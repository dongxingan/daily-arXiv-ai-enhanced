<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Leveraging LLMs for Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10093)
*Marcos Abel Zuzuárregui,Stefano Carpin*

Main category: cs.RO

TL;DR: 论文提出了一种基于大型语言模型（如ChatGPT）的端到端系统，允许用户通过自然语言指令为自主机器人分配复杂的数据收集任务，并通过IEEE任务规范标准和ROS2节点实现任务的高效执行。


<details>
  <summary>Details</summary>
Motivation: 机器人技术和人工智能在精准农业中潜力巨大，但现有系统难以适应多样化任务，且终端用户通常缺乏技术专长。

Method: 利用大型语言模型（LLMs）将自然语言指令转化为任务计划，采用IEEE任务规范标准编码任务，并通过ROS2节点执行。

Result: 实验验证了LLMs在空间推理和复杂路径规划中的优势与局限性，并展示了所提系统的有效性。

Conclusion: 该系统为终端用户提供了一种无需技术背景的任务分配方式，同时通过标准化和模块化设计提升了任务的可重用性。

Abstract: Robotics and artificial intelligence hold significant potential for advancing
precision agriculture. While robotic systems have been successfully deployed
for various tasks, adapting them to perform diverse missions remains
challenging, particularly because end users often lack technical expertise. In
this paper, we present an end-to-end system that leverages large language
models (LLMs), specifically ChatGPT, to enable users to assign complex data
collection tasks to autonomous robots using natural language instructions. To
enhance reusability, mission plans are encoded using an existing IEEE task
specification standard, and are executed on robots via ROS2 nodes that bridge
high-level mission descriptions with existing ROS libraries. Through extensive
experiments, we highlight the strengths and limitations of LLMs in this
context, particularly regarding spatial reasoning and solving complex routing
challenges, and show how our proposed implementation overcomes them.

</details>


### [2] [Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models](https://arxiv.org/abs/2506.10098)
*Christian Reichenbächer,Philipp Rank,Jochen Hipp,Oliver Bringmann*

Main category: cs.RO

TL;DR: 该论文首次将高斯混合Copula模型应用于自动驾驶系统的安全验证中，用于驾驶场景的统计建模。


<details>
  <summary>Details</summary>
Motivation: 为了基于场景的安全评估，需要了解场景参数的联合概率分布，而现有方法（如高斯混合模型和高斯Copula模型）在表达性和灵活性上存在不足。

Method: 提出高斯混合Copula模型，结合高斯混合模型的多模态表达能力和Copula的灵活性，分别建模边缘分布和依赖关系。

Result: 在1800万场景实例的评估中，高斯混合Copula模型在似然和Sinkhorn距离上优于其他方法。

Conclusion: 高斯混合Copula模型为未来基于场景的验证框架提供了有力支持。

Abstract: This paper presents the first application of Gaussian Mixture Copula Models
to the statistical modeling of driving scenarios for the safety validation of
automated driving systems. Knowledge of the joint probability distribution of
scenario parameters is essential for scenario-based safety assessment, where
risk quantification depends on the likelihood of concrete parameter
combinations. Gaussian Mixture Copula Models bring together the multimodal
expressivity of Gaussian Mixture Models and the flexibility of copulas,
enabling separate modeling of marginal distributions and dependencies. We
benchmark Gaussian Mixture Copula Models against previously proposed approaches
- Gaussian Mixture Models and Gaussian Copula Models - using real-world driving
data drawn from scenarios defined in United Nations Regulation No. 157. Our
evaluation across 18 million scenario instances demonstrates that Gaussian
Mixture Copula Models provide a better fit to the data in terms of both
likelihood and Sinkhorn distance. These results suggest that Gaussian Mixture
Copula Models are a compelling foundation for future scenario-based validation
frameworks.

</details>


### [3] [One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10106)
*Marcos Abel Zuzuárregui,Mustafa Melih Toslak,Stefano Carpin*

Main category: cs.RO

TL;DR: 论文提出了一种基于自然语言的机器人任务规划器，帮助非技术用户通过简单界面控制异构机器人，利用大语言模型和预定义原语，将人类语言转化为可执行指令。


<details>
  <summary>Details</summary>
Motivation: 人工智能在精准农业中的应用增加了效率，但也带来了复杂性和学习曲线，非技术用户难以适应。

Method: 通过大语言模型（LLMs）和预定义原语，将自然语言转化为中间描述，支持异构机器人执行复杂任务。

Result: 系统支持多种机器人，并能执行复杂任务请求，验证了其通用性和强大功能。

Conclusion: 该研究为非技术用户提供了更易用的机器人自动化工具，推动了精准农业的普及。

Abstract: Artificial intelligence is transforming precision agriculture, offering
farmers new tools to streamline their daily operations. While these
technological advances promise increased efficiency, they often introduce
additional complexity and steep learning curves that are particularly
challenging for non-technical users who must balance tech adoption with
existing workloads. In this paper, we present a natural language (NL) robotic
mission planner that enables non-specialists to control heterogeneous robots
through a common interface. By leveraging large language models (LLMs) and
predefined primitives, our architecture seamlessly translates human language
into intermediate descriptions that can be executed by different robotic
platforms. With this system, users can formulate complex agricultural missions
without writing any code. In the work presented in this paper, we extend our
previous system tailored for wheeled robot mission planning through a new class
of experiments involving robotic manipulation and computer vision tasks. Our
results demonstrate that the architecture is both general enough to support a
diverse set of robots and powerful enough to execute complex mission requests.
This work represents a significant step toward making robotic automation in
precision agriculture more accessible to non-technical users.

</details>


### [4] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: 提出了一种模块化的视觉语言导航框架，通过解耦视觉语言理解与动作规划，结合轻量级逻辑实现快速导航。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型在导航任务中的计算成本和实时部署挑战。

Method: 集成冻结的视觉语言模型Qwen2.5-VL-7B-Instruct与轻量规划逻辑，采用提示工程和双帧视觉输入策略。

Result: 在Room-to-Room基准测试中初步结果显示出泛化能力不足，但模块化设计为未来改进奠定基础。

Conclusion: 模块化框架为高效导航系统提供了可扩展性，未来可通过增强环境先验和多模态输入集成进一步优化。

Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied
AI, requiring agents to interpret natural language instructions and navigate
through visually rich, unfamiliar environments. Recent advances in large
vision-language models (LVLMs), such as CLIP and Flamingo, have significantly
improved multimodal understanding but introduced new challenges related to
computational cost and real-time deployment. In this project, we propose a
modular, plug-and-play navigation framework that decouples vision-language
understanding from action planning. By integrating a frozen vision-language
model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to
achieve flexible, fast, and adaptable navigation without extensive model
fine-tuning. Our framework leverages prompt engineering, structured history
management, and a two-frame visual input strategy to enhance decision-making
continuity across navigation steps. We evaluate our system on the Room-to-Room
benchmark within the VLN-CE setting using the Matterport3D dataset and
Habitat-Lab simulation environment. Although our initial results reveal
challenges in generalizing to unseen environments under strict evaluation
settings, our modular approach lays a foundation for scalable and efficient
navigation systems, highlighting promising directions for future improvement
through enhanced environmental priors and expanded multimodal input
integration.

</details>


### [5] [Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators](https://arxiv.org/abs/2506.10240)
*Rongfei Li,Francis Assadian*

Main category: cs.RO

TL;DR: 提出了一种基于Youla参数化的前馈-反馈自适应控制算法，用于解决3D点特征在视野外时的视觉伺服控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉伺服控制方法多依赖视野内的3D点特征，但实际应用中特征可能不在视野内，需一种更鲁棒的解决方案。

Method: 结合前馈-反馈控制与Youla参数化，设计特征估计环和自适应控制器，在线线性化解耦相机与机械臂模型。

Result: 仿真验证了控制器在视野内外均能实现稳定、快速且高精度的位姿控制。

Conclusion: 该算法鲁棒性强，易于工业机器人系统实现，适用于复杂场景。

Abstract: Image-based visual servoing (IBVS) methods have been well developed and used
in many applications, especially in pose (position and orientation) alignment.
However, most research papers focused on developing control solutions when 3D
point features can be detected inside the field of view. This work proposes an
innovative feedforward-feedback adaptive control algorithm structure with the
Youla Parameterization method. A designed feature estimation loop ensures
stable and fast motion control when point features are outside the field of
view. As 3D point features move inside the field of view, the IBVS feedback
loop preserves the precision of the pose at the end of the control period.
Also, an adaptive controller is developed in the feedback loop to stabilize the
system in the entire range of operations. The nonlinear camera and robot
manipulator model is linearized and decoupled online by an adaptive algorithm.
The adaptive controller is then computed based on the linearized model
evaluated at current linearized point. The proposed solution is robust and easy
to implement in different industrial robotic systems. Various scenarios are
used in simulations to validate the effectiveness and robust performance of the
proposed controller.

</details>


### [6] [A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures](https://arxiv.org/abs/2506.10239)
*Maximilian Mühlbauer,Freek Stulp,Sylvain Calinon,Alin Albu-Schäffer,João Silvério*

Main category: cs.RO

TL;DR: 该论文提出了一种统一的概率虚拟夹具框架，支持手动、半自动和全自动模式的切换，结合几何感知和最优阻抗增益，验证了其多模式操作的灵活性和易编程性。


<details>
  <summary>Details</summary>
Motivation: 为了提高任务执行的生产力和精度，需要在保持人类参与的同时，部分自动化任务阶段，因此需要一种能够自适应选择合适触觉反馈的虚拟夹具框架。

Method: 提出了一种基于概率动态系统的虚拟夹具，用于粗引导；扩展了概率位置轨迹夹具和视觉伺服夹具，支持几何感知和阻抗行为。

Result: 实验验证了该框架在不同机器人上的多模式操作和易编程性。

Conclusion: 该框架成功实现了手动、半自动和全自动模式的灵活切换，提升了任务执行效率和精度。

Abstract: Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the
most suitable haptic feedback for each phase of a task, based on learned or
perceived uncertainty. While keeping the human in the loop remains essential,
for instance, to ensure high precision, partial automation of certain task
phases is critical for productivity. We present a unified framework for
probabilistic VFs that seamlessly switches between manual fixtures,
semi-automated fixtures (with the human handling precise tasks), and full
autonomy. We introduce a novel probabilistic Dynamical System-based VF for
coarse guidance, enabling the robot to autonomously complete certain task
phases while keeping the human operator in the loop. For tasks requiring
precise guidance, we extend probabilistic position-based trajectory fixtures
with automation allowing for seamless human interaction as well as
geometry-awareness and optimal impedance gains. For manual tasks requiring very
precise guidance, we also extend visual servoing fixtures with the same
geometry-awareness and impedance behaviour. We validate our approach
experimentally on different robots, showcasing multiple operation modes and the
ease of programming fixtures.

</details>


### [7] [A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control](https://arxiv.org/abs/2506.10252)
*Rongfei Li,Francis Assadian*

Main category: cs.RO

TL;DR: 论文提出了一种新的控制策略，结合前馈控制器和基于Youla参数化的反馈控制器，解决了立体相机在视觉伺服控制中因过约束导致的局部极小值问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人导航和操作中，相机位姿的精确估计对任务执行至关重要。传统方法在视觉伺服控制中因过约束可能导致全局稳定性无法保证。

Method: 提出了一种结合前馈控制器和基于Youla参数化的反馈控制器的策略，确保鲁棒的伺服性能。

Result: 仿真实验表明，该方法能有效避免局部极小值，使相机准确高效地达到目标位姿。

Conclusion: 该方法为解决视觉伺服控制中的过约束问题提供了有效方案，提升了相机位姿估计的准确性和稳定性。

Abstract: In robot navigation and manipulation, accurately determining the camera's
pose relative to the environment is crucial for effective task execution. In
this paper, we systematically prove that this problem corresponds to the
Perspective-3-Point (P3P) formulation, where exactly three known 3D points and
their corresponding 2D image projections are used to estimate the pose of a
stereo camera. In image-based visual servoing (IBVS) control, the system
becomes overdetermined, as the 6 degrees of freedom (DoF) of the stereo camera
must align with 9 observed 2D features in the scene. When more constraints are
imposed than available DoFs, global stability cannot be guaranteed, as the
camera may become trapped in a local minimum far from the desired configuration
during servoing. To address this issue, we propose a novel control strategy for
accurately positioning a calibrated stereo camera. Our approach integrates a
feedforward controller with a Youla parameterization-based feedback controller,
ensuring robust servoing performance. Through simulations, we demonstrate that
our method effectively avoids local minima and enables the camera to reach the
desired pose accurately and efficiently.

</details>


### [8] [Learning Safe Control via On-the-Fly Bandit Exploration](https://arxiv.org/abs/2506.10279)
*Alexandre Capone,Ryan Cosner,Aaaron Ames,Sandra Hirche*

Main category: cs.RO

TL;DR: 论文提出了一种结合高斯过程与控制屏障函数的方法，用于在高模型不确定性下确保控制任务的安全性，无需备用控制器。


<details>
  <summary>Details</summary>
Motivation: 在高模型不确定性的控制任务中，传统方法依赖备用控制器确保安全，但可能因模型误差过大而失效。本文旨在通过动态数据收集解决这一问题。

Method: 结合高斯过程（GP）与控制屏障函数（CBF），动态收集数据以确保安全约束的可行性，避免备用控制器的需求。

Result: 方法在零均值先验动力学模型下可证明地实现安全性，是首个无需备用控制器的安全学习控制方法。

Conclusion: 通过动态探索与安全过滤的结合，本文方法在高不确定性环境中实现了安全控制，为学习控制提供了新思路。

Abstract: Control tasks with safety requirements under high levels of model uncertainty
are increasingly common. Machine learning techniques are frequently used to
address such tasks, typically by leveraging model error bounds to specify
robust constraint-based safety filters. However, if the learned model
uncertainty is very high, the corresponding filters are potentially invalid,
meaning no control input satisfies the constraints imposed by the safety
filter. While most works address this issue by assuming some form of safe
backup controller, ours tackles it by collecting additional data on the fly
using a Gaussian process bandit-type algorithm. We combine a control barrier
function with a learned model to specify a robust certificate that ensures
safety if feasible. Whenever infeasibility occurs, we leverage the control
barrier function to guide exploration, ensuring the collected data contributes
toward the closed-loop system safety. By combining a safety filter with
exploration in this manner, our method provably achieves safety in a setting
that allows for a zero-mean prior dynamics model, without requiring a backup
controller. To the best of our knowledge, it is the first safe learning-based
control method that achieves this.

</details>


### [9] [RICE: Reactive Interaction Controller for Cluttered Canopy Environment](https://arxiv.org/abs/2506.10383)
*Nidhi Homey Parayil,Thierry Peynot,Chris Lehnert*

Main category: cs.RO

TL;DR: 提出了一种新型反应控制器，用于机器臂在密集、遮挡环境中安全导航，结合末端执行器位置和实时触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 农业冠层等密集杂乱环境中，传统视觉或模型依赖方法常因物理和视觉遮挡失效，需在不破坏枝叶的情况下导航。

Method: 基于末端执行器位置和实时触觉反馈，设计了一种权衡绕过障碍与推动障碍的交互策略。

Result: 在3种实验植物设置中，35次试验均成功到达目标且未损坏任何枝叶，优于现有无模型控制器。

Conclusion: 为密集、接触丰富的可变形环境中的安全自适应交互奠定了基础，支持未来农业任务如修剪和收获。

Abstract: Robotic navigation in dense, cluttered environments such as agricultural
canopies presents significant challenges due to physical and visual occlusion
caused by leaves and branches. Traditional vision-based or model-dependent
approaches often fail in these settings, where physical interaction without
damaging foliage and branches is necessary to reach a target. We present a
novel reactive controller that enables safe navigation for a robotic arm in a
contact-rich, cluttered, deformable environment using end-effector position and
real-time tactile feedback. Our proposed framework's interaction strategy is
based on a trade-off between minimizing disturbance by maneuvering around
obstacles and pushing through them to move towards the target. We show that
over 35 trials in 3 experimental plant setups with an occluded target, the
proposed controller successfully reached the target in all trials without
breaking any branch and outperformed the state-of-the-art model-free controller
in robustness and adaptability. This work lays the foundation for safe,
adaptive interaction in cluttered, contact-rich deformable environments,
enabling future agricultural tasks such as pruning and harvesting in plant
canopies.

</details>


### [10] [Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks](https://arxiv.org/abs/2506.10287)
*Rohit Sonker,Alexandre Capone,Andrew Rothstein,Hiro Josep Farre Kaga,Egemen Kolemen,Jeff Schneider*

Main category: cs.RO

TL;DR: 提出了一种多尺度贝叶斯优化方法，用于控制核聚变中的撕裂不稳定性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 核聚变系统动态复杂、数据质量差、硬件易故障，现有机器学习工具无法全面解决这些问题。

Method: 结合高频数据驱动动态模型与低频高斯过程，通过实验间更新高斯过程快速适应新数据。

Result: 离线测试显著优于基线；现场实验成功率达50%，比历史结果提升117%。

Conclusion: 多尺度贝叶斯优化方法在核聚变控制中表现出色，为解决复杂系统问题提供了新思路。

Abstract: Machine learning algorithms often struggle to control complex real-world
systems. In the case of nuclear fusion, these challenges are exacerbated, as
the dynamics are notoriously complex, data is poor, hardware is subject to
failures, and experiments often affect dynamics beyond the experiment's
duration. Existing tools like reinforcement learning, supervised learning, and
Bayesian optimization address some of these challenges but fail to provide a
comprehensive solution. To overcome these limitations, we present a multi-scale
Bayesian optimization approach that integrates a high-frequency data-driven
dynamics model with a low-frequency Gaussian process. By updating the Gaussian
process between experiments, the method rapidly adapts to new data, refining
the predictions of the less reliable dynamical model. We validate our approach
by controlling tearing instabilities in the DIII-D nuclear fusion plant.
Offline testing on historical data shows that our method significantly
outperforms several baselines. Results on live experiments on the DIII-D
tokamak, conducted under high-performance plasma scenarios prone to
instabilities, shows a 50% success rate, marking a 117% improvement over
historical outcomes.

</details>


### [11] [Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving](https://arxiv.org/abs/2506.10317)
*Akshar Tumu,Henrik I. Christensen,Marcell Vazquez-Chanlatte,Chikao Tsuchiya,Dhaval Bhanderi*

Main category: cs.RO

TL;DR: 论文提出了一种轻量级方法，通过结合OSM地图的结构化道路元数据和道路设计手册中的车道宽度先验，改进SMERF模型的车道拓扑预测性能。


<details>
  <summary>Details</summary>
Motivation: 车道拓扑预测对自动驾驶导航至关重要，但现有方法缺乏对道路环境自然语言信息的利用。

Method: 结合OSM地图的道路元数据和车道宽度先验，增强SMERF模型的车道拓扑预测能力。

Result: 在两个地理多样化的复杂交叉口场景中，模型在车道和交通元素检测及其关联方面表现提升。

Conclusion: 该方法能够泛化并适应多样化的拓扑结构和条件。

Abstract: Lane-topology prediction is a critical component of safe and reliable
autonomous navigation. An accurate understanding of the road environment aids
this task. We observe that this information often follows conventions encoded
in natural language, through design codes that reflect the road structure and
road names that capture the road functionality. We augment this information in
a lightweight manner to SMERF, a map-prior-based online lane-topology
prediction model, by combining structured road metadata from OSM maps and
lane-width priors from Road design manuals with the road centerline encodings.
We evaluate our method on two geo-diverse complex intersection scenarios. Our
method shows improvement in both lane and traffic element detection and their
association. We report results using four topology-aware metrics to
comprehensively assess the model performance. These results demonstrate the
ability of our approach to generalize and scale to diverse topologies and
conditions.

</details>


### [12] [Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success](https://arxiv.org/abs/2506.10359)
*Che Wang,Jeroen van Baar,Chaitanya Mitash,Shuai Li,Dylan Randle,Weiyao Wang,Sumedh Sontakke,Kostas E. Bekris,Kapil Katyal*

Main category: cs.RO

TL;DR: 该研究展示了如何从稀疏标记的工业规模真实数据中自主学习机器人操作，以提升性能，重点关注多吸盘机器人抓取任务。


<details>
  <summary>Details</summary>
Motivation: 解决从杂乱堆中抓取多样化物品的挑战，满足高吞吐量的延迟要求，适用于仓库等实际场景。

Method: 利用RGB、深度和语义分割等多模态输入预测抓取质量，结合多模态预训练和微调策略。

Result: 在大规模物品抓取数据集上进行了全面实验，评估了不同物品配置、场景和物体类型，揭示了多模态训练和预训练的重要性。

Conclusion: 多模态训练和预训练对性能至关重要，模型能在预训练中学习模态间关系，微调和推理时可仅使用部分模态。

Abstract: This work demonstrates how autonomously learning aspects of robotic operation
from sparsely-labeled, real-world data of deployed, engineered solutions at
industrial scale can provide with solutions that achieve improved performance.
Specifically, it focuses on multi-suction robot picking and performs a
comprehensive study on the application of multi-modal visual encoders for
predicting the success of candidate robotic picks. Picking diverse items from
unstructured piles is an important and challenging task for robot manipulation
in real-world settings, such as warehouses. Methods for picking from clutter
must work for an open set of items while simultaneously meeting latency
constraints to achieve high throughput. The demonstrated approach utilizes
multiple input modalities, such as RGB, depth and semantic segmentation, to
estimate the quality of candidate multi-suction picks. The strategy is trained
from real-world item picking data, with a combination of multimodal pretrain
and finetune. The manuscript provides comprehensive experimental evaluation
performed over a large item-picking dataset, an item-picking dataset targeted
to include partial occlusions, and a package-picking dataset, which focuses on
containers, such as boxes and envelopes, instead of unpackaged items. The
evaluation measures performance for different item configurations, pick scenes,
and object types. Ablations help to understand the effects of in-domain
pretraining, the impact of different modalities and the importance of
finetuning. These ablations reveal both the importance of training over
multiple modalities but also the ability of models to learn during pretraining
the relationship between modalities so that during finetuning and inference,
only a subset of them can be used as input.

</details>


### [13] [Towards more efficient quantitative safety validation of residual risk for assisted and automated driving](https://arxiv.org/abs/2506.10363)
*Daniel Betschinske,Malte Schrimpf,Steven Peters,Kamil Klonecki,Jan Peter Karch,Moritz Lippert*

Main category: cs.RO

TL;DR: 论文探讨了如何提高基于FOT的宏观安全验证效率，以应对ADAS和ADS安全验证中的高成本和测试限制。


<details>
  <summary>Details</summary>
Motivation: 传统FOT方法在高自动化级别下测试成本过高且效率低下，亟需改进方法以满足国际标准（如ISO 21448）的要求。

Method: 系统识别并评估了FOT的减少方法（RAs），基于ISO 21448构建了通用模型和基础模型（以AEB为例），并通过四项标准评估RAs。

Result: 研究发现现有方法虽有一定潜力，但均存在不足，且无法完全替代FOT。

Conclusion: FOT在ADAS和ADS安全验证中仍不可或缺，未来需进一步研究以优化RAs。

Abstract: The safety validation of Advanced Driver Assistance Systems (ADAS) and
Automated Driving Systems (ADS) increasingly demands efficient and reliable
methods to quantify residual risk while adhering to international standards
such as ISO 21448. Traditionally, Field Operational Testing (FOT) has been
pivotal for macroscopic safety validation of automotive driving functions up to
SAE automation level 2. However, state-of-the-art derivations for empirical
safety demonstrations using FOT often result in impractical testing efforts,
particularly at higher automation levels. Even at lower automation levels, this
limitation - coupled with the substantial costs associated with FOT - motivates
the exploration of approaches to enhance the efficiency of FOT-based
macroscopic safety validation. Therefore, this publication systematically
identifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT,
including novel methods reported in the literature. Based on an analysis of ISO
21448, two models are derived: a generic model capturing the argumentation
components of the standard, and a base model, exemplarily applied to Automatic
Emergency Braking (AEB) systems, establishing a baseline for the real-world
driving requirement for a Quantitative Safety Validation of Residual Risk
(QSVRR). Subsequently, the RAs are assessed using four criteria:
quantifiability, threats to validity, missing links, and black box
compatibility, highlighting potential benefits, inherent limitations, and
identifying key areas for further research. Our evaluation reveals that, while
several approaches offer potential, none are free from missing links or other
substantial shortcomings. Moreover, no identified alternative can fully replace
FOT, reflecting its crucial role in the safety validation of ADAS and ADS.

</details>


### [14] [Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions](https://arxiv.org/abs/2506.10462)
*Ana Müller,Sabina Jeschke,Anja Richert*

Main category: cs.RO

TL;DR: 研究探讨了群体自适应对话设计对两种社交互动代理（SIAs）的影响，发现设计对满意度无显著影响，但揭示了多模态策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索群体敏感的对话设计在社交互动代理（SIAs）中的效果，以改进多群体交互体验。

Method: 在真实博物馆环境中，使用Furhat机器人和MetaHuman虚拟代理，结合混合检索和生成模型，与188名参与者进行交互实验。

Result: 群体敏感对话设计对满意度无显著影响，但揭示了多群体交互和不同代理形态（机器人与虚拟代理）的挑战。

Conclusion: 研究为HAI、HRI和HMI领域提供了未来群体对话适应策略的启示，强调多模态方法的重要性。

Abstract: This paper investigates the impact of a group-adaptive conversation design in
two socially interactive agents (SIAs) through two real-world studies. Both
SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped
with a conversational artificial intelligence (CAI) backend combining hybrid
retrieval and generative models. The studies were carried out in an in-the-wild
setting with a total of $N = 188$ participants who interacted with the SIAs -
in dyads, triads or larger groups - at a German museum. Although the results
did not reveal a significant effect of the group-sensitive conversation design
on perceived satisfaction, the findings provide valuable insights into the
challenges of adapting CAI for multi-party interactions and across different
embodiments (robot vs.\ virtual agent), highlighting the need for multimodal
strategies beyond linguistic pluralization. These insights contribute to the
fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and
broader Human-Machine Interaction (HMI), providing insights for future research
on effective dialogue adaptation in group settings.

</details>


### [15] [EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence](https://arxiv.org/abs/2506.10600)
*Wang Xinjie,Liu Liu,Cao Yu,Wu Ruiqi,Qin Wenkang,Wang Dehui,Sui Wei,Su Zhizhong*

Main category: cs.RO

TL;DR: EmbodiedGen是一个用于生成高质量、可控且逼真的3D资产的基础平台，支持低成本生成物理属性准确的3D世界，适用于具身智能任务。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能任务依赖传统3D图形资产，成本高且真实性有限，限制了数据驱动方法的扩展性。

Method: EmbodiedGen通过六个关键模块（如图像到3D、文本到3D等）生成多样化的交互式3D世界，利用生成式AI技术。

Result: 生成的3D资产具有高质量、可控性和物理准确性，可直接用于物理仿真引擎。

Conclusion: EmbodiedGen为具身智能研究提供了低成本、高扩展性的解决方案，解决了数据生成和评估的挑战。

Abstract: Constructing a physically realistic and accurately scaled simulated 3D world
is crucial for the training and evaluation of embodied intelligence tasks. The
diversity, realism, low cost accessibility and affordability of 3D data assets
are critical for achieving generalization and scalability in embodied AI.
However, most current embodied intelligence tasks still rely heavily on
traditional 3D computer graphics assets manually created and annotated, which
suffer from high production costs and limited realism. These limitations
significantly hinder the scalability of data driven approaches. We present
EmbodiedGen, a foundational platform for interactive 3D world generation. It
enables the scalable generation of high-quality, controllable and
photorealistic 3D assets with accurate physical properties and real-world scale
in the Unified Robotics Description Format (URDF) at low cost. These assets can
be directly imported into various physics simulation engines for fine-grained
physical control, supporting downstream tasks in training and evaluation.
EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key
modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object
Generation, Scene Generation and Layout Generation. EmbodiedGen generates
diverse and interactive 3D worlds composed of generative 3D assets, leveraging
generative AI to address the challenges of generalization and evaluation to the
needs of embodied intelligence related research. Code is available at
https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.

</details>


### [16] [An $O(n$)-Algorithm for the Higher-Order Kinematics and Inverse Dynamics of Serial Manipulators using Spatial Representation of Twists](https://arxiv.org/abs/2506.10686)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种基于空间表示的二阶逆动力学算法，并伴随四阶正逆运动学算法，适用于机器人臂的优化控制。


<details>
  <summary>Details</summary>
Motivation: 研究机器人臂控制中高效计算关节力矩/力的时间导数的需求，特别是在平坦性控制中。

Method: 采用空间表示和Lie群公式化，提出二阶逆动力学算法和四阶正逆运动学算法。

Result: 方法在7自由度Franka Emika Panda机器人上验证，展示了其高效性和紧凑性。

Conclusion: Lie群公式化方法在机器人控制中具有高效性和实用性，适用于实际应用。

Abstract: Optimal control in general, and flatness-based control in particular, of
robotic arms necessitate to compute the first and second time derivatives of
the joint torques/forces required to achieve a desired motion. In view of the
required computational efficiency, recursive $O(n)$-algorithms were proposed to
this end. Aiming at compact yet efficient formulations, a Lie group formulation
was recently proposed, making use of body-fixed and hybrid representation of
twists and wrenches. In this paper a formulation is introduced using the
spatial representation. The second-order inverse dynamics algorithm is
accompanied by a fourth-order forward and inverse kinematics algorithm. An
advantage of all Lie group formulations is that they can be parameterized in
terms of vectorial quantities that are readily available. The method is
demonstrated for the 7 DOF Franka Emika Panda robot.

</details>


### [17] [Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding](https://arxiv.org/abs/2506.10756)
*Yuhang Zhang,Haosheng Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: VLFly框架通过结合大型语言模型和视觉语言模型，为无人机提供语言引导的连续速度控制，解决了视觉语言导航中的泛化问题和离散动作空间依赖。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言导航中泛化能力不足和依赖固定离散动作空间的问题。

Method: VLFly整合指令编码器、目标检索器和路径规划器，利用单目摄像头输入生成连续速度命令。

Result: 在多样化模拟环境和真实任务中表现优异，支持开放词汇目标理解和泛化导航。

Conclusion: VLFly展示了在复杂环境中基于语言指令的鲁棒导航能力。

Abstract: Vision-and-language navigation (VLN) is a long-standing challenge in
autonomous robotics, aiming to empower agents with the ability to follow human
instructions while navigating complex environments. Two key bottlenecks remain
in this field: generalization to out-of-distribution environments and reliance
on fixed discrete action spaces. To address these challenges, we propose
Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles
(UAVs) to execute language-guided flight. Without the requirement for
localization or active ranging sensors, VLFly outputs continuous velocity
commands purely from egocentric observations captured by an onboard monocular
camera. The VLFly integrates three modules: an instruction encoder based on a
large language model (LLM) that reformulates high-level language into
structured prompts, a goal retriever powered by a vision-language model (VLM)
that matches these prompts to goal images via vision-language similarity, and a
waypoint planner that generates executable trajectories for real-time UAV
control. VLFly is evaluated across diverse simulation environments without
additional fine-tuning and consistently outperforms all baselines. Moreover,
real-world VLN tasks in indoor and outdoor environments under direct and
indirect instructions demonstrate that VLFly achieves robust open-vocabulary
goal understanding and generalized navigation capabilities, even in the
presence of abstract language input.

</details>


### [18] [In-Hand Object Pose Estimation via Visual-Tactile Fusion](https://arxiv.org/abs/2506.10787)
*Felix Nonnengießer,Alap Kshirsagar,Boris Belousov,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一种结合视觉和触觉信息的机器人手内物体姿态估计方法，通过融合RGB-D相机和触觉传感器的数据，显著提高了姿态估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉遮挡是视觉方法在机器人手内物体姿态估计中的主要挑战，需要结合其他传感器信息以提高准确性。

Method: 融合手腕RGB-D相机和指尖触觉传感器的数据，采用加权传感器融合模块和加权点云的改进ICP算法进行姿态估计。

Result: 实验表明，触觉信息的加入显著提高了姿态估计精度，平均误差为7.5毫米和16.7度，优于纯视觉方法20%。

Conclusion: 该方法在视觉遮挡情况下表现优异，并能支持精确的物体操作任务。

Abstract: Accurate in-hand pose estimation is crucial for robotic object manipulation,
but visual occlusion remains a major challenge for vision-based approaches.
This paper presents an approach to robotic in-hand object pose estimation,
combining visual and tactile information to accurately determine the position
and orientation of objects grasped by a robotic hand. We address the challenge
of visual occlusion by fusing visual information from a wrist-mounted RGB-D
camera with tactile information from vision-based tactile sensors mounted on
the fingertips of a robotic gripper. Our approach employs a weighting and
sensor fusion module to combine point clouds from heterogeneous sensor types
and control each modality's contribution to the pose estimation process. We use
an augmented Iterative Closest Point (ICP) algorithm adapted for weighted point
clouds to estimate the 6D object pose. Our experiments show that incorporating
tactile information significantly improves pose estimation accuracy,
particularly when occlusion is high. Our method achieves an average pose
estimation error of 7.5 mm and 16.7 degrees, outperforming vision-only
baselines by up to 20%. We also demonstrate the ability of our method to
perform precise object manipulation in a real-world insertion task.

</details>


### [19] [RationalVLA: A Rational Vision-Language-Action Model with Dual System](https://arxiv.org/abs/2506.10826)
*Wenxuan Song,Jiayi Chen,Wenxue Li,Xu He,Han Zhao,Pengxiang Ding Shiyan Su,Feilong Tang,Xuelian Cheng,Donglin Wang,Zongyuan Ge,Xinhu Zheng,Zhe Liu,Hesheng Wang,Yunhui Liu,Haoang Li*

Main category: cs.RO

TL;DR: RAMA是一个新的机器人操作基准，挑战模型处理未见过或缺陷指令的能力，RationalVLA模型通过双系统设计显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中指令可能模糊或不合理，现有方法假设完美对齐环境，限制了鲁棒性和泛化能力。

Method: 提出RAMA基准和RationalVLA模型，结合高层视觉语言模型与低层操作策略，通过可学习潜在空间嵌入实现推理与拒绝缺陷指令。

Result: RationalVLA在RAMA上成功率提升14.5%，任务长度平均0.94，并在标准任务中保持竞争力。

Conclusion: RationalVLA在现实应用中验证了其有效性和鲁棒性，为语言驱动的机器人操作提供了新方向。

Abstract: A fundamental requirement for real-world robotic deployment is the ability to
understand and respond to natural language instructions. Existing
language-conditioned manipulation tasks typically assume that instructions are
perfectly aligned with the environment. This assumption limits robustness and
generalization in realistic scenarios where instructions may be ambiguous,
irrelevant, or infeasible. To address this problem, we introduce RAtional
MAnipulation (RAMA), a new benchmark that challenges models with both unseen
executable instructions and defective ones that should be rejected. In RAMA, we
construct a dataset with over 14,000 samples, including diverse defective
instructions spanning six dimensions: visual, physical, semantic, motion,
safety, and out-of-context. We further propose the Rational
Vision-Language-Action model (RationalVLA). It is a dual system for robotic
arms that integrates the high-level vision-language model with the low-level
manipulation policy by introducing learnable latent space embeddings. This
design enables RationalVLA to reason over instructions, reject infeasible
commands, and execute manipulation effectively. Experiments demonstrate that
RationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher
success rate and 0.94 average task length, while maintaining competitive
performance on standard manipulation tasks. Real-world trials further validate
its effectiveness and robustness in practical applications. Our project page is
https://irpn-eai.github.io/rationalvla.

</details>


### [20] [Invariant Extended Kalman Filter for Autonomous Surface Vessels with Partial Orientation Measurements](https://arxiv.org/abs/2506.10850)
*Derek Benham,Easton Potokar,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种基于不变扩展卡尔曼滤波（InEKF）的框架，用于整合部分方向测量，以提升自主水面船只（ASV）在开放海洋环境中的状态估计精度。


<details>
  <summary>Details</summary>
Motivation: ASV在海洋科学中日益重要，但精确的状态估计（尤其是姿态）对海底测绘至关重要。传统方法依赖固定地标的相对位置测量，而开放海洋中ASV主要观测后退地平线，因此需要新的解决方案。

Method: 利用前向单目相机估计相对于地平线的滚转和俯仰角，结合双天线GPS航向测量，提出了一种新颖的InEKF框架，用于整合部分方向数据。

Result: 结果表明，所提出的部分方向测量方法在开放海洋环境中对ASV状态估计具有高效性和鲁棒性。

Conclusion: 该框架为ASV在缺乏完整方向测量的环境中提供了更准确的状态估计方法，优于传统InEKF和MEKF。

Abstract: Autonomous surface vessels (ASVs) are increasingly vital for marine science,
offering robust platforms for underwater mapping and inspection. Accurate state
estimation, particularly of vehicle pose, is paramount for precise seafloor
mapping, as even small surface deviations can have significant consequences
when sensing the seafloor below. To address this challenge, we propose an
Invariant Extended Kalman Filter (InEKF) framework designed to integrate
partial orientation measurements. While conventional estimation often relies on
relative position measurements to fixed landmarks, open ocean ASVs primarily
observe a receding horizon. We leverage forward-facing monocular cameras to
estimate roll and pitch with respect to this horizon, which provides
yaw-ambiguous partial orientation information. To effectively utilize these
measurements within the InEKF, we introduce a novel framework for incorporating
such partial orientation data. This approach contrasts with traditional InEKF
implementations that assume full orientation measurements and is particularly
relevant for planar vehicle motion constrained to a "seafaring plane." This
paper details the developed InEKF framework; its integration with horizon-based
roll/pitch observations and dual-antenna GPS heading measurements for ASV state
estimation; and provides a comparative analysis against the InEKF using full
orientation and a Multiplicative EKF (MEKF). Our results demonstrate the
efficacy and robustness of the proposed partial orientation measurements for
accurate ASV state estimation in open ocean environments.

</details>


### [21] [Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material](https://arxiv.org/abs/2506.10875)
*Guanjin Wang,Xiangxue Zhao,Shapour Azarm,Balakumar Balachandran*

Main category: cs.RO

TL;DR: 提出了一种数据驱动建模方法，结合降维、代理建模和数据同化技术，显著减少计算时间，预测精度接近高保真模拟，并可能超越模拟在长期预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究机器人运动与颗粒地形交互的基本原理，提升机器人在未知复杂地形中的导航和探索能力。

Method: 集成降维（ST-HOSVD）、代理建模（高斯过程）和数据同化技术（降阶粒子滤波），基于离线高保真模拟数据和稀疏实验数据。

Result: 计算时间大幅减少，预测精度接近模拟；结合实验数据时，长期预测可能优于模拟；能复现物理模拟的缩放关系。

Conclusion: 该方法为机器人在复杂地形中的在线和离线导航提供了高效可靠的建模工具。

Abstract: An alternative data-driven modeling approach has been proposed and employed
to gain fundamental insights into robot motion interaction with granular
terrain at certain length scales. The approach is based on an integration of
dimension reduction (Sequentially Truncated Higher-Order Singular Value
Decomposition), surrogate modeling (Gaussian Process), and data assimilation
techniques (Reduced Order Particle Filter). This approach can be used online
and is based on offline data, obtained from the offline collection of
high-fidelity simulation data and a set of sparse experimental data. The
results have shown that orders of magnitude reduction in computational time can
be obtained from the proposed data-driven modeling approach compared with
physics-based high-fidelity simulations. With only simulation data as input,
the data-driven prediction technique can generate predictions that have
comparable accuracy as simulations. With both simulation data and sparse
physical experimental measurement as input, the data-driven approach with its
embedded data assimilation techniques has the potential in outperforming only
high-fidelity simulations for the long-horizon predictions. In addition, it is
demonstrated that the data-driven modeling approach can also reproduce the
scaling relationship recovered by physics-based simulations for maximum
resistive forces, which may indicate its general predictability beyond a
case-by-case basis. The results are expected to help robot navigation and
exploration in unknown and complex terrains during both online and offline
phases.

</details>


### [22] [Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies](https://arxiv.org/abs/2506.10884)
*Dong Hae Mangalindan,Karthik Kandikonda,Ericka Rovira,Vaibhav Srivastava*

Main category: cs.RO

TL;DR: 研究机器人辅助送货中，机器人表现和信任修复策略对人类信任的影响，发现长解释最能修复信任，否认最能防止信任流失。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统效率和可靠性的提升，研究其在任务中对人类信任的影响，以优化人机协作。

Method: 使用输入-输出隐马尔可夫模型（IOHMM）分析人类行为数据，建模信任动态和行动概率。

Result: 高信任时人类更倾向自主部署机器人；长解释修复信任效果最佳，否认防止信任流失最有效。

Conclusion: 模型生成的信任估计与自报值一致，为实时调整人机信任的政策奠定基础。

Abstract: With increasing efficiency and reliability, autonomous systems are becoming
valuable assistants to humans in various tasks. In the context of
robot-assisted delivery, we investigate how robot performance and trust repair
strategies impact human trust. In this task, while handling a secondary task,
humans can choose to either send the robot to deliver autonomously or manually
control it. The trust repair strategies examined include short and long
explanations, apology and promise, and denial.
  Using data from human participants, we model human behavior using an
Input-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and
human action probabilities. Our findings indicate that humans are more likely
to deploy the robot autonomously when their trust is high. Furthermore, state
transition estimates show that long explanations are the most effective at
repairing trust following a failure, while denial is most effective at
preventing trust loss.
  We also demonstrate that the trust estimates generated by our model are
isomorphic to self-reported trust values, making them interpretable. This model
lays the groundwork for developing optimal policies that facilitate real-time
adjustment of human trust in autonomous systems.

</details>


### [23] [Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations](https://arxiv.org/abs/2506.10923)
*Xili Yi,Nima Fazeli*

Main category: cs.RO

TL;DR: Vib2Move是一种利用指尖微振动和重力精确重新定位平面物体的新方法，通过动态调节摩擦系数和协调运动规划实现高精度操作。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种能够精确重新定位平面物体的方法，利用微振动和重力实现高精度操作。

Method: 方法包括设计振动执行器动态调节摩擦系数、推导滑动运动模型以及提出协调运动规划器。

Result: 实验结果展示Vib2Move的最终定位误差低于6毫米，适用于多种平面物体。

Conclusion: 结论是Vib2Move是一种可靠且高精度的平面物体操作方法。

Abstract: We introduce Vib2Move, a novel approach for in-hand object reconfiguration
that uses fingertip micro-vibrations and gravity to precisely reposition planar
objects. Our framework comprises three key innovations. First, we design a
vibration-based actuator that dynamically modulates the effective finger-object
friction coefficient, effectively emulating changes in gripping force. Second,
we derive a sliding motion model for objects clamped in a parallel gripper with
two symmetric, variable-friction contact patches. Third, we propose a motion
planner that coordinates end-effector finger trajectories and fingertip
vibrations to achieve the desired object pose. In real-world trials, Vib2Move
consistently yields final positioning errors below 6 mm, demonstrating
reliable, high-precision manipulation across a variety of planar objects. For
more results and information, please visit https://vib2move.github.io.

</details>


### [24] [GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation](https://arxiv.org/abs/2506.10966)
*Ning Gao,Yilun Chen,Shuai Yang,Xinyi Chen,Yang Tian,Hao Li,Haifeng Huang,Hanqing Wang,Tai Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: GenManip是一个专为策略泛化研究设计的仿真平台，通过LLM驱动的任务导向场景图生成多样化任务，并引入GenManip-Bench基准评估策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有仿真平台在支持策略适应多样化指令和场景方面不足，无法满足对指令跟随基础模型（如LLMs）泛化能力的研究需求。

Method: 提出GenManip平台，利用LLM驱动的任务导向场景图自动生成大规模多样化任务，并创建GenManip-Bench基准评估策略。

Result: 评估显示，基于基础模型的模块化系统在多样化场景中泛化能力更强，而端到端策略通过数据扩展也能获益。

Conclusion: GenManip平台有望为现实条件下策略泛化的研究提供关键支持。

Abstract: Robotic manipulation in real-world settings remains challenging, especially
regarding robust generalization. Existing simulation platforms lack sufficient
support for exploring how policies adapt to varied instructions and scenarios.
Thus, they lag behind the growing interest in instruction-following foundation
models like LLMs, whose adaptability is crucial yet remains underexplored in
fair comparisons. To bridge this gap, we introduce GenManip, a realistic
tabletop simulation platform tailored for policy generalization studies. It
features an automatic pipeline via LLM-driven task-oriented scene graph to
synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To
systematically assess generalization, we present GenManip-Bench, a benchmark of
200 scenarios refined via human-in-the-loop corrections. We evaluate two policy
types: (1) modular manipulation systems integrating foundation models for
perception, reasoning, and planning, and (2) end-to-end policies trained
through scalable data collection. Results show that while data scaling benefits
end-to-end methods, modular systems enhanced with foundation models generalize
more effectively across diverse scenarios. We anticipate this platform to
facilitate critical insights for advancing policy generalization in realistic
conditions. Project Page: https://genmanip.axi404.top/.

</details>


### [25] [Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop](https://arxiv.org/abs/2506.10968)
*Justin Kerr,Kush Hari,Ethan Weber,Chung Min Kim,Brent Yi,Tyler Bonnen,Ken Goldberg,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: EyeRobot是一个通过强化学习训练机械眼球以实现任务驱动的注视行为的机器人系统，通过手眼协调完成任务。


<details>
  <summary>Details</summary>
Motivation: 人类通过主动观察来行动，因此设计一个能根据任务需求调整注视行为的机器人系统。

Method: 开发可旋转的机械眼球，收集360度摄像头数据，在模拟环境中训练手眼协调策略，采用BC-RL循环联合训练手和眼的策略。

Result: EyeRobot在五个全景工作空间操作任务中表现出有效的手眼协调行为，能够在大工作空间内高效操作。

Conclusion: EyeRobot通过任务驱动的注视行为实现了高效的手眼协调，适用于大范围操作任务。

Abstract: Humans do not passively observe the visual world -- we actively look in order
to act. Motivated by this principle, we introduce EyeRobot, a robotic system
with gaze behavior that emerges from the need to complete real-world tasks. We
develop a mechanical eyeball that can freely rotate to observe its surroundings
and train a gaze policy to control it using reinforcement learning. We
accomplish this by first collecting teleoperated demonstrations paired with a
360 camera. This data is imported into a simulation environment that supports
rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze
on top of robot demonstrations. We then introduce a BC-RL loop to train the
hand and eye jointly: the hand (BC) agent is trained from rendered eye
observations, and the eye (RL) agent is rewarded when the hand produces correct
action predictions. In this way, hand-eye coordination emerges as the eye looks
towards regions which allow the hand to complete the task. EyeRobot implements
a foveal-inspired policy architecture allowing high resolution with a small
compute budget, which we find also leads to the emergence of more stable
fixation as well as improved ability to track objects and ignore distractors.
We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring
manipulation in an arc surrounding the robot arm. Our experiments suggest
EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate
manipulation over large workspaces with a single camera. See project site for
videos: https://www.eyerobot.net/

</details>
