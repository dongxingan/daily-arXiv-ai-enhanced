<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments](https://arxiv.org/abs/2507.18808)
*Miguel Saavedra-Ruiz,Samer B. Nashed,Charlie Gauthier,Liam Paull*

Main category: cs.RO

TL;DR: Perpetua是一种用于建模半静态特征动态的方法，能够整合先验知识、跟踪多假设并预测未来特征状态。


<details>
  <summary>Details</summary>
Motivation: 复杂动态环境中，机器人系统需要处理环境变化，而现有方法无法有效预测动态特征的未来状态。

Method: 通过链式混合“持久性”和“涌现性”滤波器，在贝叶斯框架中建模特征消失或重现的概率。

Result: 实验表明，Perpetua在模拟和真实数据中比类似方法更准确，且具有在线适应性和鲁棒性。

Conclusion: Perpetua是一种高效、可扩展且通用的方法，适用于动态环境中的特征状态预测。

Abstract: Many robotic systems require extended deployments in complex, dynamic
environments. In such deployments, parts of the environment may change between
subsequent robot observations. Most robotic mapping or environment modeling
algorithms are incapable of representing dynamic features in a way that enables
predicting their future state. Instead, they opt to filter certain state
observations, either by removing them or some form of weighted averaging. This
paper introduces Perpetua, a method for modeling the dynamics of semi-static
features. Perpetua is able to: incorporate prior knowledge about the dynamics
of the feature if it exists, track multiple hypotheses, and adapt over time to
enable predicting of future feature states. Specifically, we chain together
mixtures of "persistence" and "emergence" filters to model the probability that
features will disappear or reappear in a formal Bayesian framework. The
approach is an efficient, scalable, general, and robust method for estimating
the states of features in an environment, both in the present as well as at
arbitrary future times. Through experiments on simulated and real-world data,
we find that Perpetua yields better accuracy than similar approaches while also
being online adaptable and robust to missing observations.

</details>


### [2] [Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes](https://arxiv.org/abs/2507.18819)
*Trent Weiss,Madhur Behl*

Main category: cs.RO

TL;DR: 论文提出了一种名为Gauss-Legendre Rectangle (GLR)的算法，用于高速自动驾驶赛车中的碰撞风险估计，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有碰撞风险估计方法在高速赛车场景中过于保守或简化，无法满足精确性和实时性需求。

Method: GLR算法结合Gauss-Legendre积分和非齐次泊松过程，分两阶段估计碰撞风险，考虑车辆几何和轨迹不确定性。

Result: 在446个超车场景实验中，GLR平均误差减少77%，优于五种现有方法，且运行频率达1000 Hz。

Conclusion: GLR算法在高速赛车场景中表现出色，且具有通用性，可应用于更广泛的运动规划领域。

Abstract: Overtaking in high-speed autonomous racing demands precise, real-time
estimation of collision risk; particularly in wheel-to-wheel scenarios where
safety margins are minimal. Existing methods for collision risk estimation
either rely on simplified geometric approximations, like bounding circles, or
perform Monte Carlo sampling which leads to overly conservative motion planning
behavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)
algorithm, a principled two-stage integration method that estimates collision
risk by combining Gauss-Legendre with a non-homogeneous Poisson process over
time. GLR produces accurate risk estimates that account for vehicle geometry
and trajectory uncertainty. In experiments across 446 overtaking scenarios in a
high-fidelity Formula One racing simulation, GLR outperforms five
state-of-the-art baselines achieving an average error reduction of 77% and
surpassing the next-best method by 52%, all while running at 1000 Hz. The
framework is general and applicable to broader motion planning contexts beyond
autonomous racing.

</details>


### [3] [MetaMorph -- A Metamodelling Approach For Robot Morphology](https://arxiv.org/abs/2507.18820)
*Rachel Ringe,Robin Nolte,Nima Zargham,Robert Porzel,Rainer Malaka*

Main category: cs.RO

TL;DR: MetaMorph框架通过元建模方法，从222个机器人中提取形态特征，提供了一种全面的机器人形态分类方法，弥补了现有分类方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器人外观分类方法过于宽泛或局限于拟人化特征，无法全面分类所有类型的机器人，限制了设计与交互效果的研究。

Method: 采用元建模方法，基于IEEE Robots Guide中的222个机器人数据，构建了MetaMorph框架，用于系统比较机器人视觉特征。

Result: MetaMorph能够量化机器人模型之间的视觉差异，并探索适合不同任务和场景的设计特征。

Conclusion: MetaMorph为机器人形态分类提供了更全面的工具，有助于优化机器人设计与交互效果的研究。

Abstract: Robot appearance crucially shapes Human-Robot Interaction (HRI) but is
typically described via broad categories like anthropomorphic, zoomorphic, or
technical. More precise approaches focus almost exclusively on anthropomorphic
features, which fail to classify robots across all types, limiting the ability
to draw meaningful connections between robot design and its effect on
interaction. In response, we present MetaMorph, a comprehensive framework for
classifying robot morphology. Using a metamodeling approach, MetaMorph was
synthesized from 222 robots in the IEEE Robots Guide, offering a structured
method for comparing visual features. This model allows researchers to assess
the visual distances between robot models and explore optimal design traits
tailored to different tasks and contexts.

</details>


### [4] [Equivariant Volumetric Grasping](https://arxiv.org/abs/2507.18847)
*Pinhao Song,Yutong Hu,Pengteng Li,Renaud Detry*

Main category: cs.RO

TL;DR: 提出了一种新的体积抓取模型，具有垂直轴旋转等变性，显著提高了样本效率。采用三平面体积特征表示，并引入新的特征设计，结合可变形和可转向卷积，保持等变性同时适应局部几何。实验验证了其高效性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 提高抓取模型的样本效率和计算效率，同时保持对旋转的等变性。

Method: 使用三平面体积特征表示，结合可变形可转向卷积，开发等变性抓取规划器（GIGA和IGD）。

Result: 显著降低计算和内存成本，等变性模型性能优于非等变模型。

Conclusion: 提出的投影设计和等变性模型在效率和性能上均有显著优势。

Abstract: We propose a new volumetric grasp model that is equivariant to rotations
around the vertical axis, leading to a significant improvement in sample
efficiency. Our model employs a tri-plane volumetric feature representation --
i.e., the projection of 3D features onto three canonical planes. We introduce a
novel tri-plane feature design in which features on the horizontal plane are
equivariant to 90{\deg} rotations, while the sum of features from the other two
planes remains invariant to the same transformations. This design is enabled by
a new deformable steerable convolution, which combines the adaptability of
deformable convolutions with the rotational equivariance of steerable ones.
This allows the receptive field to adapt to local object geometry while
preserving equivariance properties. We further develop equivariant adaptations
of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,
we derive a new equivariant formulation of IGD's deformable attention mechanism
and propose an equivariant generative model of grasp orientations based on flow
matching. We provide a detailed analytical justification of the proposed
equivariance properties and validate our approach through extensive simulated
and real-world experiments. Our results demonstrate that the proposed
projection-based design significantly reduces both computational and memory
costs. Moreover, the equivariant grasp models built on top of our tri-plane
features consistently outperform their non-equivariant counterparts, achieving
higher performance with only a modest computational overhead. Video and code
can be viewed in: https://mousecpn.github.io/evg-page/

</details>


### [5] [Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations](https://arxiv.org/abs/2507.19100)
*Taewon Kang,Ji-Wook Kwon,Il Bae,Jin Hyo Kim*

Main category: cs.RO

TL;DR: 提出了一种基于等边三角形构型的低成本单目视觉传感器定位方法，适用于开放空间中的群机器人定位。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在现实应用（如搜救任务）中的定位至关重要，尤其是在缺乏基础设施支持的环境中。

Method: 利用等边三角形的几何特性，通过单目视觉传感器获取的一维横向距离信息估计机器人的二维位置。

Result: 实验和仿真表明，随着运行时间增加，该方法定位误差显著低于传统航位推算系统。

Conclusion: 该方法为低成本群机器人在开放空间中的定位提供了有效解决方案。

Abstract: Localization of mobile robots is crucial for deploying robots in real-world
applications such as search and rescue missions. This work aims to develop an
accurate localization system applicable to swarm robots equipped only with
low-cost monocular vision sensors and visual markers. The system is designed to
operate in fully open spaces, without landmarks or support from positioning
infrastructures. To achieve this, we propose a localization method based on
equilateral triangular formations. By leveraging the geometric properties of
equilateral triangles, the accurate two-dimensional position of each
participating robot is estimated using one-dimensional lateral distance
information between robots, which can be reliably and accurately obtained with
a low-cost monocular vision sensor. Experimental and simulation results
demonstrate that, as travel time increases, the positioning error of the
proposed method becomes significantly smaller than that of a conventional
dead-reckoning system, another low-cost localization approach applicable to
open environments.

</details>


### [6] [A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras](https://arxiv.org/abs/2507.18886)
*Zheng Yang,Kuan Xu,Shenghai Yuan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种基于重叠平面元素的非迭代解耦方法，用于高效估计6自由度机器人位姿，避免了传统RGB-D视觉里程计的计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统RGB-D视觉里程计依赖迭代优化和特征提取，计算量大且耗时，难以满足实时性需求。

Method: 通过分离旋转和平移估计，利用场景中的重叠平面特征计算旋转矩阵，再使用核互相关器确定平移。

Result: 方法在低端i5 CPU上达到71Hz的性能，在低纹理退化环境中优于现有技术。

Conclusion: 该方法显著提高了计算效率，适用于实时机器人位姿估计。

Abstract: In this paper, we introduce a novel approach for efficiently estimating the
6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method
that capitalizes on overlapping planar elements. Conventional RGB-D visual
odometry(RGBD-VO) often relies on iterative optimization solvers to estimate
pose and involves a process of feature extraction and matching. This results in
significant computational burden and time delays. To address this, our
innovative method for RGBD-VO separates the estimation of rotation and
translation. Initially, we exploit the overlaid planar characteristics within
the scene to calculate the rotation matrix. Following this, we utilize a kernel
cross-correlator (KCC) to ascertain the translation. By sidestepping the
resource-intensive iterative optimization and feature extraction and alignment
procedures, our methodology offers improved computational efficacy, achieving a
performance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on
feature points, our technique exhibits enhanced performance in low-texture
degenerative environments compared to state-of-the-art methods.

</details>


### [7] [GEAR: Gaze-Enabled Human-Robot Collaborative Assembly](https://arxiv.org/abs/2507.18947)
*Asad Ali Shahid,Angelo Moroncelli,Drazen Brscic,Takayuki Kanda,Loris Roveda*

Main category: cs.RO

TL;DR: GEAR是一种基于视线追踪的系统，旨在通过机器人对用户视线的响应来增强人机协作。与触摸屏界面相比，GEAR在复杂任务中减少了用户的体力需求和努力，提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 复杂装配任务由于任务多变性和精确操作需求仍具挑战性，研究探索机器人辅助角色以优化人机协作。

Method: 引入GEAR系统，通过视线追踪技术让机器人响应用户视线，并与触摸屏界面进行对比实验。

Result: 实验显示，GEAR在复杂任务中显著减少用户体力需求，保持高效性能，并提升用户体验。

Conclusion: GEAR系统在复杂装配任务中有效优化人机协作，减少用户负担，提升效率与体验。

Abstract: Recent progress in robot autonomy and safety has significantly improved
human-robot interactions, enabling robots to work alongside humans on various
tasks. However, complex assembly tasks still present significant challenges due
to inherent task variability and the need for precise operations. This work
explores deploying robots in an assistive role for such tasks, where the robot
assists by fetching parts while the skilled worker provides high-level guidance
and performs the assembly. We introduce GEAR, a gaze-enabled system designed to
enhance human-robot collaboration by allowing robots to respond to the user's
gaze. We evaluate GEAR against a touch-based interface where users interact
with the robot through a touchscreen. The experimental study involved 30
participants working on two distinct assembly scenarios of varying complexity.
Results demonstrated that GEAR enabled participants to accomplish the assembly
with reduced physical demand and effort compared to the touchscreen interface,
especially for complex tasks, maintaining great performance, and receiving
objects effectively. Participants also reported enhanced user experience while
performing assembly tasks. Project page: sites.google.com/view/gear-hri

</details>


### [8] [Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots](https://arxiv.org/abs/2507.18979)
*Deokjin Lee,Junho Song,Alireza Karimi,Sehoon Oh*

Main category: cs.RO

TL;DR: 提出了一种基于频率响应函数（FRF）的优化方法，用于提升柔性关节机器人（FJR）的干扰观测器（DOB）性能。


<details>
  <summary>Details</summary>
Motivation: 柔性关节机器人（FJR）的动态特性受关节弹性和系统参数变化影响，传统DOB设计保守且性能受限。

Method: 采用FRF优化方法，最大化控制带宽并抑制振动，同时通过Nyquist稳定性准则确保闭环稳定性。

Result: 实验验证表明，该方法显著提升了系统鲁棒性和运动性能，尤其在关节弹性和系统变化条件下。

Conclusion: FRF优化方法有效解决了DOB在FJR中的性能限制，为柔性关节控制提供了新思路。

Abstract: Motion control of flexible joint robots (FJR) is challenged by inherent
flexibility and configuration-dependent variations in system dynamics. While
disturbance observers (DOB) can enhance system robustness, their performance is
often limited by the elasticity of the joints and the variations in system
parameters, which leads to a conservative design of the DOB. This paper
presents a novel frequency response function (FRF)-based optimization method
aimed at improving DOB performance, even in the presence of flexibility and
system variability. The proposed method maximizes control bandwidth and
effectively suppresses vibrations, thus enhancing overall system performance.
Closed-loop stability is rigorously proven using the Nyquist stability
criterion. Experimental validation on a FJR demonstrates that the proposed
approach significantly improves robustness and motion performance, even under
conditions of joint flexibility and system variation.

</details>


### [9] [SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research](https://arxiv.org/abs/2507.19079)
*Feng Zhu,Zihang Zhang,Kangcheng Teng,Abduhelil Yakup,Xiaohong Zhang*

Main category: cs.RO

TL;DR: 该论文介绍了SmartPNT多源集成导航、定位和姿态数据集，旨在解决现有数据集在传感器多样性和环境覆盖方面的不足，为多传感器融合和高精度导航研究提供丰富资源。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在传感器多样性和环境覆盖方面存在局限，阻碍了高精度导航和定位系统的研究进展。

Method: 数据集整合了GNSS、IMU、光学相机和LiDAR等多传感器数据，并详细记录了传感器配置、坐标系定义和校准过程。采用标准化框架确保数据一致性和可扩展性。

Result: 通过VINS-Mono和LIO-SAM等SLAM算法验证，数据集适用于复杂环境下的高级导航研究。

Conclusion: 该数据集填补了传感器多样性、数据可访问性和环境代表性方面的空白，推动了导航技术的创新。

Abstract: High-precision navigation and positioning systems are critical for
applications in autonomous vehicles and mobile mapping, where robust and
continuous localization is essential. To test and enhance the performance of
algorithms, some research institutions and companies have successively
constructed and publicly released datasets. However, existing datasets still
suffer from limitations in sensor diversity and environmental coverage. To
address these shortcomings and advance development in related fields, the
SmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset
has been developed. This dataset integrates data from multiple sensors,
including Global Navigation Satellite Systems (GNSS), Inertial Measurement
Units (IMU), optical cameras, and LiDAR, to provide a rich and versatile
resource for research in multi-sensor fusion and high-precision navigation. The
dataset construction process is thoroughly documented, encompassing sensor
configurations, coordinate system definitions, and calibration procedures for
both cameras and LiDAR. A standardized framework for data collection and
processing ensures consistency and scalability, enabling large-scale analysis.
Validation using state-of-the-art Simultaneous Localization and Mapping (SLAM)
algorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's
applicability for advanced navigation research. Covering a wide range of
real-world scenarios, including urban areas, campuses, tunnels, and suburban
environments, the dataset offers a valuable tool for advancing navigation
technologies and addressing challenges in complex environments. By providing a
publicly accessible, high-quality dataset, this work aims to bridge gaps in
sensor diversity, data accessibility, and environmental representation,
fostering further innovation in the field.

</details>


### [10] [Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen](https://arxiv.org/abs/2507.19082)
*Rachel Ringe,Leandra Thiele,Mihai Pomarlan,Nima Zargham,Robin Nolte,Lars Hurrelbrink,Rainer Malaka*

Main category: cs.RO

TL;DR: 研究探讨机器人视觉设计如何影响人类在协作烹饪场景中的布局及任务分配，发现人类偏爱生物形态机器人，且对机器人感官能力的信念受形态影响较小。


<details>
  <summary>Details</summary>
Motivation: 探索机器人视觉设计对人类协作行为和任务分配的影响，以优化人机协作体验。

Method: 在VR环境中，参与者与不同形态的机器人协作布置厨房，收集布局数据、口头反馈及问卷回答。

Result: 人类更倾向与生物形态机器人协作，对机器人感官能力的信念受形态影响较小，且对纤细机器人采取更少的避让策略。

Conclusion: 研究提出假设需后续验证，为机器人设计提供潜在优化方向。

Abstract: This study explores which factors of the visual design of a robot may
influence how humans would place it in a collaborative cooking scenario and how
these features may influence task delegation. Human participants were placed in
a Virtual Reality (VR) environment and asked to set up a kitchen for cooking
alongside a robot companion while considering the robot's morphology. We
collected multimodal data for the arrangements created by the participants,
transcripts of their think-aloud as they were performing the task, and
transcripts of their answers to structured post-task questionnaires. Based on
analyzing this data, we formulate several hypotheses: humans prefer to
collaborate with biomorphic robots; human beliefs about the sensory
capabilities of robots are less influenced by the morphology of the robot than
beliefs about action capabilities; and humans will implement fewer avoidance
strategies when sharing space with gracile robots. We intend to verify these
hypotheses in follow-up studies.

</details>


### [11] [Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL](https://arxiv.org/abs/2507.19146)
*Ahmed Abouelazm,Johannes Ratz,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 论文提出了一种基于学生-教师框架的自动课程学习方法，用于提升自动驾驶强化学习策略的鲁棒性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在复杂交通中面临挑战，现有强化学习方法依赖规则化场景生成，缺乏对常规与关键场景的平衡处理。

Method: 采用图基多智能体强化学习教师模型，自适应生成不同难度交通行为，结合学生模型（深度RL）进行训练。

Result: 教师模型能生成多样化交通行为，学生模型在自动课程训练下表现优于基于规则的方法，驾驶更平衡和自信。

Conclusion: 自动课程学习框架有效提升了自动驾驶策略的泛化能力和性能。

Abstract: Autonomous driving faces challenges in navigating complex real-world traffic,
requiring safe handling of both common and critical scenarios. Reinforcement
learning (RL), a prominent method in end-to-end driving, enables agents to
learn through trial and error in simulation. However, RL training often relies
on rule-based traffic scenarios, limiting generalization. Additionally, current
scenario generation methods focus heavily on critical scenarios, neglecting a
balance with routine driving behaviors. Curriculum learning, which
progressively trains agents on increasingly complex tasks, is a promising
approach to improving the robustness and coverage of RL driving policies.
However, existing research mainly emphasizes manually designed curricula,
focusing on scenery and actor placement rather than traffic behavior dynamics.
This work introduces a novel student-teacher framework for automatic curriculum
learning. The teacher, a graph-based multi-agent RL component, adaptively
generates traffic behaviors across diverse difficulty levels. An adaptive
mechanism adjusts task difficulty based on student performance, ensuring
exposure to behaviors ranging from common to critical. The student, though
exchangeable, is realized as a deep RL agent with partial observability,
reflecting real-world perception constraints. Results demonstrate the teacher's
ability to generate diverse traffic behaviors. The student, trained with
automatic curricula, outperformed agents trained on rule-based traffic,
achieving higher rewards and exhibiting balanced, assertive driving.

</details>


### [12] [ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination](https://arxiv.org/abs/2507.19151)
*Michael Amir,Guang Yang,Zhan Gao,Keisuke Okumura,Heedo Woo,Amanda Prorok*

Main category: cs.RO

TL;DR: ReCoDe是一个结合优化控制器和多智能体强化学习的混合框架，通过学习动态约束提升专家控制器的性能，适用于复杂协调任务。


<details>
  <summary>Details</summary>
Motivation: 手工设计的约束在多智能体环境中可能失效，需要更灵活的方法来适应复杂协调需求。

Method: ReCoDe通过局部通信学习动态约束，改进专家控制器，实现更有效的协调。

Result: 在需要复杂导航和共识的任务中，ReCoDe优于手工控制器、其他混合方法和标准MARL基线。

Conclusion: 保留用户定义的控制器并结合动态约束学习，比从头学习更高效，且适应性更强。

Abstract: Constraint-based optimization is a cornerstone of robotics, enabling the
design of controllers that reliably encode task and safety requirements such as
collision avoidance or formation adherence. However, handcrafted constraints
can fail in multi-agent settings that demand complex coordination. We introduce
ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid
framework that merges the reliability of optimization-based controllers with
the adaptability of multi-agent reinforcement learning. Rather than discarding
expert controllers, ReCoDe improves them by learning additional, dynamic
constraints that capture subtler behaviors, for example, by constraining agent
movements to prevent congestion in cluttered scenarios. Through local
communication, agents collectively constrain their allowed actions to
coordinate more effectively under changing conditions. In this work, we focus
on applications of ReCoDe to multi-agent navigation tasks requiring intricate,
context-based movements and consensus, where we show that it outperforms purely
handcrafted controllers, other hybrid approaches, and standard MARL baselines.
We give empirical (real robot) and theoretical evidence that retaining a
user-defined controller, even when it is imperfect, is more efficient than
learning from scratch, especially because ReCoDe can dynamically change the
degree to which it relies on this controller.

</details>


### [13] [Towards Multimodal Social Conversations with Robots: Using Vision-Language Models](https://arxiv.org/abs/2507.19196)
*Ruben Janssens,Tony Belpaeme*

Main category: cs.RO

TL;DR: 论文探讨了如何利用视觉语言模型提升社交机器人的多模态交互能力，以弥补当前大型语言模型在社交对话中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽能支持开放域对话，但缺乏多模态交互能力，限制了社交机器人的表现。

Method: 提出利用视觉语言模型处理广泛的视觉信息，并探讨了其适应社交机器人场景的方法与技术挑战。

Result: 视觉语言模型能够为自主社交机器人提供足够通用的多模态交互能力。

Conclusion: 视觉语言模型是提升社交机器人多模态交互能力的有前景方向，但仍需解决技术挑战和评估问题。

Abstract: Large language models have given social robots the ability to autonomously
engage in open-domain conversations. However, they are still missing a
fundamental social skill: making use of the multiple modalities that carry
social interactions. While previous work has focused on task-oriented
interactions that require referencing the environment or specific phenomena in
social interactions such as dialogue breakdowns, we outline the overall needs
of a multimodal system for social conversations with robots. We then argue that
vision-language models are able to process this wide range of visual
information in a sufficiently general manner for autonomous social robots. We
describe how to adapt them to this setting, which technical challenges remain,
and briefly discuss evaluation practices.

</details>


### [14] [Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation](https://arxiv.org/abs/2507.19242)
*Kang Xiangli,Yage He,Xianwu Gong,Zehan Liu,Yuru Bai*

Main category: cs.RO

TL;DR: 该研究提出了一种利用扩散模型定位未知物体重心的抓取方法，解决了现有方法在质量分布不均物体上的局限性。


<details>
  <summary>Details</summary>
Motivation: 在机器人抓取中，重心偏差常导致姿态不稳定，而现有的关键点或功能驱动方法存在不足。

Method: 构建了790张质量分布不均物体的图像数据集，并开发了基于基础模型的视觉驱动框架以实现重心感知抓取。

Result: 实验表明，该方法比传统关键点方法成功率提高49%，比最新功能驱动方法提高11%，且在未见物体上重心定位准确率达76%。

Conclusion: 该方法为精确稳定的抓取任务提供了新颖解决方案。

Abstract: This study presents a grasping method for objects with uneven mass
distribution by leveraging diffusion models to localize the center of gravity
(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to
postural instability, where existing keypoint-based or affordance-driven
methods exhibit limitations. We constructed a dataset of 790 images featuring
unevenly distributed objects with keypoint annotations for CoG localization. A
vision-driven framework based on foundation models was developed to achieve
CoG-aware grasping. Experimental evaluations across real-world scenarios
demonstrate that our method achieves a 49\% higher success rate compared to
conventional keypoint-based approaches and an 11\% improvement over
state-of-the-art affordance-driven methods. The system exhibits strong
generalization with a 76\% CoG localization accuracy on unseen objects,
providing a novel solution for precise and stable grasping tasks.

</details>


### [15] [How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version](https://arxiv.org/abs/2507.19335)
*Ilaria Consoli,Claudio Mattutino,Cristina Gena,Berardina de Carolis,Giuseppe Palestra*

Main category: cs.RO

TL;DR: 研究不同年龄段（儿童、年轻人和老年人）如何理解人形机器人NAO表达的情感肢体语言，揭示用户对机器人情感线索的感知差异。


<details>
  <summary>Details</summary>
Motivation: 探讨用户如何感知和响应机器人情感线索，评估机器人在不同年龄段用户中传达情感的效果。

Method: 通过分析老年参与者数据，并与年轻人和儿童数据对比，研究各年龄组的异同。

Result: 年轻和老年用户相似，但与年轻人存在差异。

Conclusion: 研究为机器人情感表达设计提供了跨年龄段的参考。

Abstract: This paper presents an empirical study investigating how individuals across
different age groups, children, young and older adults, interpret emotional
body language expressed by the humanoid robot NAO. The aim is to offer insights
into how users perceive and respond to emotional cues from robotic agents,
through an empirical evaluation of the robot's effectiveness in conveying
emotions to different groups of users. By analyzing data collected from elderly
participants and comparing these findings with previously gathered data from
young adults and children, the study highlights similarities and differences
between the groups, with younger and older users more similar but different
from young adults.

</details>
