<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 42]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods](https://arxiv.org/abs/2507.19642)
*Ahmad Suleman,Misha Urooj Khan,Zeeshan Kaleem,Ali H. Alenezi,Iqra Shabbir Sinem Coleri,Chau Yuen*

Main category: cs.RO

TL;DR: 论文提出了一种奖励增强学习框架（RARLAP），用于解决自主停车（AP）中的复杂控制问题，通过结构化奖励设计提升策略的适应性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则和模型预测的方法在自主停车中缺乏适应性和泛化能力，无法处理非线性及环境依赖的复杂性。

Method: 提出RARLAP框架，利用三种结构化奖励策略（GOR、DPR、MAR）结合策略优化方法，在高保真Unity仿真环境中训练。

Result: 实验表明，MAR策略在91%的成功率下表现最佳，轨迹更平滑且行为更鲁棒，而GOR和DPR效果不佳。

Conclusion: RARLAP通过奖励增强有效解决了自主停车的复杂控制问题，支持可扩展且高效的策略优化。

Abstract: Autonomous parking (AP) represents a critical yet complex subset of
intelligent vehicle automation, characterized by tight spatial constraints,
frequent close-range obstacle interactions, and stringent safety margins.
However, conventional rule-based and model-predictive methods often lack the
adaptability and generalization needed to handle the nonlinear and
environment-dependent complexities of AP. To address these limitations, we
propose a reward-augmented learning framework for AP (RARLAP), that mitigates
the inherent complexities of continuous-domain control by leveraging structured
reward design to induce smooth and adaptable policy behavior, trained entirely
within a high-fidelity Unity-based custom 3D simulation environment. We
systematically design and assess three structured reward strategies: goal-only
reward (GOR), dense proximity reward (DPR), and milestone-augmented reward
(MAR), each integrated with both on-policy and off-policy optimization
paradigms. Empirical evaluations demonstrate that the on-policy MAR achieves a
91\% success rate, yielding smoother trajectories and more robust behavior,
while GOR and DPR fail to guide effective learning. Convergence and trajectory
analyses demonstrate that the proposed framework enhances policy adaptability,
accelerates training, and improves safety in continuous control. Overall,
RARLAP establishes that reward augmentation effectively addresses complex
autonomous parking challenges, enabling scalable and efficient policy
optimization with both on- and off-policy methods. To support reproducibility,
the code accompanying this paper is publicly available.

</details>


### [2] [RAKOMO: Reachability-Aware K-Order Markov Path Optimization for Quadrupedal Loco-Manipulation](https://arxiv.org/abs/2507.19652)
*Mattia Risiglione,Abdelrahman Abdalla,Victor Barasuol,Kim Tien Ly,Ioannis Havoutis,Claudio Semini*

Main category: cs.RO

TL;DR: RAKOMO是一种结合K-Order Markov Optimization（KOMO）和基于可达性边界的运动规划技术，用于解决腿式机械臂的复杂运动规划问题。


<details>
  <summary>Details</summary>
Motivation: 腿式机械臂（如带机械臂的四足机器人）的运动规划需考虑复杂的运动学约束和接触不连续性，现有方法常因计算原因忽略腿部限制。

Method: RAKOMO将KOMO与基于神经网络预测的可达性边界结合，优化运动规划。

Result: 在仿真实验中，RAKOMO比基线KOMO方法更有效地完成拾取任务。

Conclusion: RAKOMO成功适应腿式机械臂的运动规划，实现了高效的任务执行。

Abstract: Legged manipulators, such as quadrupeds equipped with robotic arms, require
motion planning techniques that account for their complex kinematic constraints
in order to perform manipulation tasks both safely and effectively. However,
trajectory optimization methods often face challenges due to the hybrid
dynamics introduced by contact discontinuities, and tend to neglect leg
limitations during planning for computational reasons. In this work, we propose
RAKOMO, a path optimization technique that integrates the strengths of K-Order
Markov Optimization (KOMO) with a kinematically-aware criterion based on the
reachable region defined as reachability margin. We leverage a neural-network
to predict the margin and optimize it by incorporating it in the standard KOMO
formulation. This approach enables rapid convergence of gradient-based motion
planning -- commonly tailored for continuous systems -- while adapting it
effectively to legged manipulators, successfully executing loco-manipulation
tasks. We benchmark RAKOMO against a baseline KOMO approach through a set of
simulations for pick-and-place tasks with the HyQReal quadruped robot equipped
with a Kinova Gen3 robotic arm.

</details>


### [3] [DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning](https://arxiv.org/abs/2507.19742)
*Yanbin Li,Canran Xiao,Hongyang He,Shenghai Yuan,Zong Ke,Jiajie Yu,Zixiong Qin,Zhiguo Zhang,Wenzheng Chi,Wei Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于PPO的自适应退化优化代理（DOA），用于解决2D-SLAM在长直走廊等退化环境中的问题，通过奖励函数和迁移学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 室内环境（如长直走廊）会导致SLAM严重退化，传统监督学习方法面临数据获取瓶颈、样本质量下降和标注协议模糊等挑战。

Method: 使用PPO训练DOA，设计奖励函数引导代理感知退化环境，动态调整传感器贡献，并通过迁移学习提升泛化能力。

Result: DOA在退化检测和优化能力上优于SOTA方法，并通过消融实验验证了模型设计的合理性。

Conclusion: 提出的DOA能有效解决SLAM退化问题，具有跨环境的泛化能力，为类似问题提供了系统解决方案。

Abstract: Particle filter-based 2D-SLAM is widely used in indoor localization tasks due
to its efficiency. However, indoor environments such as long straight corridors
can cause severe degeneracy problems in SLAM. In this paper, we use Proximal
Policy Optimization (PPO) to train an adaptive degeneracy optimization agent
(DOA) to address degeneracy problem. We propose a systematic methodology to
address three critical challenges in traditional supervised learning
frameworks: (1) data acquisition bottlenecks in degenerate dataset, (2)
inherent quality deterioration of training samples, and (3) ambiguity in
annotation protocol design. We design a specialized reward function to guide
the agent in developing perception capabilities for degenerate environments.
Using the output degeneracy factor as a reference weight, the agent can
dynamically adjust the contribution of different sensors to pose optimization.
Specifically, the observation distribution is shifted towards the motion model
distribution, with the step size determined by a linear interpolation formula
related to the degeneracy factor. In addition, we employ a transfer learning
module to endow the agent with generalization capabilities across different
environments and address the inefficiency of training in degenerate
environments. Finally, we conduct ablation studies to demonstrate the
rationality of our model design and the role of transfer learning. We also
compare the proposed DOA with SOTA methods to prove its superior degeneracy
detection and optimization capabilities across various environments.

</details>


### [4] [Feeling the Force: A Nuanced Physics-based Traversability Sensor for Navigation in Unstructured Vegetation](https://arxiv.org/abs/2507.19831)
*Zaar Khizar,Johann Laconte,Roland Lenain,Romuald Aufrere*

Main category: cs.RO

TL;DR: 论文提出了一种新型传感器，用于直接测量植被对机器人施加的力，以评估其安全性和可穿越性。


<details>
  <summary>Details</summary>
Motivation: 机器人在自然环境中常遇到植被障碍，传统方法难以量化其安全性，需更精确的力测量方法。

Method: 设计了一种传感器，直接捕捉植被对机器人的反作用力，并通过实验验证其有效性。

Result: 传感器能精确测量细微的力变化，为导航决策提供量化依据。

Conclusion: 该传感器为机器人导航和未来学习算法的发展提供了重要基础。

Abstract: In many applications, robots are increasingly deployed in unstructured and
natural environments where they encounter various types of vegetation.
Vegetation presents unique challenges as a traversable obstacle, where the
mechanical properties of the plants can influence whether a robot can safely
collide with and overcome the obstacle. A more nuanced approach is required to
assess the safety and traversability of these obstacles, as collisions can
sometimes be safe and necessary for navigating through dense or unavoidable
vegetation. This paper introduces a novel sensor designed to directly measure
the applied forces exerted by vegetation on a robot: by directly capturing the
push-back forces, our sensor provides a detailed understanding of the
interactions between the robot and its surroundings. We demonstrate the
sensor's effectiveness through experimental validations, showcasing its ability
to measure subtle force variations. This force-based approach provides a
quantifiable metric that can inform navigation decisions and serve as a
foundation for developing future learning algorithms.

</details>


### [5] [Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning](https://arxiv.org/abs/2507.19555)
*Rajat Khanda,Mohammad Baqar,Sambuddha Chakrabarti,Satyasaran Changdar*

Main category: cs.RO

TL;DR: GRPO扩展到连续控制环境，提出轨迹聚类、状态感知优势估计和正则化策略更新，适用于机器人任务。


<details>
  <summary>Details</summary>
Motivation: GRPO在离散动作空间表现良好，但连续控制应用尚未探索，限制了其在机器人领域的实用性。

Method: 引入轨迹聚类、状态感知优势估计和正则化策略更新，解决高维动作空间、稀疏奖励和时序动态问题。

Result: 提供理论分析，证明收敛性和计算复杂度，为未来机器人任务验证奠定基础。

Conclusion: GRPO扩展至连续控制的理论框架为机器人应用提供了新方向。

Abstract: Group Relative Policy Optimization (GRPO) has shown promise in discrete
action spaces by eliminating value function dependencies through group-based
advantage estimation. However, its application to continuous control remains
unexplored, limiting its utility in robotics where continuous actions are
essential. This paper presents a theoretical framework extending GRPO to
continuous control environments, addressing challenges in high-dimensional
action spaces, sparse rewards, and temporal dynamics. Our approach introduces
trajectory-based policy clustering, state-aware advantage estimation, and
regularized policy updates designed for robotic applications. We provide
theoretical analysis of convergence properties and computational complexity,
establishing a foundation for future empirical validation in robotic systems
including locomotion and manipulation tasks.

</details>


### [6] [Homotopy-aware Multi-agent Navigation via Distributed Model Predictive Control](https://arxiv.org/abs/2507.19860)
*Haoze Dong,Meng Guo,Chengyi He,Zhongkui Li*

Main category: cs.RO

TL;DR: 提出了一种分布式轨迹规划框架，通过全局路径和局部轨迹协作解决多智能体在密集环境中的死锁问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体轨迹规划在密集环境中常出现死锁问题，尤其是在狭窄走廊中。

Method: 全局层面使用同伦感知的最优路径规划算法，局部层面采用模型预测控制优化轨迹，并结合在线重规划策略。

Result: 实验表明，该方法显著减少死锁，成功率从4%-13%提升至90%以上。

Conclusion: 通过全局路径的时间感知同伦属性和局部优化，有效解决了多智能体轨迹规划中的死锁问题。

Abstract: Multi-agent trajectory planning requires ensuring both safety and efficiency,
yet deadlocks remain a significant challenge, especially in obstacle-dense
environments. Such deadlocks frequently occur when multiple agents attempt to
traverse the same long and narrow corridor simultaneously. To address this, we
propose a novel distributed trajectory planning framework that bridges the gap
between global path and local trajectory cooperation. At the global level, a
homotopy-aware optimal path planning algorithm is proposed, which fully
leverages the topological structure of the environment. A reference path is
chosen from distinct homotopy classes by considering both its spatial and
temporal properties, leading to improved coordination among agents globally. At
the local level, a model predictive control-based trajectory optimization
method is used to generate dynamically feasible and collision-free
trajectories. Additionally, an online replanning strategy ensures its
adaptability to dynamic environments. Simulations and experiments validate the
effectiveness of our approach in mitigating deadlocks. Ablation studies
demonstrate that by incorporating time-aware homotopic properties into the
underlying global paths, our method can significantly reduce deadlocks and
improve the average success rate from 4%-13% to over 90% in randomly generated
dense scenarios.

</details>


### [7] [Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations](https://arxiv.org/abs/2507.19947)
*Supawich Sitdhipol,Waritwong Sukprasongdee,Ekapol Chuangsuwanich,Rina Tse*

Main category: cs.RO

TL;DR: FP-LGN模型通过三阶段课程学习，将人类空间语言与地图特征关联，实现不确定性感知的信息融合，提升人机协作任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器人感知局限，通过融合人类观察信息，实现不确定性感知的协作任务。

Method: 提出FP-LGN模型，学习地图特征与空间语言关系，通过三阶段课程学习估计概率。

Result: FP-LGN在NLL上匹配专家规则，鲁棒性更强，显著提升人机协作任务性能。

Conclusion: FP-LGN成功实现不确定性感知的信息融合，为人机协作任务提供有效解决方案。

Abstract: Fusing information from human observations can help robots overcome sensing
limitations in collaborative tasks. However, an uncertainty-aware fusion
framework requires a grounded likelihood representing the uncertainty of human
inputs. This paper presents a Feature Pyramid Likelihood Grounding Network
(FP-LGN) that grounds spatial language by learning relevant map image features
and their relationships with spatial relation semantics. The model is trained
as a probability estimator to capture aleatoric uncertainty in human language
using three-stage curriculum learning. Results showed that FP-LGN matched
expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated
greater robustness with lower standard deviation. Collaborative sensing results
demonstrated that the grounded likelihood successfully enabled
uncertainty-aware fusion of heterogeneous human language observations and robot
sensor measurements, achieving significant improvements in human-robot
collaborative task performance.

</details>


### [8] [GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning](https://arxiv.org/abs/2507.19647)
*Amin Banayeeanzade,Fatemeh Bahrani,Yutai Zhou,Erdem Bıyık*

Main category: cs.RO

TL;DR: GABRIL利用人类注视数据改进模仿学习，通过正则化损失减少因果混淆，提升性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习常因因果混淆导致性能下降，需解决这一问题。

Method: 引入GABRIL，利用人类注视数据指导表示学习，通过正则化损失聚焦因果相关特征。

Result: 在Atari和CARLA中，GABRIL性能分别提升179%和76%。

Conclusion: GABRIL有效减少因果混淆，并提供更强的可解释性。

Abstract: Imitation Learning (IL) is a widely adopted approach which enables agents to
learn from human expert demonstrations by framing the task as a supervised
learning problem. However, IL often suffers from causal confusion, where agents
misinterpret spurious correlations as causal relationships, leading to poor
performance in testing environments with distribution shift. To address this
issue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a
novel method that leverages the human gaze data gathered during the data
collection phase to guide the representation learning in IL. GABRIL utilizes a
regularization loss which encourages the model to focus on causally relevant
features identified through expert gaze and consequently mitigates the effects
of confounding variables. We validate our approach in Atari environments and
the Bench2Drive benchmark in CARLA by collecting human gaze datasets and
applying our method in both domains. Experimental results show that the
improvement of GABRIL over behavior cloning is around 179% more than the same
number for other baselines in the Atari and 76% in the CARLA setup. Finally, we
show that our method provides extra explainability when compared to regular IL
agents.

</details>


### [9] [LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models](https://arxiv.org/abs/2507.20509)
*Zhongchao Zhou,Yuxi Lu,Yaonan Zhu,Yifan Zhao,Bin He,Liang He,Wenwen Yu,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）的自适应补偿器框架，用于机器人控制，避免了从头设计控制器，并通过实验验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM在高层任务中的应用，而在反馈控制器设计中仅涉及简化系统，缺乏实际验证。本文旨在探索LLM在自适应控制中的潜力。

Method: 受模型参考自适应控制（MRAC）启发，提出LLM引导的自适应补偿器框架，利用未知系统与参考系统的差异设计补偿器。

Result: 实验表明，LLM引导的自适应补偿器优于传统方法，显著降低推理复杂度，并展现出强泛化性、适应性和鲁棒性。

Conclusion: 本研究为LLM在自动控制领域的应用开辟了新方向，具有更高的实用性和部署潜力。

Abstract: With rapid advances in code generation, reasoning, and problem-solving, Large
Language Models (LLMs) are increasingly applied in robotics. Most existing work
focuses on high-level tasks such as task decomposition. A few studies have
explored the use of LLMs in feedback controller design; however, these efforts
are restricted to overly simplified systems, fixed-structure gain tuning, and
lack real-world validation. To further investigate LLMs in automatic control,
this work targets a key subfield: adaptive control. Inspired by the framework
of model reference adaptive control (MRAC), we propose an LLM-guided adaptive
compensator framework that avoids designing controllers from scratch. Instead,
the LLMs are prompted using the discrepancies between an unknown system and a
reference system to design a compensator that aligns the response of the
unknown system with that of the reference, thereby achieving adaptivity.
Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided
adaptive controller, indirect adaptive control, learning-based adaptive
control, and MRAC, on soft and humanoid robots in both simulated and real-world
environments. Results show that the LLM-guided adaptive compensator outperforms
traditional adaptive controllers and significantly reduces reasoning complexity
compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and
reasoning-path inspection demonstrate that the LLM-guided adaptive compensator
enables a more structured design process by transforming mathematical
derivation into a reasoning task, while exhibiting strong generalizability,
adaptability, and robustness. This study opens a new direction for applying
LLMs in the field of automatic control, offering greater deployability and
practicality compared to vision-language models.

</details>


### [10] [PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction](https://arxiv.org/abs/2507.19701)
*Haichuan Li,Tomi Westerlund*

Main category: cs.RO

TL;DR: 提出一种结合学习与物理约束的混合方法，用于多模态轨迹预测，提升自动驾驶的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂城市环境中多模态轨迹预测的挑战，确保自动驾驶的安全性和高效性。

Method: 使用变分贝叶斯混合模型结合物理约束（如边界条件和MPC平滑），生成数据一致且物理可行的轨迹。

Result: 在两个基准数据集上表现优于现有方法，验证了各组件对预测准确性和可靠性的贡献。

Conclusion: 通过平衡数据驱动与物理约束，提供了一种稳健且可扩展的解决方案，适用于现实城市环境的不确定性。

Abstract: Accurate prediction of future agent trajectories is a critical challenge for
ensuring safe and efficient autonomous navigation, particularly in complex
urban environments characterized by multiple plausible future scenarios. In
this paper, we present a novel hybrid approach that integrates learning-based
with physics-based constraints to address the multi-modality inherent in
trajectory prediction. Our method employs a variational Bayesian mixture model
to effectively capture the diverse range of potential future behaviors, moving
beyond traditional unimodal assumptions. Unlike prior approaches that
predominantly treat trajectory prediction as a data-driven regression task, our
framework incorporates physical realism through sector-specific boundary
conditions and Model Predictive Control (MPC)-based smoothing. These
constraints ensure that predicted trajectories are not only data-consistent but
also physically plausible, adhering to kinematic and dynamic principles.
Furthermore, our method produces interpretable and diverse trajectory
predictions, enabling enhanced downstream decision-making and planning in
autonomous driving systems. We evaluate our approach on two benchmark datasets,
demonstrating superior performance compared to existing methods. Comprehensive
ablation studies validate the contributions of each component and highlight
their synergistic impact on prediction accuracy and reliability. By balancing
data-driven insights with physics-informed constraints, our approach offers a
robust and scalable solution for navigating the uncertainties of real-world
urban environments.

</details>


### [11] [Skin-Machine Interface with Multimodal Contact Motion Classifier](https://arxiv.org/abs/2507.19760)
*Alberto Confente,Takanori Jin,Taisuke Kobayashi,Julio Rogelio Guadarrama-Olvera,Gordon Cheng*

Main category: cs.RO

TL;DR: 提出了一种利用皮肤传感器作为复杂机器人操作界面的新框架，通过多模态触觉信息分类实现多样化机器人动作生成。


<details>
  <summary>Details</summary>
Motivation: 探索皮肤传感器作为机器人操作界面的潜力，通过多模态触觉信息提升交互的多样性和准确性。

Method: 采用基于循环神经网络的接触动作分类器，结合多模态传感和柔性支撑设计，优化分类性能。

Result: 分类器准确率超过95%，成功应用于双臂移动机械臂的多样化任务执行。

Conclusion: 该框架通过多模态传感和柔性设计显著提升了机器人操作的灵活性和准确性，为未来人机交互提供了新思路。

Abstract: This paper proposes a novel framework for utilizing skin sensors as a new
operation interface of complex robots. The skin sensors employed in this study
possess the capability to quantify multimodal tactile information at multiple
contact points. The time-series data generated from these sensors is
anticipated to facilitate the classification of diverse contact motions
exhibited by an operator. By mapping the classification results with robot
motion primitives, a diverse range of robot motions can be generated by
altering the manner in which the skin sensors are interacted with. In this
paper, we focus on a learning-based contact motion classifier employing
recurrent neural networks. This classifier is a pivotal factor in the success
of this framework. Furthermore, we elucidate the requisite conditions for
software-hardware designs. Firstly, multimodal sensing and its comprehensive
encoding significantly contribute to the enhancement of classification accuracy
and learning stability. Utilizing all modalities simultaneously as inputs to
the classifier proves to be an effective approach. Secondly, it is essential to
mount the skin sensors on a flexible and compliant support to enable the
activation of three-axis accelerometers. These accelerometers are capable of
measuring horizontal tactile information, thereby enhancing the correlation
with other modalities. Furthermore, they serve to absorb the noises generated
by the robot's movements during deployment. Through these discoveries, the
accuracy of the developed classifier surpassed 95 %, enabling the dual-arm
mobile manipulator to execute a diverse range of tasks via the Skin-Machine
Interface. https://youtu.be/UjUXT4Z4BC8

</details>


### [12] [Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation](https://arxiv.org/abs/2507.19817)
*Ziyin Xiong,Yinghan Chen,Puhao Li,Yixin Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.RO

TL;DR: Ag2x2是一个通过协调感知视觉表示实现双手机器人操作的计算框架，无需专家监督即可学习复杂技能。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作因协调控制的复杂性而具有挑战性，现有方法忽略了关键的代理特定信息。

Method: Ag2x2通过联合编码对象状态和手部运动模式，同时保持代理无关性。

Result: 在13项任务中达到73.5%的成功率，优于基线方法和专家设计的奖励策略。

Conclusion: Ag2x2为复杂双手机器人技能的可扩展学习提供了有效途径。

Abstract: Bimanual manipulation, fundamental to human daily activities, remains a
challenging task due to its inherent complexity of coordinated control. Recent
advances have enabled zero-shot learning of single-arm manipulation skills
through agent-agnostic visual representations derived from human videos;
however, these methods overlook crucial agent-specific information necessary
for bimanual coordination, such as end-effector positions. We propose Ag2x2, a
computational framework for bimanual manipulation through coordination-aware
visual representations that jointly encode object states and hand motion
patterns while maintaining agent-agnosticism. Extensive experiments demonstrate
that Ag2x2 achieves a 73.5% success rate across 13 diverse bimanual tasks from
Bi-DexHands and PerAct2, including challenging scenarios with deformable
objects like ropes. This performance outperforms baseline methods and even
surpasses the success rate of policies trained with expert-engineered rewards.
Furthermore, we show that representations learned through Ag2x2 can be
effectively leveraged for imitation learning, establishing a scalable pipeline
for skill acquisition without expert supervision. By maintaining robust
performance across diverse tasks without human demonstrations or engineered
rewards, Ag2x2 represents a step toward scalable learning of complex bimanual
robotic skills.

</details>


### [13] [A 4D Radar Camera Extrinsic Calibration Tool Based on 3D Uncertainty Perspective N Points](https://arxiv.org/abs/2507.19829)
*Chuan Cao,Xiaoning Wang,Wenqian Xi,Han Zhang,Weidong Chen,Jingchuan Wang*

Main category: cs.RO

TL;DR: 本文提出了一种针对毫米波雷达与相机系统的外参校准框架，通过3DUPnP算法显式建模雷达测量中的球坐标噪声传播，并在坐标变换中补偿非零误差期望，实验验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达与相机系统的准确外参校准对机器人多模态感知至关重要，但由于传感器噪声特性和复杂误差传播，校准仍具挑战性。

Method: 提出了一种空间3D不确定性感知的PnP算法（3DUPnP），显式建模雷达测量中的球坐标噪声传播，并在坐标变换中补偿非零误差期望。

Result: 实验验证表明，该方法在仿真中具有更好的一致性，在物理实验中提高了精度，显著优于现有CPnP基线。

Conclusion: 该研究为配备毫米波雷达和相机的机器人系统提供了一种鲁棒的校准解决方案，特别适用于自动驾驶和机器人感知应用。

Abstract: 4D imaging radar is a type of low-cost millimeter-wave radar(costing merely
10-20$\%$ of lidar systems) capable of providing range, azimuth, elevation, and
Doppler velocity information. Accurate extrinsic calibration between
millimeter-wave radar and camera systems is critical for robust multimodal
perception in robotics, yet remains challenging due to inherent sensor noise
characteristics and complex error propagation. This paper presents a systematic
calibration framework to address critical challenges through a spatial 3d
uncertainty-aware PnP algorithm (3DUPnP) that explicitly models spherical
coordinate noise propagation in radar measurements, then compensating for
non-zero error expectations during coordinate transformations. Finally,
experimental validation demonstrates significant performance improvements over
state-of-the-art CPnP baseline, including improved consistency in simulations
and enhanced precision in physical experiments. This study provides a robust
calibration solution for robotic systems equipped with millimeter-wave radar
and cameras, tailored specifically for autonomous driving and robotic
perception applications.

</details>


### [14] [PlaneHEC: Efficient Hand-Eye Calibration for Multi-view Robotic Arm via Any Point Cloud Plane Detection](https://arxiv.org/abs/2507.19851)
*Ye Wang,Haodong Jing,Yang Liao,Yongqiang Ma,Nanning Zheng*

Main category: cs.RO

TL;DR: PlaneHEC是一种无需复杂模型、仅需深度相机的手眼标定方法，利用任意平面表面实现快速最优标定。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确几何模型或人工辅助，泛化性差且复杂低效，需改进。

Method: 基于平面约束设计标定方程，结合闭式解和迭代优化提高精度。

Result: 在仿真和真实环境中表现优异，优于其他点云标定方法。

Conclusion: PlaneHEC为多智能体系统和具身智能发展提供重要贡献。

Abstract: Hand-eye calibration is an important task in vision-guided robotic systems
and is crucial for determining the transformation matrix between the camera
coordinate system and the robot end-effector. Existing methods, for multi-view
robotic systems, usually rely on accurate geometric models or manual
assistance, generalize poorly, and can be very complicated and inefficient.
Therefore, in this study, we propose PlaneHEC, a generalized hand-eye
calibration method that does not require complex models and can be accomplished
using only depth cameras, which achieves the optimal and fastest calibration
results using arbitrary planar surfaces like walls and tables. PlaneHEC
introduces hand-eye calibration equations based on planar constraints, which
makes it strongly interpretable and generalizable. PlaneHEC also uses a
comprehensive solution that starts with a closed-form solution and improves it
withiterative optimization, which greatly improves accuracy. We comprehensively
evaluated the performance of PlaneHEC in both simulated and real-world
environments and compared the results with other point-cloud-based calibration
methods, proving its superiority. Our approach achieves universal and fast
calibration with an innovative design of computational models, providing a
strong contribution to the development of multi-agent systems and embodied
intelligence.

</details>


### [15] [Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models](https://arxiv.org/abs/2507.19854)
*Anjali R. Menon,Rohit K. Sharma,Priya Singh,Chengyu Wang,Aurora M. Ferreira,Mateja Novak*

Main category: cs.RO

TL;DR: 论文提出了一种名为“Think, Act, Learn”（T-A-L）的闭环框架，通过LLM驱动的自我反思和持续学习，显著提升了机器人在动态环境中的适应能力和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的机器人系统多为开环规划，无法适应动态环境中的突发情况，限制了其鲁棒性和适应性。

Method: T-A-L框架通过“思考-行动-学习”的闭环循环，利用LLM分解任务、执行计划并收集反馈，通过自我反思和因果分析优化策略。

Result: 实验表明，T-A-L框架在复杂任务中成功率超过97%，仅需9次试验即可收敛，且能泛化到未见任务。

Conclusion: T-A-L框架为开发更鲁棒、自适应和真正自主的机器人系统迈出了重要一步。

Abstract: The integration of Large Language Models (LLMs) into robotics has unlocked
unprecedented capabilities in high-level task planning. However, most current
systems operate in an open-loop fashion, where LLMs act as one-shot planners,
rendering them brittle and unable to adapt to unforeseen circumstances in
dynamic physical environments. To overcome this limitation, this paper
introduces the "Think, Act, Learn" (T-A-L) framework, a novel architecture that
enables an embodied agent to autonomously learn and refine its policies through
continuous interaction. Our framework establishes a closed-loop cycle where an
LLM first "thinks" by decomposing high-level commands into actionable plans.
The robot then "acts" by executing these plans while gathering rich, multimodal
sensory feedback. Critically, the "learn" module processes this feedback to
facilitate LLM-driven self-reflection, allowing the agent to perform causal
analysis on its failures and generate corrective strategies. These insights are
stored in an experiential memory to guide future planning cycles. We
demonstrate through extensive experiments in both simulation and the real world
that our T-A-L agent significantly outperforms baseline methods, including
open-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our
framework achieves over a 97% success rate on complex, long-horizon tasks,
converges to a stable policy in an average of just 9 trials, and exhibits
remarkable generalization to unseen tasks. This work presents a significant
step towards developing more robust, adaptive, and truly autonomous robotic
agents.

</details>


### [16] [Bridging Simulation and Usability: A User-Friendly Framework for Scenario Generation in CARLA](https://arxiv.org/abs/2507.19883)
*Ahmed Abouelazm,Mohammad Mahmoud,Conrad Walter,Oleksandr Shchetsura,Erne Hussong,Helen Gremmelmaier,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 提出了一种无需编程的交互式场景生成框架，用于自动驾驶系统的验证，降低技术门槛并支持更广泛的用户群体。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统验证需要大规模场景测试，但现有工具依赖编程知识，限制了非技术用户的使用。

Method: 开发了一个图形化界面框架，支持手动和自动生成场景，采用基于图的场景表示方法，并与深度学习方法集成。

Result: 框架能够生成多样化和真实的测试数据集，简化了测试流程，提高了验证的可访问性。

Conclusion: 该框架为自动驾驶验证提供了高效、易用的工具，适用于研究人员、工程师和政策制定者。

Abstract: Autonomous driving promises safer roads, reduced congestion, and improved
mobility, yet validating these systems across diverse conditions remains a
major challenge. Real-world testing is expensive, time-consuming, and sometimes
unsafe, making large-scale validation impractical. In contrast, simulation
environments offer a scalable and cost-effective alternative for rigorous
verification and validation. A critical component of the validation process is
scenario generation, which involves designing and configuring traffic scenarios
to evaluate autonomous systems' responses to various events and uncertainties.
However, existing scenario generation tools often require programming
knowledge, limiting accessibility for non-technical users. To address this
limitation, we present an interactive, no-code framework for scenario
generation. Our framework features a graphical interface that enables users to
create, modify, save, load, and execute scenarios without needing coding
expertise or detailed simulation knowledge. Unlike script-based tools such as
Scenic or ScenarioRunner, our approach lowers the barrier to entry and supports
a broader user base. Central to our framework is a graph-based scenario
representation that facilitates structured management, supports both manual and
automated generation, and enables integration with deep learning-based scenario
and behavior generation methods. In automated mode, the framework can randomly
sample parameters such as actor types, behaviors, and environmental conditions,
allowing the generation of diverse and realistic test datasets. By simplifying
the scenario generation process, this framework supports more efficient testing
workflows and increases the accessibility of simulation-based validation for
researchers, engineers, and policymakers.

</details>


### [17] [High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements](https://arxiv.org/abs/2507.19914)
*Akram Khairi,Hussain Sajwani,Abdallah Mohammad Alkilany,Laith AbuAssi,Mohamad Halwani,Islam Mohamed Zaid,Ahmed Awadalla,Dewald Swart,Abdulla Ayyad,Yahya Zweiri*

Main category: cs.RO

TL;DR: 提出了一种新型触觉传感器，结合神经形态相机和滚动机制，实现快速、连续、高分辨率的3D表面扫描，显著提升速度和精度。


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器在大面积扫描时速度慢且受限于相机帧率和运动模糊，需要一种更高效的方法。

Method: 采用神经形态相机和滚动机制，结合事件驱动的多视角立体视觉方法进行3D重建，并使用贝叶斯融合策略提升精度。

Result: 扫描速度达0.5 m/s，平均绝对误差低于100微米，比现有方法快11倍；特征识别速度提升2.6倍。

Conclusion: 该方法显著提高了大面积工业表面扫描的效率和精度，具有实际应用潜力。

Abstract: Inspecting large-scale industrial surfaces like aircraft fuselages for
quality control requires capturing their precise 3D surface geometry at high
resolution. Vision-based tactile sensors (VBTSs) offer high local resolution
but require slow 'press-and-lift' measurements stitched for large areas.
Approaches with sliding or roller/belt VBTS designs provide measurements
continuity. However, they face significant challenges respectively: sliding
struggles with friction/wear and both approaches are speed-limited by
conventional camera frame rates and motion blur, making large-area scanning
time consuming. Thus, a rapid, continuous, high-resolution method is needed. We
introduce a novel tactile sensor integrating a neuromorphic camera in a rolling
mechanism to achieve this. Leveraging its high temporal resolution and
robustness to motion blur, our system uses a modified event-based multi-view
stereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning
speeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11
times faster than prior continuous tactile sensing methods. A multi-reference
Bayesian fusion strategy enhances accuracy (reducing MAE by 25.2\% compared to
EMVS) and mitigates curvature errors. We also validate high-speed feature
recognition via Braille reading 2.6 times faster than previous approaches.

</details>


### [18] [A roadmap for AI in robotics](https://arxiv.org/abs/2507.19975)
*Aude Billard,Alin Albu-Schaeffer,Michael Beetz,Wolfram Burgard,Peter Corke,Matei Ciocarlie,Ravinder Dahiya,Danica Kragic,Ken Goldberg,Yukie Nagai,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 论文探讨了AI在机器人领域的应用潜力、挑战及未来研究方向，强调了数据、算法设计、人机协作和安全性等问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，如何将其成功应用于机器人领域以解决实际物理世界中的挑战成为研究重点。

Method: 评估了自1990年代以来AI在机器人领域的成就，并提出了短期和中期的研究路线图，包括数据集更新、算法设计和人机协作等。

Result: 指出了AI在机器人应用中的关键挑战，如数据多样性、算法通用性、透明性和安全性。

Conclusion: 未来的长期挑战包括设计能够终身学习、安全部署且计算成本可持续的机器人。

Abstract: AI technologies, including deep learning, large-language models have gone
from one breakthrough to the other. As a result, we are witnessing growing
excitement in robotics at the prospect of leveraging the potential of AI to
tackle some of the outstanding barriers to the full deployment of robots in our
daily lives. However, action and sensing in the physical world pose greater and
different challenges than analysing data in isolation. As the development and
application of AI in robotic products advances, it is important to reflect on
which technologies, among the vast array of network architectures and learning
models now available in the AI field, are most likely to be successfully
applied to robots; how they can be adapted to specific robot designs, tasks,
environments; which challenges must be overcome. This article offers an
assessment of what AI for robotics has achieved since the 1990s and proposes a
short- and medium-term research roadmap listing challenges and promises. These
range from keeping up-to-date large datasets, representatives of a diversity of
tasks robots may have to perform, and of environments they may encounter, to
designing AI algorithms tailored specifically to robotics problems but generic
enough to apply to a wide range of applications and transfer easily to a
variety of robotic platforms. For robots to collaborate effectively with
humans, they must predict human behavior without relying on bias-based
profiling. Explainability and transparency in AI-driven robot control are not
optional but essential for building trust, preventing misuse, and attributing
responsibility in accidents. We close on what we view as the primary long-term
challenges, that is, to design robots capable of lifelong learning, while
guaranteeing safe deployment and usage, and sustainable computational costs.

</details>


### [19] [CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints](https://arxiv.org/abs/2507.19983)
*Yuhong Deng,Chao Tang,Cunjun Yu,Linfeng Li,David Hsu*

Main category: cs.RO

TL;DR: CLASP提出了一种基于语义关键点的通用衣物操作方法，通过稀疏的空间语义表示连接感知与动作，实现了对不同衣物类型和任务的高效处理。


<details>
  <summary>Details</summary>
Motivation: 现有衣物操作方法受限于特定任务和衣物类型，无法应对复杂的高维几何结构。

Method: 利用语义关键点（如“左袖”、“右肩”）作为中间表示，结合视觉语言模型（VLMs）进行任务规划，并通过预构建的技能库执行动作。

Result: 在仿真和真实机器人实验中，CLASP在多种任务和衣物类型上表现优于现有方法，展示了强大的性能和泛化能力。

Conclusion: CLASP通过语义关键点实现了通用衣物操作，为家庭服务机器人提供了更灵活和高效的解决方案。

Abstract: Clothes manipulation, such as folding or hanging, is a critical capability
for home service robots. Despite recent advances, most existing methods remain
limited to specific tasks and clothes types, due to the complex,
high-dimensional geometry of clothes. This paper presents CLothes mAnipulation
with Semantic keyPoints (CLASP), which aims at general-purpose clothes
manipulation over different clothes types, T-shirts, shorts, skirts, long
dresses, ... , as well as different tasks, folding, flattening, hanging, ... .
The core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right
shoulder'', etc. -- a sparse spatial-semantic representation that is salient
for both perception and action. Semantic keypoints of clothes can be reliably
extracted from RGB-D images and provide an effective intermediate
representation of clothes manipulation policies. CLASP uses semantic keypoints
to bridge high-level task planning and low-level action execution. At the high
level, it exploits vision language models (VLMs) to predict task plans over the
semantic keypoints. At the low level, it executes the plans with the help of a
simple pre-built manipulation skill library. Extensive simulation experiments
show that CLASP outperforms state-of-the-art baseline methods on multiple tasks
across diverse clothes types, demonstrating strong performance and
generalization. Further experiments with a Franka dual-arm system on four
distinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's
performance on a real robot.

</details>


### [20] [Robot Excavation and Manipulation of Geometrically Cohesive Granular Media](https://arxiv.org/abs/2507.19999)
*Laura Treers,Daniel Soto,Joonha Hwang,Michael A. D. Goodisman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 论文探讨了机器人群体如何通过操纵纠缠颗粒材料构建随机结构，并研究了材料特性对机器人性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统建筑依赖预先设计的蓝图和材料，而随机结构（aleatory architectures）依赖材料本身的特性。研究旨在探索机器人如何自主构建此类结构。

Method: 开发了一个机器人物理模型，用于与几何粘性颗粒材料（U形颗粒）交互，测试了不同初始压缩状态下的机器人性能。

Result: 机器人性能受材料初始压缩状态显著影响，运输质量变化高达75%。拉伸测试揭示了材料强度与初始压缩加载的关系。

Conclusion: 材料特性（如堆积和纠缠）对机器人操作至关重要，研究为未来理解机器人与纠缠材料的交互机制提供了方向。

Abstract: Construction throughout history typically assumes that its blueprints and
building blocks are pre-determined. However, recent work suggests that
alternative approaches can enable new paradigms for structure formation.
Aleatory architectures, or those which rely on the properties of their granular
building blocks rather than pre-planned design or computation, have thus far
relied on human intervention for their creation. We imagine that robotic swarms
could be valuable to create such aleatory structures by manipulating and
forming structures from entangled granular materials. To discover principles by
which robotic systems can effectively manipulate soft matter, we develop a
robophysical model for interaction with geometrically cohesive granular media
composed of u-shape particles. This robotic platform uses environmental signals
to autonomously coordinate excavation, transport, and deposition of material.
We test the effect of substrate initial conditions by characterizing robot
performance in two different material compaction states and observe as much as
a 75% change in transported mass depending on initial substrate compressive
loading. These discrepancies suggest the functional role that material
properties such as packing and cohesion/entanglement play in excavation and
construction. To better understand these material properties, we develop an
apparatus for tensile testing of the geometrically cohesive substrates, which
reveals how entangled material strength responds strongly to initial
compressive loading. These results explain the variation observed in robotic
performance and point to future directions for better understanding robotic
interaction mechanics with entangled materials.

</details>


### [21] [SuperMag: Vision-based Tactile Data Guided High-resolution Tactile Shape Reconstruction for Magnetic Tactile Sensors](https://arxiv.org/abs/2507.20002)
*Peiyao Hou,Danning Sun,Meng Wang,Yuzhe Huang,Zeyu Zhang,Hangxin Liu,Wanlin Li,Ziyuan Jiao*

Main category: cs.RO

TL;DR: SuperMag利用高分辨率视觉触觉传感器数据监督磁触觉传感器的超分辨率重建，提升其空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 磁触觉传感器（MBTS）设计紧凑且高频操作，但稀疏的阵列限制了其空间分辨率。

Method: 通过条件变分自编码器从低分辨率MBTS输入推断高分辨率形状，利用同步数据收集的VBTS和MBTS数据。

Result: MBTS采样频率达125Hz，形状重建推理时间低于2.5ms。

Conclusion: 跨模态协同提升了MBTS的触觉感知能力，有望用于高精度机器人任务。

Abstract: Magnetic-based tactile sensors (MBTS) combine the advantages of compact
design and high-frequency operation but suffer from limited spatial resolution
due to their sparse taxel arrays. This paper proposes SuperMag, a tactile shape
reconstruction method that addresses this limitation by leveraging
high-resolution vision-based tactile sensor (VBTS) data to supervise MBTS
super-resolution. Co-designed, open-source VBTS and MBTS with identical contact
modules enable synchronized data collection of high-resolution shapes and
magnetic signals via a symmetric calibration setup. We frame tactile shape
reconstruction as a conditional generative problem, employing a conditional
variational auto-encoder to infer high-resolution shapes from low-resolution
MBTS inputs. The MBTS achieves a sampling frequency of 125 Hz, whereas the
shape reconstruction sustains an inference time within 2.5 ms. This
cross-modality synergy advances tactile perception of the MBTS, potentially
unlocking its new capabilities in high-precision robotic tasks.

</details>


### [22] [When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation](https://arxiv.org/abs/2507.20021)
*Matin Aghaei,Mohammad Ali Alomrani,Yingxue Zhang,Mahdi Biparva*

Main category: cs.RO

TL;DR: 论文探讨了大型语言模型（LLM）在目标导航中的作用，发现几何启发式方法比LLM更能提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在目标导航中的实际贡献，验证其是否真正提升了规划能力。

Method: 通过简化InstructNav系统，使用几何启发式方法（DWFE）和轻量级语言先验（SHF）进行实验。

Result: DWFE将成功率从58.0%提升至61.1%，SPL从20.9%提升至36.0%；SHF进一步提升了性能。

Conclusion: 几何启发式方法是性能提升的主要驱动力，LLM的作用有限，需进一步优化才能发挥其潜力。

Abstract: Large language models (LLMs) are often credited with recent leaps in
ObjectGoal Navigation, yet the extent to which they improve planning remains
unclear. We revisit this question on the HM3D-v1 validation split. First, we
strip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary
GLEE detector and Intuition saliency map, and replace them with a simple
Distance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises
Success from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000
validation episodes, outperforming all previous training-free baselines.
Second, we add a lightweight language prior (SHF); on a 200-episode subset this
yields a further +2% Success and +0.9% SPL while shortening paths by five steps
on average. Qualitative trajectories confirm the trend: InstructNav back-tracks
and times-out, DWFE reaches the goal after a few islands, and SHF follows an
almost straight route. Our results indicate that frontier geometry, not
emergent LLM reasoning, drives most reported gains, and suggest that
metric-aware prompts or offline semantic graphs are necessary before
attributing navigation success to "LLM intelligence."

</details>


### [23] [Digital and Robotic Twinning for Validation of Proximity Operations and Formation Flying](https://arxiv.org/abs/2507.20034)
*Aviad Golan,Gregory Zin,Zahra Ahmed,Emily Bates,Toby Bell,Pol Francesch Huc,Samuel Y. W. Low,Juergen Bosse,Simone D'Amico*

Main category: cs.RO

TL;DR: 本文提出了一种统一的数字和机器人孪生框架，用于多模态GNC系统的验证，展示了其在低地球轨道RPO任务中的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于太空环境的复杂性，验证GNC系统具有挑战性，需要一种连接仿真与真实行为的V&V流程。

Method: 开发了数字和机器人孪生框架，结合软件和硬件在环测试，使用三个测试台（GRAND、TRON、OS）验证多模态GNC系统。

Result: 数字与机器人孪生结果一致，验证了该框架在GNC系统评估中的可靠性。

Conclusion: 混合孪生框架为GNC系统的现实评估和验证提供了可靠方法。

Abstract: In spacecraft Rendezvous, Proximity Operations (RPO), and Formation Flying
(FF), the Guidance Navigation and Control (GNC) system is safety-critical and
must meet strict performance requirements. However, validating such systems is
challenging due to the complexity of the space environment, necessitating a
verification and validation (V&V) process that bridges simulation and
real-world behavior. The key contribution of this paper is a unified,
end-to-end digital and robotic twinning framework that enables software- and
hardware-in-the-loop testing for multi-modal GNC systems. The robotic twin
includes three testbeds at Stanford's Space Rendezvous Laboratory (SLAB): the
GNSS and Radiofrequency Autonomous Navigation Testbed for Distributed Space
Systems (GRAND) to validate RF-based navigation techniques, and the Testbed for
Rendezvous and Optical Navigation (TRON) and Optical Stimulator (OS) to
validate vision-based methods. The test article for this work is an integrated
multi-modal GNC software stack for RPO and FF developed at SLAB. This paper
introduces the hybrid framework and summarizes calibration and error
characterization for the robotic twin. Then, the GNC stack's performance and
robustness is characterized using the integrated digital and robotic twinning
pipeline for a full-range RPO mission scenario in Low-Earth Orbit (LEO). The
results shown in the paper demonstrate consistency between digital and robotic
twins, validating the hybrid twinning pipeline as a reliable framework for
realistic assessment and verification of GNC systems.

</details>


### [24] [A real-time full-chain wearable sensor-based musculoskeletal simulation: an OpenSim-ROS Integration](https://arxiv.org/abs/2507.20049)
*Frederico Belmonte Klein,Zhaoyuan Wan,Huawei Wang,Ruoli Wang*

Main category: cs.RO

TL;DR: 提出了一种基于OpenSimRT、ROS和可穿戴传感器的实时集成框架，用于肌肉骨骼建模和仿真，解决了现有技术的高成本、复杂性和集成问题。


<details>
  <summary>Details</summary>
Motivation: 现有肌肉骨骼建模技术因高成本传感器、实验室设置、计算复杂性和软件集成问题而难以广泛应用。

Method: 结合OpenSimRT、ROS和可穿戴传感器，开发实时框架，验证其在下肢和上肢逆运动学及踝关节逆动力学和肌肉激活估计中的应用。

Result: 框架能够准确描述逆运动学，并有效估计踝关节动力学和下肢肌肉激活，适用于日常活动。

Conclusion: 该框架为复杂实时和可穿戴传感器的人体运动分析系统奠定了基础，对康复、机器人和外骨骼设计有潜在推动作用。

Abstract: Musculoskeletal modeling and simulations enable the accurate description and
analysis of the movement of biological systems with applications such as
rehabilitation assessment, prosthesis, and exoskeleton design. However, the
widespread usage of these techniques is limited by costly sensors,
laboratory-based setups, computationally demanding processes, and the use of
diverse software tools that often lack seamless integration. In this work, we
address these limitations by proposing an integrated, real-time framework for
musculoskeletal modeling and simulations that leverages OpenSimRT, the robotics
operating system (ROS), and wearable sensors. As a proof-of-concept, we
demonstrate that this framework can reasonably well describe inverse kinematics
of both lower and upper body using either inertial measurement units or
fiducial markers. Additionally, we show that it can effectively estimate
inverse dynamics of the ankle joint and muscle activations of major lower limb
muscles during daily activities, including walking, squatting and sit to stand,
stand to sit when combined with pressure insoles. We believe this work lays the
groundwork for further studies with more complex real-time and wearable
sensor-based human movement analysis systems and holds potential to advance
technologies in rehabilitation, robotics and exoskeleton designs.

</details>


### [25] [Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots](https://arxiv.org/abs/2507.20217)
*Wei Cui,Haoyu Wang,Wenkang Qin,Yijie Guo,Gang Han,Wen Zhao,Jiahang Cao,Zhang Zhang,Jiaru Zhong,Jingkai Sun,Pihai Sun,Shuai Shi,Botuo Jiang,Jiahao Ma,Jiaxu Wang,Hao Cheng,Zhichao Liu,Yang Wang,Zheng Zhu,Guan Huang,Jian Tang,Qiang Zhang*

Main category: cs.RO

TL;DR: Humanoid Occupancy是一个多模态占用感知系统，结合硬件和软件组件，为仿人机器人提供全面的环境理解。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人需要丰富的语义和3D几何信息以实现环境理解，而现有的占用表示方法被认为是最适合的。

Method: 采用多模态融合技术和网格占用输出，解决运动干扰和遮挡问题，并开发首个全景占用数据集。

Result: 系统实现了鲁棒的环境感知，为标准化视觉模块奠定了基础。

Conclusion: Humanoid Occupancy为仿人机器人在复杂现实场景中的广泛应用铺平了道路。

Abstract: Humanoid robot technology is advancing rapidly, with manufacturers
introducing diverse heterogeneous visual perception modules tailored to
specific scenarios. Among various perception paradigms, occupancy-based
representation has become widely recognized as particularly suitable for
humanoid robots, as it provides both rich semantic and 3D geometric information
essential for comprehensive environmental understanding. In this work, we
present Humanoid Occupancy, a generalized multimodal occupancy perception
system that integrates hardware and software components, data acquisition
devices, and a dedicated annotation pipeline. Our framework employs advanced
multi-modal fusion techniques to generate grid-based occupancy outputs encoding
both occupancy status and semantic labels, thereby enabling holistic
environmental understanding for downstream tasks such as task planning and
navigation. To address the unique challenges of humanoid robots, we overcome
issues such as kinematic interference and occlusion, and establish an effective
sensor layout strategy. Furthermore, we have developed the first panoramic
occupancy dataset specifically for humanoid robots, offering a valuable
benchmark and resource for future research and development in this domain. The
network architecture incorporates multi-modal feature fusion and temporal
information integration to ensure robust perception. Overall, Humanoid
Occupancy delivers effective environmental perception for humanoid robots and
establishes a technical foundation for standardizing universal visual modules,
paving the way for the widespread deployment of humanoid robots in complex
real-world scenarios.

</details>


### [26] [Tactile-Guided Robotic Ultrasound: Mapping Preplanned Scan Paths for Intercostal Imaging](https://arxiv.org/abs/2507.20282)
*Yifan Zhang,Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 提出了一种利用触觉信号生成超声扫描路径的方法，以解决肋间成像中扫描路径生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决机器人超声系统在肋间成像中因缺乏有效扫描路径生成方法而受限的问题。

Method: 利用触觉信号提取肋骨表面点云，并通过插值和配准生成扫描路径，同时引入自动倾斜角调整方法。

Result: 扫描路径映射的平均最近邻距离和Hausdorff距离误差分别为3.41 mm和3.65 mm，重建对象的误差为0.69 mm和2.2 mm。

Conclusion: 该方法在肋间超声成像中有效，提高了扫描路径的准确性和覆盖范围。

Abstract: Medical ultrasound (US) imaging is widely used in clinical examinations due
to its portability, real-time capability, and radiation-free nature. To address
inter- and intra-operator variability, robotic ultrasound systems have gained
increasing attention. However, their application in challenging intercostal
imaging remains limited due to the lack of an effective scan path generation
method within the constrained acoustic window. To overcome this challenge, we
explore the potential of tactile cues for characterizing subcutaneous rib
structures as an alternative signal for ultrasound segmentation-free bone
surface point cloud extraction. Compared to 2D US images, 1D tactile-related
signals offer higher processing efficiency and are less susceptible to acoustic
noise and artifacts. By leveraging robotic tracking data, a sparse tactile
point cloud is generated through a few scans along the rib, mimicking human
palpation. To robustly map the scanning trajectory into the intercostal space,
the sparse tactile bone location point cloud is first interpolated to form a
denser representation. This refined point cloud is then registered to an
image-based dense bone surface point cloud, enabling accurate scan path mapping
for individual patients. Additionally, to ensure full coverage of the object of
interest, we introduce an automated tilt angle adjustment method to visualize
structures beneath the bone. To validate the proposed method, we conducted
comprehensive experiments on four distinct phantoms. The final scanning
waypoint mapping achieved Mean Nearest Neighbor Distance (MNND) and Hausdorff
distance (HD) errors of 3.41 mm and 3.65 mm, respectively, while the
reconstructed object beneath the bone had errors of 0.69 mm and 2.2 mm compared
to the CT ground truth.

</details>


### [27] [Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance With Model Predictive Path Integral](https://arxiv.org/abs/2507.20293)
*Stepan Dergachev,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: 提出了一种结合MPPI和概率ORCA的新方法，用于多智能体导航，确保安全性和高效性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体导航中的不确定性和碰撞避免问题，适应运动约束和噪声。

Method: 将MPPI与概率ORCA结合，通过二阶锥规划引入概率安全约束。

Result: 在密集环境中表现优于ORCA-DD和B-UAVC，成功率高，Gazebo验证实用性强。

Conclusion: 该方法在多智能体导航中安全高效，适用于实际机器人平台。

Abstract: Decentralized multi-agent navigation under uncertainty is a complex task that
arises in numerous robotic applications. It requires collision avoidance
strategies that account for both kinematic constraints, sensing and action
execution noise. In this paper, we propose a novel approach that integrates the
Model Predictive Path Integral (MPPI) with a probabilistic adaptation of
Optimal Reciprocal Collision Avoidance. Our method ensures safe and efficient
multi-agent navigation by incorporating probabilistic safety constraints
directly into the MPPI sampling process via a Second-Order Cone Programming
formulation. This approach enables agents to operate independently using local
noisy observations while maintaining safety guarantees. We validate our
algorithm through extensive simulations with differential-drive robots and
benchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.
Results demonstrate that our approach outperforms them while achieving high
success rates, even in densely populated environments. Additionally, validation
in the Gazebo simulator confirms its practical applicability to robotic
platforms.

</details>


### [28] [Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/abs/2507.20370)
*Michele Grimaldi,Carlo Cernicchiaro,Sebastian Realpe Rua,Alaaeddine El-Masri-El-Chaarani,Markus Buchholz,Loizos Michael,Pere Ridao Rodriguez,Ignacio Carlucho,Yvan R. Petillot*

Main category: cs.RO

TL;DR: 论文探讨了在复杂水下环境中，通过知识图谱和检索增强生成（RAG）系统提升机器人自主性和人机协作能力，实现100%任务验证和行为完整性。


<details>
  <summary>Details</summary>
Motivation: 水下环境的复杂性和动态性（如有限能见度、不可预测的洋流和通信限制）对机器人自主性提出了高要求，同时需确保操作者的信任和监督。

Method: 采用知识图谱和RAG系统，结合大型语言模型（LLM），支持多机器人自主决策和人机交互。

Result: 实验表明，该方法实现了100%的任务验证和行为完整性，而缺乏结构化知识会导致LLM产生幻觉，影响决策质量。

Conclusion: 知识图谱和RAG系统是提升水下机器人自主性和人机协作的关键技术。

Abstract: Robotic platforms have become essential for marine operations by providing
regular and continuous access to offshore assets, such as underwater
infrastructure inspection, environmental monitoring, and resource exploration.
However, the complex and dynamic nature of underwater environments,
characterized by limited visibility, unpredictable currents, and communication
constraints, presents significant challenges that demand advanced autonomy
while ensuring operator trust and oversight. Central to addressing these
challenges are knowledge representation and reasoning techniques, particularly
knowledge graphs and retrieval-augmented generation (RAG) systems, that enable
robots to efficiently structure, retrieve, and interpret complex environmental
data. These capabilities empower robotic agents to reason, adapt, and respond
effectively to changing conditions. The primary goal of this work is to
demonstrate both multi-agent autonomy and shared autonomy, where multiple
robotic agents operate independently while remaining connected to a human
supervisor. We show how a RAG-powered large language model, augmented with
knowledge graph data and domain taxonomy, enables autonomous multi-agent
decision-making and facilitates seamless human-robot interaction, resulting in
100\% mission validation and behavior completeness. Finally, ablation studies
reveal that without structured knowledge from the graph and/or taxonomy, the
LLM is prone to hallucinations, which can compromise decision quality.

</details>


### [29] [Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning](https://arxiv.org/abs/2507.20382)
*Yuyou Zhang,Radu Corcodel,Ding Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于风险适应的分布强化学习框架，使四足机器人能够用后腿行走，释放前腿进行环境交互。


<details>
  <summary>Details</summary>
Motivation: 四足机器人的腿用作操纵器会影响运动能力，而加装手臂会增加系统复杂性。通过引入双足行走，释放前腿以实现多功能交互。

Method: 采用风险适应的分布强化学习框架，动态调整风险偏好，基于回报分布的不确定性（变异系数）。

Result: 仿真实验显示优于基线方法，实际部署展示了多功能性（如推车、探测障碍、运输负载）和鲁棒性。

Conclusion: 该方法有效解决了四足机器人双足行走的不稳定性问题，实现了多功能交互。

Abstract: Loco-manipulation of quadrupedal robots has broadened robotic applications,
but using legs as manipulators often compromises locomotion, while mounting
arms complicates the system. To mitigate this issue, we introduce bipedalism
for quadrupedal robots, thus freeing the front legs for versatile interactions
with the environment. We propose a risk-adaptive distributional Reinforcement
Learning (RL) framework designed for quadrupedal robots walking on their hind
legs, balancing worst-case conservativeness with optimal performance in this
inherently unstable task. During training, the adaptive risk preference is
dynamically adjusted based on the uncertainty of the return, measured by the
coefficient of variation of the estimated return distribution. Extensive
experiments in simulation show our method's superior performance over
baselines. Real-world deployment on a Unitree Go2 robot further demonstrates
the versatility of our policy, enabling tasks like cart pushing, obstacle
probing, and payload transport, while showcasing robustness against challenging
dynamics and external disturbances.

</details>


### [30] [Model-Structured Neural Networks to Control the Steering Dynamics of Autonomous Race Cars](https://arxiv.org/abs/2507.20427)
*Mattia Piccinini,Aniello Mungiello,Georg Jank,Gastone Pietro Rosati Papini,Francesco Biral,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种名为MS-NN-steer的新型模型结构神经网络，用于车辆转向控制，通过整合非线性车辆动力学的先验知识，提高了自动驾驶赛车运动规划的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车作为加速运动规划和控制方法开发的平台，其安全性和鲁棒性要求对决策算法有深入理解。然而，传统神经网络的“黑盒”特性难以满足这一需求。

Method: 提出MS-NN-steer，将非线性车辆动力学的先验知识融入神经网络架构中，并通过阿布扎比自动驾驶赛车联赛（A2RL）的真实数据进行验证。

Result: MS-NN-steer在小规模训练数据集上表现出更高的准确性和泛化能力，对权重初始化不敏感，且性能优于A2RL冠军团队的转向控制器。

Conclusion: MS-NN-steer为自动驾驶赛车提供了一种更安全、更可靠的转向控制解决方案，其开源实现有助于进一步研究和应用。

Abstract: Autonomous racing has gained increasing attention in recent years, as a safe
environment to accelerate the development of motion planning and control
methods for autonomous driving. Deep learning models, predominantly based on
neural networks (NNs), have demonstrated significant potential in modeling the
vehicle dynamics and in performing various tasks in autonomous driving.
However, their black-box nature is critical in the context of autonomous
racing, where safety and robustness demand a thorough understanding of the
decision-making algorithms. To address this challenge, this paper proposes
MS-NN-steer, a new Model-Structured Neural Network for vehicle steering
control, integrating the prior knowledge of the nonlinear vehicle dynamics into
the neural architecture. The proposed controller is validated using real-world
data from the Abu Dhabi Autonomous Racing League (A2RL) competition, with
full-scale autonomous race cars. In comparison with general-purpose NNs,
MS-NN-steer is shown to achieve better accuracy and generalization with small
training datasets, while being less sensitive to the weights' initialization.
Also, MS-NN-steer outperforms the steering controller used by the A2RL winning
team. Our implementation is available open-source in a GitHub repository.

</details>


### [31] [Learning Physical Interaction Skills from Human Demonstrations](https://arxiv.org/abs/2507.20445)
*Tianyu Li,Hengbo Ma,Sehoon Ha,Kwonjoon Lee*

Main category: cs.RO

TL;DR: 提出了一种名为BuddyImitation的框架，通过Embedded Interaction Graph（EIG）从人类演示中学习全身交互行为，适用于形态各异的智能体。


<details>
  <summary>Details</summary>
Motivation: 解决智能体在形态与演示者差异显著时学习物理交互技能的挑战，避免依赖手工目标或形态相似性。

Method: 提取交互动态的紧凑表示EIG，作为模仿目标训练控制策略，生成语义明确且物理可行的动作。

Result: 在多种智能体和交互场景（如格斗、握手、跳舞）中验证了框架的有效性。

Conclusion: 为形态差异显著的智能体间的协调行为提供了一条可行路径。

Abstract: Learning physical interaction skills, such as dancing, handshaking, or
sparring, remains a fundamental challenge for agents operating in human
environments, particularly when the agent's morphology differs significantly
from that of the demonstrator. Existing approaches often rely on handcrafted
objectives or morphological similarity, limiting their capacity for
generalization. Here, we introduce a framework that enables agents with diverse
embodiments to learn wholebbody interaction behaviors directly from human
demonstrations. The framework extracts a compact, transferable representation
of interaction dynamics, called the Embedded Interaction Graph (EIG), which
captures key spatiotemporal relationships between the interacting agents. This
graph is then used as an imitation objective to train control policies in
physics-based simulations, allowing the agent to generate motions that are both
semantically meaningful and physically feasible. We demonstrate BuddyImitation
on multiple agents, such as humans, quadrupedal robots with manipulators, or
mobile manipulators and various interaction scenarios, including sparring,
handshaking, rock-paper-scissors, or dancing. Our results demonstrate a
promising path toward coordinated behaviors across morphologically distinct
characters via cross embodiment interaction learning.

</details>


### [32] [Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping](https://arxiv.org/abs/2507.20516)
*Xiaofeng Jin,Ningbo Bu,Shijie Wang,Jianfei Ge,Jiangjian Xiao,Matteo Matteucci*

Main category: cs.RO

TL;DR: 该论文介绍了一个大规模、高精度的LiDAR-惯性里程计（LIO）数据集，旨在解决现有研究中LIO系统在复杂现实场景中验证不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LIO系统在复杂现实场景中的验证不足，因此需要提供更全面的数据集来评估其性能。

Method: 使用定制背包平台，配备多光束LiDAR、工业级IMU和RTK-GNSS模块，在四种不同环境中收集数据，并通过SLAM优化与RTK-GNSS锚定生成高精度地面真实数据。

Result: 数据集覆盖60,000至750,000平方米的复杂场景，提供长轨迹和高精度地面真实数据，验证了轨迹精度。

Conclusion: 该数据集为评估LIO系统在实际高精度测绘场景中的泛化能力提供了全面基准。

Abstract: This paper introduces a large-scale, high-precision LiDAR-Inertial Odometry
(LIO) dataset, aiming to address the insufficient validation of LIO systems in
complex real-world scenarios in existing research. The dataset covers four
diverse real-world environments spanning 60,000 to 750,000 square meters,
collected using a custom backpack-mounted platform equipped with multi-beam
LiDAR, an industrial-grade IMU, and RTK-GNSS modules. The dataset includes long
trajectories, complex scenes, and high-precision ground truth, generated by
fusing SLAM-based optimization with RTK-GNSS anchoring, and validated for
trajectory accuracy through the integration of oblique photogrammetry and
RTK-GNSS. This dataset provides a comprehensive benchmark for evaluating the
generalization ability of LIO systems in practical high-precision mapping
scenarios.

</details>


### [33] [Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments](https://arxiv.org/abs/2507.20538)
*Gilhwan Kang,Hogyun Kim,Byunghee Choi,Seokhwan Jeong,Young-Sik Shin,Younggun Cho*

Main category: cs.RO

TL;DR: Uni-Mapper是一个动态感知的3D点云地图合并框架，用于多模态LiDAR系统，解决了动态环境和传感器差异带来的地图统一问题。


<details>
  <summary>Details</summary>
Motivation: 统一不同地图对于多会话和多机器人协作至关重要，但动态环境和传感器差异导致点云分布和场景一致性差异，阻碍了可靠的地图对齐。

Method: Uni-Mapper包括动态物体移除、动态感知闭环和多模态LiDAR地图合并模块，采用体素自由空间哈希图和全局描述符，并通过集中锚节点策略优化位姿图。

Result: 在真实动态环境和异构LiDAR数据集上，Uni-Mapper在闭环检测、动态环境建图和多地图对齐方面优于现有方法。

Conclusion: Uni-Mapper为动态环境和多模态LiDAR系统的地图统一提供了高效解决方案。

Abstract: The unification of disparate maps is crucial for enabling scalable robot
operation across multiple sessions and collaborative multi-robot scenarios.
However, achieving a unified map robust to sensor modalities and dynamic
environments remains a challenging problem. Variations in LiDAR types and
dynamic elements lead to differences in point cloud distribution and scene
consistency, hindering reliable descriptor generation and loop closure
detection essential for accurate map alignment. To address these challenges,
this paper presents Uni-Mapper, a dynamic-aware 3D point cloud map merging
framework for multi-modal LiDAR systems. It comprises dynamic object removal,
dynamic-aware loop closure, and multi-modal LiDAR map merging modules. A
voxel-wise free space hash map is built in a coarse-to-fine manner to identify
and reject dynamic objects via temporal occupancy inconsistencies. The removal
module is integrated with a LiDAR global descriptor, which encodes preserved
static local features to ensure robust place recognition in dynamic
environments. In the final stage, multiple pose graph optimizations are
conducted for both intra-session and inter-map loop closures. We adopt a
centralized anchor-node strategy to mitigate intra-session drift errors during
map merging. In the final stage, centralized anchor-node-based pose graph
optimization is performed to address intra- and inter-map loop closures for
globally consistent map merging. Our framework is evaluated on diverse
real-world datasets with dynamic objects and heterogeneous LiDARs, showing
superior performance in loop detection across sensor modalities, robust mapping
in dynamic environments, and accurate multi-map alignment over existing
methods. Project Page: https://sparolab.github.io/research/uni_mapper.

</details>


### [34] [Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation](https://arxiv.org/abs/2507.20589)
*Francisco J. Soler Mora,Adrián Peidró Vidal,Marc Fabregat-Jaén,Luis Payá Castelló,Óscar Reinoso García*

Main category: cs.RO

TL;DR: 论文提出两种方法（分析算法和深度学习模型）用于桁架结构中可导航表面的分割，比较了它们的性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 桁架结构的检查和维护成本高且危险，现有研究多关注故障检测或机器人设计，而自主导航研究较少。

Method: 采用分析算法和深度学习模型（如PointNet、PointTransformerV3）对3D点云进行可导航表面分割。

Result: 分析算法参数调整简单且性能接近深度学习模型，而深度学习模型（如PointTransformerV3）在分割精度上更优（mIoU达97%）。

Conclusion: 研究表明两种方法均能提升桁架环境中的自主导航能力，为未来研究和实际应用提供了权衡指导。

Abstract: Reticular structures form the backbone of major infrastructure like bridges,
pylons, and airports, but their inspection and maintenance are costly and
hazardous, often requiring human intervention. While prior research has focused
on fault detection via images or robotic platform design, the autonomous
navigation of robots within these structures is less explored. This study
addresses that gap by proposing methods to detect navigable surfaces in truss
structures, enhancing the autonomy of climbing robots. The paper introduces
several approaches for binary segmentation of navigable surfaces versus
background from 3D point clouds of metallic trusses. These methods fall into
two categories: analytical algorithms and deep learning models. The analytical
approach features a custom algorithm that segments structures by analyzing the
eigendecomposition of planar patches in the point cloud. In parallel, advanced
deep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3
are trained and evaluated for the same task. Comparative analysis shows that
the analytical algorithm offers easier parameter tuning and performance
comparable to deep learning models, which, while more computationally
intensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves
a Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the
promise of both analytical and deep learning methods for improving autonomous
navigation in complex truss environments. The results highlight the trade-offs
between computational efficiency and segmentation performance, providing
valuable guidance for future research and practical applications in autonomous
infrastructure inspection and maintenance.

</details>


### [35] [FMimic: Foundation Models are Fine-grained Action Learners from Human Videos](https://arxiv.org/abs/2507.20622)
*Guangyan Chen,Meiling Wang,Te Cui,Yao Mu,Haoyang Lu,Zicai Peng,Mengxiao Hu,Tianxing Zhou,Mengyin Fu,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: FMimic利用基础模型直接从少量人类视频中学习可泛化的精细动作技能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义动作基元执行物理交互，限制了机器人系统的灵活性。

Method: 提出FMimic，利用基础模型直接从人类视频中学习精细动作技能。

Result: FMimic在单视频和五视频条件下均表现优异，在RLBench多任务和真实世界任务中分别提升39%和29%，在高精度和长时任务中分别超过基线34%和47%。

Conclusion: FMimic为视觉模仿学习提供了一种高效且泛化能力强的解决方案。

Abstract: Visual imitation learning (VIL) provides an efficient and intuitive strategy
for robotic systems to acquire novel skills. Recent advancements in foundation
models, particularly Vision Language Models (VLMs), have demonstrated
remarkable capabilities in visual and linguistic reasoning for VIL tasks.
Despite this progress, existing approaches primarily utilize these models for
learning high-level plans from human demonstrations, relying on pre-defined
motion primitives for executing physical interactions, which remains a major
bottleneck for robotic systems. In this work, we present FMimic, a novel
paradigm that harnesses foundation models to directly learn generalizable
skills at even fine-grained action levels, using only a limited number of human
videos. Extensive experiments demonstrate that our FMimic delivers strong
performance with a single human video, and significantly outperforms all other
methods with five videos. Furthermore, our method exhibits significant
improvements of over 39% and 29% in RLBench multi-task experiments and
real-world manipulation tasks, respectively, and exceeds baselines by more than
34% in high-precision tasks and 47% in long-horizon tasks.

</details>


### [36] [A Strawberry Harvesting Tool with Minimal Footprint](https://arxiv.org/abs/2507.20784)
*Mohamed Sorour,Mohamed Heshmat,Khaled Elgeneidy,Pål Johan From*

Main category: cs.RO

TL;DR: 提出了一种新型草莓采摘原型，通过激光切割茎部，提高采摘效率并延长果实保鲜期。


<details>
  <summary>Details</summary>
Motivation: 解决传统草莓采摘方法效率低、易传播病害的问题。

Method: 使用平滑捕捉器将茎部引导至精确位置，通过高温激光切割茎部，杀菌并防止病害传播。

Result: 实验显示切割时间为2.88秒，采摘周期为5.56秒，有效延长了草莓的保鲜期。

Conclusion: 该方法高效、卫生，适用于室内草莓采摘，具有实际应用潜力。

Abstract: In this paper, a novel prototype for harvesting table-top grown strawberries
is presented, that is minimalist in its footprint interacting with the fruit.
In our methodology, a smooth trapper manipulates the stem into a precise groove
location at which a distant laser beam is focused. The tool reaches
temperatures as high as 188{\deg} Celsius and as such killing germs and
preventing the spread of local plant diseases. The burnt stem wound preserves
water content and in turn the fruit shelf life. Cycle and cut times achieved
are 5.56 and 2.88 seconds respectively in successful in-door harvesting
demonstration. Extensive experiments are performed to optimize the laser spot
diameter and lateral speed against the cutting time.

</details>


### [37] [LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations](https://arxiv.org/abs/2507.20800)
*Vinil Polepalli*

Main category: cs.RO

TL;DR: LanternNet是一种新型自主机器人系统，用于检测和抑制斑点灯笼蝇（SLF），相比传统方法更高效、经济且环保。


<details>
  <summary>Details</summary>
Motivation: 斑点灯笼蝇对农业和生态系统造成严重威胁，现有控制方法效率低且环境危害大。

Method: LanternNet采用中心-辐条式机器人系统，利用YOLOv8模型识别SLF，并通过三种机器人执行灭虫、环境监测和导航任务。

Result: 实地测试显示SLF数量显著减少（p < 0.01），树木健康指标改善，且成本更低、扩展性更强。

Conclusion: LanternNet展示了机器人与AI结合在入侵物种管理中的潜力，具有广泛生态应用前景。

Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to
agriculture and ecosystems, causing widespread damage. Current control methods,
such as egg scraping, pesticides, and quarantines, prove labor-intensive,
environmentally hazardous, and inadequate for long-term SLF suppression. This
research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system
designed for scalable detection and suppression of SLF populations. A central,
tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF
identification. Three specialized robotic spokes perform targeted tasks: pest
neutralization, environmental monitoring, and navigation/mapping. Field
deployment across multiple infested sites over 5 weeks demonstrated
LanternNet's efficacy. Quantitative analysis revealed significant reductions (p
< 0.01, paired t-tests) in SLF populations and corresponding improvements in
tree health indicators across the majority of test sites. Compared to
conventional methods, LanternNet offers substantial cost advantages and
improved scalability. Furthermore, the system's adaptability for enhanced
autonomy and targeting of other invasive species presents significant potential
for broader ecological impact. LanternNet demonstrates the transformative
potential of integrating robotics and AI for advanced invasive species
management and improved environmental outcomes.

</details>


### [38] [Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics](https://arxiv.org/abs/2507.20832)
*Mihai Pomarlan,Stefano De Giorgis,Rachel Ringe,Maria M. Hedblom,Nikolaos Tsiogkas*

Main category: cs.RO

TL;DR: 论文提出了一种神经符号模块化架构，用于反应式机器人，结合神经组件和符号推理，使机器人能够识别环境元素并自主扩展知识。


<details>
  <summary>Details</summary>
Motivation: 解决人工代理在自然环境中操作时面临的挑战，如空间感知、对象功能检测和动态变化，特别是如何识别和监控与目标相关的环境元素。

Method: 结合神经网络（对象识别和图像处理）与符号表示和推理，通过本体结构整合图像模式知识，用于感知查询、动作决策和推断实体能力。

Result: 在模拟环境中，代理能够学习识别对象部分（如支撑关系中的部分），并自主扩展知识，例如识别把手并规划支撑关系的建立/破坏。

Conclusion: 展示了神经符号结合方法的潜力，使代理能够通过观察系统性地扩展知识，为复杂任务规划提供支持。

Abstract: Situationally-aware artificial agents operating with competence in natural
environments face several challenges: spatial awareness, object affordance
detection, dynamic changes and unpredictability. A critical challenge is the
agent's ability to identify and monitor environmental elements pertinent to its
objectives. Our research introduces a neurosymbolic modular architecture for
reactive robotics. Our system combines a neural component performing object
recognition over the environment and image processing techniques such as
optical flow, with symbolic representation and reasoning. The reasoning system
is grounded in the embodied cognition paradigm, via integrating image schematic
knowledge in an ontological structure. The ontology is operatively used to
create queries for the perception system, decide on actions, and infer
entities' capabilities derived from perceptual data. The combination of
reasoning and image processing allows the agent to focus its perception for
normal operation as well as discover new concepts for parts of objects involved
in particular interactions. The discovered concepts allow the robot to
autonomously acquire training data and adjust its subsymbolic perception to
recognize the parts, as well as making planning for more complex tasks feasible
by focusing search on those relevant object parts. We demonstrate our approach
in a simulated world, in which an agent learns to recognize parts of objects
involved in support relations. While the agent has no concept of handle
initially, by observing examples of supported objects hanging from a hook it
learns to recognize the parts involved in establishing support and becomes able
to plan the establishment/destruction of the support relation. This underscores
the agent's capability to expand its knowledge through observation in a
systematic way, and illustrates the potential of combining deep reasoning
[...].

</details>


### [39] [Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments](https://arxiv.org/abs/2507.20850)
*Meiting Dang,Yanping Wu,Yafei Wang,Dezong Zhao,David Flynn,Chongfeng Wei*

Main category: cs.RO

TL;DR: 提出了一种新框架，结合认知过程建模和风险感知，改进自动驾驶车辆与行人的交互。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在复杂多代理环境中与弱势道路使用者（如行人）交互时预测和决策的挑战。

Method: 采用基于自由能原理的认知过程建模，结合认知风险社会力模型和图卷积网络，优化交互动态。

Result: 仿真结果表明，框架在安全性、效率和导航流畅性上优于现有方法。

Conclusion: 新框架为自动驾驶车辆与行人的交互提供了更真实和高效的解决方案。

Abstract: Recent advances in autonomous vehicle (AV) behavior planning have shown
impressive social interaction capabilities when interacting with other road
users. However, achieving human-like prediction and decision-making in
interactions with vulnerable road users remains a key challenge in complex
multi-agent interactive environments. Existing research focuses primarily on
crowd navigation for small mobile robots, which cannot be directly applied to
AVs due to inherent differences in their decision-making strategies and dynamic
boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed
behavior patterns that cannot dynamically respond to AV actions. To overcome
these limitations, this paper proposes a novel framework for modeling
interactions between the AV and multiple pedestrians. In this framework, a
cognitive process modeling approach inspired by the Free Energy Principle is
integrated into both the AV and pedestrian models to simulate more realistic
interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk
Social Force Model adjusts goal-directed and repulsive forces using a fused
measure of cognitive uncertainty and physical risk to produce human-like
trajectories. Meanwhile, the AV leverages this fused risk to construct a
dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a
Soft Actor-Critic architecture, allowing it to make more reasonable and
informed decisions. Simulation results indicate that our proposed framework
effectively improves safety, efficiency, and smoothness of AV navigation
compared to the state-of-the-art method.

</details>


### [40] [Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling](https://arxiv.org/abs/2507.20861)
*Marco Faroni,Carlo Odesco,Andrea Zanchettin,Paolo Rocco*

Main category: cs.RO

TL;DR: 论文提出了一种基于不确定性感知的蒙特卡洛树搜索算法，用于提升复杂机器人任务（如液体倾倒）的可靠性。


<details>
  <summary>Details</summary>
Motivation: 物理仿真和学习模型在复杂机器人任务中常因认知不确定性或仿真与现实的差距而表现不佳，尤其是在数据有限的情况下。

Method: 通过结合模型不确定性估计，改进蒙特卡洛树搜索（MCTS），使其偏向预测不确定性较低的动作。

Result: 在液体倾倒任务中，该方法在数据有限的情况下仍表现出更高的成功率，优于传统方法。

Conclusion: 该方法为机器人决策提供了更鲁棒的解决方案，尤其在不确定性高的任务中。

Abstract: Physics-based simulations and learning-based models are vital for complex
robotics tasks like deformable object manipulation and liquid handling.
However, these models often struggle with accuracy due to epistemic uncertainty
or the sim-to-real gap. For instance, accurately pouring liquid from one
container to another poses challenges, particularly when models are trained on
limited demonstrations and may perform poorly in novel situations. This paper
proposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed
to mitigate these inaccuracies. By incorporating estimates of model
uncertainty, the proposed MCTS strategy biases the search towards actions with
lower predicted uncertainty. This approach enhances the reliability of planning
under uncertain conditions. Applied to a liquid pouring task, our method
demonstrates improved success rates even with models trained on minimal data,
outperforming traditional methods and showcasing its potential for robust
decision-making in robotics.

</details>


### [41] [A Human-in-the-loop Approach to Robot Action Replanning through LLM Common-Sense Reasoning](https://arxiv.org/abs/2507.20870)
*Elena Merlo,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 论文提出了一种结合自然语言输入和大语言模型（LLM）的方法，用于改进基于单次RGB视频生成的机器人执行计划，以提高其适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了推广机器人技术的应用，需要为非专家提供更易用的编程工具。单纯依赖视觉输入的观察学习在可扩展性和错误缓解方面效率不足。

Method: 通过人类参与的闭环方法，利用自然语言输入和LLM的常识推理，调整基于视觉生成的计划，避免潜在错误并根据指令适应任务。

Result: 实验证明该方法能直观有效地纠正视觉生成的错误并调整计划，无需额外演示。交互式计划优化和幻觉修正提升了系统鲁棒性。

Conclusion: 该方法为非专家提供了一种直观且高效的机器人编程工具，通过结合自然语言和LLM增强了系统的适应性和鲁棒性。

Abstract: To facilitate the wider adoption of robotics, accessible programming tools
are required for non-experts. Observational learning enables intuitive human
skills transfer through hands-on demonstrations, but relying solely on visual
input can be inefficient in terms of scalability and failure mitigation,
especially when based on a single demonstration. This paper presents a
human-in-the-loop method for enhancing the robot execution plan, automatically
generated based on a single RGB video, with natural language input to a Large
Language Model (LLM). By including user-specified goals or critical task
aspects and exploiting the LLM common-sense reasoning, the system adjusts the
vision-based plan to prevent potential failures and adapts it based on the
received instructions. Experiments demonstrated the framework intuitiveness and
effectiveness in correcting vision-derived errors and adapting plans without
requiring additional demonstrations. Moreover, interactive plan refinement and
hallucination corrections promoted system robustness.

</details>


### [42] [PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs](https://arxiv.org/abs/2507.20892)
*Sergey Bakulin,Timur Akhtyamov,Denis Fatykhov,German Devchich,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: 提出了一种结合深度学习和经典模型规划算法的混合方法，用于移动机器人的视觉导航，解决了纯数据驱动方法的训练数据需求和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的端到端模型虽然灵活，但需要大量训练数据且可解释性差，限制了实际应用。

Method: 采用分层系统，结合模型预测控制、可通行性估计、视觉地点识别和位姿估计，使用拓扑图表示目标环境。

Result: 实验证明该方法高效且比端到端方法更具可解释性。

Conclusion: 提出的混合方法在视觉导航中具有实际应用潜力。

Abstract: This work proposes a novel hybrid approach for vision-only navigation of
mobile robots, which combines advances of both deep learning approaches and
classical model-based planning algorithms. Today, purely data-driven end-to-end
models are dominant solutions to this problem. Despite advantages such as
flexibility and adaptability, the requirement of a large amount of training
data and limited interpretability are the main bottlenecks for their practical
applications. To address these limitations, we propose a hierarchical system
that utilizes recent advances in model predictive control, traversability
estimation, visual place recognition, and pose estimation, employing
topological graphs as a representation of the target environment. Using such a
combination, we provide a scalable system with a higher level of
interpretability compared to end-to-end approaches. Extensive real-world
experiments show the efficiency of the proposed method.

</details>
