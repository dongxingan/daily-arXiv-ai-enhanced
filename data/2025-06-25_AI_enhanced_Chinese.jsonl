{"id": "2506.18960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18960", "abs": "https://arxiv.org/abs/2506.18960", "authors": ["Siqi Shang", "Mingyo Seo", "Yuke Zhu", "Lilly Chin"], "title": "FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation", "comment": null, "summary": "Handling delicate and fragile objects remains a major challenge for robotic\nmanipulation, especially for rigid parallel grippers. While the simplicity and\nversatility of parallel grippers have led to widespread adoption, these\ngrippers are limited by their heavy reliance on visual feedback. Tactile\nsensing and soft robotics can add responsiveness and compliance. However,\nexisting methods typically involve high integration complexity or suffer from\nslow response times. In this work, we introduce FORTE, a tactile sensing system\nembedded in compliant gripper fingers. FORTE uses 3D-printed fin-ray grippers\nwith internal air channels to provide low-latency force and slip feedback.\nFORTE applies just enough force to grasp objects without damaging them, while\nremaining easy to fabricate and integrate. We find that FORTE can accurately\nestimate grasping forces from 0-8 N with an average error of 0.2 N, and detect\nslip events within 100 ms of occurring. We demonstrate FORTE's ability to grasp\na wide range of slippery, fragile, and deformable objects. In particular, FORTE\ngrasps fragile objects like raspberries and potato chips with a 98.6% success\nrate, and achieves 93% accuracy in detecting slip events. These results\nhighlight FORTE's potential as a robust and practical solution for enabling\ndelicate robotic manipulation. Project page: https://merge-lab.github.io/FORTE", "AI": {"tldr": "FORTE是一种嵌入柔性夹爪的触觉传感系统，通过3D打印的鳍射线夹爪和内部空气通道提供低延迟的力和滑动反馈，实现了对脆弱物体的高成功率抓取。", "motivation": "刚性平行夹爪在处理脆弱物体时依赖视觉反馈，缺乏响应性和适应性。触觉传感和软机器人技术可以解决这一问题，但现有方法集成复杂或响应慢。", "method": "FORTE采用3D打印的鳍射线夹爪，内部设计空气通道，提供低延迟的力和滑动反馈，确保抓取时施加的力适中且不损坏物体。", "result": "FORTE能准确估计0-8N的抓取力（平均误差0.2N），并在100ms内检测滑动事件，抓取脆弱物体（如覆盆子和薯片）成功率达98.6%，滑动检测准确率为93%。", "conclusion": "FORTE是一种实用且高效的解决方案，适用于脆弱物体的机器人操作。"}}
{"id": "2506.19016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19016", "abs": "https://arxiv.org/abs/2506.19016", "authors": ["Nancy Amato", "Stav Ashur", "Sariel Har-Peled%"], "title": "Faster Motion Planning via Restarts", "comment": "arXiv admin note: text overlap with arXiv:2503.04633", "summary": "Randomized methods such as PRM and RRT are widely used in motion planning.\nHowever, in some cases, their running-time suffers from inherent instability,\nleading to ``catastrophic'' performance even for relatively simple instances.\nWe apply stochastic restart techniques, some of them new, for speeding up Las\nVegas algorithms, that provide dramatic speedups in practice (a factor of $3$\n[or larger] in many cases).\n  Our experiments demonstrate that the new algorithms have faster runtimes,\nshorter paths, and greater gains from multi-threading (when compared with\nstraightforward parallel implementation). We prove the optimality of the new\nvariants. Our implementation is open source, available on github, and is easy\nto deploy and use.", "AI": {"tldr": "论文提出通过随机重启技术改进PRM和RRT等随机运动规划算法，显著提升运行效率、路径质量和多线程性能。", "motivation": "随机运动规划算法（如PRM和RRT）在某些情况下存在性能不稳定的问题，导致简单实例也可能表现不佳。", "method": "应用随机重启技术（包括一些新技术）来加速Las Vegas算法。", "result": "实验显示新算法运行更快、路径更短，多线程性能提升显著（比直接并行实现更优）。", "conclusion": "新算法在理论和实践中均表现优异，且开源实现易于部署和使用。"}}
{"id": "2506.19077", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19077", "abs": "https://arxiv.org/abs/2506.19077", "authors": ["Christoph Willibald", "Daniel Sliwowski", "Dongheui Lee"], "title": "Multimodal Anomaly Detection with a Mixture-of-Experts", "comment": "8 pages, 5 figures, 1 table, the paper has been accepted for\n  publication in the Proceedings of the 2025 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS 2025)", "summary": "With a growing number of robots being deployed across diverse applications,\nrobust multimodal anomaly detection becomes increasingly important. In robotic\nmanipulation, failures typically arise from (1) robot-driven anomalies due to\nan insufficient task model or hardware limitations, and (2) environment-driven\nanomalies caused by dynamic environmental changes or external interferences.\nConventional anomaly detection methods focus either on the first by low-level\nstatistical modeling of proprioceptive signals or the second by deep\nlearning-based visual environment observation, each with different\ncomputational and training data requirements. To effectively capture anomalies\nfrom both sources, we propose a mixture-of-experts framework that integrates\nthe complementary detection mechanisms with a visual-language model for\nenvironment monitoring and a Gaussian-mixture regression-based detector for\ntracking deviations in interaction forces and robot motions. We introduce a\nconfidence-based fusion mechanism that dynamically selects the most reliable\ndetector for each situation. We evaluate our approach on both household and\nindustrial tasks using two robotic systems, demonstrating a 60% reduction in\ndetection delay while improving frame-wise anomaly detection performance\ncompared to individual detectors.", "AI": {"tldr": "提出了一种混合专家框架，结合视觉语言模型和高斯混合回归检测器，用于机器人操作中的多模态异常检测，显著减少检测延迟并提高性能。", "motivation": "随着机器人应用的多样化，多模态异常检测的需求增加。传统方法仅针对机器人或环境驱动的异常，无法同时处理两者。", "method": "采用混合专家框架，整合视觉语言模型（环境监测）和高斯混合回归检测器（交互力与运动偏差跟踪），并通过置信度融合机制动态选择最可靠的检测器。", "result": "在家庭和工业任务中测试，检测延迟减少60%，帧级异常检测性能优于单一检测器。", "conclusion": "提出的框架有效整合了互补检测机制，显著提升了多模态异常检测的性能和效率。"}}
{"id": "2506.19112", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2506.19112", "abs": "https://arxiv.org/abs/2506.19112", "authors": ["Rom Levy", "Ari Dantus", "Zitao Yu", "Yizhar Or"], "title": "Analysis and experiments of the dissipative Twistcar: direction reversal and asymptotic approximations", "comment": null, "summary": "Underactuated wheeled vehicles are commonly studied as nonholonomic systems\nwith periodic actuation. Twistcar is a classical example inspired by a riding\ntoy, which has been analyzed using a planar model of a dynamical system with\nnonholonomic constraints. Most of the previous analyses did not account for\nenergy dissipation due to friction. In this work, we study a theoretical\ntwo-link model of the Twistcar while incorporating dissipation due to rolling\nresistance. We obtain asymptotic expressions for the system's small-amplitude\nsteady-state periodic dynamics, which reveals the possibility of reversing the\ndirection of motion upon varying the geometric and mass properties of the\nvehicle. Next, we design and construct a robotic prototype of the Twistcar\nwhose center-of-mass position can be shifted by adding and removing a massive\nblock, enabling demonstration of the Twistcar's direction reversal phenomenon.\nWe also conduct parameter fitting for the frictional resistance in order to\nimprove agreement with experiments.", "AI": {"tldr": "研究了一个考虑滚动摩擦的双连杆Twistcar模型，发现通过改变几何和质量属性可以反转运动方向，并设计了一个机器人原型验证这一现象。", "motivation": "以往对Twistcar的研究未考虑摩擦能量耗散，本研究填补了这一空白，并探索了运动方向反转的可能性。", "method": "建立了一个理论双连杆模型，结合滚动摩擦，分析了小振幅稳态周期动力学，并设计了一个可调节质心的机器人原型。", "result": "发现通过改变几何和质量属性可以反转运动方向，并通过实验验证了这一现象。", "conclusion": "研究揭示了摩擦对Twistcar动力学的重要影响，并展示了运动方向反转的实际可行性。"}}
{"id": "2506.19277", "categories": ["cs.RO", "cs.SY", "eess.SY", "68T40, 93C41", "I.2.9; I.2.8; F.2.2"], "pdf": "https://arxiv.org/pdf/2506.19277", "abs": "https://arxiv.org/abs/2506.19277", "authors": ["Jaehong Oh"], "title": "Ontology Neural Network and ORTSF: A Framework for Topological Reasoning and Delay-Robust Control", "comment": "12 pages, 5 figures, includes theoretical proofs and simulation\n  results", "summary": "The advancement of autonomous robotic systems has led to impressive\ncapabilities in perception, localization, mapping, and control. Yet, a\nfundamental gap remains: existing frameworks excel at geometric reasoning and\ndynamic stability but fall short in representing and preserving relational\nsemantics, contextual reasoning, and cognitive transparency essential for\ncollaboration in dynamic, human-centric environments. This paper introduces a\nunified architecture comprising the Ontology Neural Network (ONN) and the\nOntological Real-Time Semantic Fabric (ORTSF) to address this gap. The ONN\nformalizes relational semantic reasoning as a dynamic topological process. By\nembedding Forman-Ricci curvature, persistent homology, and semantic tensor\nstructures within a unified loss formulation, ONN ensures that relational\nintegrity and topological coherence are preserved as scenes evolve over time.\nThe ORTSF transforms reasoning traces into actionable control commands while\ncompensating for system delays. It integrates predictive and delay-aware\noperators that ensure phase margin preservation and continuity of control\nsignals, even under significant latency conditions. Empirical studies\ndemonstrate the ONN + ORTSF framework's ability to unify semantic cognition and\nrobust control, providing a mathematically principled and practically viable\nsolution for cognitive robotics.", "AI": {"tldr": "论文提出了一种结合ONN和ORTSF的统一架构，用于解决自主机器人系统中语义推理和控制的不足。", "motivation": "现有框架在几何推理和动态稳定性方面表现优异，但在关系语义表示、上下文推理和认知透明度方面存在不足，特别是在动态、以人为中心的环境中。", "method": "通过ONN（Ontology Neural Network）将关系语义推理形式化为动态拓扑过程，并利用ORTSF（Ontological Real-Time Semantic Fabric）将推理痕迹转化为可操作的控制命令。", "result": "实验证明，ONN + ORTSF框架能够统一语义认知和鲁棒控制，为认知机器人提供数学上严谨且实际可行的解决方案。", "conclusion": "该框架填补了现有技术的空白，为动态、人机协作环境中的机器人系统提供了更全面的能力。"}}
{"id": "2506.19121", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.6; I.2.9"], "pdf": "https://arxiv.org/pdf/2506.19121", "abs": "https://arxiv.org/abs/2506.19121", "authors": ["Christopher Agia", "Rohan Sinha", "Jingyun Yang", "Rika Antonova", "Marco Pavone", "Haruki Nishimura", "Masha Itkina", "Jeannette Bohg"], "title": "CUPID: Curating Data your Robot Loves with Influence Functions", "comment": "Project page: https://cupid-curation.github.io. 28 pages, 15 figures", "summary": "In robot imitation learning, policy performance is tightly coupled with the\nquality and composition of the demonstration data. Yet, developing a precise\nunderstanding of how individual demonstrations contribute to downstream\noutcomes - such as closed-loop task success or failure - remains a persistent\nchallenge. We propose CUPID, a robot data curation method based on a novel\ninfluence function-theoretic formulation for imitation learning policies. Given\na set of evaluation rollouts, CUPID estimates the influence of each training\ndemonstration on the policy's expected return. This enables ranking and\nselection of demonstrations according to their impact on the policy's\nclosed-loop performance. We use CUPID to curate data by 1) filtering out\ntraining demonstrations that harm policy performance and 2) subselecting newly\ncollected trajectories that will most improve the policy. Extensive simulated\nand hardware experiments show that our approach consistently identifies which\ndata drives test-time performance. For example, training with less than 33% of\ncurated data can yield state-of-the-art diffusion policies on the simulated\nRoboMimic benchmark, with similar gains observed in hardware. Furthermore,\nhardware experiments show that our method can identify robust strategies under\ndistribution shift, isolate spurious correlations, and even enhance the\npost-training of generalist robot policies. Additional materials are made\navailable at: https://cupid-curation.github.io.", "AI": {"tldr": "CUPID是一种基于影响函数理论的机器人数据筛选方法，用于优化模仿学习策略的性能。", "motivation": "模仿学习策略的性能与演示数据的质量和组成密切相关，但理解单个演示对最终任务成功的影响仍具挑战性。", "method": "CUPID通过估计每个训练演示对策略预期回报的影响，筛选出对性能有害的数据并选择能提升策略的新轨迹。", "result": "实验表明，CUPID能显著提升策略性能，例如仅用33%的筛选数据即可达到最佳性能，并在硬件实验中验证了其鲁棒性。", "conclusion": "CUPID是一种高效的数据筛选方法，能显著提升模仿学习策略的性能和鲁棒性。"}}
{"id": "2506.19350", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19350", "abs": "https://arxiv.org/abs/2506.19350", "authors": ["Carsten Reiners", "Minh Trinh", "Lukas Gründel", "Sven Tauchmann", "David Bitterolf", "Oliver Petrovic", "Christian Brecher"], "title": "Zero-Shot Parameter Learning of Robot Dynamics Using Bayesian Statistics and Prior Knowledge", "comment": "Carsten Reiners and Minh Trinh contributed equally to this work", "summary": "Inertial parameter identification of industrial robots is an established\nprocess, but standard methods using Least Squares or Machine Learning do not\nconsider prior information about the robot and require extensive measurements.\nInspired by Bayesian statistics, this paper presents an identification method\nwith improved generalization that incorporates prior knowledge and is able to\nlearn with only a few or without additional measurements (Zero-Shot Learning).\nFurthermore, our method is able to correctly learn not only the inertial but\nalso the mechanical and base parameters of the MABI Max 100 robot while\nensuring physical feasibility and specifying the confidence intervals of the\nresults. We also provide different types of priors for serial robots with 6\ndegrees of freedom, where datasheets or CAD models are not available.", "AI": {"tldr": "提出了一种基于贝叶斯统计的机器人惯性参数识别方法，结合先验知识，减少测量需求，实现零样本学习。", "motivation": "传统最小二乘法或机器学习方法未利用机器人先验信息且需大量测量，需改进。", "method": "采用贝叶斯方法，结合先验知识，支持少量或无测量学习，确保物理可行性并提供置信区间。", "result": "成功识别MABI Max 100机器人的惯性、机械和基础参数，适用于6自由度串联机器人。", "conclusion": "该方法显著减少测量需求，提升泛化能力，适用于缺乏数据表或CAD模型的场景。"}}
{"id": "2506.19179", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19179", "abs": "https://arxiv.org/abs/2506.19179", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Situated Haptic Interaction: Exploring the Role of Context in Affective Perception of Robotic Touch", "comment": null, "summary": "Affective interaction is not merely about recognizing emotions; it is an\nembodied, situated process shaped by context and co-created through\ninteraction. In affective computing, the role of haptic feedback within dynamic\nemotional exchanges remains underexplored. This study investigates how\nsituational emotional cues influence the perception and interpretation of\nhaptic signals given by a robot. In a controlled experiment, 32 participants\nwatched video scenarios in which a robot experienced either positive actions\n(such as being kissed), negative actions (such as being slapped) or neutral\nactions. After each video, the robot conveyed its emotional response through\nhaptic communication, delivered via a wearable vibration sleeve worn by the\nparticipant. Participants rated the robot's emotional state-its valence\n(positive or negative) and arousal (intensity)-based on the video, the haptic\nfeedback, and the combination of the two. The study reveals a dynamic interplay\nbetween visual context and touch. Participants' interpretation of haptic\nfeedback was strongly shaped by the emotional context of the video, with visual\ncontext often overriding the perceived valence of the haptic signal. Negative\nhaptic cues amplified the perceived valence of the interaction, while positive\ncues softened it. Furthermore, haptics override the participants' perception of\narousal of the video. Together, these results offer insights into how situated\nhaptic feedback can enrich affective human-robot interaction, pointing toward\nmore nuanced and embodied approaches to emotional communication with machines.", "AI": {"tldr": "研究探讨了情境情绪线索如何影响对机器人触觉信号的感知，发现视觉情境常主导触觉信号的效价感知，触觉反馈能增强或软化情绪交互。", "motivation": "情感交互不仅是情绪识别，而是受情境和互动共同塑造的过程。触觉反馈在动态情感交换中的作用尚未充分探索。", "method": "32名参与者观看机器人经历积极、消极或中性动作的视频，随后通过可穿戴振动袖接收机器人的触觉情绪反馈，并评估其情绪状态。", "result": "视觉情境强烈影响触觉信号的感知，消极触觉线索增强交互效价，积极线索软化效价；触觉反馈还主导了参与者对视频唤醒度的感知。", "conclusion": "研究揭示了情境触觉反馈如何丰富人机情感交互，为更细腻的机器情感通信提供了方向。"}}
{"id": "2506.19597", "categories": ["cs.RO", "cs.AI", "cs.AR", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19597", "abs": "https://arxiv.org/abs/2506.19597", "authors": ["Haruki Uchiito", "Akhilesh Bhat", "Koji Kusaka", "Xiaoya Zhang", "Hiraku Kinjo", "Honoka Uehara", "Motoki Koyama", "Shinji Natsume"], "title": "Robotics Under Construction: Challenges on Job Sites", "comment": "Workshop on Field Robotics, ICRA", "summary": "As labor shortages and productivity stagnation increasingly challenge the\nconstruction industry, automation has become essential for sustainable\ninfrastructure development. This paper presents an autonomous payload\ntransportation system as an initial step toward fully unmanned construction\nsites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous\nnavigation, fleet management, and GNSS-based localization to facilitate\nmaterial transport in construction site environments. While the current system\ndoes not yet incorporate dynamic environment adaptation algorithms, we have\nbegun fundamental investigations into external-sensor based perception and\nmapping system. Preliminary results highlight the potential challenges,\nincluding navigation in evolving terrain, environmental perception under\nconstruction-specific conditions, and sensor placement optimization for\nimproving autonomy and efficiency. Looking forward, we envision a construction\necosystem where collaborative autonomous agents dynamically adapt to site\nconditions, optimizing workflow and reducing human intervention. This paper\nprovides foundational insights into the future of robotics-driven construction\nautomation and identifies critical areas for further technological development.", "AI": {"tldr": "本文提出了一种基于CD110R-3履带运输车的自主载荷运输系统，旨在解决建筑行业劳动力短缺和生产力停滞问题。系统整合了自主导航、车队管理和GNSS定位技术，初步展示了无人建筑工地的潜力。", "motivation": "建筑行业面临劳动力短缺和生产力停滞的挑战，自动化成为可持续基础设施发展的关键。", "method": "基于CD110R-3履带运输车，整合自主导航、车队管理和GNSS定位技术，初步探索外部传感器感知与建图系统。", "result": "初步结果揭示了动态地形导航、建筑环境感知和传感器优化等挑战。", "conclusion": "展望未来，协作自主代理将动态适应工地条件，优化工作流程。本文为机器人驱动的建筑自动化提供了基础性见解，并指出了关键技术发展方向。"}}
{"id": "2506.19201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19201", "abs": "https://arxiv.org/abs/2506.19201", "authors": ["Hanyang Zhou", "Haozhe Lou", "Wenhao Liu", "Enyu Zhao", "Yue Wang", "Daniel Seita"], "title": "The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors", "comment": null, "summary": "Advancing dexterous manipulation with multi-fingered robotic hands requires\nrich sensory capabilities, while existing designs lack onboard thermal and\ntorque sensing. In this work, we propose the MOTIF hand, a novel multimodal and\nversatile robotic hand that extends the LEAP hand by integrating: (i) dense\ntactile information across the fingers, (ii) a depth sensor, (iii) a thermal\ncamera, (iv), IMU sensors, and (v) a visual sensor. The MOTIF hand is designed\nto be relatively low-cost (under 4000 USD) and easily reproducible. We validate\nour hand design through experiments that leverage its multimodal sensing for\ntwo representative tasks. First, we integrate thermal sensing into 3D\nreconstruction to guide temperature-aware, safe grasping. Second, we show how\nour hand can distinguish objects with identical appearance but different masses\n- a capability beyond methods that use vision only.", "AI": {"tldr": "本文提出了一种名为MOTIF的新型多模态机器人手，扩展了LEAP手的功能，集成了多种传感器，用于增强灵巧操作。", "motivation": "现有的多指机器人手缺乏热感和扭矩感知能力，限制了其在复杂任务中的应用。", "method": "MOTIF手集成了密集触觉信息、深度传感器、热成像相机、IMU传感器和视觉传感器，设计为低成本且易于复制。", "result": "实验验证了MOTIF手在温度感知安全抓取和区分外观相同但质量不同的物体方面的能力。", "conclusion": "MOTIF手通过多模态传感扩展了机器人手的应用范围，证明了其在复杂任务中的潜力。"}}
{"id": "2506.19620", "categories": ["cs.RO", "cs.FL", "cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19620", "abs": "https://arxiv.org/abs/2506.19620", "authors": ["Mustafa Adam", "Kangfeng Ye", "David A. Anisi", "Ana Cavalcanti", "Jim Woodcock", "Robert Morris"], "title": "Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment", "comment": null, "summary": "Continued adoption of agricultural robots postulates the farmer's trust in\nthe reliability, robustness and safety of the new technology. This motivates\nour work on safety assurance of agricultural robots, particularly their ability\nto detect, track and avoid obstacles and humans. This paper considers a\nprobabilistic modelling and risk analysis framework for use in the early\ndevelopment phases. Starting off with hazard identification and a risk\nassessment matrix, the behaviour of the mobile robot platform, sensor and\nperception system, and any humans present are captured using three state\nmachines. An auto-generated probabilistic model is then solved and analysed\nusing the probabilistic model checker PRISM. The result provides unique insight\ninto fundamental development and engineering aspects by quantifying the effect\nof the risk mitigation actions and risk reduction associated with distinct\ndesign concepts. These include implications of adopting a higher performance\nand more expensive Object Detection System or opting for a more elaborate\nwarning system to increase human awareness. Although this paper mainly focuses\non the initial concept-development phase, the proposed safety assurance\nframework can also be used during implementation, and subsequent deployment and\noperation phases.", "AI": {"tldr": "本文提出了一种用于农业机器人安全保证的概率建模和风险分析框架，重点关注障碍物和人类的检测、跟踪与避障。", "motivation": "农民对农业机器人的信任依赖于其可靠性、鲁棒性和安全性，因此需要确保机器人在早期开发阶段的安全性。", "method": "通过危险识别和风险评估矩阵，使用三个状态机建模机器人平台、传感器系统及人类行为，并利用PRISM概率模型检查器分析模型。", "result": "量化了不同设计概念（如高性能物体检测系统或更复杂的警告系统）对风险缓解的影响，提供了独特的开发见解。", "conclusion": "该框架不仅适用于概念开发阶段，还可用于实施、部署和操作阶段。"}}
{"id": "2506.19202", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19202", "abs": "https://arxiv.org/abs/2506.19202", "authors": ["Claire Yang", "Heer Patel", "Max Kleiman-Weiner", "Maya Cakmak"], "title": "Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Roboticists often design with the assumption that assistive robots should be\nfully autonomous. However, it remains unclear whether users prefer highly\nautonomous robots, as prior work in assistive robotics suggests otherwise. High\nrobot autonomy can reduce the user's sense of agency, which represents feeling\nin control of one's environment. How much control do users, in fact, want over\nthe actions of robots used for in-home assistance? We investigate how robot\nautonomy levels affect users' sense of agency and the autonomy level they\nprefer in contexts with varying risks. Our study asked participants to rate\ntheir sense of agency as robot users across four distinct autonomy levels and\nranked their robot preferences with respect to various household tasks. Our\nfindings revealed that participants' sense of agency was primarily influenced\nby two factors: (1) whether the robot acts autonomously, and (2) whether a\nthird party is involved in the robot's programming or operation. Notably, an\nend-user programmed robot highly preserved users' sense of agency, even though\nit acts autonomously. However, in high-risk settings, e.g., preparing a snack\nfor a child with allergies, they preferred robots that prioritized their\ncontrol significantly more. Additional contextual factors, such as trust in a\nthird party operator, also shaped their preferences.", "AI": {"tldr": "研究探讨用户对家用辅助机器人自主性的偏好及其对用户控制感的影响，发现自主性和第三方介入是关键因素，高风险任务中用户更倾向控制。", "motivation": "探讨用户是否偏好高度自主的辅助机器人，以及机器人自主性如何影响用户的控制感。", "method": "通过实验让参与者评估四种不同自主性水平下对机器人的控制感，并针对不同家庭任务排名偏好。", "result": "用户控制感受机器人自主性和第三方介入影响；高风险任务中用户更倾向控制；第三方信任也影响偏好。", "conclusion": "机器人设计需平衡自主性与用户控制感，高风险任务中应优先用户控制。"}}
{"id": "2506.19622", "categories": ["cs.RO", "cs.FL", "cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19622", "abs": "https://arxiv.org/abs/2506.19622", "authors": ["Mustafa Adam", "David A. Anisi", "Pedro Ribeiro"], "title": "A Verification Methodology for Safety Assurance of Robotic Autonomous Systems", "comment": "In Proc. of the 26th TAROS (Towards Autonomous Robotic Systems)\n  Conference, York, UK, August, 2025", "summary": "Autonomous robots deployed in shared human environments, such as agricultural\nsettings, require rigorous safety assurance to meet both functional reliability\nand regulatory compliance. These systems must operate in dynamic, unstructured\nenvironments, interact safely with humans, and respond effectively to a wide\nrange of potential hazards. This paper presents a verification workflow for the\nsafety assurance of an autonomous agricultural robot, covering the entire\ndevelopment life-cycle, from concept study and design to runtime verification.\nThe outlined methodology begins with a systematic hazard analysis and risk\nassessment to identify potential risks and derive corresponding safety\nrequirements. A formal model of the safety controller is then developed to\ncapture its behaviour and verify that the controller satisfies the specified\nsafety properties with respect to these requirements. The proposed approach is\ndemonstrated on a field robot operating in an agricultural setting. The results\nshow that the methodology can be effectively used to verify safety-critical\nproperties and facilitate the early identification of design issues,\ncontributing to the development of safer robots and autonomous systems.", "AI": {"tldr": "本文提出了一种用于农业自主机器人的安全验证工作流程，覆盖从概念设计到运行时验证的全生命周期，确保其在动态环境中的安全性和合规性。", "motivation": "农业自主机器人需要在动态、非结构化环境中安全运行并与人类互动，因此需要严格的安全保障以满足功能可靠性和法规要求。", "method": "通过系统性的危险分析和风险评估确定潜在风险，并开发安全控制器的形式化模型，验证其满足安全属性。", "result": "该方法在农业现场机器人上得到验证，能够有效识别设计问题并验证安全关键属性。", "conclusion": "该工作流程有助于开发更安全的自主机器人系统，并促进早期设计问题的发现。"}}
{"id": "2506.19212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19212", "abs": "https://arxiv.org/abs/2506.19212", "authors": ["Vincent de Bakker", "Joey Hejna", "Tyler Ga Wei Lum", "Onur Celik", "Aleksandar Taranovic", "Denis Blessing", "Gerhard Neumann", "Jeannette Bohg", "Dorsa Sadigh"], "title": "Scaffolding Dexterous Manipulation with Vision-Language Models", "comment": null, "summary": "Dexterous robotic hands are essential for performing complex manipulation\ntasks, yet remain difficult to train due to the challenges of demonstration\ncollection and high-dimensional control. While reinforcement learning (RL) can\nalleviate the data bottleneck by generating experience in simulation, it\ntypically relies on carefully designed, task-specific reward functions, which\nhinder scalability and generalization. Thus, contemporary works in dexterous\nmanipulation have often bootstrapped from reference trajectories. These\ntrajectories specify target hand poses that guide the exploration of RL\npolicies and object poses that enable dense, task-agnostic rewards. However,\nsourcing suitable trajectories - particularly for dexterous hands - remains a\nsignificant challenge. Yet, the precise details in explicit reference\ntrajectories are often unnecessary, as RL ultimately refines the motion. Our\nkey insight is that modern vision-language models (VLMs) already encode the\ncommonsense spatial and semantic knowledge needed to specify tasks and guide\nexploration effectively. Given a task description (e.g., \"open the cabinet\")\nand a visual scene, our method uses an off-the-shelf VLM to first identify\ntask-relevant keypoints (e.g., handles, buttons) and then synthesize 3D\ntrajectories for hand motion and object motion. Subsequently, we train a\nlow-level residual RL policy in simulation to track these coarse trajectories\nor \"scaffolds\" with high fidelity. Across a number of simulated tasks involving\narticulated objects and semantic understanding, we demonstrate that our method\nis able to learn robust dexterous manipulation policies. Moreover, we showcase\nthat our method transfers to real-world robotic hands without any human\ndemonstrations or handcrafted rewards.", "AI": {"tldr": "论文提出了一种利用视觉语言模型（VLM）生成粗轨迹指导强化学习（RL）策略的方法，用于训练灵巧机器人手完成复杂任务，无需人工演示或手工设计奖励函数。", "motivation": "灵巧机器人手的训练面临数据收集和高维控制的挑战，传统RL依赖任务特定奖励函数，限制了扩展性和泛化能力。", "method": "利用现成的VLM识别任务相关关键点并合成3D轨迹，训练低层RL策略跟踪这些粗轨迹。", "result": "在模拟任务中成功学习到鲁棒的灵巧操作策略，并能迁移到真实机器人手。", "conclusion": "该方法通过结合VLM和RL，实现了无需人工干预的灵巧操作训练，具有实际应用潜力。"}}
{"id": "2506.19712", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19712", "abs": "https://arxiv.org/abs/2506.19712", "authors": ["Praneeth Somisetty", "Robert Griffin", "Victor M. Baez", "Miguel F. Arevalo-Castiblanco", "Aaron T. Becker", "Jason M. O'Kane"], "title": "Estimating Spatially-Dependent GPS Errors Using a Swarm of Robots", "comment": "6 pages, 7 figures, 2025 IEEE 21st International Conference on\n  Automation Science and Engineering", "summary": "External factors, including urban canyons and adversarial interference, can\nlead to Global Positioning System (GPS) inaccuracies that vary as a function of\nthe position in the environment. This study addresses the challenge of\nestimating a static, spatially-varying error function using a team of robots.\nWe introduce a State Bias Estimation Algorithm (SBE) whose purpose is to\nestimate the GPS biases. The central idea is to use sensed estimates of the\nrange and bearing to the other robots in the team to estimate changes in bias\nacross the environment. A set of drones moves in a 2D environment, each\nsampling data from GPS, range, and bearing sensors. The biases calculated by\nthe SBE at estimated positions are used to train a Gaussian Process Regression\n(GPR) model. We use a Sparse Gaussian process-based Informative Path Planning\n(IPP) algorithm that identifies high-value regions of the environment for data\ncollection. The swarm plans paths that maximize information gain in each\niteration, further refining their understanding of the environment's positional\nbias landscape. We evaluated SBE and IPP in simulation and compared the IPP\nmethodology to an open-loop strategy.", "AI": {"tldr": "论文提出了一种通过机器人团队估计静态空间变化GPS误差的方法，结合高斯过程回归和信息路径规划算法优化数据收集。", "motivation": "解决因城市峡谷和干扰导致的GPS误差问题，提高定位精度。", "method": "使用状态偏差估计算法（SBE）和稀疏高斯过程信息路径规划（IPP）算法，通过机器人团队收集数据并优化路径。", "result": "在仿真中验证了SBE和IPP的有效性，相比开环策略，IPP能更高效地优化数据收集。", "conclusion": "提出的方法能有效估计和优化GPS误差，适用于复杂环境中的定位问题。"}}
{"id": "2506.19269", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19269", "abs": "https://arxiv.org/abs/2506.19269", "authors": ["Ziyan Zhao", "Ke Fan", "He-Yang Xu", "Ning Qiao", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Hui Shen"], "title": "AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation", "comment": null, "summary": "We present AnchorDP3, a diffusion policy framework for dual-arm robotic\nmanipulation that achieves state-of-the-art performance in highly randomized\nenvironments. AnchorDP3 integrates three key innovations: (1)\nSimulator-Supervised Semantic Segmentation, using rendered ground truth to\nexplicitly segment task-critical objects within the point cloud, which provides\nstrong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight\nmodules processing augmented point clouds per task, enabling efficient\nmulti-task learning through a shared diffusion-based action expert; (3)\nAffordance-Anchored Keypose Diffusion with Full State Supervision, replacing\ndense trajectory prediction with sparse, geometrically meaningful action\nanchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to\naffordances, drastically simplifying the prediction space; the action expert is\nforced to predict both robot joint angles and end-effector poses\nsimultaneously, which exploits geometric consistency to accelerate convergence\nand boost accuracy. Trained on large-scale, procedurally generated simulation\ndata, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark\nacross diverse tasks under extreme randomization of objects, clutter, table\nheight, lighting, and backgrounds. This framework, when integrated with the\nRoboTwin real-to-sim pipeline, has the potential to enable fully autonomous\ngeneration of deployable visuomotor policies from only scene and instruction,\ntotally eliminating human demonstrations from learning manipulation skills.", "AI": {"tldr": "AnchorDP3是一种用于双臂机器人操作的扩散策略框架，在高度随机化环境中表现优异。", "motivation": "解决在高度随机化环境中机器人操作的挑战，减少对人类示范的依赖。", "method": "结合模拟器监督语义分割、任务条件特征编码器和基于关键姿态的扩散策略，简化预测空间并提升效率。", "result": "在RoboTwin基准测试中达到98.7%的平均成功率。", "conclusion": "AnchorDP3框架有望实现完全自主的视觉运动策略生成，无需人类示范。"}}
{"id": "2506.19303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19303", "abs": "https://arxiv.org/abs/2506.19303", "authors": ["Zexiang Guo", "Hengxiang Chen", "Xinheng Mai", "Qiusang Qiu", "Gan Ma", "Zhanat Kappassov", "Qiang Li", "Nutan Chen"], "title": "Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference", "comment": "This paper has been accepted by the 2025 International Conference on\n  Climbing and Walking Robots (CLAWAR). These authors contributed equally to\n  this work: Zexiang Guo, Hengxiang Chen, Xinheng Mai", "summary": "Inferring physical properties can significantly enhance robotic manipulation\nby enabling robots to handle objects safely and efficiently through adaptive\ngrasping strategies. Previous approaches have typically relied on either\ntactile or visual data, limiting their ability to fully capture properties. We\nintroduce a novel cross-modal perception framework that integrates visual\nobservations with tactile representations within a multimodal vision-language\nmodel. Our physical reasoning framework, which employs a hierarchical feature\nalignment mechanism and a refined prompting strategy, enables our model to make\nproperty-specific predictions that strongly correlate with ground-truth\nmeasurements. Evaluated on 35 diverse objects, our approach outperforms\nexisting baselines and demonstrates strong zero-shot generalization. Keywords:\ntactile perception, visual-tactile fusion, physical property inference,\nmultimodal integration, robot perception", "AI": {"tldr": "提出了一种新颖的跨模态感知框架，结合视觉和触觉数据，通过多模态视觉语言模型提升机器人对物体物理属性的推理能力。", "motivation": "现有方法仅依赖视觉或触觉数据，无法全面捕捉物体属性，限制了机器人操作的适应性和安全性。", "method": "采用分层特征对齐机制和改进的提示策略，构建多模态视觉语言模型，融合视觉和触觉数据。", "result": "在35种不同物体上测试，性能优于现有基线，并表现出强大的零样本泛化能力。", "conclusion": "该框架显著提升了机器人对物体物理属性的推理能力，为自适应抓取策略提供了有效支持。"}}
{"id": "2506.19397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19397", "abs": "https://arxiv.org/abs/2506.19397", "authors": ["Zixi Chen", "Di Wu", "Qinghua Guan", "David Hardman", "Federico Renda", "Josie Hughes", "Thomas George Thuruthel", "Cosimo Della Santina", "Barbara Mazzolai", "Huichan Zhao", "Cesare Stefanini"], "title": "A Survey on Soft Robot Adaptability: Implementations, Applications, and Prospects", "comment": "12 pages, 4 figures, accepted by IEEE Robotics & Automation Magazine", "summary": "Soft robots, compared to rigid robots, possess inherent advantages, including\nhigher degrees of freedom, compliance, and enhanced safety, which have\ncontributed to their increasing application across various fields. Among these\nbenefits, adaptability is particularly noteworthy. In this paper, adaptability\nin soft robots is categorized into external and internal adaptability. External\nadaptability refers to the robot's ability to adjust, either passively or\nactively, to variations in environments, object properties, geometries, and\ntask dynamics. Internal adaptability refers to the robot's ability to cope with\ninternal variations, such as manufacturing tolerances or material aging, and to\ngeneralize control strategies across different robots. As the field of soft\nrobotics continues to evolve, the significance of adaptability has become\nincreasingly pronounced. In this review, we summarize various approaches to\nenhancing the adaptability of soft robots, including design, sensing, and\ncontrol strategies. Additionally, we assess the impact of adaptability on\napplications such as surgery, wearable devices, locomotion, and manipulation.\nWe also discuss the limitations of soft robotics adaptability and prospective\ndirections for future research. By analyzing adaptability through the lenses of\nimplementation, application, and challenges, this paper aims to provide a\ncomprehensive understanding of this essential characteristic in soft robotics\nand its implications for diverse applications.", "AI": {"tldr": "本文综述了软体机器人的适应性，将其分为外部和内部适应性，总结了提升适应性的设计、传感和控制策略，并探讨了其在手术、可穿戴设备等领域的应用及未来研究方向。", "motivation": "软体机器人因其高自由度、柔顺性和安全性等优势应用广泛，适应性是其关键特性，本文旨在全面理解适应性及其对应用的影响。", "method": "通过分类外部和内部适应性，总结设计、传感和控制策略，并分析其在多个领域的应用效果。", "result": "适应性对软体机器人在手术、可穿戴设备等领域的应用具有显著影响，但仍存在局限性。", "conclusion": "本文为软体机器人适应性的研究提供了全面视角，指出了未来研究方向和应用潜力。"}}
{"id": "2506.19424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19424", "abs": "https://arxiv.org/abs/2506.19424", "authors": ["Tiankai Yang", "Kaixin Chai", "Jialin Ji", "Yuze Wu", "Chao Xu", "Fei Gao"], "title": "Ground-Effect-Aware Modeling and Control for Multicopters", "comment": null, "summary": "The ground effect on multicopters introduces several challenges, such as\ncontrol errors caused by additional lift, oscillations that may occur during\nnear-ground flight due to external torques, and the influence of ground airflow\non models such as the rotor drag and the mixing matrix. This article collects\nand analyzes the dynamics data of near-ground multicopter flight through\nvarious methods, including force measurement platforms and real-world flights.\nFor the first time, we summarize the mathematical model of the external torque\nof multicopters under ground effect. The influence of ground airflow on rotor\ndrag and the mixing matrix is also verified through adequate experimentation\nand analysis. Through simplification and derivation, the differential flatness\nof the multicopter's dynamic model under ground effect is confirmed. To\nmitigate the influence of these disturbance models on control, we propose a\ncontrol method that combines dynamic inverse and disturbance models, ensuring\nconsistent control effectiveness at both high and low altitudes. In this\nmethod, the additional thrust and variations in rotor drag under ground effect\nare both considered and compensated through feedforward models. The leveling\ntorque of ground effect can be equivalently represented as variations in the\ncenter of gravity and the moment of inertia. In this way, the leveling torque\ndoes not explicitly appear in the dynamic model. The final experimental results\nshow that the method proposed in this paper reduces the control error (RMSE) by\n\\textbf{45.3\\%}. Please check the supplementary material at:\nhttps://github.com/ZJU-FAST-Lab/Ground-effect-controller.", "AI": {"tldr": "本文研究了多旋翼飞行器在近地飞行中的地面效应问题，提出了结合动态逆和扰动模型的控制方法，显著降低了控制误差。", "motivation": "地面效应对多旋翼飞行器的控制带来额外升力、振荡和气流干扰等挑战，需要建立数学模型并设计补偿方法。", "method": "通过力测量平台和实际飞行数据收集分析，建立地面效应下的外部扭矩数学模型，提出动态逆与扰动模型结合的控制方法。", "result": "实验表明，该方法将控制误差（RMSE）降低了45.3%。", "conclusion": "提出的方法有效补偿了地面效应的影响，确保了高低空飞行中的控制一致性。"}}
{"id": "2506.19498", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.10; I.4.8; H.5.2"], "pdf": "https://arxiv.org/pdf/2506.19498", "abs": "https://arxiv.org/abs/2506.19498", "authors": ["Yiteng Chen", "Wenbo Li", "Shiyi Wang", "Huiping Zhuang", "Qingyao Wu"], "title": "T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models", "comment": "submitted to NeurIPS 2025", "summary": "Building a general robotic manipulation system capable of performing a wide\nvariety of tasks in real-world settings is a challenging task. Vision-Language\nModels (VLMs) have demonstrated remarkable potential in robotic manipulation\ntasks, primarily due to the extensive world knowledge they gain from\nlarge-scale datasets. In this process, Spatial Representations (such as points\nrepresenting object positions or vectors representing object orientations) act\nas a bridge between VLMs and real-world scene, effectively grounding the\nreasoning abilities of VLMs and applying them to specific task scenarios.\nHowever, existing VLM-based robotic approaches often adopt a fixed spatial\nrepresentation extraction scheme for various tasks, resulting in insufficient\nrepresentational capability or excessive extraction time. In this work, we\nintroduce T-Rex, a Task-Adaptive Framework for Spatial Representation\nExtraction, which dynamically selects the most appropriate spatial\nrepresentation extraction scheme for each entity based on specific task\nrequirements. Our key insight is that task complexity determines the types and\ngranularity of spatial representations, and Stronger representational\ncapabilities are typically associated with Higher overall system operation\ncosts. Through comprehensive experiments in real-world robotic environments, we\nshow that our approach delivers significant advantages in spatial\nunderstanding, efficiency, and stability without additional training.", "AI": {"tldr": "论文提出了一种任务自适应的空间表示提取框架T-Rex，动态选择最适合任务需求的表示方案，提升了机器人操作的空间理解、效率和稳定性。", "motivation": "现有基于视觉语言模型（VLM）的机器人方法通常采用固定的空间表示提取方案，导致表示能力不足或提取时间过长。", "method": "提出T-Rex框架，根据任务复杂度动态选择空间表示的类型和粒度。", "result": "实验表明，该方法在空间理解、效率和稳定性方面具有显著优势，且无需额外训练。", "conclusion": "T-Rex通过任务自适应的空间表示提取，有效提升了机器人操作的性能。"}}
{"id": "2506.19579", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19579", "abs": "https://arxiv.org/abs/2506.19579", "authors": ["Federico Tavella", "Kathryn Mearns", "Angelo Cangelosi"], "title": "Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects", "comment": null, "summary": "Robotic scene understanding increasingly relies on vision-language models\n(VLMs) to generate natural language descriptions of the environment. In this\nwork, we present a comparative study of captioning strategies for tabletop\nscenes captured by a robotic arm equipped with an RGB camera. The robot\ncollects images of objects from multiple viewpoints, and we evaluate several\nmodels that generate scene descriptions. We compare the performance of various\ncaptioning models, like BLIP and VLMs. Our experiments examine the trade-offs\nbetween single-view and multi-view captioning, and difference between\nrecognising real-world and 3D printed objects. We quantitatively evaluate\nobject identification accuracy, completeness, and naturalness of the generated\ncaptions. Results show that VLMs can be used in robotic settings where common\nobjects need to be recognised, but fail to generalise to novel representations.\nOur findings provide practical insights into deploying foundation models for\nembodied agents in real-world settings.", "AI": {"tldr": "比较了机器人场景理解中不同视觉语言模型（VLMs）的标题生成策略，评估了单视角与多视角、真实物体与3D打印物体的识别效果。", "motivation": "研究如何利用视觉语言模型（VLMs）生成机器人场景的自然语言描述，以提升机器人对环境的理解能力。", "method": "通过机器人手臂配备的RGB相机采集多视角图像，比较BLIP和VLMs等模型的标题生成性能，评估对象识别准确性、完整性和描述自然性。", "result": "VLMs在常见物体识别中表现良好，但对新颖表示泛化能力不足。", "conclusion": "研究为实际部署基础模型于机器人场景提供了实用见解。"}}
{"id": "2506.19602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19602", "abs": "https://arxiv.org/abs/2506.19602", "authors": ["Leonardo Zamora Yanez", "Jacob Rogatinsky", "Dominic Recco", "Sang-Yoep Lee", "Grace Matthews", "Andrew P. Sabelhaus", "Tommaso Ranzani"], "title": "Soft Robotic Delivery of Coiled Anchors for Cardiac Interventions", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Trans-catheter cardiac intervention has become an increasingly available\noption for high-risk patients without the complications of open heart surgery.\nHowever, current catheterbased platforms suffer from a lack of dexterity, force\napplication, and compliance required to perform complex intracardiac\nprocedures. An exemplary task that would significantly ease minimally invasive\nintracardiac procedures is the implantation of anchor coils, which can be used\nto fix and implant various devices in the beating heart. We introduce a robotic\nplatform capable of delivering anchor coils. We develop a kineto-statics model\nof the robotic platform and demonstrate low positional error. We leverage the\npassive compliance and high force output of the actuator in a multi-anchor\ndelivery procedure against a motile in-vitro simulator with millimeter level\naccuracy.", "AI": {"tldr": "论文介绍了一种用于心脏内锚定线圈植入的机器人平台，解决了现有导管平台在灵活性和力量应用上的不足。", "motivation": "当前导管平台在复杂心脏内手术中缺乏灵活性、力量应用和适应性，限制了其在高风险患者中的应用。", "method": "开发了一种机器人平台，建立了其运动-静力学模型，并利用其被动适应性和高力量输出进行多锚定植入。", "result": "实验显示该平台具有低位置误差，并在模拟器中实现了毫米级精度的锚定植入。", "conclusion": "该机器人平台为复杂心脏内手术提供了更高的灵活性和精确性，有望改善高风险患者的治疗效果。"}}
{"id": "2506.19699", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19699", "abs": "https://arxiv.org/abs/2506.19699", "authors": ["Jian Hou", "Xin Zhou", "Qihan Yang", "Adam J. Spiers"], "title": "UniTac-NV: A Unified Tactile Representation For Non-Vision-Based Tactile Sensors", "comment": "7 pages, 8 figures. Accepted version to appear in: 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Generalizable algorithms for tactile sensing remain underexplored, primarily\ndue to the diversity of sensor modalities. Recently, many methods for\ncross-sensor transfer between optical (vision-based) tactile sensors have been\ninvestigated, yet little work focus on non-optical tactile sensors. To address\nthis gap, we propose an encoder-decoder architecture to unify tactile data\nacross non-vision-based sensors. By leveraging sensor-specific encoders, the\nframework creates a latent space that is sensor-agnostic, enabling cross-sensor\ndata transfer with low errors and direct use in downstream applications. We\nleverage this network to unify tactile data from two commercial tactile\nsensors: the Xela uSkin uSPa 46 and the Contactile PapillArray. Both were\nmounted on a UR5e robotic arm, performing force-controlled pressing sequences\nagainst distinct object shapes (circular, square, and hexagonal prisms) and two\nmaterials (rigid PLA and flexible TPU). Another more complex unseen object was\nalso included to investigate the model's generalization capabilities. We show\nthat alignment in latent space can be implicitly learned from joint autoencoder\ntraining with matching contacts collected via different sensors. We further\ndemonstrate the practical utility of our approach through contact geometry\nestimation, where downstream models trained on one sensor's latent\nrepresentation can be directly applied to another without retraining.", "AI": {"tldr": "提出了一种编码器-解码器架构，用于统一非视觉触觉传感器的数据，实现跨传感器数据转换和下游应用。", "motivation": "现有研究多关注光学触觉传感器的跨传感器转换，而忽略了非光学触觉传感器。本文旨在填补这一空白。", "method": "采用传感器特定编码器构建传感器无关的潜在空间，通过联合自编码器训练实现数据对齐。实验使用两种商业触觉传感器（Xela uSkin uSPa 46和Contactile PapillArray）进行验证。", "result": "模型在潜在空间中对齐数据，实现了低误差的跨传感器转换，并成功应用于接触几何估计任务。", "conclusion": "该方法为非光学触觉传感器的通用化算法提供了有效解决方案，展示了潜在空间对齐的实用价值。"}}
{"id": "2506.19781", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.19781", "abs": "https://arxiv.org/abs/2506.19781", "authors": ["Boyi Liu", "Qianyi Zhang", "Qiang Yang", "Jianhao Jiao", "Jagmohan Chauhan", "Dimitrios Kanoulas"], "title": "The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication", "comment": null, "summary": "The integration of satellite communication into mobile devices represents a\nparadigm shift in connectivity, yet the performance characteristics under\nmotion and environmental occlusion remain poorly understood. We present the\nStarlink Robot, the first mobile robotic platform equipped with Starlink\nsatellite internet, comprehensive sensor suite including upward-facing camera,\nLiDAR, and IMU, designed to systematically study satellite communication\nperformance during movement. Our multi-modal dataset captures synchronized\ncommunication metrics, motion dynamics, sky visibility, and 3D environmental\ncontext across diverse scenarios including steady-state motion, variable\nspeeds, and different occlusion conditions. This platform and dataset enable\nresearchers to develop motion-aware communication protocols, predict\nconnectivity disruptions, and optimize satellite communication for emerging\nmobile applications from smartphones to autonomous vehicles. The project is\navailable at https://github.com/StarlinkRobot.", "AI": {"tldr": "论文介绍了Starlink Robot，首个配备Starlink卫星互联网的移动机器人平台，用于研究运动和环境遮挡下的卫星通信性能。", "motivation": "卫星通信在移动设备中的集成是一个重大转变，但其在运动和环境遮挡下的性能特性尚未充分理解。", "method": "开发了配备Starlink卫星互联网和多模态传感器（如摄像头、LiDAR和IMU）的移动机器人平台，收集同步通信指标、运动动态、天空可见性和3D环境数据。", "result": "生成了一个多模态数据集，涵盖稳态运动、变速和不同遮挡条件下的通信性能。", "conclusion": "该平台和数据集为开发运动感知通信协议、预测连接中断以及优化卫星通信提供了基础，适用于智能手机到自动驾驶车辆等新兴移动应用。"}}
{"id": "2506.19815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19815", "abs": "https://arxiv.org/abs/2506.19815", "authors": ["Runsheng Wang", "Xinyue Zhu", "Ava Chen", "Jingxi Xu", "Lauren Winterbottom", "Dawn M. Nilsen", "Joel Stein", "Matei Ciocarlie"], "title": "ReactEMG: Zero-Shot, Low-Latency Intent Detection via sEMG", "comment": null, "summary": "Surface electromyography (sEMG) signals show promise for effective\nhuman-computer interfaces, particularly in rehabilitation and prosthetics.\nHowever, challenges remain in developing systems that respond quickly and\nreliably to user intent, across different subjects and without requiring\ntime-consuming calibration. In this work, we propose a framework for EMG-based\nintent detection that addresses these challenges. Unlike traditional gesture\nrecognition models that wait until a gesture is completed before classifying\nit, our approach uses a segmentation strategy to assign intent labels at every\ntimestep as the gesture unfolds. We introduce a novel masked modeling strategy\nthat aligns muscle activations with their corresponding user intents, enabling\nrapid onset detection and stable tracking of ongoing gestures. In evaluations\nagainst baseline methods, considering both accuracy and stability for device\ncontrol, our approach surpasses state-of-the-art performance in zero-shot\ntransfer conditions, demonstrating its potential for wearable robotics and\nnext-generation prosthetic systems. Our project page is available at:\nhttps://reactemg.github.io", "AI": {"tldr": "提出了一种基于表面肌电信号（sEMG）的意图检测框架，通过分段策略和掩码建模实现快速意图识别，适用于康复和假肢领域。", "motivation": "解决现有sEMG系统在不同用户间快速、可靠识别意图且无需耗时校准的挑战。", "method": "采用分段策略实时标记意图，并引入掩码建模对齐肌肉激活与用户意图，实现快速检测和稳定跟踪。", "result": "在零样本迁移条件下，方法在准确性和稳定性上优于现有技术。", "conclusion": "该框架在可穿戴机器人和下一代假肢系统中具有应用潜力。"}}
{"id": "2506.19816", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19816", "abs": "https://arxiv.org/abs/2506.19816", "authors": ["Hao Li", "Shuai Yang", "Yilun Chen", "Yang Tian", "Xiaoda Yang", "Xinyi Chen", "Hanqing Wang", "Tai Wang", "Feng Zhao", "Dahua Lin", "Jiangmiao Pang"], "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation", "comment": "36 pages, 21 figures", "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.", "AI": {"tldr": "CronusVLA扩展了单帧视觉语言动作模型至多帧范式，通过高效的后训练阶段提升性能，减少计算冗余。", "motivation": "现有视觉语言动作模型受限于单帧观察，无法充分利用多帧历史信息，计算成本高。", "method": "1. 单帧预训练；2. 多帧编码；3. 跨帧解码。通过特征分块和动作适应机制优化。", "result": "在SimperEnv上成功率70.9%，LIBERO上比OpenVLA提升12.7%，真实机器人实验表现优异。", "conclusion": "CronusVLA通过多帧处理和高效推理，显著提升了性能与鲁棒性。"}}
{"id": "2506.19827", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19827", "abs": "https://arxiv.org/abs/2506.19827", "authors": ["Ola Elmaghraby", "Eslam Mounier", "Paulo Ricardo Marques de Araujo", "Aboelmagd Noureldin"], "title": "Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital Maps for GNSS-Challenged Environments", "comment": null, "summary": "In Global Navigation Satellite System (GNSS)-denied environments such as\nindoor parking structures or dense urban canyons, achieving accurate and robust\nvehicle positioning remains a significant challenge. This paper proposes a\ncost-effective, vision-based multi-sensor navigation system that integrates\nmonocular depth estimation, semantic filtering, and visual map registration\n(VMR) with 3-D digital maps. Extensive testing in real-world indoor and outdoor\ndriving scenarios demonstrates the effectiveness of the proposed system,\nachieving sub-meter accuracy of 92% indoors and more than 80% outdoors, with\nconsistent horizontal positioning and heading average root mean-square errors\nof approximately 0.98 m and 1.25 {\\deg}, respectively. Compared to the\nbaselines examined, the proposed solution significantly reduced drift and\nimproved robustness under various conditions, achieving positioning accuracy\nimprovements of approximately 88% on average. This work highlights the\npotential of cost-effective monocular vision systems combined with 3D maps for\nscalable, GNSS-independent navigation in land vehicles.", "AI": {"tldr": "论文提出了一种低成本、基于视觉的多传感器导航系统，用于GNSS受限环境中的车辆定位，结合单目深度估计、语义过滤和视觉地图注册，实现了高精度定位。", "motivation": "在GNSS受限环境（如室内停车场或密集城市峡谷）中，实现准确且鲁棒的车辆定位是一个重大挑战。", "method": "提出了一种结合单目深度估计、语义过滤和视觉地图注册（VMR）的多传感器导航系统，并与3D数字地图集成。", "result": "在真实驾驶场景中测试，室内定位精度达到92%的亚米级，室外超过80%，水平定位和航向平均均方根误差分别为0.98米和1.25度。与基线相比，定位精度平均提高了88%。", "conclusion": "该研究表明低成本单目视觉系统结合3D地图在陆地车辆导航中具有潜力，可实现不依赖GNSS的可扩展导航。"}}
{"id": "2506.19842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19842", "abs": "https://arxiv.org/abs/2506.19842", "authors": ["Tengbo Yu", "Guanxing Lu", "Zaijia Yang", "Haoyuan Deng", "Season Si Chen", "Jiwen Lu", "Wenbo Ding", "Guoqiang Hu", "Yansong Tang", "Ziwei Wang"], "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model", "comment": null, "summary": "Multi-task robotic bimanual manipulation is becoming increasingly popular as\nit enables sophisticated tasks that require diverse dual-arm collaboration\npatterns. Compared to unimanual manipulation, bimanual tasks pose challenges to\nunderstanding the multi-body spatiotemporal dynamics. An existing method\nManiGaussian pioneers encoding the spatiotemporal dynamics into the visual\nrepresentation via Gaussian world model for single-arm settings, which ignores\nthe interaction of multiple embodiments for dual-arm systems with significant\nperformance drop. In this paper, we propose ManiGaussian++, an extension of\nManiGaussian framework that improves multi-task bimanual manipulation by\ndigesting multi-body scene dynamics through a hierarchical Gaussian world\nmodel. To be specific, we first generate task-oriented Gaussian Splatting from\nintermediate visual features, which aims to differentiate acting and\nstabilizing arms for multi-body spatiotemporal dynamics modeling. We then build\na hierarchical Gaussian world model with the leader-follower architecture,\nwhere the multi-body spatiotemporal dynamics is mined for intermediate visual\nrepresentation via future scene prediction. The leader predicts Gaussian\nSplatting deformation caused by motions of the stabilizing arm, through which\nthe follower generates the physical consequences resulted from the movement of\nthe acting arm. As a result, our method significantly outperforms the current\nstate-of-the-art bimanual manipulation techniques by an improvement of 20.2% in\n10 simulated tasks, and achieves 60% success rate on average in 9 challenging\nreal-world tasks. Our code is available at\nhttps://github.com/April-Yz/ManiGaussian_Bimanual.", "AI": {"tldr": "ManiGaussian++扩展了ManiGaussian框架，通过分层高斯世界模型改进多任务双手机器人操作，显著提升了性能。", "motivation": "双手机器人操作任务复杂，现有方法ManiGaussian忽略多体交互，导致性能下降。", "method": "提出分层高斯世界模型，通过任务导向的高斯泼溅和领导者-跟随者架构建模多体时空动态。", "result": "在10个模拟任务中性能提升20.2%，在9个真实任务中平均成功率60%。", "conclusion": "ManiGaussian++有效解决了双手机器人操作中的多体动态建模问题，性能显著优于现有技术。"}}
