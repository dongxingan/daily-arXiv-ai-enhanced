<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10106)
*Marcos Abel Zuzuárregui,Mustafa Melih Toslak,Stefano Carpin*

Main category: cs.RO

TL;DR: 论文提出了一种基于自然语言的机器人任务规划器，帮助非技术用户通过简单界面控制异构机器人，利用大语言模型和预定义原语实现复杂农业任务的无代码操作。


<details>
  <summary>Details</summary>
Motivation: 人工智能在精准农业中的应用虽提高了效率，但也增加了复杂性，非技术用户难以适应。本文旨在通过自然语言界面降低技术门槛。

Method: 采用大语言模型（LLMs）和预定义原语，将人类语言转化为中间描述，供不同机器人平台执行。

Result: 系统支持异构机器人执行复杂任务，实验验证了其在机器人操作和计算机视觉任务中的通用性和强大功能。

Conclusion: 该研究为非技术用户提供了更易用的机器人自动化工具，推动了精准农业的普及。

Abstract: Artificial intelligence is transforming precision agriculture, offering
farmers new tools to streamline their daily operations. While these
technological advances promise increased efficiency, they often introduce
additional complexity and steep learning curves that are particularly
challenging for non-technical users who must balance tech adoption with
existing workloads. In this paper, we present a natural language (NL) robotic
mission planner that enables non-specialists to control heterogeneous robots
through a common interface. By leveraging large language models (LLMs) and
predefined primitives, our architecture seamlessly translates human language
into intermediate descriptions that can be executed by different robotic
platforms. With this system, users can formulate complex agricultural missions
without writing any code. In the work presented in this paper, we extend our
previous system tailored for wheeled robot mission planning through a new class
of experiments involving robotic manipulation and computer vision tasks. Our
results demonstrate that the architecture is both general enough to support a
diverse set of robots and powerful enough to execute complex mission requests.
This work represents a significant step toward making robotic automation in
precision agriculture more accessible to non-technical users.

</details>


### [2] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: 提出了一种模块化的导航框架，将视觉语言理解与动作规划解耦，结合轻量级规划逻辑，实现快速灵活的导航。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言导航（VLN）中计算成本高和实时部署的挑战，同时提升多模态理解能力。

Method: 使用冻结的视觉语言模型Qwen2.5-VL-7B-Instruct，结合提示工程、结构化历史管理和双帧视觉输入策略。

Result: 在Room-to-Room基准测试中初步结果显示出泛化能力不足，但模块化设计为未来改进奠定了基础。

Conclusion: 模块化方法为高效导航系统提供了可扩展的框架，未来可通过增强环境先验和多模态输入集成进一步优化。

Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied
AI, requiring agents to interpret natural language instructions and navigate
through visually rich, unfamiliar environments. Recent advances in large
vision-language models (LVLMs), such as CLIP and Flamingo, have significantly
improved multimodal understanding but introduced new challenges related to
computational cost and real-time deployment. In this project, we propose a
modular, plug-and-play navigation framework that decouples vision-language
understanding from action planning. By integrating a frozen vision-language
model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to
achieve flexible, fast, and adaptable navigation without extensive model
fine-tuning. Our framework leverages prompt engineering, structured history
management, and a two-frame visual input strategy to enhance decision-making
continuity across navigation steps. We evaluate our system on the Room-to-Room
benchmark within the VLN-CE setting using the Matterport3D dataset and
Habitat-Lab simulation environment. Although our initial results reveal
challenges in generalizing to unseen environments under strict evaluation
settings, our modular approach lays a foundation for scalable and efficient
navigation systems, highlighting promising directions for future improvement
through enhanced environmental priors and expanded multimodal input
integration.

</details>


### [3] [Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators](https://arxiv.org/abs/2506.10240)
*Rongfei Li,Francis Assadian*

Main category: cs.RO

TL;DR: 提出了一种基于Youla参数化的前馈-反馈自适应控制算法，用于解决3D点特征在视野外时的视觉伺服控制问题，并通过自适应控制器保证系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉伺服控制方法多依赖视野内的3D点特征，而视野外特征的控制问题尚未充分研究。

Method: 结合前馈-反馈结构和Youla参数化方法，设计特征估计环路和自适应控制器，在线线性化解耦相机与机器人模型。

Result: 仿真验证了控制器在视野内外均能实现稳定、快速且高精度的位姿控制。

Conclusion: 所提方法鲁棒性强，易于在不同工业机器人系统中实现，适用于复杂场景。

Abstract: Image-based visual servoing (IBVS) methods have been well developed and used
in many applications, especially in pose (position and orientation) alignment.
However, most research papers focused on developing control solutions when 3D
point features can be detected inside the field of view. This work proposes an
innovative feedforward-feedback adaptive control algorithm structure with the
Youla Parameterization method. A designed feature estimation loop ensures
stable and fast motion control when point features are outside the field of
view. As 3D point features move inside the field of view, the IBVS feedback
loop preserves the precision of the pose at the end of the control period.
Also, an adaptive controller is developed in the feedback loop to stabilize the
system in the entire range of operations. The nonlinear camera and robot
manipulator model is linearized and decoupled online by an adaptive algorithm.
The adaptive controller is then computed based on the linearized model
evaluated at current linearized point. The proposed solution is robust and easy
to implement in different industrial robotic systems. Various scenarios are
used in simulations to validate the effectiveness and robust performance of the
proposed controller.

</details>


### [4] [A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures](https://arxiv.org/abs/2506.10239)
*Maximilian Mühlbauer,Freek Stulp,Sylvain Calinon,Alin Albu-Schäffer,João Silvério*

Main category: cs.RO

TL;DR: 论文提出了一种统一的概率虚拟夹具（VFs）框架，能够根据任务阶段自适应选择最合适的触觉反馈，支持手动、半自动和全自动模式的切换。


<details>
  <summary>Details</summary>
Motivation: 在保持人类参与的同时，通过部分自动化提高任务效率，同时确保高精度。

Method: 结合概率动态系统实现粗引导，扩展概率位置轨迹夹具和视觉伺服夹具，支持几何感知和最优阻抗增益。

Result: 实验验证了框架在不同机器人上的有效性，展示了多种操作模式和编程的便捷性。

Conclusion: 该框架为任务阶段提供了灵活的自动化支持，同时保持了人类操作的精确性。

Abstract: Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the
most suitable haptic feedback for each phase of a task, based on learned or
perceived uncertainty. While keeping the human in the loop remains essential,
for instance, to ensure high precision, partial automation of certain task
phases is critical for productivity. We present a unified framework for
probabilistic VFs that seamlessly switches between manual fixtures,
semi-automated fixtures (with the human handling precise tasks), and full
autonomy. We introduce a novel probabilistic Dynamical System-based VF for
coarse guidance, enabling the robot to autonomously complete certain task
phases while keeping the human operator in the loop. For tasks requiring
precise guidance, we extend probabilistic position-based trajectory fixtures
with automation allowing for seamless human interaction as well as
geometry-awareness and optimal impedance gains. For manual tasks requiring very
precise guidance, we also extend visual servoing fixtures with the same
geometry-awareness and impedance behaviour. We validate our approach
experimentally on different robots, showcasing multiple operation modes and the
ease of programming fixtures.

</details>


### [5] [A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control](https://arxiv.org/abs/2506.10252)
*Rongfei Li,Francis Assadian*

Main category: cs.RO

TL;DR: 论文提出了一种新的控制策略，结合前馈控制器和基于Youla参数化的反馈控制器，解决了立体相机在视觉伺服控制中因过约束导致的局部极小值问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人导航和操作中，准确确定相机相对于环境的姿态是关键。然而，在基于图像的视觉伺服控制中，过约束问题可能导致相机陷入局部极小值，影响任务执行。

Method: 提出了一种集成前馈控制器和基于Youla参数化反馈控制器的策略，用于精确校准立体相机的姿态控制。

Result: 仿真结果表明，该方法有效避免了局部极小值，使相机能够准确高效地到达目标姿态。

Conclusion: 该方法为立体相机在视觉伺服控制中的姿态定位问题提供了有效的解决方案。

Abstract: In robot navigation and manipulation, accurately determining the camera's
pose relative to the environment is crucial for effective task execution. In
this paper, we systematically prove that this problem corresponds to the
Perspective-3-Point (P3P) formulation, where exactly three known 3D points and
their corresponding 2D image projections are used to estimate the pose of a
stereo camera. In image-based visual servoing (IBVS) control, the system
becomes overdetermined, as the 6 degrees of freedom (DoF) of the stereo camera
must align with 9 observed 2D features in the scene. When more constraints are
imposed than available DoFs, global stability cannot be guaranteed, as the
camera may become trapped in a local minimum far from the desired configuration
during servoing. To address this issue, we propose a novel control strategy for
accurately positioning a calibrated stereo camera. Our approach integrates a
feedforward controller with a Youla parameterization-based feedback controller,
ensuring robust servoing performance. Through simulations, we demonstrate that
our method effectively avoids local minima and enables the camera to reach the
desired pose accurately and efficiently.

</details>


### [6] [Learning Safe Control via On-the-Fly Bandit Exploration](https://arxiv.org/abs/2506.10279)
*Alexandre Capone,Ryan Cosner,Aaaron Ames,Sandra Hirche*

Main category: cs.RO

TL;DR: 论文提出了一种结合高斯过程与控制屏障函数的方法，用于在模型不确定性高的情况下确保控制任务的安全性，无需依赖备用控制器。


<details>
  <summary>Details</summary>
Motivation: 在高模型不确定性的控制任务中，传统方法依赖备用控制器确保安全性，但可能导致效率低下或无效。本文旨在通过动态数据收集和探索解决这一问题。

Method: 结合高斯过程（GP）的bandit算法与控制屏障函数（CBF），动态收集数据以验证安全性，并在不可行时引导探索。

Result: 该方法在零均值先验动力学模型下可证明地确保安全性，且无需备用控制器。

Conclusion: 这是首个无需备用控制器的安全学习控制方法，为高不确定性任务提供了新解决方案。

Abstract: Control tasks with safety requirements under high levels of model uncertainty
are increasingly common. Machine learning techniques are frequently used to
address such tasks, typically by leveraging model error bounds to specify
robust constraint-based safety filters. However, if the learned model
uncertainty is very high, the corresponding filters are potentially invalid,
meaning no control input satisfies the constraints imposed by the safety
filter. While most works address this issue by assuming some form of safe
backup controller, ours tackles it by collecting additional data on the fly
using a Gaussian process bandit-type algorithm. We combine a control barrier
function with a learned model to specify a robust certificate that ensures
safety if feasible. Whenever infeasibility occurs, we leverage the control
barrier function to guide exploration, ensuring the collected data contributes
toward the closed-loop system safety. By combining a safety filter with
exploration in this manner, our method provably achieves safety in a setting
that allows for a zero-mean prior dynamics model, without requiring a backup
controller. To the best of our knowledge, it is the first safe learning-based
control method that achieves this.

</details>


### [7] [RICE: Reactive Interaction Controller for Cluttered Canopy Environment](https://arxiv.org/abs/2506.10383)
*Nidhi Homey Parayil,Thierry Peynot,Chris Lehnert*

Main category: cs.RO

TL;DR: 提出一种新型反应式控制器，用于机器臂在密集、遮挡的农业环境中安全导航，结合末端执行器位置和实时触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 传统视觉或模型依赖方法在农业冠层等密集、遮挡环境中易失效，需物理交互但不损坏植物。

Method: 基于权衡绕过障碍和推动障碍的策略，结合位置和触觉反馈。

Result: 在35次实验中，控制器成功到达目标且未损坏植物，优于现有无模型控制器。

Conclusion: 为农业任务（如修剪和收获）提供了安全、自适应交互的基础。

Abstract: Robotic navigation in dense, cluttered environments such as agricultural
canopies presents significant challenges due to physical and visual occlusion
caused by leaves and branches. Traditional vision-based or model-dependent
approaches often fail in these settings, where physical interaction without
damaging foliage and branches is necessary to reach a target. We present a
novel reactive controller that enables safe navigation for a robotic arm in a
contact-rich, cluttered, deformable environment using end-effector position and
real-time tactile feedback. Our proposed framework's interaction strategy is
based on a trade-off between minimizing disturbance by maneuvering around
obstacles and pushing through them to move towards the target. We show that
over 35 trials in 3 experimental plant setups with an occluded target, the
proposed controller successfully reached the target in all trials without
breaking any branch and outperformed the state-of-the-art model-free controller
in robustness and adaptability. This work lays the foundation for safe,
adaptive interaction in cluttered, contact-rich deformable environments,
enabling future agricultural tasks such as pruning and harvesting in plant
canopies.

</details>


### [8] [Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks](https://arxiv.org/abs/2506.10287)
*Rohit Sonker,Alexandre Capone,Andrew Rothstein,Hiro Josep Farre Kaga,Egemen Kolemen,Jeff Schneider*

Main category: cs.RO

TL;DR: 提出了一种多尺度贝叶斯优化方法，用于控制核聚变中的撕裂不稳定性，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 核聚变系统动态复杂、数据质量差、硬件易故障，现有机器学习工具无法全面解决这些问题。

Method: 结合高频数据驱动动态模型与低频高斯过程，通过实验间更新高斯过程快速适应新数据。

Result: 在DIII-D托卡马克实验中，成功率提高117%，达到50%。

Conclusion: 多尺度贝叶斯优化方法在复杂动态系统中表现出色，为核聚变控制提供了有效解决方案。

Abstract: Machine learning algorithms often struggle to control complex real-world
systems. In the case of nuclear fusion, these challenges are exacerbated, as
the dynamics are notoriously complex, data is poor, hardware is subject to
failures, and experiments often affect dynamics beyond the experiment's
duration. Existing tools like reinforcement learning, supervised learning, and
Bayesian optimization address some of these challenges but fail to provide a
comprehensive solution. To overcome these limitations, we present a multi-scale
Bayesian optimization approach that integrates a high-frequency data-driven
dynamics model with a low-frequency Gaussian process. By updating the Gaussian
process between experiments, the method rapidly adapts to new data, refining
the predictions of the less reliable dynamical model. We validate our approach
by controlling tearing instabilities in the DIII-D nuclear fusion plant.
Offline testing on historical data shows that our method significantly
outperforms several baselines. Results on live experiments on the DIII-D
tokamak, conducted under high-performance plasma scenarios prone to
instabilities, shows a 50% success rate, marking a 117% improvement over
historical outcomes.

</details>


### [9] [Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving](https://arxiv.org/abs/2506.10317)
*Akshar Tumu,Henrik I. Christensen,Marcell Vazquez-Chanlatte,Chikao Tsuchiya,Dhaval Bhanderi*

Main category: cs.RO

TL;DR: 论文提出了一种轻量级方法，通过结合OSM地图的结构化道路元数据和道路设计手册中的车道宽度先验，改进了SMERF模型的车道拓扑预测能力。


<details>
  <summary>Details</summary>
Motivation: 车道拓扑预测对自动驾驶导航至关重要，而道路环境信息通常遵循自然语言编码的惯例（如设计规范和道路名称）。

Method: 在SMERF模型中结合OSM地图的道路元数据和车道宽度先验，增强车道拓扑预测。

Result: 在两个地理多样化的复杂交叉口场景中，模型在车道和交通元素检测及其关联方面表现提升。

Conclusion: 该方法能够泛化并适应多样化的拓扑结构和条件。

Abstract: Lane-topology prediction is a critical component of safe and reliable
autonomous navigation. An accurate understanding of the road environment aids
this task. We observe that this information often follows conventions encoded
in natural language, through design codes that reflect the road structure and
road names that capture the road functionality. We augment this information in
a lightweight manner to SMERF, a map-prior-based online lane-topology
prediction model, by combining structured road metadata from OSM maps and
lane-width priors from Road design manuals with the road centerline encodings.
We evaluate our method on two geo-diverse complex intersection scenarios. Our
method shows improvement in both lane and traffic element detection and their
association. We report results using four topology-aware metrics to
comprehensively assess the model performance. These results demonstrate the
ability of our approach to generalize and scale to diverse topologies and
conditions.

</details>


### [10] [Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success](https://arxiv.org/abs/2506.10359)
*Che Wang,Jeroen van Baar,Chaitanya Mitash,Shuai Li,Dylan Randle,Weiyao Wang,Sumedh Sontakke,Kostas E. Bekris,Kapil Katyal*

Main category: cs.RO

TL;DR: 本文展示了如何通过从稀疏标记的工业规模真实数据中自主学习机器人操作，实现性能提升，重点研究了多模态视觉编码器在多吸盘机器人抓取中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决从无序堆中抓取多样化物品的挑战，满足高吞吐量的实时性要求，适用于仓库等实际场景。

Method: 利用RGB、深度和语义分割等多模态输入预测抓取质量，结合多模态预训练和微调策略。

Result: 在大规模物品抓取数据集、部分遮挡数据集和包裹抓取数据集上进行了全面实验，验证了方法的有效性。

Conclusion: 多模态训练和预训练对性能至关重要，模型能在微调和推理时仅使用部分模态输入。

Abstract: This work demonstrates how autonomously learning aspects of robotic operation
from sparsely-labeled, real-world data of deployed, engineered solutions at
industrial scale can provide with solutions that achieve improved performance.
Specifically, it focuses on multi-suction robot picking and performs a
comprehensive study on the application of multi-modal visual encoders for
predicting the success of candidate robotic picks. Picking diverse items from
unstructured piles is an important and challenging task for robot manipulation
in real-world settings, such as warehouses. Methods for picking from clutter
must work for an open set of items while simultaneously meeting latency
constraints to achieve high throughput. The demonstrated approach utilizes
multiple input modalities, such as RGB, depth and semantic segmentation, to
estimate the quality of candidate multi-suction picks. The strategy is trained
from real-world item picking data, with a combination of multimodal pretrain
and finetune. The manuscript provides comprehensive experimental evaluation
performed over a large item-picking dataset, an item-picking dataset targeted
to include partial occlusions, and a package-picking dataset, which focuses on
containers, such as boxes and envelopes, instead of unpackaged items. The
evaluation measures performance for different item configurations, pick scenes,
and object types. Ablations help to understand the effects of in-domain
pretraining, the impact of different modalities and the importance of
finetuning. These ablations reveal both the importance of training over
multiple modalities but also the ability of models to learn during pretraining
the relationship between modalities so that during finetuning and inference,
only a subset of them can be used as input.

</details>


### [11] [Towards more efficient quantitative safety validation of residual risk for assisted and automated driving](https://arxiv.org/abs/2506.10363)
*Daniel Betschinske,Malte Schrimpf,Steven Peters,Kamil Klonecki,Jan Peter Karch,Moritz Lippert*

Main category: cs.RO

TL;DR: 论文探讨了如何通过减少现场操作测试（FOT）的负担来高效验证高级驾驶辅助系统（ADAS）和自动驾驶系统（ADS）的安全性，提出了两种模型并评估了现有方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 传统的FOT方法在高自动化级别下测试成本高且效率低，因此需要探索更高效的验证方法以满足国际标准（如ISO 21448）。

Method: 基于ISO 21448，论文提出了两种模型：通用模型和基础模型（以自动紧急制动系统为例），并评估了现有减少FOT负担的方法。

Result: 评估发现，现有方法虽有一定潜力，但均存在局限性，无法完全替代FOT在安全验证中的关键作用。

Conclusion: FOT在ADAS和ADS的安全验证中仍不可或缺，未来研究需进一步优化现有方法以弥补其不足。

Abstract: The safety validation of Advanced Driver Assistance Systems (ADAS) and
Automated Driving Systems (ADS) increasingly demands efficient and reliable
methods to quantify residual risk while adhering to international standards
such as ISO 21448. Traditionally, Field Operational Testing (FOT) has been
pivotal for macroscopic safety validation of automotive driving functions up to
SAE automation level 2. However, state-of-the-art derivations for empirical
safety demonstrations using FOT often result in impractical testing efforts,
particularly at higher automation levels. Even at lower automation levels, this
limitation - coupled with the substantial costs associated with FOT - motivates
the exploration of approaches to enhance the efficiency of FOT-based
macroscopic safety validation. Therefore, this publication systematically
identifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT,
including novel methods reported in the literature. Based on an analysis of ISO
21448, two models are derived: a generic model capturing the argumentation
components of the standard, and a base model, exemplarily applied to Automatic
Emergency Braking (AEB) systems, establishing a baseline for the real-world
driving requirement for a Quantitative Safety Validation of Residual Risk
(QSVRR). Subsequently, the RAs are assessed using four criteria:
quantifiability, threats to validity, missing links, and black box
compatibility, highlighting potential benefits, inherent limitations, and
identifying key areas for further research. Our evaluation reveals that, while
several approaches offer potential, none are free from missing links or other
substantial shortcomings. Moreover, no identified alternative can fully replace
FOT, reflecting its crucial role in the safety validation of ADAS and ADS.

</details>


### [12] [Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions](https://arxiv.org/abs/2506.10462)
*Ana Müller,Sabina Jeschke,Anja Richert*

Main category: cs.RO

TL;DR: 研究探讨了群体自适应对话设计对两种社交交互代理（SIAs）的影响，发现其对满意度无显著影响，但为多群体交互和不同体现形式的挑战提供了见解。


<details>
  <summary>Details</summary>
Motivation: 探索群体敏感的对话设计在社交交互代理（SIAs）中的实际效果，以及其在多群体交互和不同体现形式（机器人与虚拟代理）中的适应性。

Method: 通过两项真实世界研究，使用Furhat社交机器人和MetaHuman虚拟代理，结合混合检索和生成模型的对话AI后端，与188名参与者互动。

Result: 群体敏感对话设计对满意度无显著影响，但揭示了在多群体交互和不同体现形式中适应对话AI的挑战。

Conclusion: 研究为未来群体设置中有效对话适应的研究提供了见解，强调了超越语言多元化的多模态策略需求。

Abstract: This paper investigates the impact of a group-adaptive conversation design in
two socially interactive agents (SIAs) through two real-world studies. Both
SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped
with a conversational artificial intelligence (CAI) backend combining hybrid
retrieval and generative models. The studies were carried out in an in-the-wild
setting with a total of $N = 188$ participants who interacted with the SIAs -
in dyads, triads or larger groups - at a German museum. Although the results
did not reveal a significant effect of the group-sensitive conversation design
on perceived satisfaction, the findings provide valuable insights into the
challenges of adapting CAI for multi-party interactions and across different
embodiments (robot vs.\ virtual agent), highlighting the need for multimodal
strategies beyond linguistic pluralization. These insights contribute to the
fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and
broader Human-Machine Interaction (HMI), providing insights for future research
on effective dialogue adaptation in group settings.

</details>


### [13] [EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence](https://arxiv.org/abs/2506.10600)
*Wang Xinjie,Liu Liu,Cao Yu,Wu Ruiqi,Qin Wenkang,Wang Dehui,Sui Wei,Su Zhizhong*

Main category: cs.RO

TL;DR: EmbodiedGen是一个用于生成高质量、可控且逼真的3D资产的基础平台，支持低成本生成具有物理属性的3D世界，适用于具身智能任务。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能任务依赖传统3D图形资产，成本高且真实性有限，限制了数据驱动方法的扩展性。

Method: EmbodiedGen通过六个关键模块（如图像到3D、文本到3D等）生成多样化的交互式3D世界，利用生成式AI技术。

Result: 生成的3D资产具有高真实性和物理准确性，可直接用于物理模拟引擎，支持下游任务。

Conclusion: EmbodiedGen解决了具身智能研究中数据扩展性和真实性的挑战，是一个易用且功能全面的工具包。

Abstract: Constructing a physically realistic and accurately scaled simulated 3D world
is crucial for the training and evaluation of embodied intelligence tasks. The
diversity, realism, low cost accessibility and affordability of 3D data assets
are critical for achieving generalization and scalability in embodied AI.
However, most current embodied intelligence tasks still rely heavily on
traditional 3D computer graphics assets manually created and annotated, which
suffer from high production costs and limited realism. These limitations
significantly hinder the scalability of data driven approaches. We present
EmbodiedGen, a foundational platform for interactive 3D world generation. It
enables the scalable generation of high-quality, controllable and
photorealistic 3D assets with accurate physical properties and real-world scale
in the Unified Robotics Description Format (URDF) at low cost. These assets can
be directly imported into various physics simulation engines for fine-grained
physical control, supporting downstream tasks in training and evaluation.
EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key
modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object
Generation, Scene Generation and Layout Generation. EmbodiedGen generates
diverse and interactive 3D worlds composed of generative 3D assets, leveraging
generative AI to address the challenges of generalization and evaluation to the
needs of embodied intelligence related research. Code is available at
https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.

</details>


### [14] [An $O(n$)-Algorithm for the Higher-Order Kinematics and Inverse Dynamics of Serial Manipulators using Spatial Representation of Twists](https://arxiv.org/abs/2506.10686)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种基于空间表示的二阶逆动力学算法，并配合四阶正向和逆向运动学算法，适用于机器人臂的最优控制。


<details>
  <summary>Details</summary>
Motivation: 为了提高计算效率，特别是在机器人臂的平坦性控制中，需要高效计算关节扭矩/力的时间导数。

Method: 采用空间表示法，结合Lie群理论，提出二阶逆动力学算法和四阶运动学算法。

Result: 该方法在7自由度Franka Emika Panda机器人上验证了其有效性。

Conclusion: Lie群表示法能以矢量形式高效实现，适用于机器人控制。

Abstract: Optimal control in general, and flatness-based control in particular, of
robotic arms necessitate to compute the first and second time derivatives of
the joint torques/forces required to achieve a desired motion. In view of the
required computational efficiency, recursive $O(n)$-algorithms were proposed to
this end. Aiming at compact yet efficient formulations, a Lie group formulation
was recently proposed, making use of body-fixed and hybrid representation of
twists and wrenches. In this paper a formulation is introduced using the
spatial representation. The second-order inverse dynamics algorithm is
accompanied by a fourth-order forward and inverse kinematics algorithm. An
advantage of all Lie group formulations is that they can be parameterized in
terms of vectorial quantities that are readily available. The method is
demonstrated for the 7 DOF Franka Emika Panda robot.

</details>


### [15] [Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding](https://arxiv.org/abs/2506.10756)
*Yuhang Zhang,Haosheng Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: VLFly是一种为无人机设计的视觉语言导航框架，通过连续速度命令实现语言引导飞行，无需定位或测距传感器，在多样环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言导航中的泛化问题和离散动作空间依赖，提升无人机在复杂环境中的导航能力。

Method: 结合大型语言模型（LLM）编码指令、视觉语言模型（VLM）匹配目标图像，以及路径规划模块生成连续轨迹。

Result: 在模拟和真实环境中均优于基线方法，支持开放词汇目标理解和抽象语言输入。

Conclusion: VLFly展示了强大的泛化能力和实时控制潜力，适用于无人机导航任务。

Abstract: Vision-and-language navigation (VLN) is a long-standing challenge in
autonomous robotics, aiming to empower agents with the ability to follow human
instructions while navigating complex environments. Two key bottlenecks remain
in this field: generalization to out-of-distribution environments and reliance
on fixed discrete action spaces. To address these challenges, we propose
Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles
(UAVs) to execute language-guided flight. Without the requirement for
localization or active ranging sensors, VLFly outputs continuous velocity
commands purely from egocentric observations captured by an onboard monocular
camera. The VLFly integrates three modules: an instruction encoder based on a
large language model (LLM) that reformulates high-level language into
structured prompts, a goal retriever powered by a vision-language model (VLM)
that matches these prompts to goal images via vision-language similarity, and a
waypoint planner that generates executable trajectories for real-time UAV
control. VLFly is evaluated across diverse simulation environments without
additional fine-tuning and consistently outperforms all baselines. Moreover,
real-world VLN tasks in indoor and outdoor environments under direct and
indirect instructions demonstrate that VLFly achieves robust open-vocabulary
goal understanding and generalized navigation capabilities, even in the
presence of abstract language input.

</details>


### [16] [In-Hand Object Pose Estimation via Visual-Tactile Fusion](https://arxiv.org/abs/2506.10787)
*Felix Nonnengießer,Alap Kshirsagar,Boris Belousov,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一种结合视觉和触觉信息的机器人手内物体姿态估计方法，显著提高了遮挡情况下的精度。


<details>
  <summary>Details</summary>
Motivation: 视觉遮挡是视觉方法的主要挑战，需要结合触觉信息提高姿态估计的准确性。

Method: 融合手腕RGB-D相机和指尖视觉触觉传感器的数据，使用加权点云的增强ICP算法估计6D姿态。

Result: 平均姿态估计误差为7.5毫米和16.7度，比纯视觉方法提升20%。

Conclusion: 结合触觉信息显著提高了遮挡情况下的姿态估计精度，适用于实际物体操作任务。

Abstract: Accurate in-hand pose estimation is crucial for robotic object manipulation,
but visual occlusion remains a major challenge for vision-based approaches.
This paper presents an approach to robotic in-hand object pose estimation,
combining visual and tactile information to accurately determine the position
and orientation of objects grasped by a robotic hand. We address the challenge
of visual occlusion by fusing visual information from a wrist-mounted RGB-D
camera with tactile information from vision-based tactile sensors mounted on
the fingertips of a robotic gripper. Our approach employs a weighting and
sensor fusion module to combine point clouds from heterogeneous sensor types
and control each modality's contribution to the pose estimation process. We use
an augmented Iterative Closest Point (ICP) algorithm adapted for weighted point
clouds to estimate the 6D object pose. Our experiments show that incorporating
tactile information significantly improves pose estimation accuracy,
particularly when occlusion is high. Our method achieves an average pose
estimation error of 7.5 mm and 16.7 degrees, outperforming vision-only
baselines by up to 20%. We also demonstrate the ability of our method to
perform precise object manipulation in a real-world insertion task.

</details>


### [17] [RationalVLA: A Rational Vision-Language-Action Model with Dual System](https://arxiv.org/abs/2506.10826)
*Wenxuan Song,Jiayi Chen,Wenxue Li,Xu He,Han Zhao,Pengxiang Ding Shiyan Su,Feilong Tang,Xuelian Cheng,Donglin Wang,Zongyuan Ge,Xinhu Zheng,Zhe Liu,Hesheng Wang,Yunhui Liu,Haoang Li*

Main category: cs.RO

TL;DR: 论文提出了RAMA基准和RationalVLA模型，用于处理机器人执行自然语言指令时的模糊、无关或不可行问题，显著提升了任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有语言条件操控任务假设指令与环境完美对齐，限制了实际场景中的鲁棒性和泛化能力。

Method: 构建包含14,000多样本的数据集，提出RationalVLA模型，结合高层视觉语言模型与低层操控策略，通过可学习潜在空间嵌入实现指令推理与拒绝。

Result: RationalVLA在RAMA基准上比现有方法任务成功率提升14.5%，平均任务长度降低0.94，并在实际应用中验证了有效性。

Conclusion: RAMA和RationalVLA为处理复杂指令提供了新方法，显著提升了机器人操控的鲁棒性和实用性。

Abstract: A fundamental requirement for real-world robotic deployment is the ability to
understand and respond to natural language instructions. Existing
language-conditioned manipulation tasks typically assume that instructions are
perfectly aligned with the environment. This assumption limits robustness and
generalization in realistic scenarios where instructions may be ambiguous,
irrelevant, or infeasible. To address this problem, we introduce RAtional
MAnipulation (RAMA), a new benchmark that challenges models with both unseen
executable instructions and defective ones that should be rejected. In RAMA, we
construct a dataset with over 14,000 samples, including diverse defective
instructions spanning six dimensions: visual, physical, semantic, motion,
safety, and out-of-context. We further propose the Rational
Vision-Language-Action model (RationalVLA). It is a dual system for robotic
arms that integrates the high-level vision-language model with the low-level
manipulation policy by introducing learnable latent space embeddings. This
design enables RationalVLA to reason over instructions, reject infeasible
commands, and execute manipulation effectively. Experiments demonstrate that
RationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher
success rate and 0.94 average task length, while maintaining competitive
performance on standard manipulation tasks. Real-world trials further validate
its effectiveness and robustness in practical applications. Our project page is
https://irpn-eai.github.io/rationalvla.

</details>


### [18] [Invariant Extended Kalman Filter for Autonomous Surface Vessels with Partial Orientation Measurements](https://arxiv.org/abs/2506.10850)
*Derek Benham,Easton Potokar,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种基于不变扩展卡尔曼滤波（InEKF）的框架，用于整合部分方向测量，以提高自主水面船只（ASV）在开放海洋环境中的状态估计精度。


<details>
  <summary>Details</summary>
Motivation: 由于ASV在海洋科学中的重要性，精确的状态估计（尤其是姿态）对海底测绘至关重要。传统方法依赖固定地标的相对位置测量，而开放海洋中ASV主要观测地平线，因此需要新的解决方案。

Method: 利用前向单目相机估计地平线相关的滚转和俯仰角，结合双天线GPS航向测量，开发了一种新的InEKF框架以整合部分方向数据。

Result: 实验结果表明，提出的方法在开放海洋环境中对ASV状态估计具有高效性和鲁棒性。

Conclusion: 该框架为ASV在缺乏全方向测量时的状态估计提供了有效解决方案，优于传统InEKF和MEKF方法。

Abstract: Autonomous surface vessels (ASVs) are increasingly vital for marine science,
offering robust platforms for underwater mapping and inspection. Accurate state
estimation, particularly of vehicle pose, is paramount for precise seafloor
mapping, as even small surface deviations can have significant consequences
when sensing the seafloor below. To address this challenge, we propose an
Invariant Extended Kalman Filter (InEKF) framework designed to integrate
partial orientation measurements. While conventional estimation often relies on
relative position measurements to fixed landmarks, open ocean ASVs primarily
observe a receding horizon. We leverage forward-facing monocular cameras to
estimate roll and pitch with respect to this horizon, which provides
yaw-ambiguous partial orientation information. To effectively utilize these
measurements within the InEKF, we introduce a novel framework for incorporating
such partial orientation data. This approach contrasts with traditional InEKF
implementations that assume full orientation measurements and is particularly
relevant for planar vehicle motion constrained to a "seafaring plane." This
paper details the developed InEKF framework; its integration with horizon-based
roll/pitch observations and dual-antenna GPS heading measurements for ASV state
estimation; and provides a comparative analysis against the InEKF using full
orientation and a Multiplicative EKF (MEKF). Our results demonstrate the
efficacy and robustness of the proposed partial orientation measurements for
accurate ASV state estimation in open ocean environments.

</details>


### [19] [Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material](https://arxiv.org/abs/2506.10875)
*Guanjin Wang,Xiangxue Zhao,Shapour Azarm,Balakumar Balachandran*

Main category: cs.RO

TL;DR: 提出了一种基于数据驱动的建模方法，结合降维、代理建模和数据同化技术，显著减少计算时间并保持高精度，适用于机器人导航和复杂地形探索。


<details>
  <summary>Details</summary>
Motivation: 研究机器人运动与颗粒地形交互的基本原理，提出高效且准确的建模方法。

Method: 整合降维（ST-HOSVD）、代理建模（高斯过程）和数据同化（降阶粒子滤波）技术，基于离线高保真模拟数据和稀疏实验数据。

Result: 计算时间显著减少，预测精度与模拟相当；结合实验数据后，长期预测优于纯模拟。

Conclusion: 该方法在机器人导航和复杂地形探索中具有潜力，展现了超越个案预测的通用性。

Abstract: An alternative data-driven modeling approach has been proposed and employed
to gain fundamental insights into robot motion interaction with granular
terrain at certain length scales. The approach is based on an integration of
dimension reduction (Sequentially Truncated Higher-Order Singular Value
Decomposition), surrogate modeling (Gaussian Process), and data assimilation
techniques (Reduced Order Particle Filter). This approach can be used online
and is based on offline data, obtained from the offline collection of
high-fidelity simulation data and a set of sparse experimental data. The
results have shown that orders of magnitude reduction in computational time can
be obtained from the proposed data-driven modeling approach compared with
physics-based high-fidelity simulations. With only simulation data as input,
the data-driven prediction technique can generate predictions that have
comparable accuracy as simulations. With both simulation data and sparse
physical experimental measurement as input, the data-driven approach with its
embedded data assimilation techniques has the potential in outperforming only
high-fidelity simulations for the long-horizon predictions. In addition, it is
demonstrated that the data-driven modeling approach can also reproduce the
scaling relationship recovered by physics-based simulations for maximum
resistive forces, which may indicate its general predictability beyond a
case-by-case basis. The results are expected to help robot navigation and
exploration in unknown and complex terrains during both online and offline
phases.

</details>


### [20] [Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies](https://arxiv.org/abs/2506.10884)
*Dong Hae Mangalindan,Karthik Kandikonda,Ericka Rovira,Vaibhav Srivastava*

Main category: cs.RO

TL;DR: 研究探讨了机器人性能与信任修复策略对人类信任的影响，发现长解释最能修复信任，而否认最能防止信任流失。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统效率与可靠性提升，研究其在机器人辅助交付任务中如何影响人类信任。

Method: 使用输入-输出隐马尔可夫模型（IOHMM）建模人类行为，分析信任动态与行动概率。

Result: 高信任时人类更倾向自主部署机器人；长解释修复信任效果最佳，否认防止信任流失最有效。

Conclusion: 模型信任估计与自报告值一致，为实时调整人类对自主系统信任的最优策略奠定基础。

Abstract: With increasing efficiency and reliability, autonomous systems are becoming
valuable assistants to humans in various tasks. In the context of
robot-assisted delivery, we investigate how robot performance and trust repair
strategies impact human trust. In this task, while handling a secondary task,
humans can choose to either send the robot to deliver autonomously or manually
control it. The trust repair strategies examined include short and long
explanations, apology and promise, and denial.
  Using data from human participants, we model human behavior using an
Input-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and
human action probabilities. Our findings indicate that humans are more likely
to deploy the robot autonomously when their trust is high. Furthermore, state
transition estimates show that long explanations are the most effective at
repairing trust following a failure, while denial is most effective at
preventing trust loss.
  We also demonstrate that the trust estimates generated by our model are
isomorphic to self-reported trust values, making them interpretable. This model
lays the groundwork for developing optimal policies that facilitate real-time
adjustment of human trust in autonomous systems.

</details>


### [21] [Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations](https://arxiv.org/abs/2506.10923)
*Xili Yi,Nima Fazeli*

Main category: cs.RO

TL;DR: Vib2Move是一种利用指尖微振动和重力精确重新定位平面物体的新方法，通过动态调节摩擦系数和协调运动规划，实现高精度操作。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种能够精确重新定位平面物体的方法，利用振动和重力实现高效操作。

Method: 方法包括设计振动执行器、推导滑动运动模型以及提出协调运动规划器。

Result: 实验结果显示，Vib2Move的最终定位误差低于6毫米，适用于多种平面物体。

Conclusion: 结论表明，Vib2Move是一种可靠且高精度的操作方法，适用于平面物体的重新配置。

Abstract: We introduce Vib2Move, a novel approach for in-hand object reconfiguration
that uses fingertip micro-vibrations and gravity to precisely reposition planar
objects. Our framework comprises three key innovations. First, we design a
vibration-based actuator that dynamically modulates the effective finger-object
friction coefficient, effectively emulating changes in gripping force. Second,
we derive a sliding motion model for objects clamped in a parallel gripper with
two symmetric, variable-friction contact patches. Third, we propose a motion
planner that coordinates end-effector finger trajectories and fingertip
vibrations to achieve the desired object pose. In real-world trials, Vib2Move
consistently yields final positioning errors below 6 mm, demonstrating
reliable, high-precision manipulation across a variety of planar objects. For
more results and information, please visit https://vib2move.github.io.

</details>


### [22] [GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation](https://arxiv.org/abs/2506.10966)
*Ning Gao,Yilun Chen,Shuai Yang,Xinyi Chen,Yang Tian,Hao Li,Haifeng Huang,Hanqing Wang,Tai Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: GenManip是一个面向策略泛化研究的仿真平台，通过LLM驱动的任务场景图生成多样化任务，并引入GenManip-Bench基准评估策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有仿真平台在支持策略适应多样化指令和场景方面不足，无法满足对指令跟随基础模型（如LLMs）适应性研究的兴趣。

Method: 提出GenManip平台，利用LLM驱动的任务场景图自动生成大规模多样化任务，并开发GenManip-Bench基准进行系统评估。

Result: 模块化系统结合基础模型在多样化场景中泛化效果优于端到端策略，数据扩展对后者有帮助但有限。

Conclusion: GenManip平台有望为现实条件下策略泛化研究提供关键支持。

Abstract: Robotic manipulation in real-world settings remains challenging, especially
regarding robust generalization. Existing simulation platforms lack sufficient
support for exploring how policies adapt to varied instructions and scenarios.
Thus, they lag behind the growing interest in instruction-following foundation
models like LLMs, whose adaptability is crucial yet remains underexplored in
fair comparisons. To bridge this gap, we introduce GenManip, a realistic
tabletop simulation platform tailored for policy generalization studies. It
features an automatic pipeline via LLM-driven task-oriented scene graph to
synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To
systematically assess generalization, we present GenManip-Bench, a benchmark of
200 scenarios refined via human-in-the-loop corrections. We evaluate two policy
types: (1) modular manipulation systems integrating foundation models for
perception, reasoning, and planning, and (2) end-to-end policies trained
through scalable data collection. Results show that while data scaling benefits
end-to-end methods, modular systems enhanced with foundation models generalize
more effectively across diverse scenarios. We anticipate this platform to
facilitate critical insights for advancing policy generalization in realistic
conditions. Project Page: https://genmanip.axi404.top/.

</details>


### [23] [Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop](https://arxiv.org/abs/2506.10968)
*Justin Kerr,Kush Hari,Ethan Weber,Chung Min Kim,Brent Yi,Tyler Bonnen,Ken Goldberg,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: EyeRobot是一个通过强化学习训练机械眼球实现任务导向凝视行为的机器人系统，结合手眼协调完成大范围工作空间操作任务。


<details>
  <summary>Details</summary>
Motivation: 人类通过主动观察来完成任务，受此启发，开发具有任务导向凝视行为的机器人系统。

Method: 通过360度摄像头收集遥操作数据，在仿真环境中训练凝视策略，结合行为克隆（BC）和强化学习（RL）联合训练手和眼。

Result: EyeRobot在手眼协调任务中表现出稳定的凝视行为和高效的操作能力，适用于大范围工作空间。

Conclusion: EyeRobot展示了任务导向凝视行为的有效性，为机器人操作提供了新的解决方案。

Abstract: Humans do not passively observe the visual world -- we actively look in order
to act. Motivated by this principle, we introduce EyeRobot, a robotic system
with gaze behavior that emerges from the need to complete real-world tasks. We
develop a mechanical eyeball that can freely rotate to observe its surroundings
and train a gaze policy to control it using reinforcement learning. We
accomplish this by first collecting teleoperated demonstrations paired with a
360 camera. This data is imported into a simulation environment that supports
rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze
on top of robot demonstrations. We then introduce a BC-RL loop to train the
hand and eye jointly: the hand (BC) agent is trained from rendered eye
observations, and the eye (RL) agent is rewarded when the hand produces correct
action predictions. In this way, hand-eye coordination emerges as the eye looks
towards regions which allow the hand to complete the task. EyeRobot implements
a foveal-inspired policy architecture allowing high resolution with a small
compute budget, which we find also leads to the emergence of more stable
fixation as well as improved ability to track objects and ignore distractors.
We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring
manipulation in an arc surrounding the robot arm. Our experiments suggest
EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate
manipulation over large workspaces with a single camera. See project site for
videos: https://www.eyerobot.net/

</details>
