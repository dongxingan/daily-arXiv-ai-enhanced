{"id": "2508.20457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20457", "abs": "https://arxiv.org/abs/2508.20457", "authors": ["Joonho Lee", "Yunho Kim", "Seokjoon Kim", "Quan Nguyen", "Youngjin Heo"], "title": "Learning Fast, Tool aware Collision Avoidance for Collaborative Robots", "comment": null, "summary": "Ensuring safe and efficient operation of collaborative robots in human\nenvironments is challenging, especially in dynamic settings where both obstacle\nmotion and tasks change over time. Current robot controllers typically assume\nfull visibility and fixed tools, which can lead to collisions or overly\nconservative behavior. In our work, we introduce a tool-aware collision\navoidance system that adjusts in real time to different tool sizes and modes of\ntool-environment interaction. Using a learned perception model, our system\nfilters out robot and tool components from the point cloud, reasons about\noccluded area, and predicts collision under partial observability. We then use\na control policy trained via constrained reinforcement learning to produce\nsmooth avoidance maneuvers in under 10 milliseconds. In simulated and\nreal-world tests, our approach outperforms traditional approaches (APF, MPPI)\nin dynamic environments, while maintaining sub-millimeter accuracy. Moreover,\nour system operates with approximately 60% lower computational cost compared to\na state-of-the-art GPU-based planner. Our approach provides modular, efficient,\nand effective collision avoidance for robots operating in dynamic environments.\nWe integrate our method into a collaborative robot application and demonstrate\nits practical use for safe and responsive operation.", "AI": {"tldr": "提出了一种工具感知的实时碰撞避免系统，能够在动态环境中适应不同工具尺寸和交互模式，通过感知模型处理部分可观测性，使用约束强化学习训练控制策略，在保持高精度的同时显著降低计算成本。", "motivation": "当前机器人控制器通常假设完全可见性和固定工具，这在动态环境中容易导致碰撞或过于保守的行为，需要开发能够实时适应工具变化和部分可观测性的碰撞避免系统。", "method": "使用学习感知模型从点云中过滤机器人和工具组件，推理遮挡区域，预测部分可观测下的碰撞风险，并通过约束强化学习训练控制策略生成平滑的避障动作。", "result": "在仿真和真实测试中优于传统方法（APF、MPPI），保持亚毫米级精度，计算成本比最先进的GPU规划器降低约60%，响应时间低于10毫秒。", "conclusion": "该方法为动态环境中的机器人操作提供了模块化、高效且有效的碰撞避免解决方案，成功集成到协作机器人应用中，实现了安全且响应迅速的操作。"}}
{"id": "2508.20547", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20547", "abs": "https://arxiv.org/abs/2508.20547", "authors": ["Yunpeng Mei", "Hongjie Cao", "Yinqiu Xia", "Wei Xiao", "Zhaohan Feng", "Gang Wang", "Jie Chen"], "title": "SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes", "comment": null, "summary": "Real-time interactive grasp synthesis for dynamic objects remains challenging\nas existing methods fail to achieve low-latency inference while maintaining\npromptability. To bridge this gap, we propose SPGrasp (spatiotemporal\nprompt-driven dynamic grasp synthesis), a novel framework extending segment\nanything model v2 (SAMv2) for video stream grasp estimation. Our core\ninnovation integrates user prompts with spatiotemporal context, enabling\nreal-time interaction with end-to-end latency as low as 59 ms while ensuring\ntemporal consistency for dynamic objects. In benchmark evaluations, SPGrasp\nachieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on\nJacquard. On the challenging GraspNet-1Billion dataset under continuous\ntracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,\nrepresenting a 58.5% reduction compared to the prior state-of-the-art\npromptable method RoG-SAM while maintaining competitive accuracy. Real-world\nexperiments involving 13 moving objects demonstrate a 94.8% success rate in\ninteractive grasping scenarios. These results confirm SPGrasp effectively\nresolves the latency-interactivity trade-off in dynamic grasp synthesis. Code\nis available at https://github.com/sejmoonwei/SPGrasp.", "AI": {"tldr": "SPGrasp是一个基于SAMv2的实时动态抓取合成框架，通过整合用户提示和时空上下文，实现了59ms的低延迟推理，在多个基准测试中达到90%以上的抓取准确率，比现有方法延迟降低58.5%。", "motivation": "解决现有动态物体实时交互抓取合成方法无法同时实现低延迟推理和提示性的问题，弥合延迟与交互性之间的权衡差距。", "method": "扩展SAMv2模型用于视频流抓取估计，核心创新是将用户提示与时空上下文整合，确保动态物体的时间一致性，实现端到端的实时交互。", "result": "在OCID上达到90.6%的实例级抓取准确率，Jacquard上93.8%，GraspNet-1Billion连续跟踪下92.0%准确率，每帧延迟73.1ms，比RoG-SAM延迟降低58.5%。真实世界实验中13个移动物体的交互抓取成功率达到94.8%。", "conclusion": "SPGrasp有效解决了动态抓取合成中的延迟-交互性权衡问题，实现了实时低延迟的交互式抓取合成，代码已开源。"}}
{"id": "2508.20561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20561", "abs": "https://arxiv.org/abs/2508.20561", "authors": ["Kipp McAdam Freud", "Yijiong Lin", "Nathan F. Lepora"], "title": "SimShear: Sim-to-Real Shear-based Tactile Servoing", "comment": "2025 Conference on Robot Learning (CoRL)", "summary": "We present SimShear, a sim-to-real pipeline for tactile control that enables\nthe use of shear information without explicitly modeling shear dynamics in\nsimulation. Shear, arising from lateral movements across contact surfaces, is\ncritical for tasks involving dynamic object interactions but remains\nchallenging to simulate. To address this, we introduce shPix2pix, a\nshear-conditioned U-Net GAN that transforms simulated tactile images absent of\nshear, together with a vector encoding shear information, into realistic\nequivalents with shear deformations. This method outperforms baseline pix2pix\napproaches in simulating tactile images and in pose/shear prediction. We apply\nSimShear to two control tasks using a pair of low-cost desktop robotic arms\nequipped with a vision-based tactile sensor: (i) a tactile tracking task, where\na follower arm tracks a surface moved by a leader arm, and (ii) a collaborative\nco-lifting task, where both arms jointly hold an object while the leader\nfollows a prescribed trajectory. Our method maintains contact errors within 1\nto 2 mm across varied trajectories where shear sensing is essential, validating\nthe feasibility of sim-to-real shear modeling with rigid-body simulators and\nopening new directions for simulation in tactile robotics.", "AI": {"tldr": "SimShear是一个从仿真到现实的触觉控制管道，通过shPix2pix GAN将无剪切信息的仿真触觉图像转换为包含剪切变形的真实图像，成功应用于桌面机械臂的触觉跟踪和协作举升任务。", "motivation": "剪切力在动态物体交互任务中至关重要，但由于剪切动力学难以在仿真中准确建模，限制了仿真到现实触觉控制的应用。", "method": "提出shPix2pix（剪切条件U-Net GAN），将无剪切信息的仿真触觉图像与剪切编码向量结合，生成包含剪切变形的真实触觉图像。使用刚性体仿真器和低成本桌面机械臂进行验证。", "result": "在触觉图像仿真和姿态/剪切预测方面优于基线pix2pix方法。在触觉跟踪和协作举升任务中，接触误差控制在1-2毫米以内。", "conclusion": "该方法验证了使用刚性体仿真器进行仿真到现实剪切建模的可行性，为触觉机器人学的仿真研究开辟了新方向。"}}
{"id": "2508.20661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20661", "abs": "https://arxiv.org/abs/2508.20661", "authors": ["TianChen Huang", "Wei Gao", "Runchen Xu", "Shiwu Zhang"], "title": "Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking", "comment": null, "summary": "Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer.", "AI": {"tldr": "基于物理基础的两阶段框架，结合XCoM/LIPM步代模板与轻量残差规划器，实现人形机器人在0.2米宽豆腐梁上的可靠穿行", "motivation": "解决人形机器人穿行窄梁时面临的挑战：接触点稀疏、安全关键性高，以及纯学习策略的脆弱性", "method": "1. 平坦地面训练阶段：轻量跟踪器通过对经验步代添加小随机扰动学习稳定接触排程和强壁随随核心跟踪能力\n2. 豆腐训练阶段：高层规划器预测摆动脚的体弧残差(Δx, Δy, Δψ)，精炼模板步代以优先确保安全准确落脚", "result": "在Unitree G1机器人上，系统可靠穿行0.2米宽、3米长的豆腐梁。在模拟和实际环境中，残差精炼方案在成功率、中心线遵循度和安全余量方面均超过单纯模板和单一基准方案", "conclusion": "结构化的步代接口实现了透明分析和低摩擦模拟到实际转移，该框架通过结合物理模型与学习残差的方式，提高了人形机器人在极窄环境下的稳定性和可靠性"}}
{"id": "2508.20664", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.20664", "abs": "https://arxiv.org/abs/2508.20664", "authors": ["Kan Chen", "Zhen Meng", "Xiangmin Xu", "Jiaming Yang", "Emma Li", "Philip G. Zhao"], "title": "Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse", "comment": "This paper has submitted to IEEE Transactions on Mobile Computing", "summary": "Real-time human-device interaction in industrial Metaverse faces challenges\nsuch as high computational load, limited bandwidth, and strict latency. This\npaper proposes a task-oriented edge-assisted cross-system framework using\ndigital twins (DTs) to enable responsive interactions. By predicting operator\nmotions, the system supports: 1) proactive Metaverse rendering for visual\nfeedback, and 2) preemptive control of remote devices. The DTs are decoupled\ninto two virtual functions-visual display and robotic control-optimizing both\nperformance and adaptability. To enhance generalizability, we introduce the\nHuman-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which\ndynamically adjusts prediction horizons. Evaluation on two tasks demonstrates\nthe framework's effectiveness: in a Trajectory-Based Drawing Control task, it\nreduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene\nrepresentation task for nuclear decommissioning, it achieves a PSNR of 22.11,\nSSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's\ncapability to ensure spatial precision and visual fidelity in real-time,\nhigh-risk industrial environments.", "AI": {"tldr": "这篇论文提出了一种基于数字偶生的任务导向边缘协助框架，通过预测操作员动作来实现工业元宇宙中的实时人机交互，在轨迹绘图和3D场景重现任务中都取得了显著性能提升。", "motivation": "解决工业元宇宙中实时人机交互遇到的高计算负荷、带宽限制和严格延迟要求等挑战，需要一种能够确保空间精度和视觉保真度的解决方案。", "method": "提出任务导向边缘协助跨系统框架，使用数字偶生技术，将数字偶生解耦为视觉显示和机器人控制两个虚拟功能。采用HITL-MAML算法动态调整预测边界，支持主动元宇宙渲染和预免式远程设备控制。", "result": "在轨迹绘图控制任务中，将权重RMSE从0.0712米降低到0.0101米；在核取消除实时3D场景重现任务中，达到PSNR 22.11、SSIM 0.8729和LPIPS 0.1298的优异性能。", "conclusion": "该框架能够在实时、高风险工业环境中确保空间精度和视觉保真度，为工业元宇宙人机交互提供了高效解决方案。"}}
{"id": "2508.20740", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20740", "abs": "https://arxiv.org/abs/2508.20740", "authors": ["Yuki Tanaka", "Seiichiro Katsura"], "title": "Non-expert to Expert Motion Translation Using Generative Adversarial Networks", "comment": null, "summary": "Decreasing skilled workers is a very serious problem in the world. To deal\nwith this problem, the skill transfer from experts to robots has been\nresearched. These methods which teach robots by human motion are called\nimitation learning. Experts' skills generally appear in not only position data,\nbut also force data. Thus, position and force data need to be saved and\nreproduced. To realize this, a lot of research has been conducted in the\nframework of a motion-copying system. Recent research uses machine learning\nmethods to generate motion commands. However, most of them could not change\ntasks by following human intention. Some of them can change tasks by\nconditional training, but the labels are limited. Thus, we propose the flexible\nmotion translation method by using Generative Adversarial Networks. The\nproposed method enables users to teach robots tasks by inputting data, and\nskills by a trained model. We evaluated the proposed system with a 3-DOF\ncalligraphy robot.", "AI": {"tldr": "通过生成对抗网络实现灵活的运动翻译方法，解决传统份丝学习方法无法根据人类意图切换任务的问题", "motivation": "应对技能工人减少的全球急诫问题，需要将专家技能传输给机器人。传统份丝学习方法存在任务切换不灵活、标签限制等问题", "method": "使用生成对抗网络（GAN）实现灵活的运动翻译方法，允许用户通过输入数据教掌任务，通过训练模型传递技能", "result": "在3-DOF书法机器人上进行了评估，验证了所提方法的可行性", "conclusion": "提出的方法能够有效实现份丝学习中的灵活任务切换，为机器人技能传输提供了新的解决方案"}}
{"id": "2508.20688", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20688", "abs": "https://arxiv.org/abs/2508.20688", "authors": ["Thanh Thi Nguyen", "Quoc Viet Hung Nguyen", "Jonathan Kua", "Imran Razzak", "Dung Nguyen", "Saeid Nahavandi"], "title": "Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning", "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "summary": "Enabling multiple autonomous machines to perform reliably requires the\ndevelopment of efficient cooperative control algorithms. This paper presents a\nsurvey of algorithms that have been developed for controlling and coordinating\nautonomous machines in complex environments. We especially focus on task\nallocation methods using computational intelligence (CI) and deep reinforcement\nlearning (RL). The advantages and disadvantages of the surveyed methods are\nanalysed thoroughly. We also propose and discuss in detail various future\nresearch directions that shed light on how to improve existing algorithms or\ncreate new methods to enhance the employability and performance of autonomous\nmachines in real-world applications. The findings indicate that CI and deep RL\nmethods provide viable approaches to addressing complex task allocation\nproblems in dynamic and uncertain environments. The recent development of deep\nRL has greatly contributed to the literature on controlling and coordinating\nautonomous machines, and it has become a growing trend in this area. It is\nenvisaged that this paper will provide researchers and engineers with a\ncomprehensive overview of progress in machine learning research related to\nautonomous machines. It also highlights underexplored areas, identifies\nemerging methodologies, and suggests new avenues for exploration in future\nresearch within this domain.", "AI": {"tldr": "这篇论文对自主机器协同控制算法进行了综述性调研，重点分析了计算智能和深度强化学习在任务分配中的应用。", "motivation": "为了提高多自主机器系统的可靠性和性能，需要开发高效的协同控制算法。", "method": "调研分析了使用计算智能(CI)和深度强化学习(RL)的任务分配方法，详细评估了各种方法的优缺点。", "result": "研究发现CI和深度RL方法能有效解决动态不确定环境中的复杂任务分配问题，深度RL成为该领域的发展趋势。", "conclusion": "论文为研究人员和工程师提供了全面的概览，指出了未充分探索的领域，识别了新兴方法论，并建议了未来研究的新方向。"}}
{"id": "2508.20812", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20812", "abs": "https://arxiv.org/abs/2508.20812", "authors": ["Lorenzo Busellato", "Federico Cunico", "Diego Dall'Alba", "Marco Emporio", "Andrea Giachetti", "Riccardo Muradore", "Marco Cristani"], "title": "Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting", "comment": null, "summary": "To enable flexible, high-throughput automation in settings where people and\nrobots share workspaces, collaborative robotic cells must reconcile stringent\nsafety guarantees with the need for responsive and effective behavior. A\ndynamic obstacle is the stochastic, task-dependent variability of human motion:\nwhen robots fall back on purely reactive or worst-case envelopes, they brake\nunnecessarily, stall task progress, and tamper with the fluidity that true\nHuman-Robot Interaction demands. In recent years, learning-based human-motion\nprediction has rapidly advanced, although most approaches produce worst-case\nscenario forecasts that often do not treat prediction uncertainty in a\nwell-structured way, resulting in over-conservative planning algorithms,\nlimiting their flexibility. We introduce Uncertainty-Aware Predictive Control\nBarrier Functions (UA-PCBFs), a unified framework that fuses probabilistic\nhuman hand motion forecasting with the formal safety guarantees of Control\nBarrier Functions. In contrast to other variants, our framework allows for\ndynamic adjustment of the safety margin thanks to the human motion uncertainty\nestimation provided by a forecasting module. Thanks to uncertainty estimation,\nUA-PCBFs empower collaborative robots with a deeper understanding of future\nhuman states, facilitating more fluid and intelligent interactions through\ninformed motion planning. We validate UA-PCBFs through comprehensive real-world\nexperiments with an increasing level of realism, including automated setups (to\nperform exactly repeatable motions) with a robotic hand and direct human-robot\ninteractions (to validate promptness, usability, and human confidence).\nRelative to state-of-the-art HRI architectures, UA-PCBFs show better\nperformance in task-critical metrics, significantly reducing the number of\nviolations of the robot's safe space during interaction with respect to the\nstate-of-the-art.", "AI": {"tldr": "提出了不确定性感知预测控制屏障函数(UA-PCBFs)框架，将概率性人体运动预测与控制屏障函数的形式化安全保证相结合，实现人机协作中更流畅智能的交互", "motivation": "解决人机协作环境中，传统方法因过度保守的安全策略导致机器人频繁制动、任务停滞的问题，需要平衡安全保证与响应性行为的需求", "method": "融合概率性人手运动预测与控制屏障函数，通过运动预测模块提供的不确定性估计动态调整安全边界，实现更智能的运动规划", "result": "在真实世界实验中，相比最先进的人机交互架构，UA-PCBFs在任务关键指标上表现更好，显著减少了机器人安全空间被侵犯的次数", "conclusion": "UA-PCBFs框架通过不确定性感知的预测控制，为人机协作提供了更流畅、智能且安全的交互方式，在保持形式化安全保证的同时提升了交互效率"}}
{"id": "2508.20831", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20831", "abs": "https://arxiv.org/abs/2508.20831", "authors": ["Rui Chen", "Domenico Chiaradia", "Antonio Frisoli", "Daniele Leonardis"], "title": "A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation", "comment": null, "summary": "This paper presents a novel fabric-based thermal-haptic interface for virtual\nreality and teleoperation. It integrates pneumatic actuation and conductive\nfabric with an innovative ultra-lightweight design, achieving only 2~g for each\nfinger unit. By embedding heating elements within textile pneumatic chambers,\nthe system delivers modulated pressure and thermal stimuli to fingerpads\nthrough a fully soft, wearable interface.\n  Comprehensive characterization demonstrates rapid thermal modulation with\nheating rates up to 3$^{\\circ}$C/s, enabling dynamic thermal feedback for\nvirtual or teleoperation interactions. The pneumatic subsystem generates forces\nup to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance\nenhances cooling efficiency with minimal force reduction. Experimental\nvalidation conducted with two different user studies shows high temperature\nidentification accuracy (0.98 overall) across three thermal levels, and\nsignificant manipulation improvements in a virtual pick-and-place tasks.\nResults show enhanced success rates (88.5\\% to 96.4\\%, p = 0.029) and improved\nforce control precision (p = 0.013) when haptic feedback is enabled, validating\nthe effectiveness of the integrated thermal-haptic approach for advanced\nhuman-machine interaction applications.", "AI": {"tldr": "本文提出了一种基于织物的新型热触觉接口，集成了气动驱动和导电织物，单个手指单元仅重2克，能够提供压力刺激和热刺激。", "motivation": "为虚拟现实和遥操作开发轻量化、柔软可穿戴的热触觉反馈接口，提升人机交互体验。", "method": "采用气动驱动和导电织物技术，将加热元件嵌入纺织气动腔室中，通过优化指垫-执行器间隙提高冷却效率。", "result": "实现了快速热调制（加热速率达3°C/s），产生高达8.93N的力，温度识别准确率达0.98，虚拟操作任务成功率从88.5%提升至96.4%。", "conclusion": "集成的热触觉方法有效提升了人机交互性能，验证了该技术在高级人机交互应用中的有效性。"}}
{"id": "2508.20836", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.20836", "abs": "https://arxiv.org/abs/2508.20836", "authors": ["Ahmed A. Elgohary", "Rohan Palanikumar", "Sameh A. Eisa"], "title": "Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration", "comment": null, "summary": "In a recent effort, we successfully proposed a categorically novel approach\nto mimic the phenomenoa of hovering and source seeking by flapping insects and\nhummingbirds using a new extremum seeking control (ESC) approach. Said ESC\napproach was shown capable of characterizing the physics of hovering and source\nseeking by flapping systems, providing at the same time uniquely novel\nopportunity for a model-free, real-time biomimicry control design. In this\npaper, we experimentally test and verify, for the first time in the literature,\nthe potential of ESC in flapping robots to achieve model-free, real-time\ncontrolled hovering and source seeking. The results of this paper, while being\nrestricted to 1D, confirm the premise of introducing ESC as a natural control\nmethod and biomimicry mechanism to the field of flapping flight and robotics.", "AI": {"tldr": "本文首次实验验证了极值搜索控制(ESC)在扑翼机器人中实现无模型实时悬停和源追踪的潜力，证实了ESC作为扑翼飞行自然控制方法和仿生机制的有效性。", "motivation": "模仿昆虫和蜂鸟的悬停和源追踪现象，为扑翼系统提供无模型、实时的仿生控制设计机会。", "method": "采用新型极值搜索控制(ESC)方法，在1D环境下进行实验测试和验证。", "result": "实验结果证实了ESC作为扑翼飞行自然控制方法和仿生机制的前提，成功实现了模型无关的实时控制悬停和源追踪。", "conclusion": "ESC被证明是扑翼机器人和飞行控制领域中一种有效的自然控制方法和仿生机制，尽管目前仅限于1D验证，但为后续研究奠定了基础。"}}
{"id": "2508.20840", "categories": ["cs.RO", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.20840", "abs": "https://arxiv.org/abs/2508.20840", "authors": ["Qiao Sun", "Liujia Yang", "Wei Tang", "Wei Huang", "Kaixin Xu", "Yongchao Chen", "Mingyu Liu", "Jiange Yang", "Haoyi Zhu", "Yating Wang", "Tong He", "Yilun Chen", "Xili Dai", "Nanyang Ye", "Qinying Gu"], "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning", "comment": null, "summary": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence.", "AI": {"tldr": "提出Primitive Embodied World Models (PEWM)新范式，通过限制视频生成为固定短时程，解决具身数据稀缺问题，实现细粒度语言-动作对齐和高效长时程任务控制", "motivation": "基于视频生成的具身世界模型依赖大规模交互数据，但具身数据稀缺、收集困难和高维度特性限制了语言与动作的细粒度对齐，阻碍了长时程视频生成的发展", "method": "PEWM范式：1) 限制视频生成为固定短时程；2) 配备模块化视觉语言模型(VLM)规划器和起始-目标热图引导机制(SGG)；3) 利用视频模型的时空视觉先验和VLM的语义感知能力", "result": "实现了细粒度语言概念与机器人动作视觉表示的对齐，降低了学习复杂度，提高了数据收集效率，减少了推理延迟，支持原始级策略的组合泛化", "conclusion": "PEWM框架通过连接细粒度物理交互与高层推理，为可扩展、可解释和通用目的的具身智能开辟了新途径"}}
{"id": "2508.20871", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20871", "abs": "https://arxiv.org/abs/2508.20871", "authors": ["Liding Zhang", "Kuanqi Cai", "Zhenshan Bing", "Chaoqun Wang", "Alois Knoll"], "title": "Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic Programming Heuristics", "comment": null, "summary": "Optimal path planning involves finding a feasible state sequence between a\nstart and a goal that optimizes an objective. This process relies on heuristic\nfunctions to guide the search direction. While a robust function can improve\nsearch efficiency and solution quality, current methods often overlook\navailable environmental data and simplify the function structure due to the\ncomplexity of information relationships. This study introduces Genetic Informed\nTrees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a\nwider array of environmental data, such as repulsive forces from obstacles and\nthe dynamic importance of vertices, to refine heuristic functions for better\nguidance. Furthermore, we integrated reinforced genetic programming (RGP),\nwhich combines genetic programming with reward system feedback to mutate\ngenotype-generative heuristic functions for GIT*. RGP leverages a multitude of\ndata types, thereby improving computational efficiency and solution quality\nwithin a set timeframe. Comparative analyses demonstrate that GIT* surpasses\nexisting single-query, sampling-based planners in problems ranging from R^4 to\nR^16 and was tested on a real-world mobile manipulation task. A video\nshowcasing our experimental results is available at\nhttps://youtu.be/URjXbc_BiYg", "AI": {"tldr": "GIT*算法通过整合环境数据和强化遗传编程改进启发函数，在R^4到R^16的高维路径规划问题中超越现有方法", "motivation": "现有路径规划方法往往忽视可用环境数据并简化启发函数结构，限制了搜索效率和解决方案质量", "method": "提出遗传信息树(GIT*)算法，整合障碍物排斥力和顶点动态重要性等环境数据；结合强化遗传编程(RGP)来进化生成启发函数", "result": "GIT*在R^4到R^16维度问题中优于现有单查询采样规划器，并在真实移动操作任务中验证有效性", "conclusion": "通过充分利用环境数据和遗传编程优化启发函数，GIT*显著提高了高维路径规划的计算效率和解决方案质量"}}
{"id": "2508.20884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20884", "abs": "https://arxiv.org/abs/2508.20884", "authors": ["Liding Zhang", "Qiyang Zong", "Yu Zhang", "Zhenshan Bing", "Alois Knoll"], "title": "Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning", "comment": null, "summary": "Efficient motion planning algorithms are essential in robotics. Optimizing\nessential parameters, such as batch size and nearest neighbor selection in\nsampling-based methods, can enhance performance in the planning process.\nHowever, existing approaches often lack environmental adaptability. Inspired by\nthe method of the deep fuzzy neural networks, this work introduces\nLearning-based Informed Trees (LIT*), a sampling-based deep fuzzy\nlearning-based planner that dynamically adjusts batch size and nearest neighbor\nparameters to obstacle distributions in the configuration spaces. By encoding\nboth global and local ratios via valid and invalid states, LIT* differentiates\nbetween obstacle-sparse and obstacle-dense regions, leading to lower-cost paths\nand reduced computation time. Experimental results in high-dimensional spaces\ndemonstrate that LIT* achieves faster convergence and improved solution\nquality. It outperforms state-of-the-art single-query, sampling-based planners\nin environments ranging from R^8 to R^14 and is successfully validated on a\ndual-arm robot manipulation task. A video showcasing our experimental results\nis available at: https://youtu.be/NrNs9zebWWk", "AI": {"tldr": "LIT*是一种基于深度模糊学习的采样运动规划器，通过动态调整批处理大小和最近邻参数来适应障碍物分布，在8-14维空间中实现更快收敛和更优路径质量", "motivation": "现有运动规划算法缺乏环境适应性，无法根据障碍物分布动态调整关键参数，限制了在复杂环境中的性能表现", "method": "基于深度模糊神经网络，提出LIT*算法，通过编码全局和局部有效/无效状态比例来区分稀疏和密集障碍区域，动态调整批处理大小和最近邻选择参数", "result": "在高维空间(R^8到R^14)实验中，LIT*相比最先进的单查询采样规划器实现了更快收敛速度、更低成本路径和更短计算时间，并在双臂机器人操作任务中成功验证", "conclusion": "LIT*通过深度模糊学习方法实现了环境自适应的运动规划，显著提升了采样规划算法在高维复杂环境中的性能和效率"}}
{"id": "2508.20898", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20898", "abs": "https://arxiv.org/abs/2508.20898", "authors": ["Jiaxi Huang", "Yan Huang", "Yixian Zhao", "Wenchao Meng", "Jinming Xu"], "title": "CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems", "comment": "Accepted by IROS2025", "summary": "Collaborative learning enhances the performance and adaptability of\nmulti-robot systems in complex tasks but faces significant challenges due to\nhigh communication overhead and data heterogeneity inherent in multi-robot\ntasks. To this end, we propose CoCoL, a Communication efficient decentralized\nCollaborative Learning method tailored for multi-robot systems with\nheterogeneous local datasets. Leveraging a mirror descent framework, CoCoL\nachieves remarkable communication efficiency with approximate Newton-type\nupdates by capturing the similarity between objective functions of robots, and\nreduces computational costs through inexact sub-problem solutions. Furthermore,\nthe integration of a gradient tracking scheme ensures its robustness against\ndata heterogeneity. Experimental results on three representative multi robot\ncollaborative learning tasks show the superiority of the proposed CoCoL in\nsignificantly reducing both the number of communication rounds and total\nbandwidth consumption while maintaining state-of-the-art accuracy. These\nbenefits are particularly evident in challenging scenarios involving non-IID\n(non-independent and identically distributed) data distribution, streaming\ndata, and time-varying network topologies.", "AI": {"tldr": "CoCoL是一种面向多机器人系统的通信高效去中心化协作学习方法，通过镜像下降框架和近似牛顿更新显著减少通信开销，同时保持最先进精度", "motivation": "多机器人协作学习面临高通信开销和数据异构性的挑战，需要开发通信高效的分布式学习方法", "method": "基于镜像下降框架，采用近似牛顿型更新捕获机器人目标函数相似性，结合梯度跟踪方案增强对数据异构性的鲁棒性，通过不精确子问题解降低计算成本", "result": "在三个代表性多机器人协作学习任务中，CoCoL显著减少了通信轮数和总带宽消耗，同时在非IID数据分布、流式数据和时变网络拓扑等挑战性场景中表现出色", "conclusion": "CoCoL为多机器人系统提供了一种通信高效的协作学习解决方案，在保持高精度的同时大幅降低了通信需求，特别适用于现实世界中的复杂多机器人应用场景"}}
{"id": "2508.20899", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20899", "abs": "https://arxiv.org/abs/2508.20899", "authors": ["Liding Zhang", "Zeqi Li", "Kuanqi Cai", "Qian Huang", "Zhenshan Bing", "Alois Knoll"], "title": "Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments", "comment": null, "summary": "Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS", "AI": {"tldr": "提出GODHS框架，结合语言模型和分层搜索策略，在陌生环境中高效搜索目标物体", "motivation": "传统场景表示只能捕捉静态语义，缺乏可解释的上下文推理，限制了在完全陌生环境中指导物体搜索的能力", "method": "语言增强的分层导航框架，利用大语言模型推断场景语义，通过多层次决策层次指导搜索过程，结合启发式运动规划器生成探索路径", "result": "在Isaac Sim中的综合评估显示，GODHS相比传统非语义搜索策略具有更高的搜索效率", "conclusion": "该框架成功整合了语义感知和空间推理，为机器人在复杂非结构化环境中的物体搜索提供了有效解决方案"}}
{"id": "2508.20926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20926", "abs": "https://arxiv.org/abs/2508.20926", "authors": ["Gabriel Manuel Garcia", "Antoine Richard", "Miguel Olivares-Mendez"], "title": "PLUME: Procedural Layer Underground Modeling Engine", "comment": null, "summary": "As space exploration advances, underground environments are becoming\nincreasingly attractive due to their potential to provide shelter, easier\naccess to resources, and enhanced scientific opportunities. Although such\nenvironments exist on Earth, they are often not easily accessible and do not\naccurately represent the diversity of underground environments found throughout\nthe solar system. This paper presents PLUME, a procedural generation framework\naimed at easily creating 3D underground environments. Its flexible structure\nallows for the continuous enhancement of various underground features, aligning\nwith our expanding understanding of the solar system. The environments\ngenerated using PLUME can be used for AI training, evaluating robotics\nalgorithms, 3D rendering, and facilitating rapid iteration on developed\nexploration algorithms. In this paper, it is demonstrated that PLUME has been\nused along with a robotic simulator. PLUME is open source and has been released\non Github. https://github.com/Gabryss/P.L.U.M.E", "AI": {"tldr": "PLUME是一个程序化生成框架，用于创建3D地下环境，支持AI训练、机器人算法评估和太空探索算法迭代，已开源发布。", "motivation": "随着太空探索发展，地下环境因其提供庇护、资源获取和科研机会的潜力而备受关注，但地球上的地下环境难以准确模拟太阳系多样性。", "method": "开发了PLUME程序化生成框架，采用灵活结构可不断扩展地下特征，与太阳系认知发展保持同步。", "result": "PLUME已成功与机器人模拟器集成使用，能够生成用于多种用途的3D地下环境。", "conclusion": "PLUME框架为地下环境研究提供了有效的工具，支持太空探索相关技术的开发和测试，已开源促进社区使用和发展。"}}
{"id": "2508.20959", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20959", "abs": "https://arxiv.org/abs/2508.20959", "authors": ["Curtis C. Johnson", "Daniel Webb", "David Hill", "Marc D. Killpack"], "title": "Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing", "comment": "In submission to IEEE Sensors", "summary": "Scaling tactile sensing for robust whole-body manipulation is a significant\nchallenge, often limited by wiring complexity, data throughput, and system\nreliability. This paper presents a complete architecture designed to overcome\nthese barriers. Our approach pairs open-source, fabric-based sensors with\ncustom readout electronics that reduce signal crosstalk to less than 3.3%\nthrough hardware-based mitigation. Critically, we introduce a novel,\ndaisy-chained SPI bus topology that avoids the practical limitations of common\nwireless protocols and the prohibitive wiring complexity of USB hub-based\nsystems. This architecture streams synchronized data from over 8,000 taxels\nacross 1 square meter of sensing area at update rates exceeding 50 FPS,\nconfirming its suitability for real-time control. We validate the system's\nefficacy in a whole-body grasping task where, without feedback, the robot's\nopen-loop trajectory results in an uncontrolled application of force that\nslowly crushes a deformable cardboard box. With real-time tactile feedback, the\nrobot transforms this motion into a gentle, stable grasp, successfully\nmanipulating the object without causing structural damage. This work provides a\nrobust and well-characterized platform to enable future research in advanced\nwhole-body control and physical human-robot interaction.", "AI": {"tldr": "提出了一种可扩展的触觉传感架构，通过织物传感器和定制电子设备减少串扰，使用菊花链SPI总线拓扑实现8000多个触点的实时数据同步，验证了在全身抓取任务中的有效性。", "motivation": "解决触觉传感在全身操作中的扩展性挑战，包括布线复杂性、数据吞吐量和系统可靠性限制。", "method": "采用开源织物传感器配合定制读取电子设备，硬件层面减少信号串扰至3.3%以下；引入新颖的菊花链SPI总线拓扑，避免无线协议限制和USB集线器系统的复杂布线。", "result": "实现了超过8000个触点在1平方米传感区域上以超过50FPS的更新率同步数据流传输；在全身抓取任务中，实时触觉反馈使机器人能够实现轻柔稳定的抓取，避免对可变形物体造成结构损坏。", "conclusion": "该工作提供了一个稳健且充分表征的平台，为未来先进的全身控制和物理人机交互研究奠定了基础。"}}
{"id": "2508.20981", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20981", "abs": "https://arxiv.org/abs/2508.20981", "authors": ["Jiajie Li", "Boyang Sun", "Luca Di Giammarino", "Hermann Blum", "Marc Pollefeys"], "title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection", "comment": null, "summary": "Reliable localization is critical for robot navigation, yet most existing\nsystems implicitly assume that all viewing directions at a location are equally\ninformative. In practice, localization becomes unreliable when the robot\nobserves unmapped, ambiguous, or uninformative regions. To address this, we\npresent ActLoc, an active viewpoint-aware planning framework for enhancing\nlocalization accuracy for general robot navigation tasks. At its core, ActLoc\nemploys a largescale trained attention-based model for viewpoint selection. The\nmodel encodes a metric map and the camera poses used during map construction,\nand predicts localization accuracy across yaw and pitch directions at arbitrary\n3D locations. These per-point accuracy distributions are incorporated into a\npath planner, enabling the robot to actively select camera orientations that\nmaximize localization robustness while respecting task and motion constraints.\nActLoc achieves stateof-the-art results on single-viewpoint selection and\ngeneralizes effectively to fulltrajectory planning. Its modular design makes it\nreadily applicable to diverse robot navigation and inspection tasks.", "AI": {"tldr": "ActLoc是一个主动视角感知规划框架，通过注意力模型预测不同视角的定位精度，指导机器人选择最优视角来提升定位可靠性", "motivation": "现有定位系统假设所有视角都同样有效，但实际上面对未映射、模糊或无信息区域时定位会不可靠，需要主动选择信息丰富的视角", "method": "使用大规模训练的注意力模型编码度量地图和建图时的相机位姿，预测任意3D位置在不同偏航和俯仰角度的定位精度分布，并将其整合到路径规划器中", "result": "在单视角选择和完整轨迹规划任务上都达到了最先进的性能，能够有效提升定位鲁棒性", "conclusion": "ActLoc的模块化设计使其适用于各种机器人导航和检测任务，通过主动视角选择显著提高了定位可靠性"}}
{"id": "2508.20982", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20982", "abs": "https://arxiv.org/abs/2508.20982", "authors": ["Junhao Gong", "Kit-Wa Sou", "Shoujie Li", "Changqing Guo", "Yan Huang", "Chuqiao Lyu", "Ziwu Song", "Wenbo Ding"], "title": "UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception", "comment": "Accepted to IROS 2025", "summary": "Visuotactile sensors provide high-resolution tactile information but are\nincapable of perceiving the material features of objects. We present UltraTac,\nan integrated sensor that combines visuotactile imaging with ultrasound sensing\nthrough a coaxial optoacoustic architecture. The design shares structural\ncomponents and achieves consistent sensing regions for both modalities.\nAdditionally, we incorporate acoustic matching into the traditional\nvisuotactile sensor structure, enabling integration of the ultrasound sensing\nmodality without compromising visuotactile performance. Through tactile\nfeedback, we dynamically adjust the operating state of the ultrasound module to\nachieve flexible functional coordination. Systematic experiments demonstrate\nthree key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),\nmaterial classification (average accuracy: 99.20%), and texture-material\ndual-mode object recognition achieving 92.11% accuracy on a 15-class task.\nFinally, we integrate the sensor into a robotic manipulation system to\nconcurrently detect container surface patterns and internal content, which\nverifies its potential for advanced human-machine interaction and precise\nrobotic manipulation.", "AI": {"tldr": "UltraTac是一种新型集成传感器，通过同轴光声架构将视觉触觉成像与超声波传感相结合，实现了接近感知、材料分类和纹理-材料双模式物体识别三大功能", "motivation": "传统视觉触觉传感器虽然能提供高分辨率触觉信息，但无法感知物体的材料特性，限制了其在机器人操作和人机交互中的应用", "method": "采用同轴光声架构设计，共享结构组件并实现一致的传感区域；在传统视觉触觉传感器结构中融入声学匹配，在不影响视觉触觉性能的前提下集成超声波传感模块；通过触觉反馈动态调整超声波模块工作状态", "result": "实现了3-8厘米范围内的接近感知（R²=0.90）、材料分类（平均准确率99.20%）以及纹理-材料双模式物体识别（15类任务准确率92.11%）；成功集成到机器人操作系统中，能同时检测容器表面图案和内部内容", "conclusion": "UltraTac传感器展示了在高级人机交互和精确机器人操作中的潜力，通过多模态传感融合解决了传统传感器的局限性"}}
{"id": "2508.21007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21007", "abs": "https://arxiv.org/abs/2508.21007", "authors": ["Mateusz Jaszczuk", "Nadia Figueroa"], "title": "Rapid Mismatch Estimation via Neural Network Informed Variational Inference", "comment": "Accepted at 9th Annual Conference on Robot Learning. Project Website\n  - https://mateusz-jaszczuk.github.io/rme/", "summary": "With robots increasingly operating in human-centric environments, ensuring\nsoft and safe physical interactions, whether with humans, surroundings, or\nother machines, is essential. While compliant hardware can facilitate such\ninteractions, this work focuses on impedance controllers that allow\ntorque-controlled robots to safely and passively respond to contact while\naccurately executing tasks. From inverse dynamics to quadratic\nprogramming-based controllers, the effectiveness of these methods relies on\naccurate dynamics models of the robot and the object it manipulates. Any model\nmismatch results in task failures and unsafe behaviors. Thus, we introduce\nRapid Mismatch Estimation (RME), an adaptive, controller-agnostic,\nprobabilistic framework that estimates end-effector dynamics mismatches online,\nwithout relying on external force-torque sensors. From the robot's\nproprioceptive feedback, a Neural Network Model Mismatch Estimator generates a\nprior for a Variational Inference solver, which rapidly converges to the\nunknown parameters while quantifying uncertainty. With a real 7-DoF manipulator\ndriven by a state-of-the-art passive impedance controller, RME adapts to sudden\nchanges in mass and center of mass at the end-effector in $\\sim400$ ms, in\nstatic and dynamic settings. We demonstrate RME in a collaborative scenario\nwhere a human attaches an unknown basket to the robot's end-effector and\ndynamically adds/removes heavy items, showcasing fast and safe adaptation to\nchanging dynamics during physical interaction without any external sensory\nsystem.", "AI": {"tldr": "提出Rapid Mismatch Estimation (RME)框架，用于在线估计末端执行器动力学失配，无需外部力传感器，实现快速安全的动力学适应", "motivation": "随着机器人在人机环境中的广泛应用，需要确保柔软安全的物理交互。现有阻抗控制方法依赖精确的动力学模型，模型失配会导致任务失败和不安全行为", "method": "基于神经网络模型失配估计器和变分推断求解器的概率框架，从本体感受反馈中快速估计未知参数并量化不确定性", "result": "在7自由度机械臂上，RME能在约400毫秒内适应末端执行器质量和质心的突然变化，在静态和动态场景中均有效", "conclusion": "RME框架能够实现快速安全的动力学适应，在协作场景中展示了对变化动力学的快速适应能力，无需外部传感系统"}}
{"id": "2508.21043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21043", "abs": "https://arxiv.org/abs/2508.21043", "authors": ["Zhi Su", "Bike Zhang", "Nima Rahmanian", "Yuman Gao", "Qiayuan Liao", "Caitlin Regan", "Koushil Sreenath", "S. Shankar Sastry"], "title": "HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning", "comment": "8 pages, 7 figures", "summary": "Humanoid robots have recently achieved impressive progress in locomotion and\nwhole-body control, yet they remain constrained in tasks that demand rapid\ninteraction with dynamic environments through manipulation. Table tennis\nexemplifies such a challenge: with ball speeds exceeding 5 m/s, players must\nperceive, predict, and act within sub-second reaction times, requiring both\nagility and precision. To address this, we present a hierarchical framework for\nhumanoid table tennis that integrates a model-based planner for ball trajectory\nprediction and racket target planning with a reinforcement learning-based\nwhole-body controller. The planner determines striking position, velocity and\ntiming, while the controller generates coordinated arm and leg motions that\nmimic human strikes and maintain stability and agility across consecutive\nrallies. Moreover, to encourage natural movements, human motion references are\nincorporated during training. We validate our system on a general-purpose\nhumanoid robot, achieving up to 106 consecutive shots with a human opponent and\nsustained exchanges against another humanoid. These results demonstrate\nreal-world humanoid table tennis with sub-second reactive control, marking a\nstep toward agile and interactive humanoid behaviors.", "AI": {"tldr": "提出了一种人形机器人打乒乓球的层次化框架，结合了基于模型的轨迹预测规划和强化学习全身控制，实现了亚秒级反应时间的连续击球能力", "motivation": "人形机器人在动态环境中的快速交互能力仍然受限，乒乓球运动需要亚秒级反应时间，同时要求敏捷性和精确性，这代表了人机交互的重要挑战", "method": "分层框架：基于模型的规划器进行球轨迹预测和球拍目标规划，强化学习全身控制器生成协调的臂腿运动，训练中融入人类运动参考以鼓励自然动作", "result": "在通用人形机器人上实现最多106次连续击球，能够与人类对手持续对打，并与其他机器人进行持续交换", "conclusion": "实现了现实世界中亚秒级反应控制的人形机器人乒乓球运动，朝着敏捷和交互式人形行为迈出了一步"}}
{"id": "2508.21063", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21063", "abs": "https://arxiv.org/abs/2508.21063", "authors": ["Ruixuan Liu", "Philip Huang", "Ava Pun", "Kangle Deng", "Shobhit Aggarwal", "Kevin Tang", "Michelle Liu", "Deva Ramanan", "Jun-Yan Zhu", "Jiaoyang Li", "Changliu Liu"], "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation", "comment": "12 pages, 10 figures, 2 tables", "summary": "Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.", "AI": {"tldr": "Prompt-to-Product是一个自动化流水线，能够从自然语言提示生成现实世界的组装产品，使用乐高积木作为组装平台，通过双手机器人系统自动构建实体产品。", "motivation": "传统组装产品的创建需要大量人工努力和专业知识，包括设计组装方案和实际构建产品。本文旨在降低从创意想法创建组装产品的门槛和人工成本。", "method": "利用乐高积木作为组装平台，自动化创建积木组装结构的过程。系统根据用户设计要求生成物理可构建的积木设计，然后使用双手机器人系统构建实际的组装产品。", "result": "通过全面的用户研究表明，Prompt-to-Product显著降低了从想象创意创建组装产品的门槛，并减少了人工努力。", "conclusion": "该系统成功实现了将用户想象力转化为现实世界产品的自动化流程，为组装产品创建提供了更便捷的解决方案。"}}
{"id": "2508.21065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21065", "abs": "https://arxiv.org/abs/2508.21065", "authors": ["Jiahe Pan", "Jiaxu Xing", "Rudolf Reiter", "Yifan Zhai", "Elie Aljalbout", "Davide Scaramuzza"], "title": "Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation", "comment": null, "summary": "Learning control policies in simulation enables rapid, safe, and\ncost-effective development of advanced robotic capabilities. However,\ntransferring these policies to the real world remains difficult due to the\nsim-to-real gap, where unmodeled dynamics and environmental disturbances can\ndegrade policy performance. Existing approaches, such as domain randomization\nand Real2Sim2Real pipelines, can improve policy robustness, but either struggle\nunder out-of-distribution conditions or require costly offline retraining. In\nthis work, we approach these problems from a different perspective. Instead of\nrelying on diverse training conditions before deployment, we focus on rapidly\nadapting the learned policy in the real world in an online fashion. To achieve\nthis, we propose a novel online adaptive learning framework that unifies\nresidual dynamics learning with real-time policy adaptation inside a\ndifferentiable simulation. Starting from a simple dynamics model, our framework\nrefines the model continuously with real-world data to capture unmodeled\neffects and disturbances such as payload changes and wind. The refined dynamics\nmodel is embedded in a differentiable simulation framework, enabling gradient\nbackpropagation through the dynamics and thus rapid, sample-efficient policy\nupdates beyond the reach of classical RL methods like PPO. All components of\nour system are designed for rapid adaptation, enabling the policy to adjust to\nunseen disturbances within 5 seconds of training. We validate the approach on\nagile quadrotor control under various disturbances in both simulation and the\nreal world. Our framework reduces hovering error by up to 81% compared to\nL1-MPC and 55% compared to DATT, while also demonstrating robustness in\nvision-based control without explicit state estimation.", "AI": {"tldr": "这篇论文提出了一种在线适应性学习框架，通过统一残差动力学书和可微模拟来实现快速的实时策略适应，解决了模拟到实际转移中的动力学差异问题。", "motivation": "解决模拟到实际转移中的动力学差异问题，现有方法要么面临分布外条件挑战，要么需要程序重新训练。需要一种能够在实际环境中快速适应的方案。", "method": "提出了一种在线适应性学习框架，统一残差动力学书与可微模拟。从简单的动力学模型出发，使用实际数据持续精炼模型，并将精炼后的模型嵌入可微模拟框架中实现梯度传播。", "result": "在模拟和实际环境中验证了方案，在5秒内完成训练适应。与L1-MPC相比悬停错误减少81%，与DATT相比减少55%，同时在无显式状态估计的视觉基控制中也表现出稳健性。", "conclusion": "该框架通过在线适应学习有效解决了模拟到实际转移的挑战，能够快速适应未见干扰，为机器人控制提供了一种高效的解决方案。"}}
