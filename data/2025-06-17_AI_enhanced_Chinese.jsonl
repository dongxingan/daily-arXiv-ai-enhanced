{"id": "2506.12082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12082", "abs": "https://arxiv.org/abs/2506.12082", "authors": ["Harith S. Gallage", "Bailey F. De Sousa", "Benjamin I. Chesnik", "Chaikel G. Brownstein", "Anson Paul", "Ronghuai Qi"], "title": "Design and Development of a Robotic Transcatheter Delivery System for Aortic Valve Replacement", "comment": "1 page with 2 figures. This abstract has been accepted by the 2025\n  International Conference on Robotics and Automation (ICRA) Workshop on\n  Robot-Assisted Endovascular Interventions", "summary": "Minimally invasive transcatheter approaches are increasingly adopted for\naortic stenosis treatment, where optimal commissural and coronary alignment is\nimportant. Achieving precise alignment remains clinically challenging, even\nwith contemporary robotic transcatheter aortic valve replacement (TAVR)\ndevices, as this task is still performed manually. This paper proposes the\ndevelopment of a robotic transcatheter delivery system featuring an\nomnidirectional bending joint and an actuation system designed to enhance\npositional accuracy and precision in TAVR procedures. The preliminary\nexperimental results validate the functionality of this novel robotic system.", "AI": {"tldr": "提出了一种新型机器人导管输送系统，用于提高TAVR手术中的位置精度。", "motivation": "尽管现代机器人TAVR设备已广泛应用，但实现精确的瓣膜对齐仍依赖手动操作，存在临床挑战。", "method": "开发了一种具有全向弯曲关节和驱动系统的机器人导管输送系统。", "result": "初步实验结果验证了该系统的功能。", "conclusion": "该系统有望提升TAVR手术的精确性和准确性。"}}
{"id": "2506.12089", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12089", "abs": "https://arxiv.org/abs/2506.12089", "authors": ["Razan Ghzouli", "Atieh Hanna", "Endre Erös", "Rebekka Wohlrab"], "title": "Using Behavior Trees in Risk Assessment", "comment": "8 pages, 5 figures", "summary": "Cyber-physical production systems increasingly involve collaborative robotic\nmissions, requiring more demand for robust and safe missions. Industries rely\non risk assessments to identify potential failures and implement measures to\nmitigate their risks. Although it is recommended to conduct risk assessments\nearly in the design of robotic missions, the state of practice in the industry\nis different. Safety experts often struggle to completely understand robotics\nmissions at the early design stages of projects and to ensure that the output\nof risk assessments is adequately considered during implementation.\n  This paper presents a design science study that conceived a model-based\napproach for early risk assessment in a development-centric way. Our approach\nsupports risk assessment activities by using the behavior-tree model. We\nevaluated the approach together with five practitioners from four companies.\nOur findings highlight the potential of the behavior-tree model in supporting\nearly identification, visualisation, and bridging the gap between code\nimplementation and risk assessments' outputs. This approach is the first\nattempt to use the behavior-tree model to support risk assessment; thus, the\nfindings highlight the need for further development.", "AI": {"tldr": "本文提出了一种基于行为树模型的早期风险评估方法，旨在解决工业中安全专家在设计阶段难以完全理解机器人任务的问题。", "motivation": "工业中早期风险评估的实践与理论脱节，安全专家难以在设计阶段充分理解机器人任务并确保风险评估结果在实施中被考虑。", "method": "采用设计科学研究方法，提出基于行为树模型的早期风险评估方法，并与四家公司的五位从业者共同评估。", "result": "行为树模型支持早期风险识别、可视化，并弥合了代码实施与风险评估输出之间的差距。", "conclusion": "该方法是首次尝试使用行为树模型支持风险评估，结果表明其潜力，但需进一步开发。"}}
{"id": "2506.12095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12095", "abs": "https://arxiv.org/abs/2506.12095", "authors": ["Khang Nguyen", "An T. Le", "Jan Peters", "Minh Nhat Vu"], "title": "DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion", "comment": null, "summary": "Achieving robust robot learning for humanoid locomotion is a fundamental\nchallenge in model-based reinforcement learning (MBRL), where environmental\nstochasticity and randomness can hinder efficient exploration and learning\nstability. The environmental, so-called aleatoric, uncertainty can be amplified\nin high-dimensional action spaces with complex contact dynamics, and further\nentangled with epistemic uncertainty in the models during learning phases. In\nthis work, we propose DoublyAware, an uncertainty-aware extension of Temporal\nDifference Model Predictive Control (TD-MPC) that explicitly decomposes\nuncertainty into two disjoint interpretable components, i.e., planning and\npolicy uncertainties. To handle the planning uncertainty, DoublyAware employs\nconformal prediction to filter candidate trajectories using quantile-calibrated\nrisk bounds, ensuring statistical consistency and robustness against stochastic\ndynamics. Meanwhile, policy rollouts are leveraged as structured informative\npriors to support the learning phase with Group-Relative Policy Constraint\n(GRPC) optimizers that impose a group-based adaptive trust-region in the latent\naction space. This principled combination enables the robot agent to prioritize\nhigh-confidence, high-reward behavior while maintaining effective, targeted\nexploration under uncertainty. Evaluated on the HumanoidBench locomotion suite\nwith the Unitree 26-DoF H1-2 humanoid, DoublyAware demonstrates improved sample\nefficiency, accelerated convergence, and enhanced motion feasibility compared\nto RL baselines. Our simulation results emphasize the significance of\nstructured uncertainty modeling for data-efficient and reliable decision-making\nin TD-MPC-based humanoid locomotion learning.", "AI": {"tldr": "论文提出DoublyAware方法，通过分解不确定性和结合规划与策略优化，提升人形机器人运动的鲁棒性和学习效率。", "motivation": "解决模型强化学习中环境随机性和高维动作空间带来的不确定性挑战，提升学习稳定性和探索效率。", "method": "提出DoublyAware方法，分解不确定性为规划和策略两部分，使用conformal prediction和GRPC优化器分别处理。", "result": "在HumanoidBench测试中，DoublyAware表现出更高的样本效率、收敛速度和运动可行性。", "conclusion": "结构化不确定性建模对TD-MPC框架下人形机器人运动学习的数据效率和可靠性至关重要。"}}
{"id": "2506.12184", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12184", "abs": "https://arxiv.org/abs/2506.12184", "authors": ["Stanley Lewis", "Vishal Chandra", "Tom Gao", "Odest Chadwicke Jenkins"], "title": "SPLATART: Articulated Gaussian Splatting with Estimated Object Structure", "comment": "7 pages, Accepted to the 2025 RSS Workshop on Gaussian\n  Representations for Robot Autonomy. Contact: Stanley Lewis, stanlew@umich.edu", "summary": "Representing articulated objects remains a difficult problem within the field\nof robotics. Objects such as pliers, clamps, or cabinets require\nrepresentations that capture not only geometry and color information, but also\npart seperation, connectivity, and joint parametrization. Furthermore, learning\nthese representations becomes even more difficult with each additional degree\nof freedom. Complex articulated objects such as robot arms may have seven or\nmore degrees of freedom, and the depth of their kinematic tree may be notably\ngreater than the tools, drawers, and cabinets that are the typical subjects of\narticulated object research. To address these concerns, we introduce SPLATART -\na pipeline for learning Gaussian splat representations of articulated objects\nfrom posed images, of which a subset contains image space part segmentations.\nSPLATART disentangles the part separation task from the articulation estimation\ntask, allowing for post-facto determination of joint estimation and\nrepresentation of articulated objects with deeper kinematic trees than\npreviously exhibited. In this work, we present data on the SPLATART pipeline as\napplied to the syntheic Paris dataset objects, and qualitative results on a\nreal-world object under spare segmentation supervision. We additionally present\non articulated serial chain manipulators to demonstrate usage on deeper\nkinematic tree structures.", "AI": {"tldr": "SPLATART是一种从姿态图像中学习铰接对象高斯样条表示的管道，能够分离部件分割与关节估计任务，适用于更深的运动树结构。", "motivation": "铰接对象的表示在机器人领域仍然是一个难题，尤其是对于具有多自由度和复杂运动树的对象。", "method": "SPLATART管道通过从姿态图像中学习高斯样条表示，分离部件分割和关节估计任务。", "result": "在合成Paris数据集和真实世界对象上展示了SPLATART的效果，并验证了其在更深运动树结构上的适用性。", "conclusion": "SPLATART为解决复杂铰接对象的表示问题提供了一种有效方法，尤其适用于多自由度和深层运动树结构。"}}
{"id": "2506.12273", "categories": ["cs.RO", "cs.SY", "eess.SY", "93B30 (Primary), 93B35 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.12273", "abs": "https://arxiv.org/abs/2506.12273", "authors": ["Rongfei Li", "Francis Assadian"], "title": "Role of Uncertainty in Model Development and Control Design for a Manufacturing Process", "comment": "35 pages, 26 figures, Book Chapter. Published in: Role of Uncertainty\n  in Model Development and Control Design for a Manufacturing Process,\n  IntechOpen, 2022. For published version, see this http URL:\n  https://doi.org/10.5772/intechopen.104780", "summary": "The use of robotic technology has drastically increased in manufacturing in\nthe 21st century. But by utilizing their sensory cues, humans still outperform\nmachines, especially in the micro scale manufacturing, which requires\nhigh-precision robot manipulators. These sensory cues naturally compensate for\nhigh level of uncertainties that exist in the manufacturing environment.\nUncertainties in performing manufacturing tasks may come from measurement\nnoise, model inaccuracy, joint compliance (e.g., elasticity) etc. Although\nadvanced metrology sensors and high-precision microprocessors, which are\nutilized in nowadays robots, have compensated for many structural and dynamic\nerrors in robot positioning, but a well-designed control algorithm still works\nas a comparable and cheaper alternative to reduce uncertainties in automated\nmanufacturing. Our work illustrates that a multi-robot control system can\nreduce various uncertainties to a great amount.", "AI": {"tldr": "论文探讨了多机器人控制系统在减少制造环境中不确定性方面的有效性，尤其是在高精度微尺度制造中。", "motivation": "人类在微尺度制造中仍优于机器人，主要依赖感官线索补偿环境不确定性。机器人虽配备先进传感器和微处理器，但控制算法仍是一种经济高效的替代方案。", "method": "提出了一种多机器人控制系统，旨在减少制造任务中的不确定性。", "result": "研究表明，多机器人控制系统能显著减少多种不确定性。", "conclusion": "多机器人控制系统是减少制造环境中不确定性的有效且经济的方法。"}}
{"id": "2506.12239", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12239", "abs": "https://arxiv.org/abs/2506.12239", "authors": ["Jayjun Lee", "Nima Fazeli"], "title": "ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation", "comment": "Accepted to RSS 2025 | Project page:\n  https://jayjunlee.github.io/vitascope/", "summary": "Mastering dexterous, contact-rich object manipulation demands precise\nestimation of both in-hand object poses and external contact\nlocations$\\unicode{x2013}$tasks particularly challenging due to partial and\nnoisy observations. We present ViTaSCOPE: Visuo-Tactile Simultaneous Contact\nand Object Pose Estimation, an object-centric neural implicit representation\nthat fuses vision and high-resolution tactile feedback. By representing objects\nas signed distance fields and distributed tactile feedback as neural shear\nfields, ViTaSCOPE accurately localizes objects and registers extrinsic contacts\nonto their 3D geometry as contact fields. Our method enables seamless reasoning\nover complementary visuo-tactile cues by leveraging simulation for scalable\ntraining and zero-shot transfers to the real-world by bridging the sim-to-real\ngap. We evaluate our method through comprehensive simulated and real-world\nexperiments, demonstrating its capabilities in dexterous manipulation\nscenarios.", "AI": {"tldr": "ViTaSCOPE结合视觉与高分辨率触觉反馈，通过神经隐式表示实现物体姿态与外部接触位置的精确估计。", "motivation": "在部分和噪声观测下，精确估计物体姿态和接触位置是灵巧操作的挑战。", "method": "采用神经隐式表示，将物体建模为有符号距离场，触觉反馈建模为神经剪切场，融合视觉与触觉数据。", "result": "在仿真和真实实验中验证了方法的有效性，支持灵巧操作。", "conclusion": "ViTaSCOPE通过融合视觉与触觉反馈，解决了物体姿态与接触位置估计的难题。"}}
{"id": "2506.12314", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.12314", "abs": "https://arxiv.org/abs/2506.12314", "authors": ["Xiaoshuai Ma", "Haoxiang Qi", "Qingqing Li", "Haochen Xu", "Xuechao Chen", "Junyao Gao", "Zhangguo Yu", "Qiang Huang"], "title": "Explosive Output to Enhance Jumping Ability: A Variable Reduction Ratio Design Paradigm for Humanoid Robots Knee Joint", "comment": null, "summary": "Enhancing the explosive power output of the knee joints is critical for\nimproving the agility and obstacle-crossing capabilities of humanoid robots.\nHowever, a mismatch between the knee-to-center-of-mass (CoM) transmission ratio\nand jumping demands, coupled with motor performance degradation at high speeds,\nrestricts the duration of high-power output and limits jump performance. To\naddress these problems, this paper introduces a novel knee joint design\nparadigm employing a dynamically decreasing reduction ratio for explosive\noutput during jump. Analysis of motor output characteristics and knee\nkinematics during jumping inspired a coupling strategy in which the reduction\nratio gradually decreases as the joint extends. A high initial ratio rapidly\nincreases torque at jump initiation, while its gradual reduction minimizes\nmotor speed increments and power losses, thereby maintaining sustained\nhigh-power output. A compact and efficient linear actuator-driven guide-rod\nmechanism realizes this coupling strategy, supported by parameter optimization\nguided by explosive jump control strategies. Experimental validation\ndemonstrated a 63 cm vertical jump on a single-joint platform (a theoretical\nimprovement of 28.1\\% over the optimal fixed-ratio joints). Integrated into a\nhumanoid robot, the proposed design enabled a 1.1 m long jump, a 0.5 m vertical\njump, and a 0.5 m box jump.", "AI": {"tldr": "论文提出了一种新型膝关节设计，通过动态减小减速比来提升跳跃时的爆发力输出，显著提高了人形机器人的跳跃性能。", "motivation": "提升膝关节的爆发力输出对人形机器人的敏捷性和越障能力至关重要，但传统设计因减速比与跳跃需求不匹配以及电机高速性能下降而受限。", "method": "采用动态减小减速比的策略，通过线性执行器驱动的导杆机构实现，并结合参数优化和跳跃控制策略。", "result": "实验验证显示单关节平台垂直跳跃高度达63厘米（比固定减速比设计提升28.1%），集成到人形机器人后实现了1.1米远跳、0.5米高跳和0.5米箱跳。", "conclusion": "动态减速比设计有效解决了传统膝关节在跳跃中的性能限制，显著提升了人形机器人的爆发力和跳跃能力。"}}
{"id": "2506.12248", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12248", "abs": "https://arxiv.org/abs/2506.12248", "authors": ["Jennifer Grannen", "Siddharth Karamcheti", "Blake Wulfe", "Dorsa Sadigh"], "title": "ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration", "comment": "Accepted by IEEE Robotics and Automation Letters 2025", "summary": "Collaborative robots must quickly adapt to their partner's intent and\npreferences to proactively identify helpful actions. This is especially true in\nsituated settings where human partners can continually teach robots new\nhigh-level behaviors, visual concepts, and physical skills (e.g., through\ndemonstration), growing the robot's capabilities as the human-robot pair work\ntogether to accomplish diverse tasks. In this work, we argue that robots should\nbe able to infer their partner's goals from early interactions and use this\ninformation to proactively plan behaviors ahead of explicit instructions from\nthe user. Building from the strong commonsense priors and steerability of large\nlanguage models, we introduce ProVox (\"Proactive Voice\"), a novel framework\nthat enables robots to efficiently personalize and adapt to individual\ncollaborators. We design a meta-prompting protocol that empowers users to\ncommunicate their distinct preferences, intent, and expected robot behaviors\nahead of starting a physical interaction. ProVox then uses the personalized\nprompt to condition a proactive language model task planner that anticipates a\nuser's intent from the current interaction context and robot capabilities to\nsuggest helpful actions; in doing so, we alleviate user burden, minimizing the\namount of time partners spend explicitly instructing and supervising the robot.\nWe evaluate ProVox through user studies grounded in household manipulation\ntasks (e.g., assembling lunch bags) that measure the efficiency of the\ncollaboration, as well as features such as perceived helpfulness, ease of use,\nand reliability. Our analysis suggests that both meta-prompting and proactivity\nare critical, resulting in 38.7% faster task completion times and 31.9% less\nuser burden relative to non-active baselines. Supplementary material, code, and\nvideos can be found at https://provox-2025.github.io.", "AI": {"tldr": "ProVox框架通过个性化提示和主动语言模型任务规划，使机器人能快速推断用户意图并提前规划行为，减少用户负担，提升协作效率。", "motivation": "协作机器人需快速适应用户意图和偏好，尤其是在动态环境中，通过主动推断目标减少用户显式指令的需求。", "method": "提出ProVox框架，结合元提示协议和主动语言模型任务规划，从交互上下文中推断用户意图并建议行为。", "result": "用户研究表明，ProVox显著提升任务完成速度（38.7%）并减少用户负担（31.9%）。", "conclusion": "元提示和主动性是提升人机协作效率的关键，ProVox为动态环境中的个性化协作提供了有效解决方案。"}}
{"id": "2506.13019", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13019", "abs": "https://arxiv.org/abs/2506.13019", "authors": ["Jiachen Li", "Jian Chu", "Feiyang Zhao", "Shihao Li", "Wei Li", "Dongmei Chen"], "title": "Constrained Optimal Planning to Minimize Battery Degradation of Autonomous Mobile Robots", "comment": null, "summary": "This paper proposes an optimization framework that addresses both cycling\ndegradation and calendar aging of batteries for autonomous mobile robot (AMR)\nto minimize battery degradation while ensuring task completion. A rectangle\nmethod of piecewise linear approximation is employed to linearize the bilinear\noptimization problem. We conduct a case study to validate the efficiency of the\nproposed framework in achieving an optimal path planning for AMRs while\nreducing battery aging.", "AI": {"tldr": "提出了一种优化框架，用于减少自主移动机器人（AMR）电池的循环退化和日历老化，同时确保任务完成。", "motivation": "解决AMR电池的循环退化和日历老化问题，以延长电池寿命并确保任务完成。", "method": "采用分段线性近似的矩形方法，将双线性优化问题线性化。", "result": "通过案例研究验证了框架在实现AMR最优路径规划并减少电池老化方面的效率。", "conclusion": "该框架能有效优化AMR路径规划，减少电池老化。"}}
{"id": "2506.12261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12261", "abs": "https://arxiv.org/abs/2506.12261", "authors": ["Sreevishakh Vasudevan", "Som Sagar", "Ransalu Senanayake"], "title": "Strategic Vantage Selection for Learning Viewpoint-Agnostic Manipulation Policies", "comment": null, "summary": "Vision-based manipulation has shown remarkable success, achieving promising\nperformance across a range of tasks. However, these manipulation policies often\nfail to generalize beyond their training viewpoints, which is a persistent\nchallenge in achieving perspective-agnostic manipulation, especially in\nsettings where the camera is expected to move at runtime. Although collecting\ndata from many angles seems a natural solution, such a naive approach is both\nresource-intensive and degrades manipulation policy performance due to\nexcessive and unstructured visual diversity. This paper proposes Vantage, a\nframework that systematically identifies and integrates data from optimal\nperspectives to train robust, viewpoint-agnostic policies. By formulating\nviewpoint selection as a continuous optimization problem, we iteratively\nfine-tune policies on a few vantage points. Since we leverage Bayesian\noptimization to efficiently navigate the infinite space of potential camera\nconfigurations, we are able to balance exploration of novel views and\nexploitation of high-performing ones, thereby ensuring data collection from a\nminimal number of effective viewpoints. We empirically evaluate this framework\non diverse standard manipulation tasks using multiple policy learning methods,\ndemonstrating that fine-tuning with data from strategic camera placements\nyields substantial performance gains, achieving average improvements of up to\n46.19% when compared to fixed, random, or heuristic-based strategies.", "AI": {"tldr": "论文提出Vantage框架，通过优化视角选择训练鲁棒的视角无关操作策略，显著提升性能。", "motivation": "解决视觉操作策略因视角变化导致的泛化能力不足问题，避免数据收集的资源浪费。", "method": "利用贝叶斯优化选择最优视角，迭代微调策略，平衡探索与利用。", "result": "在多种任务中，Vantage框架平均性能提升46.19%，优于固定或随机策略。", "conclusion": "Vantage框架通过系统化视角选择，显著提升视角无关操作策略的性能和鲁棒性。"}}
{"id": "2506.13149", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13149", "abs": "https://arxiv.org/abs/2506.13149", "authors": ["Jaehong Oh"], "title": "Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots", "comment": null, "summary": "This paper presents SEGO (Semantic Graph Ontology), a cognitive mapping\narchitecture designed to integrate geometric perception, semantic reasoning,\nand explanation generation into a unified framework for human-centric\ncollaborative robotics. SEGO constructs dynamic cognitive scene graphs that\nrepresent not only the spatial configuration of the environment but also the\nsemantic relations and ontological consistency among detected objects. The\narchitecture seamlessly combines SLAM-based localization, deep-learning-based\nobject detection and tracking, and ontology-driven reasoning to enable\nreal-time, semantically coherent mapping.", "AI": {"tldr": "SEGO是一种认知映射架构，结合几何感知、语义推理和解释生成，用于人机协作机器人。", "motivation": "旨在为协作机器人提供一个统一的框架，整合环境的空间配置和语义关系。", "method": "结合SLAM定位、深度学习目标检测与跟踪，以及本体驱动推理，构建动态认知场景图。", "result": "实现了实时、语义一致的映射。", "conclusion": "SEGO为协作机器人提供了高效的语义理解和场景建模能力。"}}
{"id": "2506.13421", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13421", "abs": "https://arxiv.org/abs/2506.13421", "authors": ["Dongliang Zheng", "Yebin Wang", "Stefano Di Cairano", "Panagiotis Tsiotras"], "title": "Delayed Expansion AGT: Kinodynamic Planning with Application to Tractor-Trailer Parking", "comment": null, "summary": "Kinodynamic planning of articulated vehicles in cluttered environments faces\nadditional challenges arising from high-dimensional state space and complex\nsystem dynamics. Built upon [1],[2], this work proposes the DE-AGT algorithm\nthat grows a tree using pre-computed motion primitives (MPs) and A* heuristics.\nThe first feature of DE-AGT is a delayed expansion of MPs. In particular, the\nMPs are divided into different modes, which are ranked online. With the MP\nclassification and prioritization, DE-AGT expands the most promising mode of\nMPs first, which eliminates unnecessary computation and finds solutions faster.\nTo obtain the cost-to-go heuristic for nonholonomic articulated vehicles, we\nrely on supervised learning and train neural networks for fast and accurate\ncost-to-go prediction. The learned heuristic is used for online mode ranking\nand node selection. Another feature of DE-AGT is the improved goal-reaching.\nExactly reaching a goal state usually requires a constant connection checking\nwith the goal by solving steering problems -- non-trivial and time-consuming\nfor articulated vehicles. The proposed termination scheme overcomes this\nchallenge by tightly integrating a light-weight trajectory tracking controller\nwith the search process. DE-AGT is implemented for autonomous parking of a\ngeneral car-like tractor with 3-trailer. Simulation results show an average of\n10x acceleration compared to a previous method.", "AI": {"tldr": "DE-AGT算法通过预计算运动基元和A*启发式方法，解决了高维状态空间和复杂系统动力学下的运动规划问题，显著提升了计算效率。", "motivation": "针对高维状态空间和复杂动力学的运动规划问题，传统方法计算效率低，DE-AGT旨在通过优化运动基元扩展和启发式学习提升效率。", "method": "DE-AGT采用延迟扩展运动基元模式，结合监督学习训练神经网络预测启发式成本，并改进目标到达机制。", "result": "仿真结果显示，DE-AGT比之前方法平均加速10倍。", "conclusion": "DE-AGT通过优化运动基元扩展和启发式学习，显著提升了运动规划效率，适用于复杂环境中的车辆运动规划。"}}
{"id": "2506.12312", "categories": ["cs.RO", "cs.CL", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2506.12312", "abs": "https://arxiv.org/abs/2506.12312", "authors": ["Kan Hatakeyama-Sato", "Toshihiko Nishida", "Kenta Kitamura", "Yoshitaka Ushiku", "Koichi Takahashi", "Yuta Nabae", "Teruaki Hayakawa"], "title": "Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research", "comment": null, "summary": "This review explores the potential of foundation models to advance laboratory\nautomation in the materials and chemical sciences. It emphasizes the dual roles\nof these models: cognitive functions for experimental planning and data\nanalysis, and physical functions for hardware operations. While traditional\nlaboratory automation has relied heavily on specialized, rigid systems,\nfoundation models offer adaptability through their general-purpose intelligence\nand multimodal capabilities. Recent advancements have demonstrated the\nfeasibility of using large language models (LLMs) and multimodal robotic\nsystems to handle complex and dynamic laboratory tasks. However, significant\nchallenges remain, including precision manipulation of hardware, integration of\nmultimodal data, and ensuring operational safety. This paper outlines a roadmap\nhighlighting future directions, advocating for close interdisciplinary\ncollaboration, benchmark establishment, and strategic human-AI integration to\nrealize fully autonomous experimental laboratories.", "AI": {"tldr": "综述探讨了基础模型在材料和化学科学实验室自动化中的潜力，强调其认知和物理功能，并提出未来发展方向。", "motivation": "传统实验室自动化依赖专用系统，缺乏灵活性；基础模型通过通用智能和多模态能力提供适应性。", "method": "利用大型语言模型（LLMs）和多模态机器人系统处理复杂动态任务。", "result": "展示了基础模型在实验室自动化中的可行性，但仍面临硬件操作精度、多模态数据整合和安全等挑战。", "conclusion": "提出未来路线图，强调跨学科合作、基准建立和人机协同，以实现完全自主的实验室。"}}
{"id": "2506.13498", "categories": ["cs.RO", "cs.HC", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13498", "abs": "https://arxiv.org/abs/2506.13498", "authors": ["Toshiaki Tsuji", "Yasuhiro Kato", "Gokhan Solak", "Heng Zhang", "Tadej Petrič", "Francesco Nori", "Arash Ajoudani"], "title": "A Survey on Imitation Learning for Contact-Rich Tasks in Robotics", "comment": "47pages, 1 figures", "summary": "This paper comprehensively surveys research trends in imitation learning for\ncontact-rich robotic tasks. Contact-rich tasks, which require complex physical\ninteractions with the environment, represent a central challenge in robotics\ndue to their nonlinear dynamics and sensitivity to small positional deviations.\nThe paper examines demonstration collection methodologies, including teaching\nmethods and sensory modalities crucial for capturing subtle interaction\ndynamics. We then analyze imitation learning approaches, highlighting their\napplications to contact-rich manipulation. Recent advances in multimodal\nlearning and foundation models have significantly enhanced performance in\ncomplex contact tasks across industrial, household, and healthcare domains.\nThrough systematic organization of current research and identification of\nchallenges, this survey provides a foundation for future advancements in\ncontact-rich robotic manipulation.", "AI": {"tldr": "本文综述了模仿学习在接触密集型机器人任务中的研究趋势，分析了演示收集方法和学习方法的进展，并探讨了未来发展方向。", "motivation": "接触密集型任务因其非线性动态和对微小位置偏差的敏感性，是机器人领域的核心挑战，需要深入研究模仿学习在此类任务中的应用。", "method": "通过系统整理演示收集方法（如教学方法和感官模态）和模仿学习方法（如多模态学习和基础模型），分析其在接触密集型任务中的应用。", "result": "多模态学习和基础模型的进步显著提升了工业、家庭和医疗领域中复杂接触任务的性能。", "conclusion": "本文为未来接触密集型机器人操作的研究提供了系统性基础和挑战分析。"}}
{"id": "2506.12374", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.10; I.4.8; H.5.2"], "pdf": "https://arxiv.org/pdf/2506.12374", "abs": "https://arxiv.org/abs/2506.12374", "authors": ["Wenbo Li", "Shiyi Wang", "Yiteng Chen", "Huiping Zhuang", "Qingyao Wu"], "title": "AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making", "comment": "submitted to NeurIPS 2025", "summary": "Vision-Language Models (VLMs) encode knowledge and reasoning capabilities for\nrobotic manipulation within high-dimensional representation spaces. However,\ncurrent approaches often project them into compressed intermediate\nrepresentations, discarding important task-specific information such as\nfine-grained spatial or semantic details. To address this, we propose\nAntiGrounding, a new framework that reverses the instruction grounding process.\nIt lifts candidate actions directly into the VLM representation space, renders\ntrajectories from multiple views, and uses structured visual question answering\nfor instruction-based decision making. This enables zero-shot synthesis of\noptimal closed-loop robot trajectories for new tasks. We also propose an\noffline policy refinement module that leverages past experience to enhance\nlong-term performance. Experiments in both simulation and real-world\nenvironments show that our method outperforms baselines across diverse robotic\nmanipulation tasks.", "AI": {"tldr": "提出AntiGrounding框架，通过逆向指令接地过程，直接在VLM表示空间中生成候选动作，并利用多视角渲染和结构化视觉问答实现零样本任务合成。", "motivation": "当前方法将视觉语言模型（VLM）的高维表示压缩为中间表示，丢失了任务关键信息（如细粒度空间或语义细节）。", "method": "提出AntiGrounding框架，逆向指令接地过程，在VLM表示空间中生成动作，多视角渲染轨迹，结构化视觉问答辅助决策，并引入离线策略优化模块。", "result": "在仿真和真实环境中，该方法在多样化机器人操作任务中优于基线。", "conclusion": "AntiGrounding框架通过保留高维表示中的关键信息，实现了零样本任务合成和性能提升。"}}
{"id": "2506.12507", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12507", "abs": "https://arxiv.org/abs/2506.12507", "authors": ["Pablo Gonzalez-Oliveras", "Olov Engwall", "Ali Reza Majlesi"], "title": "Sense and Sensibility: What makes a social robot convincing to high-school students?", "comment": "14 pages; 8 figures; 3 tables; RSS 2025 (Robotics: Science & Systems)", "summary": "This study with 40 high-school students demonstrates the high influence of a\nsocial educational robot on students' decision-making for a set of eight\ntrue-false questions on electric circuits, for which the theory had been\ncovered in the students' courses. The robot argued for the correct answer on\nsix questions and the wrong on two, and 75% of the students were persuaded by\nthe robot to perform beyond their expected capacity, positively when the robot\nwas correct and negatively when it was wrong. Students with more experience of\nusing large language models were even more likely to be influenced by the\nrobot's stance -- in particular for the two easiest questions on which the\nrobot was wrong -- suggesting that familiarity with AI can increase\nsusceptibility to misinformation by AI.\n  We further examined how three different levels of portrayed robot certainty,\ndisplayed using semantics, prosody and facial signals, affected how the\nstudents aligned with the robot's answer on specific questions and how\nconvincing they perceived the robot to be on these questions. The students\naligned with the robot's answers in 94.4% of the cases when the robot was\nportrayed as Certain, 82.6% when it was Neutral and 71.4% when it was\nUncertain. The alignment was thus high for all conditions, highlighting\nstudents' general susceptibility to accept the robot's stance, but alignment in\nthe Uncertain condition was significantly lower than in the Certain. Post-test\nquestionnaire answers further show that students found the robot most\nconvincing when it was portrayed as Certain. These findings highlight the need\nfor educational robots to adjust their display of certainty based on the\nreliability of the information they convey, to promote students' critical\nthinking and reduce undue influence.", "AI": {"tldr": "研究表明，社交教育机器人对学生决策有显著影响，尤其是熟悉AI的学生更容易被误导。机器人表现出的确定性程度直接影响学生的接受度。", "motivation": "探讨社交教育机器人对学生决策的影响，以及机器人表现出的确定性如何改变学生的接受度和感知可信度。", "method": "40名高中生参与实验，机器人针对8道判断题提供答案（6对2错），并展示三种确定性水平（确定、中立、不确定）。", "result": "75%学生受机器人影响，94.4%在机器人确定时接受答案，71.4%在不确定时接受。熟悉AI的学生更容易被误导。", "conclusion": "教育机器人应根据信息可靠性调整确定性表现，以促进学生批判性思维并减少不当影响。"}}
{"id": "2506.12525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12525", "abs": "https://arxiv.org/abs/2506.12525", "authors": ["Peng Wang", "Minh Huy Pham", "Zhihao Guo", "Wei Zhou"], "title": "A Spatial Relationship Aware Dataset for Robotics", "comment": "7 pages; 7 figures, 1 table", "summary": "Robotic task planning in real-world environments requires not only object\nrecognition but also a nuanced understanding of spatial relationships between\nobjects. We present a spatial-relationship-aware dataset of nearly 1,000\nrobot-acquired indoor images, annotated with object attributes, positions, and\ndetailed spatial relationships. Captured using a Boston Dynamics Spot robot and\nlabelled with a custom annotation tool, the dataset reflects complex scenarios\nwith similar or identical objects and intricate spatial arrangements. We\nbenchmark six state-of-the-art scene-graph generation models on this dataset,\nanalysing their inference speed and relational accuracy. Our results highlight\nsignificant differences in model performance and demonstrate that integrating\nexplicit spatial relationships into foundation models, such as ChatGPT 4o,\nsubstantially improves their ability to generate executable, spatially-aware\nplans for robotics. The dataset and annotation tool are publicly available at\nhttps://github.com/PengPaulWang/SpatialAwareRobotDataset, supporting further\nresearch in spatial reasoning for robotics.", "AI": {"tldr": "论文提出了一种空间关系感知的数据集，用于机器人任务规划，并测试了六种先进场景图生成模型的性能，结果表明显式空间关系能显著提升模型生成空间感知计划的能力。", "motivation": "机器人任务规划需要理解物体间的空间关系，但现有数据集缺乏对复杂空间关系的详细标注。", "method": "使用Boston Dynamics Spot机器人采集近1000张室内图像，并通过自定义标注工具标注物体属性、位置和空间关系。", "result": "测试了六种场景图生成模型，发现显式空间关系能显著提升模型性能，尤其是生成可执行的空间感知计划。", "conclusion": "数据集和标注工具公开可用，支持机器人空间推理的进一步研究。"}}
{"id": "2506.12536", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12536", "abs": "https://arxiv.org/abs/2506.12536", "authors": ["Farida Mohsen", "Ali Safa"], "title": "Deep Fusion of Ultra-Low-Resolution Thermal Camera and Gyroscope Data for Lighting-Robust and Compute-Efficient Rotational Odometry", "comment": null, "summary": "Accurate rotational odometry is crucial for autonomous robotic systems,\nparticularly for small, power-constrained platforms such as drones and mobile\nrobots. This study introduces thermal-gyro fusion, a novel sensor fusion\napproach that integrates ultra-low-resolution thermal imaging with gyroscope\nreadings for rotational odometry. Unlike RGB cameras, thermal imaging is\ninvariant to lighting conditions and, when fused with gyroscopic data,\nmitigates drift which is a common limitation of inertial sensors. We first\ndevelop a multimodal data acquisition system to collect synchronized thermal\nand gyroscope data, along with rotational speed labels, across diverse\nenvironments. Subsequently, we design and train a lightweight Convolutional\nNeural Network (CNN) that fuses both modalities for rotational speed\nestimation. Our analysis demonstrates that thermal-gyro fusion enables a\nsignificant reduction in thermal camera resolution without significantly\ncompromising accuracy, thereby improving computational efficiency and memory\nutilization. These advantages make our approach well-suited for real-time\ndeployment in resource-constrained robotic systems. Finally, to facilitate\nfurther research, we publicly release our dataset as supplementary material.", "AI": {"tldr": "提出了一种新型的热成像与陀螺仪融合方法，用于旋转里程计，适用于资源受限的机器人系统。", "motivation": "精确的旋转里程计对小型、功率受限的机器人平台至关重要，而传统方法在光照变化和惯性传感器漂移方面存在局限。", "method": "开发了多模态数据采集系统，结合热成像和陀螺仪数据，并设计轻量级CNN进行融合估计旋转速度。", "result": "热成像-陀螺仪融合显著降低了热成像分辨率需求，同时保持精度，提高了计算效率和内存利用率。", "conclusion": "该方法适用于实时部署，并公开了数据集以促进进一步研究。"}}
{"id": "2506.12676", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12676", "abs": "https://arxiv.org/abs/2506.12676", "authors": ["Yingyi Kuang", "Luis J. Manso", "George Vogiatzis"], "title": "Goal-based Self-Adaptive Generative Adversarial Imitation Learning (Goal-SAGAIL) for Multi-goal Robotic Manipulation Tasks", "comment": "6 pages, 5 figures", "summary": "Reinforcement learning for multi-goal robot manipulation tasks poses\nsignificant challenges due to the diversity and complexity of the goal space.\nTechniques such as Hindsight Experience Replay (HER) have been introduced to\nimprove learning efficiency for such tasks. More recently, researchers have\ncombined HER with advanced imitation learning methods such as Generative\nAdversarial Imitation Learning (GAIL) to integrate demonstration data and\naccelerate training speed. However, demonstration data often fails to provide\nenough coverage for the goal space, especially when acquired from human\nteleoperation. This biases the learning-from-demonstration process toward\nmastering easier sub-tasks instead of tackling the more challenging ones. In\nthis work, we present Goal-based Self-Adaptive Generative Adversarial Imitation\nLearning (Goal-SAGAIL), a novel framework specifically designed for multi-goal\nrobot manipulation tasks. By integrating self-adaptive learning principles with\ngoal-conditioned GAIL, our approach enhances imitation learning efficiency,\neven when limited, suboptimal demonstrations are available. Experimental\nresults validate that our method significantly improves learning efficiency\nacross various multi-goal manipulation scenarios -- including complex in-hand\nmanipulation tasks -- using suboptimal demonstrations provided by both\nsimulation and human experts.", "AI": {"tldr": "提出了一种名为Goal-SAGAIL的新框架，结合自适应性学习和目标条件GAIL，提升多目标机器人操作任务的模仿学习效率。", "motivation": "多目标机器人操作任务的目标空间复杂多样，现有方法如HER和GAIL在演示数据不足时效果有限，尤其是人类远程操作提供的演示数据覆盖不足。", "method": "结合自适应性学习与目标条件GAIL，提出Goal-SAGAIL框架，优化模仿学习效率。", "result": "实验表明，该方法在多种多目标操作任务中显著提升学习效率，包括复杂的手内操作任务。", "conclusion": "Goal-SAGAIL框架有效解决了演示数据不足的问题，提升了多目标机器人操作任务的模仿学习效率。"}}
{"id": "2506.12678", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12678", "abs": "https://arxiv.org/abs/2506.12678", "authors": ["Pranay Gupta", "Henny Admoni", "Andrea Bajcsy"], "title": "Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence", "comment": "15 pages, 11 figures", "summary": "End-to-end visuomotor policies trained using behavior cloning have shown a\nremarkable ability to generate complex, multi-modal low-level robot behaviors.\nHowever, at deployment time, these policies still struggle to act reliably when\nfaced with out-of-distribution (OOD) visuals induced by objects, backgrounds,\nor environment changes. Prior works in interactive imitation learning solicit\ncorrective expert demonstrations under the OOD conditions -- but this can be\ncostly and inefficient. We observe that task success under OOD conditions does\nnot always warrant novel robot behaviors. In-distribution (ID) behaviors can\ndirectly be transferred to OOD conditions that share functional similarities\nwith ID conditions. For example, behaviors trained to interact with\nin-distribution (ID) pens can apply to interacting with a visually-OOD pencil.\nThe key challenge lies in disambiguating which ID observations functionally\ncorrespond to the OOD observation for the task at hand. We propose that an\nexpert can provide this OOD-to-ID functional correspondence. Thus, instead of\ncollecting new demonstrations and re-training at every OOD encounter, our\nmethod: (1) detects the need for feedback by first checking if current\nobservations are OOD and then identifying whether the most similar training\nobservations show divergent behaviors, (2) solicits functional correspondence\nfeedback to disambiguate between those behaviors, and (3) intervenes on the OOD\nobservations with the functionally corresponding ID observations to perform\ndeployment-time generalization. We validate our method across diverse\nreal-world robotic manipulation tasks with a Franka Panda robotic manipulator.\nOur results show that test-time functional correspondences can improve the\ngeneralization of a vision-based diffusion policy to OOD objects and\nenvironment conditions with low feedback.", "AI": {"tldr": "论文提出了一种通过专家反馈实现端到端视觉运动策略在分布外（OOD）条件下泛化的方法，避免了昂贵的重新训练。", "motivation": "现有行为克隆训练的端到端策略在OOD条件下表现不佳，而传统的交互式模仿学习需要大量专家纠正演示，成本高且低效。", "method": "方法包括检测OOD观察、获取专家功能对应反馈，并用对应ID观察干预OOD观察以实现泛化。", "result": "实验证明，该方法能显著提升视觉扩散策略在OOD条件下的泛化能力，且反馈成本低。", "conclusion": "通过功能对应反馈，无需重新训练即可实现OOD条件下的高效泛化。"}}
{"id": "2506.12710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12710", "abs": "https://arxiv.org/abs/2506.12710", "authors": ["Yuqi Ping", "Tianhao Liang", "Huahao Ding", "Guangyu Lei", "Junwei Wu", "Xuan Zou", "Kuan Shi", "Rui Shao", "Chiya Zhang", "Weizheng Zhang", "Weijie Yuan", "Tingting Zhang"], "title": "Multimodal Large Language Models-Enabled UAV Swarm: Towards Efficient and Intelligent Autonomous Aerial Systems", "comment": "8 pages, 5 figures,submitted to IEEE wcm", "summary": "Recent breakthroughs in multimodal large language models (MLLMs) have endowed\nAI systems with unified perception, reasoning and natural-language interaction\nacross text, image and video streams. Meanwhile, Unmanned Aerial Vehicle (UAV)\nswarms are increasingly deployed in dynamic, safety-critical missions that\ndemand rapid situational understanding and autonomous adaptation. This paper\nexplores potential solutions for integrating MLLMs with UAV swarms to enhance\nthe intelligence and adaptability across diverse tasks. Specifically, we first\noutline the fundamental architectures and functions of UAVs and MLLMs. Then, we\nanalyze how MLLMs can enhance the UAV system performance in terms of target\ndetection, autonomous navigation, and multi-agent coordination, while exploring\nsolutions for integrating MLLMs into UAV systems. Next, we propose a practical\ncase study focused on the forest fire fighting. To fully reveal the\ncapabilities of the proposed framework, human-machine interaction, swarm task\nplanning, fire assessment, and task execution are investigated. Finally, we\ndiscuss the challenges and future research directions for the MLLMs-enabled UAV\nswarm. An experiment illustration video could be found online at\nhttps://youtu.be/zwnB9ZSa5A4.", "AI": {"tldr": "论文探讨了将多模态大语言模型（MLLMs）与无人机群（UAV）结合，以提升其在动态任务中的智能和适应性，并以森林灭火为例展示了其潜力。", "motivation": "无人机群在动态、安全关键任务中需要快速的情境理解和自主适应能力，而MLLMs的突破为AI系统提供了跨模态的感知、推理和交互能力。", "method": "论文首先概述了无人机和MLLMs的基础架构与功能，分析了MLLMs如何提升无人机系统的目标检测、自主导航和多智能体协调能力，并提出了集成方案。", "result": "通过森林灭火的案例研究，展示了该框架在人机交互、群体任务规划、火情评估和任务执行方面的能力。", "conclusion": "论文讨论了MLLMs赋能无人机群的挑战和未来研究方向，并提供了实验视频链接。"}}
{"id": "2506.12742", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12742", "abs": "https://arxiv.org/abs/2506.12742", "authors": ["Yuchen Liu", "Alexiy Buynitsky", "Ruiqi Ni", "Ahmed H. Qureshi"], "title": "Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments", "comment": null, "summary": "Physics-informed Neural Motion Planners (PiNMPs) provide a data-efficient\nframework for solving the Eikonal Partial Differential Equation (PDE) and\nrepresenting the cost-to-go function for motion planning. However, their\nscalability remains limited by spectral bias and the complex loss landscape of\nPDE-driven training. Domain decomposition mitigates these issues by dividing\nthe environment into smaller subdomains, but existing methods enforce\ncontinuity only at individual spatial points. While effective for function\napproximation, these methods fail to capture the spatial connectivity required\nfor motion planning, where the cost-to-go function depends on both the start\nand goal coordinates rather than a single query point. We propose Finite Basis\nNeural Time Fields (FB-NTFields), a novel neural field representation for\nscalable cost-to-go estimation. Instead of enforcing continuity in output\nspace, FB-NTFields construct a latent space representation, computing the\ncost-to-go as a distance between the latent embeddings of start and goal\ncoordinates. This enables global spatial coherence while integrating domain\ndecomposition, ensuring efficient large-scale motion planning. We validate\nFB-NTFields in complex synthetic and real-world scenarios, demonstrating\nsubstantial improvements over existing PiNMPs. Finally, we deploy our method on\na Unitree B1 quadruped robot, successfully navigating indoor environments. The\nsupplementary videos can be found at https://youtu.be/OpRuCbLNOwM.", "AI": {"tldr": "FB-NTFields提出了一种新的神经场表示方法，通过构建潜在空间表示来解决PiNMPs在运动规划中的可扩展性问题，显著提升了性能。", "motivation": "PiNMPs在解决Eikonal PDE和运动规划中的成本函数表示时存在可扩展性问题，主要受限于频谱偏差和PDE驱动的复杂损失地形。", "method": "FB-NTFields通过构建潜在空间表示，将成本函数计算为起点和终点坐标的潜在嵌入之间的距离，同时结合域分解，确保全局空间一致性。", "result": "在复杂合成和真实场景中验证了FB-NTFields的优越性，显著优于现有PiNMPs，并在四足机器人上成功部署。", "conclusion": "FB-NTFields为大规模运动规划提供了一种高效且可扩展的解决方案，解决了PiNMPs的关键限制。"}}
{"id": "2506.12762", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12762", "abs": "https://arxiv.org/abs/2506.12762", "authors": ["Adrian Rubio-Solis", "Luciano Nava-Balanzar", "Tomas Salgado-Jimenez"], "title": "On-board Sonar Data Classification for Path Following in Underwater Vehicles using Fast Interval Type-2 Fuzzy Extreme Learning Machine", "comment": null, "summary": "In autonomous underwater missions, the successful completion of predefined\npaths mainly depends on the ability of underwater vehicles to recognise their\nsurroundings. In this study, we apply the concept of Fast Interval Type-2 Fuzzy\nExtreme Learning Machine (FIT2-FELM) to train a Takagi-Sugeno-Kang IT2 Fuzzy\nInference System (TSK IT2-FIS) for on-board sonar data classification using an\nunderwater vehicle called BlueROV2. The TSK IT2-FIS is integrated into a\nHierarchical Navigation Strategy (HNS) as the main navigation engine to infer\nlocal motions and provide the BlueROV2 with full autonomy to follow an\nobstacle-free trajectory in a water container of 2.5m x 2.5m x 3.5m. Compared\nto traditional navigation architectures, using the proposed method, we observe\na robust path following behaviour in the presence of uncertainty and noise. We\nfound that the proposed approach provides the BlueROV with a more complete\nsensory picture about its surroundings while real-time navigation planning is\nperformed by the concurrent execution of two or more tasks.", "AI": {"tldr": "论文提出了一种基于FIT2-FELM的TSK IT2-FIS方法，用于水下自主导航，通过HNS实现实时路径规划和避障。", "motivation": "水下自主任务的成功依赖于对环境的准确识别，传统导航架构在不确定性和噪声下表现不佳。", "method": "应用FIT2-FELM训练TSK IT2-FIS，集成到HNS中作为导航引擎，实现实时路径规划和避障。", "result": "在2.5m x 2.5m x 3.5m的水箱中，BlueROV2表现出鲁棒的路径跟踪能力，且能同时执行多任务。", "conclusion": "该方法为水下自主导航提供了更完整的环境感知和实时规划能力。"}}
{"id": "2506.12769", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12769", "abs": "https://arxiv.org/abs/2506.12769", "authors": ["Junpeng Yue", "Zepeng Wang", "Yuxuan Wang", "Weishuai Zeng", "Jiangxing Wang", "Xinrun Xu", "Yu Zhang", "Sipeng Zheng", "Ziluo Ding", "Zongqing Lu"], "title": "RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control", "comment": null, "summary": "This paper focuses on a critical challenge in robotics: translating\ntext-driven human motions into executable actions for humanoid robots, enabling\nefficient and cost-effective learning of new behaviors. While existing\ntext-to-motion generation methods achieve semantic alignment between language\nand motion, they often produce kinematically or physically infeasible motions\nunsuitable for real-world deployment. To bridge this sim-to-real gap, we\npropose Reinforcement Learning from Physical Feedback (RLPF), a novel framework\nthat integrates physics-aware motion evaluation with text-conditioned motion\ngeneration. RLPF employs a motion tracking policy to assess feasibility in a\nphysics simulator, generating rewards for fine-tuning the motion generator.\nFurthermore, RLPF introduces an alignment verification module to preserve\nsemantic fidelity to text instructions. This joint optimization ensures both\nphysical plausibility and instruction alignment. Extensive experiments show\nthat RLPF greatly outperforms baseline methods in generating physically\nfeasible motions while maintaining semantic correspondence with text\ninstruction, enabling successful deployment on real humanoid robots.", "AI": {"tldr": "论文提出RLPF框架，通过物理反馈强化学习解决文本驱动动作生成的物理可行性问题，实现语义与物理的双重优化。", "motivation": "现有文本到动作生成方法虽能实现语义对齐，但常产生物理不可行的动作，无法实际部署。", "method": "RLPF结合物理感知动作评估与文本条件动作生成，通过运动跟踪策略评估可行性并生成奖励，同时引入对齐验证模块保持语义忠实度。", "result": "实验表明RLPF在生成物理可行动作并保持语义对齐方面显著优于基线方法，成功应用于真实人形机器人。", "conclusion": "RLPF有效解决了文本驱动动作生成的物理可行性问题，为机器人行为学习提供了高效且低成本的方法。"}}
{"id": "2506.12779", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12779", "abs": "https://arxiv.org/abs/2506.12779", "authors": ["Yuxuan Wang", "Ming Yang", "Weishuai Zeng", "Yu Zhang", "Xinrun Xu", "Haobin Jiang", "Ziluo Ding", "Zongqing Lu"], "title": "From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots", "comment": null, "summary": "Achieving general agile whole-body control on humanoid robots remains a major\nchallenge due to diverse motion demands and data conflicts. While existing\nframeworks excel in training single motion-specific policies, they struggle to\ngeneralize across highly varied behaviors due to conflicting control\nrequirements and mismatched data distributions. In this work, we propose\nBumbleBee (BB), an expert-generalist learning framework that combines motion\nclustering and sim-to-real adaptation to overcome these challenges. BB first\nleverages an autoencoder-based clustering method to group behaviorally similar\nmotions using motion features and motion descriptions. Expert policies are then\ntrained within each cluster and refined with real-world data through iterative\ndelta action modeling to bridge the sim-to-real gap. Finally, these experts are\ndistilled into a unified generalist controller that preserves agility and\nrobustness across all motion types. Experiments on two simulations and a real\nhumanoid robot demonstrate that BB achieves state-of-the-art general whole-body\ncontrol, setting a new benchmark for agile, robust, and generalizable humanoid\nperformance in the real world.", "AI": {"tldr": "BumbleBee (BB) 是一种专家-通才学习框架，通过运动聚类和仿真到现实的适应，实现人形机器人的通用敏捷全身控制。", "motivation": "现有框架在训练单一运动策略时表现优异，但在处理多样化行为时因控制需求冲突和数据分布不匹配而难以泛化。", "method": "BB 使用基于自动编码器的聚类方法分组行为相似的运动，训练专家策略并通过迭代增量动作建模适应现实数据，最终将这些专家蒸馏为统一控制器。", "result": "实验表明，BB 在仿真和真实人形机器人上实现了最先进的通用全身控制，为现实世界中的敏捷、鲁棒和可泛化性能设定了新标准。", "conclusion": "BB 框架成功解决了多样化运动需求和数据冲突的挑战，为通用敏捷全身控制提供了有效解决方案。"}}
{"id": "2506.12851", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12851", "abs": "https://arxiv.org/abs/2506.12851", "authors": ["Weiji Xie", "Jinrui Han", "Jiakun Zheng", "Huanyu Li", "Xinzhe Liu", "Jiyuan Shi", "Weinan Zhang", "Chenjia Bai", "Xuelong Li"], "title": "KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills", "comment": null, "summary": "Humanoid robots are promising to acquire various skills by imitating human\nbehaviors. However, existing algorithms are only capable of tracking smooth,\nlow-speed human motions, even with delicate reward and curriculum design. This\npaper presents a physics-based humanoid control framework, aiming to master\nhighly-dynamic human behaviors such as Kungfu and dancing through multi-steps\nmotion processing and adaptive motion tracking. For motion processing, we\ndesign a pipeline to extract, filter out, correct, and retarget motions, while\nensuring compliance with physical constraints to the maximum extent. For motion\nimitation, we formulate a bi-level optimization problem to dynamically adjust\nthe tracking accuracy tolerance based on the current tracking error, creating\nan adaptive curriculum mechanism. We further construct an asymmetric\nactor-critic framework for policy training. In experiments, we train whole-body\ncontrol policies to imitate a set of highly-dynamic motions. Our method\nachieves significantly lower tracking errors than existing approaches and is\nsuccessfully deployed on the Unitree G1 robot, demonstrating stable and\nexpressive behaviors. The project page is https://kungfu-bot.github.io.", "AI": {"tldr": "本文提出了一种基于物理的人形机器人控制框架，通过多步骤运动处理和自适应运动跟踪，实现了对高动态人类行为（如功夫和舞蹈）的模仿。", "motivation": "现有算法仅能跟踪平滑、低速的人类动作，而本文旨在解决高动态行为的模仿问题。", "method": "设计了运动处理流程（提取、过滤、校正、重定向）和自适应运动跟踪的双层优化问题，并采用非对称actor-critic框架进行策略训练。", "result": "实验表明，该方法跟踪误差显著低于现有方法，并在Unitree G1机器人上实现了稳定且富有表现力的行为。", "conclusion": "该框架成功实现了高动态行为的模仿，为机器人技能学习提供了新思路。"}}
{"id": "2506.13079", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.13079", "abs": "https://arxiv.org/abs/2506.13079", "authors": ["Qidi Fang", "Hang Yu", "Shijie Fang", "Jindan Huang", "Qiuyu Chen", "Reuben M. Aronson", "Elaine S. Short"], "title": "CHARM: Considering Human Attributes for Reinforcement Modeling", "comment": null, "summary": "Reinforcement Learning from Human Feedback has recently achieved significant\nsuccess in various fields, and its performance is highly related to feedback\nquality. While much prior work acknowledged that human teachers'\ncharacteristics would affect human feedback patterns, there is little work that\nhas closely investigated the actual effects. In this work, we designed an\nexploratory study investigating how human feedback patterns are associated with\nhuman characteristics. We conducted a public space study with two long horizon\ntasks and 46 participants. We found that feedback patterns are not only\ncorrelated with task statistics, such as rewards, but also correlated with\nparticipants' characteristics, especially robot experience and educational\nbackground. Additionally, we demonstrated that human feedback value can be more\naccurately predicted with human characteristics compared to only using task\nstatistics. All human feedback and characteristics we collected, and codes for\nour data collection and predicting more accurate human feedback are available\nat https://github.com/AABL-Lab/CHARM", "AI": {"tldr": "研究了人类反馈模式与个体特征的关系，发现机器人经验和教育背景显著影响反馈质量，且结合个体特征能更准确预测反馈价值。", "motivation": "探索人类教师的个体特征如何影响反馈模式，填补了相关研究的空白。", "method": "设计了公开空间实验，包含两个长期任务和46名参与者，收集反馈和个体特征数据。", "result": "反馈模式与任务统计（如奖励）和个体特征（如机器人经验、教育背景）相关，结合个体特征能更准确预测反馈价值。", "conclusion": "人类个体特征对反馈质量有显著影响，未来研究应考虑这些因素以提高反馈效果。"}}
{"id": "2506.13087", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13087", "abs": "https://arxiv.org/abs/2506.13087", "authors": ["Zeyu Zhang", "Ziyuan Jiao"], "title": "IKDiffuser: Fast and Diverse Inverse Kinematics Solution Generation for Multi-arm Robotic Systems", "comment": "under review", "summary": "Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has\nprimarily been successful with single serial manipulators. For multi-arm\nrobotic systems, IK remains challenging due to complex self-collisions, coupled\njoints, and high-dimensional redundancy. These complexities make traditional IK\nsolvers slow, prone to failure, and lacking in solution diversity. In this\npaper, we present IKDiffuser, a diffusion-based model designed for fast and\ndiverse IK solution generation for multi-arm robotic systems. IKDiffuser learns\nthe joint distribution over the configuration space, capturing complex\ndependencies and enabling seamless generalization to multi-arm robotic systems\nof different structures. In addition, IKDiffuser can incorporate additional\nobjectives during inference without retraining, offering versatility and\nadaptability for task-specific requirements. In experiments on 6 different\nmulti-arm systems, the proposed IKDiffuser achieves superior solution accuracy,\nprecision, diversity, and computational efficiency compared to existing\nsolvers. The proposed IKDiffuser framework offers a scalable, unified approach\nto solving multi-arm IK problems, facilitating the potential of multi-arm\nrobotic systems in real-time manipulation tasks.", "AI": {"tldr": "IKDiffuser是一种基于扩散的模型，用于快速生成多臂机器人系统的多样逆运动学解。", "motivation": "多臂机器人系统的逆运动学问题因复杂性（如自碰撞、耦合关节和高维冗余）而难以解决，传统方法效率低且缺乏多样性。", "method": "IKDiffuser通过学习配置空间的联合分布，捕捉复杂依赖关系，并支持在推理时加入额外目标而无需重新训练。", "result": "在6种多臂系统上的实验表明，IKDiffuser在准确性、精度、多样性和计算效率上优于现有方法。", "conclusion": "IKDiffuser为多臂机器人系统提供了一种可扩展的统一解决方案，支持实时操作任务。"}}
{"id": "2506.13100", "categories": ["cs.RO", "cs.CV", "93C85", "I.4"], "pdf": "https://arxiv.org/pdf/2506.13100", "abs": "https://arxiv.org/abs/2506.13100", "authors": ["Zhanhua Xin", "Zhihao Wang", "Shenghao Zhang", "Wanchao Chi", "Yan Meng", "Shihan Kong", "Yan Xiong", "Chong Zhang", "Yuzhen Liu", "Junzhi Yu"], "title": "A Novel ViDAR Device With Visual Inertial Encoder Odometry and Reinforcement Learning-Based Active SLAM Method", "comment": "12 pages, 13 figures", "summary": "In the field of multi-sensor fusion for simultaneous localization and mapping\n(SLAM), monocular cameras and IMUs are widely used to build simple and\neffective visual-inertial systems. However, limited research has explored the\nintegration of motor-encoder devices to enhance SLAM performance. By\nincorporating such devices, it is possible to significantly improve active\ncapability and field of view (FOV) with minimal additional cost and structural\ncomplexity. This paper proposes a novel visual-inertial-encoder tightly coupled\nodometry (VIEO) based on a ViDAR (Video Detection and Ranging) device. A ViDAR\ncalibration method is introduced to ensure accurate initialization for VIEO. In\naddition, a platform motion decoupled active SLAM method based on deep\nreinforcement learning (DRL) is proposed. Experimental data demonstrate that\nthe proposed ViDAR and the VIEO algorithm significantly increase cross-frame\nco-visibility relationships compared to its corresponding visual-inertial\nodometry (VIO) algorithm, improving state estimation accuracy. Additionally,\nthe DRL-based active SLAM algorithm, with the ability to decouple from platform\nmotion, can increase the diversity weight of the feature points and further\nenhance the VIEO algorithm's performance. The proposed methodology sheds fresh\ninsights into both the updated platform design and decoupled approach of active\nSLAM systems in complex environments.", "AI": {"tldr": "论文提出了一种基于ViDAR设备的视觉-惯性-编码器紧耦合里程计（VIEO），并通过深度强化学习（DRL）提出了一种平台运动解耦的主动SLAM方法，显著提升了状态估计精度和特征点多样性。", "motivation": "现有SLAM系统中，单目相机和IMU被广泛使用，但电机编码器设备的集成研究较少。通过引入电机编码器，可以低成本、低复杂度地提升主动能力和视野范围。", "method": "提出VIEO算法，结合ViDAR设备进行校准；提出基于DRL的平台运动解耦主动SLAM方法。", "result": "实验表明，VIEO算法显著提升了跨帧共视关系，状态估计精度优于传统VIO算法；DRL方法进一步增强了特征点多样性。", "conclusion": "该方法为复杂环境下的平台设计和主动SLAM系统提供了新思路。"}}
{"id": "2506.13105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13105", "abs": "https://arxiv.org/abs/2506.13105", "authors": ["Fen Liu", "Chengfeng Jia", "Na Zhang", "Shenghai Yuan", "Rong Su"], "title": "Underwater target 6D State Estimation via UUV Attitude Enhance Observability", "comment": "Paper has been accepted in IROS 2025", "summary": "Accurate relative state observation of Unmanned Underwater Vehicles (UUVs)\nfor tracking uncooperative targets remains a significant challenge due to the\nabsence of GPS, complex underwater dynamics, and sensor limitations. Existing\nlocalization approaches rely on either global positioning infrastructure or\nmulti-UUV collaboration, both of which are impractical for a single UUV\noperating in large or unknown environments. To address this, we propose a novel\npersistent relative 6D state estimation framework that enables a single UUV to\nestimate its relative motion to a non-cooperative target using only successive\nnoisy range measurements from two monostatic sonar sensors. Our key\ncontribution is an observability-enhanced attitude control strategy, which\noptimally adjusts the UUV's orientation to improve the observability of\nrelative state estimation using a Kalman filter, effectively mitigating the\nimpact of sensor noise and drift accumulation. Additionally, we introduce a\nrigorously proven Lyapunov-based tracking control strategy that guarantees\nlong-term stability by ensuring that the UUV maintains an optimal measurement\nrange, preventing localization errors from diverging over time. Through\ntheoretical analysis and simulations, we demonstrate that our method\nsignificantly improves 6D relative state estimation accuracy and robustness\ncompared to conventional approaches. This work provides a scalable,\ninfrastructure-free solution for UUVs tracking uncooperative targets\nunderwater.", "AI": {"tldr": "提出了一种新型的6D状态估计框架，使单个UUV仅通过两个单静态声纳传感器的连续噪声测距测量，估计其与非合作目标的相对运动。", "motivation": "由于缺乏GPS、复杂的水下动力学和传感器限制，UUV对非合作目标的相对状态观测仍具挑战性。现有方法依赖全局定位基础设施或多UUV协作，不适用于单个UUV在大型或未知环境中操作。", "method": "提出了一种可观测性增强的姿态控制策略，通过卡尔曼滤波器优化UUV方向以提高相对状态估计的可观测性，并引入基于Lyapunov的跟踪控制策略保证长期稳定性。", "result": "理论分析和仿真表明，该方法显著提高了6D相对状态估计的准确性和鲁棒性。", "conclusion": "该研究为UUV在水下跟踪非合作目标提供了一种可扩展、无需基础设施的解决方案。"}}
{"id": "2506.13106", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13106", "abs": "https://arxiv.org/abs/2506.13106", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Rong Su"], "title": "Autonomous 3D Moving Target Encirclement and Interception with Range measurement", "comment": "Paper has been accepted into IROS 2025", "summary": "Commercial UAVs are an emerging security threat as they are capable of\ncarrying hazardous payloads or disrupting air traffic. To counter UAVs, we\nintroduce an autonomous 3D target encirclement and interception strategy.\nUnlike traditional ground-guided systems, this strategy employs autonomous\ndrones to track and engage non-cooperative hostile UAVs, which is effective in\nnon-line-of-sight conditions, GPS denial, and radar jamming, where conventional\ndetection and neutralization from ground guidance fail. Using two noisy\nreal-time distances measured by drones, guardian drones estimate the relative\nposition from their own to the target using observation and velocity\ncompensation methods, based on anti-synchronization (AS) and an X$-$Y circular\nmotion combined with vertical jitter. An encirclement control mechanism is\nproposed to enable UAVs to adaptively transition from encircling and protecting\na target to encircling and monitoring a hostile target. Upon breaching a\nwarning threshold, the UAVs may even employ a suicide attack to neutralize the\nhostile target. We validate this strategy through real-world UAV experiments\nand simulated analysis in MATLAB, demonstrating its effectiveness in detecting,\nencircling, and intercepting hostile drones. More details:\nhttps://youtu.be/5eHW56lPVto.", "AI": {"tldr": "提出了一种自主3D目标包围与拦截策略，用于对抗商用无人机（UAV）的安全威胁，适用于非视距、GPS拒止和雷达干扰环境。", "motivation": "商用无人机可能携带危险载荷或干扰空中交通，传统地面引导系统在复杂环境下失效，需自主解决方案。", "method": "利用无人机实时测量的噪声距离，通过观测和速度补偿方法估计目标相对位置，结合反同步（AS）和X-Y圆周运动加垂直抖动实现包围控制。", "result": "实验和仿真验证了该策略在检测、包围和拦截敌对无人机方面的有效性。", "conclusion": "该策略为复杂环境下的无人机对抗提供了高效自主解决方案。"}}
{"id": "2506.13198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13198", "abs": "https://arxiv.org/abs/2506.13198", "authors": ["Bin-Bin Hu", "Bayu Jayawardhana", "Ming Cao"], "title": "Equilibrium-Driven Smooth Separation and Navigation of Marsupial Robotic Systems", "comment": null, "summary": "In this paper, we propose an equilibrium-driven controller that enables a\nmarsupial carrier-passenger robotic system to achieve smooth carrier-passenger\nseparation and then to navigate the passenger robot toward a predetermined\ntarget point. Particularly, we design a potential gradient in the form of a\ncubic polynomial for the passenger's controller as a function of the\ncarrier-passenger and carrier-target distances in the moving carrier's frame.\nThis introduces multiple equilibrium points corresponding to the zero state of\nthe error dynamic system during carrier-passenger separation. The change of\nequilibrium points is associated with the change in their attraction regions,\nenabling smooth carrier-passenger separation and afterwards seamless navigation\ntoward the target. Finally, simulations demonstrate the effectiveness and\nadaptability of the proposed controller in environments containing obstacles.", "AI": {"tldr": "提出一种基于平衡驱动的控制器，实现载体-乘客机器人系统的平滑分离及乘客机器人导航至目标点。", "motivation": "解决载体-乘客机器人系统在分离和导航过程中的平滑性与适应性挑战。", "method": "设计基于立方多项式的势梯度控制器，利用载体-乘客和目标距离动态调整平衡点。", "result": "仿真验证控制器在含障碍环境中的有效性和适应性。", "conclusion": "该方法能实现平滑分离和导航，适用于复杂环境。"}}
{"id": "2506.13202", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13202", "abs": "https://arxiv.org/abs/2506.13202", "authors": ["Bin-Bin Hu", "Yanxin Zhou", "Henglai Wei", "Shuo Cheng", "Chen Lv"], "title": "C2TE: Coordinated Constrained Task Execution Design for Ordering-Flexible Multi-Vehicle Platoon Merging", "comment": null, "summary": "In this paper, we propose a distributed coordinated constrained task\nexecution (C2TE) algorithm that enables a team of vehicles from different lanes\nto cooperatively merge into an {\\it ordering-flexible platoon} maneuvering on\nthe desired lane. Therein, the platoon is flexible in the sense that no\nspecific spatial ordering sequences of vehicles are predetermined. To attain\nsuch a flexible platoon, we first separate the multi-vehicle platoon (MVP)\nmerging mission into two stages, namely, pre-merging regulation and {\\it\nordering-flexible platoon} merging, and then formulate them into distributed\nconstraint-based optimization problems. Particularly, by encoding\nlongitudinal-distance regulation and same-lane collision avoidance subtasks\ninto the corresponding control barrier function (CBF) constraints, the proposed\nalgorithm in Stage 1 can safely enlarge sufficient longitudinal distances among\nadjacent vehicles. Then, by encoding lateral convergence, longitudinal-target\nattraction, and neighboring collision avoidance subtasks into CBF constraints,\nthe proposed algorithm in Stage~2 can efficiently achieve the {\\it\nordering-flexible platoon}. Note that the {\\it ordering-flexible platoon} is\nrealized through the interaction of the longitudinal-target attraction and\ntime-varying neighboring collision avoidance constraints simultaneously.\nFeasibility guarantee and rigorous convergence analysis are both provided under\nstrong nonlinear couplings induced by flexible orderings. Finally, experiments\nusing three autonomous mobile vehicles (AMVs) are conducted to verify the\neffectiveness and flexibility of the proposed algorithm, and extensive\nsimulations are performed to demonstrate its robustness, adaptability, and\nscalability when tackling vehicles' sudden breakdown, new appearing, different\nnumber of lanes, mixed autonomy, and large-scale scenarios, respectively.", "AI": {"tldr": "提出一种分布式协调约束任务执行（C2TE）算法，实现多车道车辆协作合并为顺序灵活的编队。", "motivation": "解决多车辆编队合并时顺序灵活性的问题，避免预设固定空间顺序的限制。", "method": "将任务分为两个阶段：预合并调节和顺序灵活编队合并，分别用分布式约束优化问题建模，并利用控制屏障函数（CBF）约束实现安全与效率。", "result": "实验和仿真验证了算法的有效性、灵活性、鲁棒性和可扩展性。", "conclusion": "该算法在多场景下表现优异，适用于动态和复杂环境。"}}
{"id": "2506.13367", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13367", "abs": "https://arxiv.org/abs/2506.13367", "authors": ["Utkarsh Bajpai", "Julius Rückin", "Cyrill Stachniss", "Marija Popović"], "title": "Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation", "comment": "7 pages, 3 figures", "summary": "Mobile robots exploring indoor environments increasingly rely on\nvision-language models to perceive high-level semantic cues in camera images,\nsuch as object categories. Such models offer the potential to substantially\nadvance robot behaviour for tasks such as object-goal navigation (ObjectNav),\nwhere the robot must locate objects specified in natural language by exploring\nthe environment. Current ObjectNav methods heavily depend on prompt engineering\nfor perception and do not address the semantic uncertainty induced by\nvariations in prompt phrasing. Ignoring semantic uncertainty can lead to\nsuboptimal exploration, which in turn limits performance. Hence, we propose a\nsemantic uncertainty-informed active perception pipeline for ObjectNav in\nindoor environments. We introduce a novel probabilistic sensor model for\nquantifying semantic uncertainty in vision-language models and incorporate it\ninto a probabilistic geometric-semantic map to enhance spatial understanding.\nBased on this map, we develop a frontier exploration planner with an\nuncertainty-informed multi-armed bandit objective to guide efficient object\nsearch. Experimental results demonstrate that our method achieves ObjectNav\nsuccess rates comparable to those of state-of-the-art approaches, without\nrequiring extensive prompt engineering.", "AI": {"tldr": "提出了一种基于语义不确定性的主动感知管道，用于室内环境中的目标导航任务，通过量化视觉语言模型的语义不确定性并融入概率几何语义地图，提升导航效率。", "motivation": "当前目标导航方法依赖提示工程且忽略语义不确定性，导致探索效率低下，限制了性能。", "method": "提出了一种概率传感器模型量化语义不确定性，结合概率几何语义地图，并开发了基于不确定性信息的多臂老虎机目标的前沿探索规划器。", "result": "实验结果表明，该方法在不依赖大量提示工程的情况下，达到了与最先进方法相当的目标导航成功率。", "conclusion": "该方法通过量化语义不确定性并融入导航策略，显著提升了目标导航的效率和性能。"}}
{"id": "2506.13420", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13420", "abs": "https://arxiv.org/abs/2506.13420", "authors": ["Jiang Wang", "Yaozhong Kang", "Linya Fu", "Kazuhiro Nakadai", "He Kong"], "title": "Observability-Aware Active Calibration of Multi-Sensor Extrinsics for Ground Robots via Online Trajectory Optimization", "comment": "Accepted and to appear in the IEEE Sensors Journal", "summary": "Accurate calibration of sensor extrinsic parameters for ground robotic\nsystems (i.e., relative poses) is crucial for ensuring spatial alignment and\nachieving high-performance perception. However, existing calibration methods\ntypically require complex and often human-operated processes to collect data.\nMoreover, most frameworks neglect acoustic sensors, thereby limiting the\nassociated systems' auditory perception capabilities. To alleviate these\nissues, we propose an observability-aware active calibration method for ground\nrobots with multimodal sensors, including a microphone array, a LiDAR\n(exteroceptive sensors), and wheel encoders (proprioceptive sensors). Unlike\ntraditional approaches, our method enables active trajectory optimization for\nonline data collection and calibration, contributing to the development of more\nintelligent robotic systems. Specifically, we leverage the Fisher information\nmatrix (FIM) to quantify parameter observability and adopt its minimum\neigenvalue as an optimization metric for trajectory generation via B-spline\ncurves. Through planning and replanning of robot trajectory online, the method\nenhances the observability of multi-sensor extrinsic parameters. The\neffectiveness and advantages of our method have been demonstrated through\nnumerical simulations and real-world experiments. For the benefit of the\ncommunity, we have also open-sourced our code and data at\nhttps://github.com/AISLAB-sustech/Multisensor-Calibration.", "AI": {"tldr": "提出了一种基于可观测性感知的主动校准方法，用于地面机器人的多模态传感器外参校准，通过轨迹优化提升校准效率。", "motivation": "现有校准方法复杂且依赖人工操作，且常忽略声学传感器，限制了系统的听觉感知能力。", "method": "利用Fisher信息矩阵量化参数可观测性，通过B样条曲线优化轨迹生成，实现在线数据收集与校准。", "result": "通过数值模拟和实际实验验证了方法的有效性和优势，并开源了代码和数据。", "conclusion": "该方法为多传感器外参校准提供了更智能的解决方案，推动了机器人系统的发展。"}}
{"id": "2506.13425", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13425", "abs": "https://arxiv.org/abs/2506.13425", "authors": ["Sai Srinivas Jeevanandam", "Sandeep Inuganti", "Shreedhar Govil", "Didier Stricker", "Jason Rambach"], "title": "JENGA: Object selection and pose estimation for robotic grasping from a stack", "comment": null, "summary": "Vision-based robotic object grasping is typically investigated in the context\nof isolated objects or unstructured object sets in bin picking scenarios.\nHowever, there are several settings, such as construction or warehouse\nautomation, where a robot needs to interact with a structured object formation\nsuch as a stack. In this context, we define the problem of selecting suitable\nobjects for grasping along with estimating an accurate 6DoF pose of these\nobjects. To address this problem, we propose a camera-IMU based approach that\nprioritizes unobstructed objects on the higher layers of stacks and introduce a\ndataset for benchmarking and evaluation, along with a suitable evaluation\nmetric that combines object selection with pose accuracy. Experimental results\nshow that although our method can perform quite well, this is a challenging\nproblem if a completely error-free solution is needed. Finally, we show results\nfrom the deployment of our method for a brick-picking application in a\nconstruction scenario.", "AI": {"tldr": "论文提出了一种基于相机-IMU的方法，用于在结构化物体堆叠场景中选择适合抓取的物体并估计其6DoF位姿，同时引入了数据集和评估指标。", "motivation": "在建筑或仓库自动化等场景中，机器人需要与结构化物体堆叠（如砖块堆）交互，而现有研究多关注孤立物体或无结构物体。", "method": "采用相机-IMU结合的方法，优先选择堆叠高层未被遮挡的物体，并开发了数据集和评估指标。", "result": "实验表明方法表现良好，但完全无误差的解决方案仍具挑战性。", "conclusion": "方法在建筑场景的砖块抓取应用中展示了实用性。"}}
{"id": "2506.13428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13428", "abs": "https://arxiv.org/abs/2506.13428", "authors": ["Jiaming Chen", "Yiyu Jiang", "Aoshen Huang", "Yang Li", "Wei Pan"], "title": "VLM-SFD: VLM-Assisted Siamese Flow Diffusion Framework for Dual-Arm Cooperative Manipulation", "comment": null, "summary": "Dual-arm cooperative manipulation holds great promise for tackling complex\nreal-world tasks that demand seamless coordination and adaptive dynamics.\nDespite substantial progress in learning-based motion planning, most approaches\nstruggle to generalize across diverse manipulation tasks and adapt to dynamic,\nunstructured environments, particularly in scenarios involving interactions\nbetween two objects such as assembly, tool use, and bimanual grasping. To\naddress these challenges, we introduce a novel VLM-Assisted Siamese Flow\nDiffusion (VLM-SFD) framework for efficient imitation learning in dual-arm\ncooperative manipulation. The proposed VLM-SFD framework exhibits outstanding\nadaptability, significantly enhancing the ability to rapidly adapt and\ngeneralize to diverse real-world tasks from only a minimal number of human\ndemonstrations. Specifically, we propose a Siamese Flow Diffusion Network\n(SFDNet) employs a dual-encoder-decoder Siamese architecture to embed two\ntarget objects into a shared latent space, while a diffusion-based conditioning\nprocess-conditioned by task instructions-generates two-stream object-centric\nmotion flows that guide dual-arm coordination. We further design a dynamic task\nassignment strategy that seamlessly maps the predicted 2D motion flows into 3D\nspace and incorporates a pre-trained vision-language model (VLM) to adaptively\nassign the optimal motion to each robotic arm over time. Experiments validate\nthe effectiveness of the proposed method, demonstrating its ability to\ngeneralize to diverse manipulation tasks while maintaining high efficiency and\nadaptability. The code and demo videos are publicly available on our project\nwebsite https://sites.google.com/view/vlm-sfd/.", "AI": {"tldr": "论文提出了一种名为VLM-SFD的新框架，用于双臂协作操作的模仿学习，通过结合视觉语言模型和扩散模型，显著提高了任务适应性和泛化能力。", "motivation": "双臂协作操作在复杂任务中具有潜力，但现有方法难以适应动态环境和多样化任务，尤其是在涉及两个物体交互的场景中。", "method": "提出了VLM-SFD框架，包括Siamese Flow Diffusion Network（SFDNet）和动态任务分配策略，结合扩散模型和视觉语言模型生成双流运动流。", "result": "实验验证了该方法的有效性，能够从少量人类演示中快速适应和泛化到多样化任务。", "conclusion": "VLM-SFD框架在双臂协作操作中表现出色，具有高效性和适应性，代码和演示视频已公开。"}}
{"id": "2506.13432", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13432", "abs": "https://arxiv.org/abs/2506.13432", "authors": ["Jonas Haack", "Franek Stark", "Shubham Vyas", "Frank Kirchner", "Shivesh Kumar"], "title": "Adaptive Model-Base Control of Quadrupeds via Online System Identification using Kalman Filter", "comment": "6 pages, 5 figures, 1 table, accepted for IEEE IROS 2025", "summary": "Many real-world applications require legged robots to be able to carry\nvariable payloads. Model-based controllers such as model predictive control\n(MPC) have become the de facto standard in research for controlling these\nsystems. However, most model-based control architectures use fixed plant\nmodels, which limits their applicability to different tasks. In this paper, we\npresent a Kalman filter (KF) formulation for online identification of the mass\nand center of mass (COM) of a four-legged robot. We evaluate our method on a\nquadrupedal robot carrying various payloads and find that it is more robust to\nstrong measurement noise than classical recursive least squares (RLS) methods.\nMoreover, it improves the tracking performance of the model-based controller\nwith varying payloads when the model parameters are adjusted at runtime.", "AI": {"tldr": "本文提出了一种基于卡尔曼滤波的在线质量与质心识别方法，用于四足机器人，提高了模型控制器的鲁棒性和跟踪性能。", "motivation": "现实应用中，四足机器人需要适应不同负载，而传统基于固定模型的控制器限制了其适用性。", "method": "采用卡尔曼滤波器在线识别机器人的质量和质心，并与经典递归最小二乘法进行比较。", "result": "该方法在强测量噪声下比递归最小二乘法更鲁棒，并能提升模型控制器在变负载下的跟踪性能。", "conclusion": "卡尔曼滤波器在线识别方法有效提升了四足机器人在变负载场景下的控制性能。"}}
{"id": "2506.13453", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13453", "abs": "https://arxiv.org/abs/2506.13453", "authors": ["YR Darr", "MA Niazi"], "title": "Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics", "comment": null, "summary": "The self-organization of robots for the formation of structures and shapes is\na stimulating application of the swarm robotic system. It involves a large\nnumber of autonomous robots of heterogeneous behavior, coordination among them,\nand their interaction with the dynamic environment. This process of complex\nstructure formation is considered a complex system, which needs to be modeled\nby using any modeling approach. Although the formal specification approach\nalong with other formal methods has been used to model the behavior of robots\nin a swarm. However, to the best of our knowledge, the formal specification\napproach has not been used to model the self-organization process in swarm\nrobotic systems for shape formation. In this paper, we use a formal\nspecification approach to model the shape formation task of swarm robots. We\nuse Z (Zed) language of formal specification, which is a state-based language,\nto model the states of the entities of the systems. We demonstrate the\neffectiveness of Z for the self-organized shape formation. The presented formal\nspecification model gives the outlines for designing and implementing the swarm\nrobotic system for the formation of complex shapes and structures. It also\nprovides the foundation for modeling the complex shape formation process for\nswarm robotics using a multi-agent system in a simulation-based environment.\nKeywords: Swarm robotics, Self-organization, Formal specification, Complex\nsystems", "AI": {"tldr": "本文提出了一种使用形式化规范方法（Z语言）建模群机器人自组织形状形成任务的方法，填补了该领域的研究空白。", "motivation": "群机器人自组织形成复杂结构是一个复杂系统，但形式化规范方法尚未用于建模这一过程。本文旨在填补这一空白。", "method": "使用基于状态的Z语言对系统实体的状态进行建模，并验证其在自组织形状形成中的有效性。", "result": "提出的形式化规范模型为设计和实现群机器人系统提供了框架，并为多智能体系统仿真奠定了基础。", "conclusion": "形式化规范方法（Z语言）适用于群机器人自组织形状形成的建模，为复杂系统研究提供了新思路。"}}
{"id": "2506.13478", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13478", "abs": "https://arxiv.org/abs/2506.13478", "authors": ["Hemjyoti Das", "Minh Nhat Vu", "Christian Ott"], "title": "Learning Swing-up Maneuvers for a Suspended Aerial Manipulation Platform in a Hierarchical Control Framework", "comment": "6 pages, 10 figures", "summary": "In this work, we present a novel approach to augment a model-based control\nmethod with a reinforcement learning (RL) agent and demonstrate a swing-up\nmaneuver with a suspended aerial manipulation platform. These platforms are\ntargeted towards a wide range of applications on construction sites involving\ncranes, with swing-up maneuvers allowing it to perch at a given location,\ninaccessible with purely the thrust force of the platform. Our proposed\napproach is based on a hierarchical control framework, which allows different\ntasks to be executed according to their assigned priorities. An RL agent is\nthen subsequently utilized to adjust the reference set-point of the\nlower-priority tasks to perform the swing-up maneuver, which is confined in the\nnullspace of the higher-priority tasks, such as maintaining a specific\norientation and position of the end-effector. Our approach is validated using\nextensive numerical simulation studies.", "AI": {"tldr": "提出了一种结合模型控制与强化学习的层次化控制框架，用于实现悬挂式空中平台的摆动动作。", "motivation": "悬挂式空中平台在建筑工地等场景有广泛应用，但仅靠推力难以完成特定位置的摆动动作。", "method": "采用层次化控制框架，优先级任务（如保持末端执行器位置）由模型控制完成，低优先级任务（摆动动作）由强化学习代理调整参考点。", "result": "通过数值模拟验证了方法的有效性。", "conclusion": "该方法为复杂任务提供了灵活且高效的解决方案。"}}
{"id": "2506.13536", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13536", "abs": "https://arxiv.org/abs/2506.13536", "authors": ["Vaibhav Saxena", "Matthew Bronars", "Nadun Ranawaka Arachchige", "Kuancheng Wang", "Woo Chul Shin", "Soroush Nasiriany", "Ajay Mandlekar", "Danfei Xu"], "title": "What Matters in Learning from Large-Scale Datasets for Robot Manipulation", "comment": null, "summary": "Imitation learning from large multi-task demonstration datasets has emerged\nas a promising path for building generally-capable robots. As a result, 1000s\nof hours have been spent on building such large-scale datasets around the\nglobe. Despite the continuous growth of such efforts, we still lack a\nsystematic understanding of what data should be collected to improve the\nutility of a robotics dataset and facilitate downstream policy learning. In\nthis work, we conduct a large-scale dataset composition study to answer this\nquestion. We develop a data generation framework to procedurally emulate common\nsources of diversity in existing datasets (such as sensor placements and object\ntypes and arrangements), and use it to generate large-scale robot datasets with\ncontrolled compositions, enabling a suite of dataset composition studies that\nwould be prohibitively expensive in the real world. We focus on two practical\nsettings: (1) what types of diversity should be emphasized when future\nresearchers collect large-scale datasets for robotics, and (2) how should\ncurrent practitioners retrieve relevant demonstrations from existing datasets\nto maximize downstream policy performance on tasks of interest. Our study\nyields several critical insights -- for example, we find that camera poses and\nspatial arrangements are crucial dimensions for both diversity in collection\nand alignment in retrieval. In real-world robot learning settings, we find that\nnot only do our insights from simulation carry over, but our retrieval\nstrategies on existing datasets such as DROID allow us to consistently\noutperform existing training strategies by up to 70%. More results at\nhttps://robo-mimiclabs.github.io/", "AI": {"tldr": "该论文研究了如何通过控制数据集组成来优化机器人模仿学习的效果，提出了数据生成框架，并得出相机位姿和空间布局是关键因素的结论。", "motivation": "尽管大规模多任务演示数据集在机器人模仿学习中具有潜力，但目前缺乏对如何优化数据集组成的系统性理解。", "method": "开发了一个数据生成框架，模拟现有数据集的多样性来源，并生成受控组成的大规模数据集，用于研究数据集组成的影响。", "result": "研究发现相机位姿和空间布局是关键因素，且在真实机器人学习中，模拟中的结论依然适用，检索策略能提升性能达70%。", "conclusion": "论文为未来数据集收集和现有数据集检索提供了实用指导，强调了多样性和对齐的重要性。"}}
{"id": "2506.13622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13622", "abs": "https://arxiv.org/abs/2506.13622", "authors": ["Martino Gulisano", "Matteo Masoni", "Marco Gabiccini", "Massimo Guiggiani"], "title": "Disturbance-aware minimum-time planning strategies for motorsport vehicles with probabilistic safety certificates", "comment": "24 pages, 11 figures, paper under review", "summary": "This paper presents a disturbance-aware framework that embeds robustness into\nminimum-lap-time trajectory optimization for motorsport. Two formulations are\nintroduced. (i) Open-loop, horizon-based covariance propagation uses worst-case\nuncertainty growth over a finite window to tighten tire-friction and\ntrack-limit constraints. (ii) Closed-loop, covariance-aware planning\nincorporates a time-varying LQR feedback law in the optimizer, providing a\nfeedback-consistent estimate of disturbance attenuation and enabling sharper\nyet reliable constraint tightening. Both methods yield reference trajectories\nfor human or artificial drivers: in autonomous applications the modelled\ncontroller can replicate the on-board implementation, while for human driving\naccuracy increases with the extent to which the driver can be approximated by\nthe assumed time-varying LQR policy. Computational tests on a representative\nBarcelona-Catalunya sector show that both schemes meet the prescribed safety\nprobability, yet the closed-loop variant incurs smaller lap-time penalties than\nthe more conservative open-loop solution, while the nominal (non-robust)\ntrajectory remains infeasible under the same uncertainties. By accounting for\nuncertainty growth and feedback action during planning, the proposed framework\ndelivers trajectories that are both performance-optimal and probabilistically\nsafe, advancing minimum-time optimization toward real-world deployment in\nhigh-performance motorsport and autonomous racing.", "AI": {"tldr": "本文提出了一种扰动感知框架，将鲁棒性嵌入到赛车运动的最小圈速轨迹优化中，分为开环和闭环两种方法，均能生成安全且性能优化的轨迹。", "motivation": "赛车运动中的轨迹优化需要在高性能要求下同时保证安全性，传统方法在不确定性下可能失效，因此需要一种鲁棒性强的优化框架。", "method": "（i）开环方法基于有限窗口的最坏情况不确定性增长收紧约束；（ii）闭环方法结合时变LQR反馈律，提供更精确的约束收紧。", "result": "两种方法均满足安全概率要求，闭环方法在圈速损失上优于开环方法，且非鲁棒轨迹在相同不确定性下不可行。", "conclusion": "该框架通过考虑不确定性和反馈作用，生成了性能最优且概率安全的轨迹，推动了最小时间优化在赛车运动中的实际应用。"}}
{"id": "2506.13640", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13640", "abs": "https://arxiv.org/abs/2506.13640", "authors": ["Cedric Le Gentil", "Cedric Pradalier", "Timothy D. Barfoot"], "title": "Towards Efficient Occupancy Mapping via Gaussian Process Latent Field Shaping", "comment": "Presented at RSS 2025 Workshop: Gaussian Representations for Robot\n  Autonomy: Challenges and Opportunities", "summary": "Occupancy mapping has been a key enabler of mobile robotics. Originally based\non a discrete grid representation, occupancy mapping has evolved towards\ncontinuous representations that can predict the occupancy status at any\nlocation and account for occupancy correlations between neighbouring areas.\nGaussian Process (GP) approaches treat this task as a binary classification\nproblem using both observations of occupied and free space. Conceptually, a GP\nlatent field is passed through a logistic function to obtain the output class\nwithout actually manipulating the GP latent field. In this work, we propose to\nact directly on the latent function to efficiently integrate free space\ninformation as a prior based on the shape of the sensor's field-of-view. A\nmajor difference with existing methods is the change in the classification\nproblem, as we distinguish between free and unknown space. The `occupied' area\nis the infinitesimally thin location where the class transitions from free to\nunknown. We demonstrate in simulated environments that our approach is sound\nand leads to competitive reconstruction accuracy.", "AI": {"tldr": "该论文提出了一种基于高斯过程的占用映射方法，通过直接操作潜在函数来高效整合自由空间信息，并区分自由与未知空间，提高了重建精度。", "motivation": "占用映射是移动机器人的关键技术，传统离散网格表示已发展为连续表示。现有高斯过程方法将任务视为二分类问题，但未直接操作潜在函数。本文旨在通过直接操作潜在函数，更高效地整合自由空间信息。", "method": "提出直接操作高斯过程潜在函数的方法，将自由空间信息作为先验，并区分自由与未知空间。占用区域定义为自由到未知的过渡点。", "result": "在模拟环境中验证了方法的有效性，重建精度具有竞争力。", "conclusion": "该方法通过直接操作潜在函数和区分自由与未知空间，实现了高效的占用映射，为移动机器人提供了更精确的环境表示。"}}
{"id": "2506.13679", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13679", "abs": "https://arxiv.org/abs/2506.13679", "authors": ["Yuqing Wen", "Kefan Gu", "Haoxuan Liu", "Yucheng Zhao", "Tiancai Wang", "Haoqiang Fan", "Xiaoyan Sun"], "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently made significant advance in\nmulti-task, end-to-end robotic control, due to the strong generalization\ncapabilities of Vision-Language Models (VLMs). A fundamental challenge in\ndeveloping such models is effectively aligning the vision-language space with\nthe robotic action space. Existing approaches typically rely on directly\nfine-tuning VLMs using expert demonstrations. However, this strategy suffers\nfrom a spatio-temporal gap, resulting in considerable data inefficiency and\nheavy reliance on human labor. Spatially, VLMs operate within a high-level\nsemantic space, whereas robotic actions are grounded in low-level 3D physical\nspace; temporally, VLMs primarily interpret the present, while VLA models\nanticipate future actions. To overcome these challenges, we propose a novel\ntraining paradigm, ROSA, which leverages robot state estimation to improve\nalignment between vision-language and action spaces. By integrating robot state\nestimation data obtained via an automated process, ROSA enables the VLA model\nto gain enhanced spatial understanding and self-awareness, thereby boosting\nperformance and generalization. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of ROSA, particularly in\nlow-data regimes.", "AI": {"tldr": "ROSA是一种新的训练范式，通过机器人状态估计提升视觉-语言-动作（VLA）模型的空间对齐能力，解决了现有方法的数据效率低和依赖人工的问题。", "motivation": "现有方法通过直接微调视觉语言模型（VLMs）存在时空差距，导致数据效率低下且依赖人工。", "method": "提出ROSA训练范式，利用机器人状态估计数据自动对齐视觉-语言与动作空间。", "result": "在模拟和真实环境中验证了ROSA的有效性，尤其在低数据情况下表现优异。", "conclusion": "ROSA通过增强空间理解和自感知能力，显著提升了VLA模型的性能和泛化能力。"}}
{"id": "2506.13704", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13704", "abs": "https://arxiv.org/abs/2506.13704", "authors": ["V. Sripada", "A. Khan", "J. Föcker", "S. Parsa", "Susmitha P", "H Maior", "A. Ghalamzan-E"], "title": "HARMONI: Haptic-Guided Assistance for Unified Robotic Tele-Manipulation and Tele-Navigation", "comment": "To appear in IEEE CASE 2025", "summary": "Shared control, which combines human expertise with autonomous assistance, is\ncritical for effective teleoperation in complex environments. While recent\nadvances in haptic-guided teleoperation have shown promise, they are often\nlimited to simplified tasks involving 6- or 7-DoF manipulators and rely on\nseparate control strategies for navigation and manipulation. This increases\nboth cognitive load and operational overhead. In this paper, we present a\nunified tele-mobile manipulation framework that leverages haptic-guided shared\ncontrol. The system integrates a 9-DoF follower mobile manipulator and a 7-DoF\nleader robotic arm, enabling seamless transitions between tele-navigation and\ntele-manipulation through real-time haptic feedback. A user study with 20\nparticipants under real-world conditions demonstrates that our framework\nsignificantly improves task accuracy and efficiency without increasing\ncognitive load. These findings highlight the potential of haptic-guided shared\ncontrol for enhancing operator performance in demanding teleoperation\nscenarios.", "AI": {"tldr": "提出了一种基于触觉引导共享控制的统一远程移动操作框架，显著提高了任务准确性和效率，同时未增加认知负担。", "motivation": "现有触觉引导远程操作多限于简化任务，且导航与操作策略分离，增加了认知和操作负担。", "method": "整合9-DoF移动机械臂和7-DoF机械臂，通过实时触觉反馈实现导航与操作的无缝切换。", "result": "20名参与者的实验表明，框架显著提升了任务准确性和效率。", "conclusion": "触觉引导共享控制有望在复杂远程操作中提升操作者表现。"}}
{"id": "2506.13725", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13725", "abs": "https://arxiv.org/abs/2506.13725", "authors": ["Wenxuan Song", "Jiayi Chen", "Pengxiang Ding", "Yuxin Huang", "Han Zhao", "Donglin Wang", "Haoang Li"], "title": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding", "comment": "16 pages", "summary": "In recent years, Vision-Language-Action (VLA) models have become a vital\nresearch direction in robotics due to their impressive multimodal understanding\nand generalization capabilities. Despite the progress, their practical\ndeployment is severely constrained by inference speed bottlenecks, particularly\nin high-frequency and dexterous manipulation tasks. While recent studies have\nexplored Jacobi decoding as a more efficient alternative to traditional\nautoregressive decoding, its practical benefits are marginal due to the lengthy\niterations. To address it, we introduce consistency distillation training to\npredict multiple correct action tokens in each iteration, thereby achieving\nacceleration. Besides, we design mixed-label supervision to mitigate the error\naccumulation during distillation. Although distillation brings acceptable\nspeedup, we identify that certain inefficient iterations remain a critical\nbottleneck. To tackle this, we propose an early-exit decoding strategy that\nmoderately relaxes convergence conditions, which further improves average\ninference efficiency. Experimental results show that the proposed method\nachieves more than 4 times inference acceleration across different baselines\nwhile maintaining high task success rates in both simulated and real-world\nrobot tasks. These experiments validate that our approach provides an efficient\nand general paradigm for accelerating multimodal decision-making in robotics.\nOur project page is available at https://irpn-eai.github.io/CEED-VLA/.", "AI": {"tldr": "本文提出了一种通过一致性蒸馏训练和早期退出解码策略加速视觉-语言-动作（VLA）模型推理的方法，实现了4倍以上的加速，同时保持高任务成功率。", "motivation": "VLA模型在机器人领域具有重要应用，但其推理速度瓶颈限制了实际部署。传统方法如雅可比解码效率有限，需要改进。", "method": "采用一致性蒸馏训练预测多个动作标记，结合混合标签监督减少误差积累，并提出早期退出解码策略放松收敛条件。", "result": "实验表明，该方法在不同基准上实现了4倍以上的推理加速，同时在模拟和真实机器人任务中保持高成功率。", "conclusion": "该方法为加速机器人多模态决策提供了高效且通用的解决方案。"}}
{"id": "2506.13739", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.13739", "abs": "https://arxiv.org/abs/2506.13739", "authors": ["Guy Laban", "Micol Spitale", "Minja Axelsson", "Nida Itrat Abbasi", "Hatice Gunes"], "title": "Critical Insights about Robots for Mental Wellbeing", "comment": null, "summary": "Social robots are increasingly being explored as tools to support emotional\nwellbeing, particularly in non-clinical settings. Drawing on a range of\nempirical studies and practical deployments, this paper outlines six key\ninsights that highlight both the opportunities and challenges in using robots\nto promote mental wellbeing. These include (1) the lack of a single, objective\nmeasure of wellbeing, (2) the fact that robots don't need to act as companions\nto be effective, (3) the growing potential of virtual interactions, (4) the\nimportance of involving clinicians in the design process, (5) the difference\nbetween one-off and long-term interactions, and (6) the idea that adaptation\nand personalization are not always necessary for positive outcomes. Rather than\npositioning robots as replacements for human therapists, we argue that they are\nbest understood as supportive tools that must be designed with care, grounded\nin evidence, and shaped by ethical and psychological considerations. Our aim is\nto inform future research and guide responsible, effective use of robots in\nmental health and wellbeing contexts.", "AI": {"tldr": "本文总结了社交机器人在促进心理健康方面的六个关键见解，包括测量方法的多样性、机器人角色的灵活性、虚拟互动的潜力、临床设计的必要性、互动时长的差异以及个性化需求的非必要性。", "motivation": "探讨社交机器人作为非临床环境中支持情感健康的工具，明确其机会与挑战。", "method": "基于实证研究和实际部署，提炼出六个关键见解。", "result": "机器人应被视为辅助工具，而非人类治疗师的替代品，需基于证据和伦理设计。", "conclusion": "为未来研究和机器人心理健康应用的负责任使用提供指导。"}}
{"id": "2506.13751", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13751", "abs": "https://arxiv.org/abs/2506.13751", "authors": ["Haoru Xue", "Xiaoyu Huang", "Dantong Niu", "Qiayuan Liao", "Thomas Kragerud", "Jan Tommy Gravdahl", "Xue Bin Peng", "Guanya Shi", "Trevor Darrell", "Koushil Screenath", "Shankar Sastry"], "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction", "comment": null, "summary": "Vision-language-action (VLA) models have demonstrated strong semantic\nunderstanding and zero-shot generalization, yet most existing systems assume an\naccurate low-level controller with hand-crafted action \"vocabulary\" such as\nend-effector pose or root velocity. This assumption confines prior work to\nquasi-static tasks and precludes the agile, whole-body behaviors required by\nhumanoid whole-body control (WBC) tasks. To capture this gap in the literature,\nwe start by introducing the first sim-to-real-ready, vision-language,\nclosed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10\ncategories. We then propose LeVERB: Latent Vision-Language-Encoded Robot\nBehavior, a hierarchical latent instruction-following framework for humanoid\nvision-language WBC, the first of its kind. At the top level, a vision-language\npolicy learns a latent action vocabulary from synthetically rendered kinematic\ndemonstrations; at the low level, a reinforcement-learned WBC policy consumes\nthese latent verbs to generate dynamics-level commands. In our benchmark,\nLeVERB can zero-shot attain a 80% success rate on simple visual navigation\ntasks, and 58.5% success rate overall, outperforming naive hierarchical\nwhole-body VLA implementation by 7.8 times.", "AI": {"tldr": "论文提出LeVERB框架，解决人形机器人视觉语言动作（VLA）模型在全身控制（WBC）任务中的不足，并通过新基准测试验证其性能。", "motivation": "现有VLA模型依赖低层控制器和手工动作词汇，限制了其在动态任务中的应用，尤其是人形机器人全身控制任务。", "method": "提出LeVERB框架，分层次设计：高层视觉语言策略学习潜在动作词汇，低层强化学习策略生成动态命令。", "result": "LeVERB在基准测试中，简单视觉导航任务成功率达80%，整体成功率58.5%，优于基线方法7.8倍。", "conclusion": "LeVERB填补了VLA模型在人形机器人全身控制任务中的空白，展示了零样本泛化能力。"}}
{"id": "2506.13753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13753", "abs": "https://arxiv.org/abs/2506.13753", "authors": ["Stav Ashur", "Nancy M. Amato", "Sariel Har-Peled"], "title": "Edge Nearest Neighbor in Sampling-Based Motion Planning", "comment": null, "summary": "Neighborhood finders and nearest neighbor queries are fundamental parts of\nsampling based motion planning algorithms. Using different distance metrics or\notherwise changing the definition of a neighborhood produces different\nalgorithms with unique empiric and theoretical properties. In \\cite{l-pa-06}\nLaValle suggests a neighborhood finder for the Rapidly-exploring Random Tree\nRRT\n  algorithm \\cite{l-rrtnt-98} which finds the nearest neighbor of the sampled\npoint on the swath of the tree, that is on the set of all of the points on the\ntree edges, using a hierarchical data structure. In this paper we implement\nsuch a neighborhood finder and show, theoretically and experimentally, that\nthis results in more efficient algorithms, and suggest a variant of the\nRapidly-exploring Random Graph RRG algorithm \\cite{f-isaom-10} that better\nexploits the exploration properties of the newly described subroutine for\nfinding narrow passages.", "AI": {"tldr": "本文实现了一种基于层次数据结构的邻域查找器，用于RRT算法，并通过理论和实验证明其效率更高，同时提出了一种改进的RRG算法变体。", "motivation": "邻域查找和最近邻查询是基于采样的运动规划算法的核心部分，不同的距离度量或邻域定义会产生不同特性的算法。", "method": "实现了一种基于层次数据结构的邻域查找器，用于在RRT算法中查找采样点的最近邻。", "result": "理论和实验证明该方法提高了算法效率。", "conclusion": "提出了一种改进的RRG算法变体，更好地利用了新描述的邻域查找子程序来探索狭窄通道。"}}
{"id": "2506.13761", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13761", "abs": "https://arxiv.org/abs/2506.13761", "authors": ["Chuanruo Ning", "Kuan Fang", "Wei-Chiu Ma"], "title": "Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins", "comment": null, "summary": "Recent advancements in open-world robot manipulation have been largely driven\nby vision-language models (VLMs). While these models exhibit strong\ngeneralization ability in high-level planning, they struggle to predict\nlow-level robot controls due to limited physical-world understanding. To\naddress this issue, we propose a model predictive control framework for\nopen-world manipulation that combines the semantic reasoning capabilities of\nVLMs with physically-grounded, interactive digital twins of the real-world\nenvironments. By constructing and simulating the digital twins, our approach\ngenerates feasible motion trajectories, simulates corresponding outcomes, and\nprompts the VLM with future observations to evaluate and select the most\nsuitable outcome based on language instructions of the task. To further enhance\nthe capability of pre-trained VLMs in understanding complex scenes for robotic\ncontrol, we leverage the flexible rendering capabilities of the digital twin to\nsynthesize the scene at various novel, unoccluded viewpoints. We validate our\napproach on a diverse set of complex manipulation tasks, demonstrating superior\nperformance compared to baseline methods for language-conditioned robotic\ncontrol using VLMs.", "AI": {"tldr": "提出了一种结合视觉语言模型（VLM）和数字孪生的模型预测控制框架，用于开放世界机器人操作，通过模拟生成可行运动轨迹并优化任务执行。", "motivation": "现有视觉语言模型在高层次规划中表现优异，但在低层次机器人控制预测方面因物理世界理解不足而受限。", "method": "结合VLM的语义推理能力和数字孪生的物理模拟，生成运动轨迹并通过未来观察提示VLM评估任务执行。", "result": "在多样复杂操作任务中验证，性能优于基于VLM的语言条件机器人控制基线方法。", "conclusion": "该方法通过数字孪生增强VLM的物理世界理解能力，显著提升了开放世界机器人操作的性能。"}}
{"id": "2506.13762", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13762", "abs": "https://arxiv.org/abs/2506.13762", "authors": ["Zifan Zhao", "Siddhant Haldar", "Jinda Cui", "Lerrel Pinto", "Raunaq Bhirangi"], "title": "Touch begins where vision ends: Generalizable policies for contact-rich manipulation", "comment": null, "summary": "Data-driven approaches struggle with precise manipulation; imitation learning\nrequires many hard-to-obtain demonstrations, while reinforcement learning\nyields brittle, non-generalizable policies. We introduce VisuoTactile Local\n(ViTaL) policy learning, a framework that solves fine-grained manipulation\ntasks by decomposing them into two phases: a reaching phase, where a\nvision-language model (VLM) enables scene-level reasoning to localize the\nobject of interest, and a local interaction phase, where a reusable,\nscene-agnostic ViTaL policy performs contact-rich manipulation using egocentric\nvision and tactile sensing. This approach is motivated by the observation that\nwhile scene context varies, the low-level interaction remains consistent across\ntask instances. By training local policies once in a canonical setting, they\ncan generalize via a localize-then-execute strategy. ViTaL achieves around 90%\nsuccess on contact-rich tasks in unseen environments and is robust to\ndistractors. ViTaL's effectiveness stems from three key insights: (1)\nfoundation models for segmentation enable training robust visual encoders via\nbehavior cloning; (2) these encoders improve the generalizability of policies\nlearned using residual RL; and (3) tactile sensing significantly boosts\nperformance in contact-rich tasks. Ablation studies validate each of these\ninsights, and we demonstrate that ViTaL integrates well with high-level VLMs,\nenabling robust, reusable low-level skills. Results and videos are available at\nhttps://vitalprecise.github.io.", "AI": {"tldr": "ViTaL框架通过分阶段策略（定位与局部交互）解决精细操作任务，结合视觉语言模型和触觉传感，实现高成功率和泛化能力。", "motivation": "传统数据驱动方法在精细操作上表现不佳，模仿学习需要大量演示，强化学习策略脆弱且难以泛化。", "method": "ViTaL将任务分解为定位阶段（使用视觉语言模型）和局部交互阶段（使用可复用的ViTaL策略，结合视觉和触觉传感）。", "result": "在未见环境中，ViTaL在接触密集型任务中达到约90%的成功率，且对干扰物具有鲁棒性。", "conclusion": "ViTaL通过结合基础模型、残差强化学习和触觉传感，实现了高效、可泛化的精细操作策略。"}}
