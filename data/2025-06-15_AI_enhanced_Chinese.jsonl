{"id": "2506.10093", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10093", "abs": "https://arxiv.org/abs/2506.10093", "authors": ["Marcos Abel Zuzuárregui", "Stefano Carpin"], "title": "Leveraging LLMs for Mission Planning in Precision Agriculture", "comment": "Published in Proceedings of 2025 International Conference on Robotics\n  and Automation (ICRA)", "summary": "Robotics and artificial intelligence hold significant potential for advancing\nprecision agriculture. While robotic systems have been successfully deployed\nfor various tasks, adapting them to perform diverse missions remains\nchallenging, particularly because end users often lack technical expertise. In\nthis paper, we present an end-to-end system that leverages large language\nmodels (LLMs), specifically ChatGPT, to enable users to assign complex data\ncollection tasks to autonomous robots using natural language instructions. To\nenhance reusability, mission plans are encoded using an existing IEEE task\nspecification standard, and are executed on robots via ROS2 nodes that bridge\nhigh-level mission descriptions with existing ROS libraries. Through extensive\nexperiments, we highlight the strengths and limitations of LLMs in this\ncontext, particularly regarding spatial reasoning and solving complex routing\nchallenges, and show how our proposed implementation overcomes them.", "AI": {"tldr": "论文提出了一种基于大型语言模型（如ChatGPT）的端到端系统，允许用户通过自然语言指令为自主机器人分配复杂的数据收集任务，并通过IEEE任务规范标准和ROS2节点实现任务的高效执行。", "motivation": "机器人技术和人工智能在精准农业中潜力巨大，但现有系统难以适应多样化任务，且终端用户通常缺乏技术专长。", "method": "利用大型语言模型（LLMs）将自然语言指令转化为任务计划，采用IEEE任务规范标准编码任务，并通过ROS2节点执行。", "result": "实验验证了LLMs在空间推理和复杂路径规划中的优势与局限性，并展示了所提系统的有效性。", "conclusion": "该系统为终端用户提供了一种无需技术背景的任务分配方式，同时通过标准化和模块化设计提升了任务的可重用性。"}}
{"id": "2506.10098", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10098", "abs": "https://arxiv.org/abs/2506.10098", "authors": ["Christian Reichenbächer", "Philipp Rank", "Jochen Hipp", "Oliver Bringmann"], "title": "Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models", "comment": "8 pages, 4 figures; This work has been submitted to the IEEE for\n  possible publication", "summary": "This paper presents the first application of Gaussian Mixture Copula Models\nto the statistical modeling of driving scenarios for the safety validation of\nautomated driving systems. Knowledge of the joint probability distribution of\nscenario parameters is essential for scenario-based safety assessment, where\nrisk quantification depends on the likelihood of concrete parameter\ncombinations. Gaussian Mixture Copula Models bring together the multimodal\nexpressivity of Gaussian Mixture Models and the flexibility of copulas,\nenabling separate modeling of marginal distributions and dependencies. We\nbenchmark Gaussian Mixture Copula Models against previously proposed approaches\n- Gaussian Mixture Models and Gaussian Copula Models - using real-world driving\ndata drawn from scenarios defined in United Nations Regulation No. 157. Our\nevaluation across 18 million scenario instances demonstrates that Gaussian\nMixture Copula Models provide a better fit to the data in terms of both\nlikelihood and Sinkhorn distance. These results suggest that Gaussian Mixture\nCopula Models are a compelling foundation for future scenario-based validation\nframeworks.", "AI": {"tldr": "该论文首次将高斯混合Copula模型应用于自动驾驶系统的安全验证中，用于驾驶场景的统计建模。", "motivation": "为了基于场景的安全评估，需要了解场景参数的联合概率分布，而现有方法（如高斯混合模型和高斯Copula模型）在表达性和灵活性上存在不足。", "method": "提出高斯混合Copula模型，结合高斯混合模型的多模态表达能力和Copula的灵活性，分别建模边缘分布和依赖关系。", "result": "在1800万场景实例的评估中，高斯混合Copula模型在似然和Sinkhorn距离上优于其他方法。", "conclusion": "高斯混合Copula模型为未来基于场景的验证框架提供了有力支持。"}}
{"id": "2506.10106", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10106", "abs": "https://arxiv.org/abs/2506.10106", "authors": ["Marcos Abel Zuzuárregui", "Mustafa Melih Toslak", "Stefano Carpin"], "title": "One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture", "comment": "Accepted to International Federation of Automatic Control (IFAC)\n  Sensing, Control and Automation Technologies for Agriculture - 8th\n  AGRICONTROL 2025", "summary": "Artificial intelligence is transforming precision agriculture, offering\nfarmers new tools to streamline their daily operations. While these\ntechnological advances promise increased efficiency, they often introduce\nadditional complexity and steep learning curves that are particularly\nchallenging for non-technical users who must balance tech adoption with\nexisting workloads. In this paper, we present a natural language (NL) robotic\nmission planner that enables non-specialists to control heterogeneous robots\nthrough a common interface. By leveraging large language models (LLMs) and\npredefined primitives, our architecture seamlessly translates human language\ninto intermediate descriptions that can be executed by different robotic\nplatforms. With this system, users can formulate complex agricultural missions\nwithout writing any code. In the work presented in this paper, we extend our\nprevious system tailored for wheeled robot mission planning through a new class\nof experiments involving robotic manipulation and computer vision tasks. Our\nresults demonstrate that the architecture is both general enough to support a\ndiverse set of robots and powerful enough to execute complex mission requests.\nThis work represents a significant step toward making robotic automation in\nprecision agriculture more accessible to non-technical users.", "AI": {"tldr": "论文提出了一种基于自然语言的机器人任务规划器，帮助非技术用户通过简单界面控制异构机器人，利用大语言模型和预定义原语，将人类语言转化为可执行指令。", "motivation": "人工智能在精准农业中的应用增加了效率，但也带来了复杂性和学习曲线，非技术用户难以适应。", "method": "通过大语言模型（LLMs）和预定义原语，将自然语言转化为中间描述，支持异构机器人执行复杂任务。", "result": "系统支持多种机器人，并能执行复杂任务请求，验证了其通用性和强大功能。", "conclusion": "该研究为非技术用户提供了更易用的机器人自动化工具，推动了精准农业的普及。"}}
{"id": "2506.10172", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10172", "abs": "https://arxiv.org/abs/2506.10172", "authors": ["Yicheng Duan", "Kaiyu tang"], "title": "A Navigation Framework Utilizing Vision-Language Models", "comment": null, "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied\nAI, requiring agents to interpret natural language instructions and navigate\nthrough visually rich, unfamiliar environments. Recent advances in large\nvision-language models (LVLMs), such as CLIP and Flamingo, have significantly\nimproved multimodal understanding but introduced new challenges related to\ncomputational cost and real-time deployment. In this project, we propose a\nmodular, plug-and-play navigation framework that decouples vision-language\nunderstanding from action planning. By integrating a frozen vision-language\nmodel, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to\nachieve flexible, fast, and adaptable navigation without extensive model\nfine-tuning. Our framework leverages prompt engineering, structured history\nmanagement, and a two-frame visual input strategy to enhance decision-making\ncontinuity across navigation steps. We evaluate our system on the Room-to-Room\nbenchmark within the VLN-CE setting using the Matterport3D dataset and\nHabitat-Lab simulation environment. Although our initial results reveal\nchallenges in generalizing to unseen environments under strict evaluation\nsettings, our modular approach lays a foundation for scalable and efficient\nnavigation systems, highlighting promising directions for future improvement\nthrough enhanced environmental priors and expanded multimodal input\nintegration.", "AI": {"tldr": "提出了一种模块化的视觉语言导航框架，通过解耦视觉语言理解与动作规划，结合轻量级逻辑实现快速导航。", "motivation": "解决大型视觉语言模型在导航任务中的计算成本和实时部署挑战。", "method": "集成冻结的视觉语言模型Qwen2.5-VL-7B-Instruct与轻量规划逻辑，采用提示工程和双帧视觉输入策略。", "result": "在Room-to-Room基准测试中初步结果显示出泛化能力不足，但模块化设计为未来改进奠定基础。", "conclusion": "模块化框架为高效导航系统提供了可扩展性，未来可通过增强环境先验和多模态输入集成进一步优化。"}}
{"id": "2506.10240", "categories": ["cs.RO", "cs.SY", "eess.SY", "93B52 (Primary), 93C85 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.10240", "abs": "https://arxiv.org/abs/2506.10240", "authors": ["Rongfei Li", "Francis Assadian"], "title": "Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators", "comment": "22 pages, 13 figures. To appear in: Innovative Adaptive Image-Based\n  Visual Servoing Control of 6 DoFs Industrial Robot Manipulators, IntechOpen,\n  2024. For published version, see this http URL:\n  https://doi.org/10.5772/intechopen.1004857", "summary": "Image-based visual servoing (IBVS) methods have been well developed and used\nin many applications, especially in pose (position and orientation) alignment.\nHowever, most research papers focused on developing control solutions when 3D\npoint features can be detected inside the field of view. This work proposes an\ninnovative feedforward-feedback adaptive control algorithm structure with the\nYoula Parameterization method. A designed feature estimation loop ensures\nstable and fast motion control when point features are outside the field of\nview. As 3D point features move inside the field of view, the IBVS feedback\nloop preserves the precision of the pose at the end of the control period.\nAlso, an adaptive controller is developed in the feedback loop to stabilize the\nsystem in the entire range of operations. The nonlinear camera and robot\nmanipulator model is linearized and decoupled online by an adaptive algorithm.\nThe adaptive controller is then computed based on the linearized model\nevaluated at current linearized point. The proposed solution is robust and easy\nto implement in different industrial robotic systems. Various scenarios are\nused in simulations to validate the effectiveness and robust performance of the\nproposed controller.", "AI": {"tldr": "提出了一种基于Youla参数化的前馈-反馈自适应控制算法，用于解决3D点特征在视野外时的视觉伺服控制问题。", "motivation": "现有视觉伺服控制方法多依赖视野内的3D点特征，但实际应用中特征可能不在视野内，需一种更鲁棒的解决方案。", "method": "结合前馈-反馈控制与Youla参数化，设计特征估计环和自适应控制器，在线线性化解耦相机与机械臂模型。", "result": "仿真验证了控制器在视野内外均能实现稳定、快速且高精度的位姿控制。", "conclusion": "该算法鲁棒性强，易于工业机器人系统实现，适用于复杂场景。"}}
{"id": "2506.10239", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10239", "abs": "https://arxiv.org/abs/2506.10239", "authors": ["Maximilian Mühlbauer", "Freek Stulp", "Sylvain Calinon", "Alin Albu-Schäffer", "João Silvério"], "title": "A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures", "comment": null, "summary": "Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the\nmost suitable haptic feedback for each phase of a task, based on learned or\nperceived uncertainty. While keeping the human in the loop remains essential,\nfor instance, to ensure high precision, partial automation of certain task\nphases is critical for productivity. We present a unified framework for\nprobabilistic VFs that seamlessly switches between manual fixtures,\nsemi-automated fixtures (with the human handling precise tasks), and full\nautonomy. We introduce a novel probabilistic Dynamical System-based VF for\ncoarse guidance, enabling the robot to autonomously complete certain task\nphases while keeping the human operator in the loop. For tasks requiring\nprecise guidance, we extend probabilistic position-based trajectory fixtures\nwith automation allowing for seamless human interaction as well as\ngeometry-awareness and optimal impedance gains. For manual tasks requiring very\nprecise guidance, we also extend visual servoing fixtures with the same\ngeometry-awareness and impedance behaviour. We validate our approach\nexperimentally on different robots, showcasing multiple operation modes and the\nease of programming fixtures.", "AI": {"tldr": "该论文提出了一种统一的概率虚拟夹具框架，支持手动、半自动和全自动模式的切换，结合几何感知和最优阻抗增益，验证了其多模式操作的灵活性和易编程性。", "motivation": "为了提高任务执行的生产力和精度，需要在保持人类参与的同时，部分自动化任务阶段，因此需要一种能够自适应选择合适触觉反馈的虚拟夹具框架。", "method": "提出了一种基于概率动态系统的虚拟夹具，用于粗引导；扩展了概率位置轨迹夹具和视觉伺服夹具，支持几何感知和阻抗行为。", "result": "实验验证了该框架在不同机器人上的多模式操作和易编程性。", "conclusion": "该框架成功实现了手动、半自动和全自动模式的灵活切换，提升了任务执行效率和精度。"}}
{"id": "2506.10252", "categories": ["cs.RO", "cs.SY", "eess.SY", "93B52 (Primary), 93C85 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.10252", "abs": "https://arxiv.org/abs/2506.10252", "authors": ["Rongfei Li", "Francis Assadian"], "title": "A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control", "comment": "36 pages, 19 figures, Journal, Published in: Applied Sciences, 2025,\n  vol. 15, article 4991. For published version, see this http URL:\n  https://doi.org/10.3390/app15094991", "summary": "In robot navigation and manipulation, accurately determining the camera's\npose relative to the environment is crucial for effective task execution. In\nthis paper, we systematically prove that this problem corresponds to the\nPerspective-3-Point (P3P) formulation, where exactly three known 3D points and\ntheir corresponding 2D image projections are used to estimate the pose of a\nstereo camera. In image-based visual servoing (IBVS) control, the system\nbecomes overdetermined, as the 6 degrees of freedom (DoF) of the stereo camera\nmust align with 9 observed 2D features in the scene. When more constraints are\nimposed than available DoFs, global stability cannot be guaranteed, as the\ncamera may become trapped in a local minimum far from the desired configuration\nduring servoing. To address this issue, we propose a novel control strategy for\naccurately positioning a calibrated stereo camera. Our approach integrates a\nfeedforward controller with a Youla parameterization-based feedback controller,\nensuring robust servoing performance. Through simulations, we demonstrate that\nour method effectively avoids local minima and enables the camera to reach the\ndesired pose accurately and efficiently.", "AI": {"tldr": "论文提出了一种新的控制策略，结合前馈控制器和基于Youla参数化的反馈控制器，解决了立体相机在视觉伺服控制中因过约束导致的局部极小值问题。", "motivation": "在机器人导航和操作中，相机位姿的精确估计对任务执行至关重要。传统方法在视觉伺服控制中因过约束可能导致全局稳定性无法保证。", "method": "提出了一种结合前馈控制器和基于Youla参数化的反馈控制器的策略，确保鲁棒的伺服性能。", "result": "仿真实验表明，该方法能有效避免局部极小值，使相机准确高效地达到目标位姿。", "conclusion": "该方法为解决视觉伺服控制中的过约束问题提供了有效方案，提升了相机位姿估计的准确性和稳定性。"}}
{"id": "2506.10279", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.10279", "abs": "https://arxiv.org/abs/2506.10279", "authors": ["Alexandre Capone", "Ryan Cosner", "Aaaron Ames", "Sandra Hirche"], "title": "Learning Safe Control via On-the-Fly Bandit Exploration", "comment": "arXiv admin note: text overlap with arXiv:2311.02133", "summary": "Control tasks with safety requirements under high levels of model uncertainty\nare increasingly common. Machine learning techniques are frequently used to\naddress such tasks, typically by leveraging model error bounds to specify\nrobust constraint-based safety filters. However, if the learned model\nuncertainty is very high, the corresponding filters are potentially invalid,\nmeaning no control input satisfies the constraints imposed by the safety\nfilter. While most works address this issue by assuming some form of safe\nbackup controller, ours tackles it by collecting additional data on the fly\nusing a Gaussian process bandit-type algorithm. We combine a control barrier\nfunction with a learned model to specify a robust certificate that ensures\nsafety if feasible. Whenever infeasibility occurs, we leverage the control\nbarrier function to guide exploration, ensuring the collected data contributes\ntoward the closed-loop system safety. By combining a safety filter with\nexploration in this manner, our method provably achieves safety in a setting\nthat allows for a zero-mean prior dynamics model, without requiring a backup\ncontroller. To the best of our knowledge, it is the first safe learning-based\ncontrol method that achieves this.", "AI": {"tldr": "论文提出了一种结合高斯过程与控制屏障函数的方法，用于在高模型不确定性下确保控制任务的安全性，无需备用控制器。", "motivation": "在高模型不确定性的控制任务中，传统方法依赖备用控制器确保安全，但可能因模型误差过大而失效。本文旨在通过动态数据收集解决这一问题。", "method": "结合高斯过程（GP）与控制屏障函数（CBF），动态收集数据以确保安全约束的可行性，避免备用控制器的需求。", "result": "方法在零均值先验动力学模型下可证明地实现安全性，是首个无需备用控制器的安全学习控制方法。", "conclusion": "通过动态探索与安全过滤的结合，本文方法在高不确定性环境中实现了安全控制，为学习控制提供了新思路。"}}
{"id": "2506.10383", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.10383", "abs": "https://arxiv.org/abs/2506.10383", "authors": ["Nidhi Homey Parayil", "Thierry Peynot", "Chris Lehnert"], "title": "RICE: Reactive Interaction Controller for Cluttered Canopy Environment", "comment": "This work has been submitted to the IEEE RAL for possible publication", "summary": "Robotic navigation in dense, cluttered environments such as agricultural\ncanopies presents significant challenges due to physical and visual occlusion\ncaused by leaves and branches. Traditional vision-based or model-dependent\napproaches often fail in these settings, where physical interaction without\ndamaging foliage and branches is necessary to reach a target. We present a\nnovel reactive controller that enables safe navigation for a robotic arm in a\ncontact-rich, cluttered, deformable environment using end-effector position and\nreal-time tactile feedback. Our proposed framework's interaction strategy is\nbased on a trade-off between minimizing disturbance by maneuvering around\nobstacles and pushing through them to move towards the target. We show that\nover 35 trials in 3 experimental plant setups with an occluded target, the\nproposed controller successfully reached the target in all trials without\nbreaking any branch and outperformed the state-of-the-art model-free controller\nin robustness and adaptability. This work lays the foundation for safe,\nadaptive interaction in cluttered, contact-rich deformable environments,\nenabling future agricultural tasks such as pruning and harvesting in plant\ncanopies.", "AI": {"tldr": "提出了一种新型反应控制器，用于机器臂在密集、遮挡环境中安全导航，结合末端执行器位置和实时触觉反馈。", "motivation": "农业冠层等密集杂乱环境中，传统视觉或模型依赖方法常因物理和视觉遮挡失效，需在不破坏枝叶的情况下导航。", "method": "基于末端执行器位置和实时触觉反馈，设计了一种权衡绕过障碍与推动障碍的交互策略。", "result": "在3种实验植物设置中，35次试验均成功到达目标且未损坏任何枝叶，优于现有无模型控制器。", "conclusion": "为密集、接触丰富的可变形环境中的安全自适应交互奠定了基础，支持未来农业任务如修剪和收获。"}}
{"id": "2506.10287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10287", "abs": "https://arxiv.org/abs/2506.10287", "authors": ["Rohit Sonker", "Alexandre Capone", "Andrew Rothstein", "Hiro Josep Farre Kaga", "Egemen Kolemen", "Jeff Schneider"], "title": "Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks", "comment": null, "summary": "Machine learning algorithms often struggle to control complex real-world\nsystems. In the case of nuclear fusion, these challenges are exacerbated, as\nthe dynamics are notoriously complex, data is poor, hardware is subject to\nfailures, and experiments often affect dynamics beyond the experiment's\nduration. Existing tools like reinforcement learning, supervised learning, and\nBayesian optimization address some of these challenges but fail to provide a\ncomprehensive solution. To overcome these limitations, we present a multi-scale\nBayesian optimization approach that integrates a high-frequency data-driven\ndynamics model with a low-frequency Gaussian process. By updating the Gaussian\nprocess between experiments, the method rapidly adapts to new data, refining\nthe predictions of the less reliable dynamical model. We validate our approach\nby controlling tearing instabilities in the DIII-D nuclear fusion plant.\nOffline testing on historical data shows that our method significantly\noutperforms several baselines. Results on live experiments on the DIII-D\ntokamak, conducted under high-performance plasma scenarios prone to\ninstabilities, shows a 50% success rate, marking a 117% improvement over\nhistorical outcomes.", "AI": {"tldr": "提出了一种多尺度贝叶斯优化方法，用于控制核聚变中的撕裂不稳定性，显著优于现有方法。", "motivation": "核聚变系统动态复杂、数据质量差、硬件易故障，现有机器学习工具无法全面解决这些问题。", "method": "结合高频数据驱动动态模型与低频高斯过程，通过实验间更新高斯过程快速适应新数据。", "result": "离线测试显著优于基线；现场实验成功率达50%，比历史结果提升117%。", "conclusion": "多尺度贝叶斯优化方法在核聚变控制中表现出色，为解决复杂系统问题提供了新思路。"}}
{"id": "2506.10317", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10317", "abs": "https://arxiv.org/abs/2506.10317", "authors": ["Akshar Tumu", "Henrik I. Christensen", "Marcell Vazquez-Chanlatte", "Chikao Tsuchiya", "Dhaval Bhanderi"], "title": "Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving", "comment": "4 pages, 3 figures, Accepted at RSS 2025 Workshop -\n  RobotEvaluation@RSS2025", "summary": "Lane-topology prediction is a critical component of safe and reliable\nautonomous navigation. An accurate understanding of the road environment aids\nthis task. We observe that this information often follows conventions encoded\nin natural language, through design codes that reflect the road structure and\nroad names that capture the road functionality. We augment this information in\na lightweight manner to SMERF, a map-prior-based online lane-topology\nprediction model, by combining structured road metadata from OSM maps and\nlane-width priors from Road design manuals with the road centerline encodings.\nWe evaluate our method on two geo-diverse complex intersection scenarios. Our\nmethod shows improvement in both lane and traffic element detection and their\nassociation. We report results using four topology-aware metrics to\ncomprehensively assess the model performance. These results demonstrate the\nability of our approach to generalize and scale to diverse topologies and\nconditions.", "AI": {"tldr": "论文提出了一种轻量级方法，通过结合OSM地图的结构化道路元数据和道路设计手册中的车道宽度先验，改进SMERF模型的车道拓扑预测性能。", "motivation": "车道拓扑预测对自动驾驶导航至关重要，但现有方法缺乏对道路环境自然语言信息的利用。", "method": "结合OSM地图的道路元数据和车道宽度先验，增强SMERF模型的车道拓扑预测能力。", "result": "在两个地理多样化的复杂交叉口场景中，模型在车道和交通元素检测及其关联方面表现提升。", "conclusion": "该方法能够泛化并适应多样化的拓扑结构和条件。"}}
{"id": "2506.10359", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10359", "abs": "https://arxiv.org/abs/2506.10359", "authors": ["Che Wang", "Jeroen van Baar", "Chaitanya Mitash", "Shuai Li", "Dylan Randle", "Weiyao Wang", "Sumedh Sontakke", "Kostas E. Bekris", "Kapil Katyal"], "title": "Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success", "comment": "Accepted to Robotics: Science and Systems (RSS 2025), 15 pages", "summary": "This work demonstrates how autonomously learning aspects of robotic operation\nfrom sparsely-labeled, real-world data of deployed, engineered solutions at\nindustrial scale can provide with solutions that achieve improved performance.\nSpecifically, it focuses on multi-suction robot picking and performs a\ncomprehensive study on the application of multi-modal visual encoders for\npredicting the success of candidate robotic picks. Picking diverse items from\nunstructured piles is an important and challenging task for robot manipulation\nin real-world settings, such as warehouses. Methods for picking from clutter\nmust work for an open set of items while simultaneously meeting latency\nconstraints to achieve high throughput. The demonstrated approach utilizes\nmultiple input modalities, such as RGB, depth and semantic segmentation, to\nestimate the quality of candidate multi-suction picks. The strategy is trained\nfrom real-world item picking data, with a combination of multimodal pretrain\nand finetune. The manuscript provides comprehensive experimental evaluation\nperformed over a large item-picking dataset, an item-picking dataset targeted\nto include partial occlusions, and a package-picking dataset, which focuses on\ncontainers, such as boxes and envelopes, instead of unpackaged items. The\nevaluation measures performance for different item configurations, pick scenes,\nand object types. Ablations help to understand the effects of in-domain\npretraining, the impact of different modalities and the importance of\nfinetuning. These ablations reveal both the importance of training over\nmultiple modalities but also the ability of models to learn during pretraining\nthe relationship between modalities so that during finetuning and inference,\nonly a subset of them can be used as input.", "AI": {"tldr": "该研究展示了如何从稀疏标记的工业规模真实数据中自主学习机器人操作，以提升性能，重点关注多吸盘机器人抓取任务。", "motivation": "解决从杂乱堆中抓取多样化物品的挑战，满足高吞吐量的延迟要求，适用于仓库等实际场景。", "method": "利用RGB、深度和语义分割等多模态输入预测抓取质量，结合多模态预训练和微调策略。", "result": "在大规模物品抓取数据集上进行了全面实验，评估了不同物品配置、场景和物体类型，揭示了多模态训练和预训练的重要性。", "conclusion": "多模态训练和预训练对性能至关重要，模型能在预训练中学习模态间关系，微调和推理时可仅使用部分模态。"}}
{"id": "2506.10363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10363", "abs": "https://arxiv.org/abs/2506.10363", "authors": ["Daniel Betschinske", "Malte Schrimpf", "Steven Peters", "Kamil Klonecki", "Jan Peter Karch", "Moritz Lippert"], "title": "Towards more efficient quantitative safety validation of residual risk for assisted and automated driving", "comment": null, "summary": "The safety validation of Advanced Driver Assistance Systems (ADAS) and\nAutomated Driving Systems (ADS) increasingly demands efficient and reliable\nmethods to quantify residual risk while adhering to international standards\nsuch as ISO 21448. Traditionally, Field Operational Testing (FOT) has been\npivotal for macroscopic safety validation of automotive driving functions up to\nSAE automation level 2. However, state-of-the-art derivations for empirical\nsafety demonstrations using FOT often result in impractical testing efforts,\nparticularly at higher automation levels. Even at lower automation levels, this\nlimitation - coupled with the substantial costs associated with FOT - motivates\nthe exploration of approaches to enhance the efficiency of FOT-based\nmacroscopic safety validation. Therefore, this publication systematically\nidentifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT,\nincluding novel methods reported in the literature. Based on an analysis of ISO\n21448, two models are derived: a generic model capturing the argumentation\ncomponents of the standard, and a base model, exemplarily applied to Automatic\nEmergency Braking (AEB) systems, establishing a baseline for the real-world\ndriving requirement for a Quantitative Safety Validation of Residual Risk\n(QSVRR). Subsequently, the RAs are assessed using four criteria:\nquantifiability, threats to validity, missing links, and black box\ncompatibility, highlighting potential benefits, inherent limitations, and\nidentifying key areas for further research. Our evaluation reveals that, while\nseveral approaches offer potential, none are free from missing links or other\nsubstantial shortcomings. Moreover, no identified alternative can fully replace\nFOT, reflecting its crucial role in the safety validation of ADAS and ADS.", "AI": {"tldr": "论文探讨了如何提高基于FOT的宏观安全验证效率，以应对ADAS和ADS安全验证中的高成本和测试限制。", "motivation": "传统FOT方法在高自动化级别下测试成本过高且效率低下，亟需改进方法以满足国际标准（如ISO 21448）的要求。", "method": "系统识别并评估了FOT的减少方法（RAs），基于ISO 21448构建了通用模型和基础模型（以AEB为例），并通过四项标准评估RAs。", "result": "研究发现现有方法虽有一定潜力，但均存在不足，且无法完全替代FOT。", "conclusion": "FOT在ADAS和ADS安全验证中仍不可或缺，未来需进一步研究以优化RAs。"}}
{"id": "2506.10462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10462", "abs": "https://arxiv.org/abs/2506.10462", "authors": ["Ana Müller", "Sabina Jeschke", "Anja Richert"], "title": "Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions", "comment": "Accepted as a regular paper at the 2025 IEEE International Conference\n  on Robot and Human Interactive Communication (RO-MAN). \\c{opyright} IEEE.\n  This is the preprint version. The final version will appear in the IEEE\n  proceedings", "summary": "This paper investigates the impact of a group-adaptive conversation design in\ntwo socially interactive agents (SIAs) through two real-world studies. Both\nSIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped\nwith a conversational artificial intelligence (CAI) backend combining hybrid\nretrieval and generative models. The studies were carried out in an in-the-wild\nsetting with a total of $N = 188$ participants who interacted with the SIAs -\nin dyads, triads or larger groups - at a German museum. Although the results\ndid not reveal a significant effect of the group-sensitive conversation design\non perceived satisfaction, the findings provide valuable insights into the\nchallenges of adapting CAI for multi-party interactions and across different\nembodiments (robot vs.\\ virtual agent), highlighting the need for multimodal\nstrategies beyond linguistic pluralization. These insights contribute to the\nfields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and\nbroader Human-Machine Interaction (HMI), providing insights for future research\non effective dialogue adaptation in group settings.", "AI": {"tldr": "研究探讨了群体自适应对话设计对两种社交互动代理（SIAs）的影响，发现设计对满意度无显著影响，但揭示了多模态策略的重要性。", "motivation": "探索群体敏感的对话设计在社交互动代理（SIAs）中的效果，以改进多群体交互体验。", "method": "在真实博物馆环境中，使用Furhat机器人和MetaHuman虚拟代理，结合混合检索和生成模型，与188名参与者进行交互实验。", "result": "群体敏感对话设计对满意度无显著影响，但揭示了多群体交互和不同代理形态（机器人与虚拟代理）的挑战。", "conclusion": "研究为HAI、HRI和HMI领域提供了未来群体对话适应策略的启示，强调多模态方法的重要性。"}}
{"id": "2506.10600", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10600", "abs": "https://arxiv.org/abs/2506.10600", "authors": ["Wang Xinjie", "Liu Liu", "Cao Yu", "Wu Ruiqi", "Qin Wenkang", "Wang Dehui", "Sui Wei", "Su Zhizhong"], "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence", "comment": null, "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.", "AI": {"tldr": "EmbodiedGen是一个用于生成高质量、可控且逼真的3D资产的基础平台，支持低成本生成物理属性准确的3D世界，适用于具身智能任务。", "motivation": "当前具身智能任务依赖传统3D图形资产，成本高且真实性有限，限制了数据驱动方法的扩展性。", "method": "EmbodiedGen通过六个关键模块（如图像到3D、文本到3D等）生成多样化的交互式3D世界，利用生成式AI技术。", "result": "生成的3D资产具有高质量、可控性和物理准确性，可直接用于物理仿真引擎。", "conclusion": "EmbodiedGen为具身智能研究提供了低成本、高扩展性的解决方案，解决了数据生成和评估的挑战。"}}
{"id": "2506.10686", "categories": ["cs.RO", "cs.SC", "math.GR", "math.OC", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2506.10686", "abs": "https://arxiv.org/abs/2506.10686", "authors": ["Andreas Mueller"], "title": "An $O(n$)-Algorithm for the Higher-Order Kinematics and Inverse Dynamics of Serial Manipulators using Spatial Representation of Twists", "comment": null, "summary": "Optimal control in general, and flatness-based control in particular, of\nrobotic arms necessitate to compute the first and second time derivatives of\nthe joint torques/forces required to achieve a desired motion. In view of the\nrequired computational efficiency, recursive $O(n)$-algorithms were proposed to\nthis end. Aiming at compact yet efficient formulations, a Lie group formulation\nwas recently proposed, making use of body-fixed and hybrid representation of\ntwists and wrenches. In this paper a formulation is introduced using the\nspatial representation. The second-order inverse dynamics algorithm is\naccompanied by a fourth-order forward and inverse kinematics algorithm. An\nadvantage of all Lie group formulations is that they can be parameterized in\nterms of vectorial quantities that are readily available. The method is\ndemonstrated for the 7 DOF Franka Emika Panda robot.", "AI": {"tldr": "本文提出了一种基于空间表示的二阶逆动力学算法，并伴随四阶正逆运动学算法，适用于机器人臂的优化控制。", "motivation": "研究机器人臂控制中高效计算关节力矩/力的时间导数的需求，特别是在平坦性控制中。", "method": "采用空间表示和Lie群公式化，提出二阶逆动力学算法和四阶正逆运动学算法。", "result": "方法在7自由度Franka Emika Panda机器人上验证，展示了其高效性和紧凑性。", "conclusion": "Lie群公式化方法在机器人控制中具有高效性和实用性，适用于实际应用。"}}
{"id": "2506.10756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10756", "abs": "https://arxiv.org/abs/2506.10756", "authors": ["Yuhang Zhang", "Haosheng Yu", "Jiaping Xiao", "Mir Feroskhan"], "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding", "comment": null, "summary": "Vision-and-language navigation (VLN) is a long-standing challenge in\nautonomous robotics, aiming to empower agents with the ability to follow human\ninstructions while navigating complex environments. Two key bottlenecks remain\nin this field: generalization to out-of-distribution environments and reliance\non fixed discrete action spaces. To address these challenges, we propose\nVision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles\n(UAVs) to execute language-guided flight. Without the requirement for\nlocalization or active ranging sensors, VLFly outputs continuous velocity\ncommands purely from egocentric observations captured by an onboard monocular\ncamera. The VLFly integrates three modules: an instruction encoder based on a\nlarge language model (LLM) that reformulates high-level language into\nstructured prompts, a goal retriever powered by a vision-language model (VLM)\nthat matches these prompts to goal images via vision-language similarity, and a\nwaypoint planner that generates executable trajectories for real-time UAV\ncontrol. VLFly is evaluated across diverse simulation environments without\nadditional fine-tuning and consistently outperforms all baselines. Moreover,\nreal-world VLN tasks in indoor and outdoor environments under direct and\nindirect instructions demonstrate that VLFly achieves robust open-vocabulary\ngoal understanding and generalized navigation capabilities, even in the\npresence of abstract language input.", "AI": {"tldr": "VLFly框架通过结合大型语言模型和视觉语言模型，为无人机提供语言引导的连续速度控制，解决了视觉语言导航中的泛化问题和离散动作空间依赖。", "motivation": "解决视觉语言导航中泛化能力不足和依赖固定离散动作空间的问题。", "method": "VLFly整合指令编码器、目标检索器和路径规划器，利用单目摄像头输入生成连续速度命令。", "result": "在多样化模拟环境和真实任务中表现优异，支持开放词汇目标理解和泛化导航。", "conclusion": "VLFly展示了在复杂环境中基于语言指令的鲁棒导航能力。"}}
{"id": "2506.10787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10787", "abs": "https://arxiv.org/abs/2506.10787", "authors": ["Felix Nonnengießer", "Alap Kshirsagar", "Boris Belousov", "Jan Peters"], "title": "In-Hand Object Pose Estimation via Visual-Tactile Fusion", "comment": "8 pages", "summary": "Accurate in-hand pose estimation is crucial for robotic object manipulation,\nbut visual occlusion remains a major challenge for vision-based approaches.\nThis paper presents an approach to robotic in-hand object pose estimation,\ncombining visual and tactile information to accurately determine the position\nand orientation of objects grasped by a robotic hand. We address the challenge\nof visual occlusion by fusing visual information from a wrist-mounted RGB-D\ncamera with tactile information from vision-based tactile sensors mounted on\nthe fingertips of a robotic gripper. Our approach employs a weighting and\nsensor fusion module to combine point clouds from heterogeneous sensor types\nand control each modality's contribution to the pose estimation process. We use\nan augmented Iterative Closest Point (ICP) algorithm adapted for weighted point\nclouds to estimate the 6D object pose. Our experiments show that incorporating\ntactile information significantly improves pose estimation accuracy,\nparticularly when occlusion is high. Our method achieves an average pose\nestimation error of 7.5 mm and 16.7 degrees, outperforming vision-only\nbaselines by up to 20%. We also demonstrate the ability of our method to\nperform precise object manipulation in a real-world insertion task.", "AI": {"tldr": "提出了一种结合视觉和触觉信息的机器人手内物体姿态估计方法，通过融合RGB-D相机和触觉传感器的数据，显著提高了姿态估计的准确性。", "motivation": "视觉遮挡是视觉方法在机器人手内物体姿态估计中的主要挑战，需要结合其他传感器信息以提高准确性。", "method": "融合手腕RGB-D相机和指尖触觉传感器的数据，采用加权传感器融合模块和加权点云的改进ICP算法进行姿态估计。", "result": "实验表明，触觉信息的加入显著提高了姿态估计精度，平均误差为7.5毫米和16.7度，优于纯视觉方法20%。", "conclusion": "该方法在视觉遮挡情况下表现优异，并能支持精确的物体操作任务。"}}
{"id": "2506.10826", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10826", "abs": "https://arxiv.org/abs/2506.10826", "authors": ["Wenxuan Song", "Jiayi Chen", "Wenxue Li", "Xu He", "Han Zhao", "Pengxiang Ding Shiyan Su", "Feilong Tang", "Xuelian Cheng", "Donglin Wang", "Zongyuan Ge", "Xinhu Zheng", "Zhe Liu", "Hesheng Wang", "Yunhui Liu", "Haoang Li"], "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System", "comment": "14 pages", "summary": "A fundamental requirement for real-world robotic deployment is the ability to\nunderstand and respond to natural language instructions. Existing\nlanguage-conditioned manipulation tasks typically assume that instructions are\nperfectly aligned with the environment. This assumption limits robustness and\ngeneralization in realistic scenarios where instructions may be ambiguous,\nirrelevant, or infeasible. To address this problem, we introduce RAtional\nMAnipulation (RAMA), a new benchmark that challenges models with both unseen\nexecutable instructions and defective ones that should be rejected. In RAMA, we\nconstruct a dataset with over 14,000 samples, including diverse defective\ninstructions spanning six dimensions: visual, physical, semantic, motion,\nsafety, and out-of-context. We further propose the Rational\nVision-Language-Action model (RationalVLA). It is a dual system for robotic\narms that integrates the high-level vision-language model with the low-level\nmanipulation policy by introducing learnable latent space embeddings. This\ndesign enables RationalVLA to reason over instructions, reject infeasible\ncommands, and execute manipulation effectively. Experiments demonstrate that\nRationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher\nsuccess rate and 0.94 average task length, while maintaining competitive\nperformance on standard manipulation tasks. Real-world trials further validate\nits effectiveness and robustness in practical applications. Our project page is\nhttps://irpn-eai.github.io/rationalvla.", "AI": {"tldr": "RAMA是一个新的机器人操作基准，挑战模型处理未见过或缺陷指令的能力，RationalVLA模型通过双系统设计显著提升性能。", "motivation": "现实场景中指令可能模糊或不合理，现有方法假设完美对齐环境，限制了鲁棒性和泛化能力。", "method": "提出RAMA基准和RationalVLA模型，结合高层视觉语言模型与低层操作策略，通过可学习潜在空间嵌入实现推理与拒绝缺陷指令。", "result": "RationalVLA在RAMA上成功率提升14.5%，任务长度平均0.94，并在标准任务中保持竞争力。", "conclusion": "RationalVLA在现实应用中验证了其有效性和鲁棒性，为语言驱动的机器人操作提供了新方向。"}}
{"id": "2506.10850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10850", "abs": "https://arxiv.org/abs/2506.10850", "authors": ["Derek Benham", "Easton Potokar", "Joshua G. Mangelson"], "title": "Invariant Extended Kalman Filter for Autonomous Surface Vessels with Partial Orientation Measurements", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics. 8 pages,\n  4 figures, 2 tables", "summary": "Autonomous surface vessels (ASVs) are increasingly vital for marine science,\noffering robust platforms for underwater mapping and inspection. Accurate state\nestimation, particularly of vehicle pose, is paramount for precise seafloor\nmapping, as even small surface deviations can have significant consequences\nwhen sensing the seafloor below. To address this challenge, we propose an\nInvariant Extended Kalman Filter (InEKF) framework designed to integrate\npartial orientation measurements. While conventional estimation often relies on\nrelative position measurements to fixed landmarks, open ocean ASVs primarily\nobserve a receding horizon. We leverage forward-facing monocular cameras to\nestimate roll and pitch with respect to this horizon, which provides\nyaw-ambiguous partial orientation information. To effectively utilize these\nmeasurements within the InEKF, we introduce a novel framework for incorporating\nsuch partial orientation data. This approach contrasts with traditional InEKF\nimplementations that assume full orientation measurements and is particularly\nrelevant for planar vehicle motion constrained to a \"seafaring plane.\" This\npaper details the developed InEKF framework; its integration with horizon-based\nroll/pitch observations and dual-antenna GPS heading measurements for ASV state\nestimation; and provides a comparative analysis against the InEKF using full\norientation and a Multiplicative EKF (MEKF). Our results demonstrate the\nefficacy and robustness of the proposed partial orientation measurements for\naccurate ASV state estimation in open ocean environments.", "AI": {"tldr": "提出了一种基于不变扩展卡尔曼滤波（InEKF）的框架，用于整合部分方向测量，以提升自主水面船只（ASV）在开放海洋环境中的状态估计精度。", "motivation": "ASV在海洋科学中日益重要，但精确的状态估计（尤其是姿态）对海底测绘至关重要。传统方法依赖固定地标的相对位置测量，而开放海洋中ASV主要观测后退地平线，因此需要新的解决方案。", "method": "利用前向单目相机估计相对于地平线的滚转和俯仰角，结合双天线GPS航向测量，提出了一种新颖的InEKF框架，用于整合部分方向数据。", "result": "结果表明，所提出的部分方向测量方法在开放海洋环境中对ASV状态估计具有高效性和鲁棒性。", "conclusion": "该框架为ASV在缺乏完整方向测量的环境中提供了更准确的状态估计方法，优于传统InEKF和MEKF。"}}
{"id": "2506.10875", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.10875", "abs": "https://arxiv.org/abs/2506.10875", "authors": ["Guanjin Wang", "Xiangxue Zhao", "Shapour Azarm", "Balakumar Balachandran"], "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material", "comment": null, "summary": "An alternative data-driven modeling approach has been proposed and employed\nto gain fundamental insights into robot motion interaction with granular\nterrain at certain length scales. The approach is based on an integration of\ndimension reduction (Sequentially Truncated Higher-Order Singular Value\nDecomposition), surrogate modeling (Gaussian Process), and data assimilation\ntechniques (Reduced Order Particle Filter). This approach can be used online\nand is based on offline data, obtained from the offline collection of\nhigh-fidelity simulation data and a set of sparse experimental data. The\nresults have shown that orders of magnitude reduction in computational time can\nbe obtained from the proposed data-driven modeling approach compared with\nphysics-based high-fidelity simulations. With only simulation data as input,\nthe data-driven prediction technique can generate predictions that have\ncomparable accuracy as simulations. With both simulation data and sparse\nphysical experimental measurement as input, the data-driven approach with its\nembedded data assimilation techniques has the potential in outperforming only\nhigh-fidelity simulations for the long-horizon predictions. In addition, it is\ndemonstrated that the data-driven modeling approach can also reproduce the\nscaling relationship recovered by physics-based simulations for maximum\nresistive forces, which may indicate its general predictability beyond a\ncase-by-case basis. The results are expected to help robot navigation and\nexploration in unknown and complex terrains during both online and offline\nphases.", "AI": {"tldr": "提出了一种数据驱动建模方法，结合降维、代理建模和数据同化技术，显著减少计算时间，预测精度接近高保真模拟，并可能超越模拟在长期预测中的表现。", "motivation": "研究机器人运动与颗粒地形交互的基本原理，提升机器人在未知复杂地形中的导航和探索能力。", "method": "集成降维（ST-HOSVD）、代理建模（高斯过程）和数据同化技术（降阶粒子滤波），基于离线高保真模拟数据和稀疏实验数据。", "result": "计算时间大幅减少，预测精度接近模拟；结合实验数据时，长期预测可能优于模拟；能复现物理模拟的缩放关系。", "conclusion": "该方法为机器人在复杂地形中的在线和离线导航提供了高效可靠的建模工具。"}}
{"id": "2506.10884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10884", "abs": "https://arxiv.org/abs/2506.10884", "authors": ["Dong Hae Mangalindan", "Karthik Kandikonda", "Ericka Rovira", "Vaibhav Srivastava"], "title": "Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies", "comment": null, "summary": "With increasing efficiency and reliability, autonomous systems are becoming\nvaluable assistants to humans in various tasks. In the context of\nrobot-assisted delivery, we investigate how robot performance and trust repair\nstrategies impact human trust. In this task, while handling a secondary task,\nhumans can choose to either send the robot to deliver autonomously or manually\ncontrol it. The trust repair strategies examined include short and long\nexplanations, apology and promise, and denial.\n  Using data from human participants, we model human behavior using an\nInput-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and\nhuman action probabilities. Our findings indicate that humans are more likely\nto deploy the robot autonomously when their trust is high. Furthermore, state\ntransition estimates show that long explanations are the most effective at\nrepairing trust following a failure, while denial is most effective at\npreventing trust loss.\n  We also demonstrate that the trust estimates generated by our model are\nisomorphic to self-reported trust values, making them interpretable. This model\nlays the groundwork for developing optimal policies that facilitate real-time\nadjustment of human trust in autonomous systems.", "AI": {"tldr": "研究机器人辅助送货中，机器人表现和信任修复策略对人类信任的影响，发现长解释最能修复信任，否认最能防止信任流失。", "motivation": "随着自主系统效率和可靠性的提升，研究其在任务中对人类信任的影响，以优化人机协作。", "method": "使用输入-输出隐马尔可夫模型（IOHMM）分析人类行为数据，建模信任动态和行动概率。", "result": "高信任时人类更倾向自主部署机器人；长解释修复信任效果最佳，否认防止信任流失最有效。", "conclusion": "模型生成的信任估计与自报值一致，为实时调整人机信任的政策奠定基础。"}}
{"id": "2506.10923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10923", "abs": "https://arxiv.org/abs/2506.10923", "authors": ["Xili Yi", "Nima Fazeli"], "title": "Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations", "comment": "11 pages, 12 figures", "summary": "We introduce Vib2Move, a novel approach for in-hand object reconfiguration\nthat uses fingertip micro-vibrations and gravity to precisely reposition planar\nobjects. Our framework comprises three key innovations. First, we design a\nvibration-based actuator that dynamically modulates the effective finger-object\nfriction coefficient, effectively emulating changes in gripping force. Second,\nwe derive a sliding motion model for objects clamped in a parallel gripper with\ntwo symmetric, variable-friction contact patches. Third, we propose a motion\nplanner that coordinates end-effector finger trajectories and fingertip\nvibrations to achieve the desired object pose. In real-world trials, Vib2Move\nconsistently yields final positioning errors below 6 mm, demonstrating\nreliable, high-precision manipulation across a variety of planar objects. For\nmore results and information, please visit https://vib2move.github.io.", "AI": {"tldr": "Vib2Move是一种利用指尖微振动和重力精确重新定位平面物体的新方法，通过动态调节摩擦系数和协调运动规划实现高精度操作。", "motivation": "研究动机是开发一种能够精确重新定位平面物体的方法，利用微振动和重力实现高精度操作。", "method": "方法包括设计振动执行器动态调节摩擦系数、推导滑动运动模型以及提出协调运动规划器。", "result": "实验结果展示Vib2Move的最终定位误差低于6毫米，适用于多种平面物体。", "conclusion": "结论是Vib2Move是一种可靠且高精度的平面物体操作方法。"}}
{"id": "2506.10966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10966", "abs": "https://arxiv.org/abs/2506.10966", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation", "comment": null, "summary": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.", "AI": {"tldr": "GenManip是一个专为策略泛化研究设计的仿真平台，通过LLM驱动的任务导向场景图生成多样化任务，并引入GenManip-Bench基准评估策略泛化能力。", "motivation": "现有仿真平台在支持策略适应多样化指令和场景方面不足，无法满足对指令跟随基础模型（如LLMs）泛化能力的研究需求。", "method": "提出GenManip平台，利用LLM驱动的任务导向场景图自动生成大规模多样化任务，并创建GenManip-Bench基准评估策略。", "result": "评估显示，基于基础模型的模块化系统在多样化场景中泛化能力更强，而端到端策略通过数据扩展也能获益。", "conclusion": "GenManip平台有望为现实条件下策略泛化的研究提供关键支持。"}}
{"id": "2506.10968", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10968", "abs": "https://arxiv.org/abs/2506.10968", "authors": ["Justin Kerr", "Kush Hari", "Ethan Weber", "Chung Min Kim", "Brent Yi", "Tyler Bonnen", "Ken Goldberg", "Angjoo Kanazawa"], "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop", "comment": "Project page: https://www.eyerobot.net/", "summary": "Humans do not passively observe the visual world -- we actively look in order\nto act. Motivated by this principle, we introduce EyeRobot, a robotic system\nwith gaze behavior that emerges from the need to complete real-world tasks. We\ndevelop a mechanical eyeball that can freely rotate to observe its surroundings\nand train a gaze policy to control it using reinforcement learning. We\naccomplish this by first collecting teleoperated demonstrations paired with a\n360 camera. This data is imported into a simulation environment that supports\nrendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze\non top of robot demonstrations. We then introduce a BC-RL loop to train the\nhand and eye jointly: the hand (BC) agent is trained from rendered eye\nobservations, and the eye (RL) agent is rewarded when the hand produces correct\naction predictions. In this way, hand-eye coordination emerges as the eye looks\ntowards regions which allow the hand to complete the task. EyeRobot implements\na foveal-inspired policy architecture allowing high resolution with a small\ncompute budget, which we find also leads to the emergence of more stable\nfixation as well as improved ability to track objects and ignore distractors.\nWe evaluate EyeRobot on five panoramic workspace manipulation tasks requiring\nmanipulation in an arc surrounding the robot arm. Our experiments suggest\nEyeRobot exhibits hand-eye coordination behaviors which effectively facilitate\nmanipulation over large workspaces with a single camera. See project site for\nvideos: https://www.eyerobot.net/", "AI": {"tldr": "EyeRobot是一个通过强化学习训练机械眼球以实现任务驱动的注视行为的机器人系统，通过手眼协调完成任务。", "motivation": "人类通过主动观察来行动，因此设计一个能根据任务需求调整注视行为的机器人系统。", "method": "开发可旋转的机械眼球，收集360度摄像头数据，在模拟环境中训练手眼协调策略，采用BC-RL循环联合训练手和眼的策略。", "result": "EyeRobot在五个全景工作空间操作任务中表现出有效的手眼协调行为，能够在大工作空间内高效操作。", "conclusion": "EyeRobot通过任务驱动的注视行为实现了高效的手眼协调，适用于大范围操作任务。"}}
