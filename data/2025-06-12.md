<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 33]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras](https://arxiv.org/abs/2506.09098)
*Yangjie Cui,Boyang Gao,Yiwei Zhang,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: 论文提出了一种基于小波去噪的检测Transformer网络（WD-DETR），用于解决事件相机中密集事件表示中的噪声问题，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 密集事件表示中的噪声问题降低了表示质量并增加了漏检概率，现有研究对此关注不足。

Method: 提出WD-DETR网络，包括密集事件表示、小波去噪方法、Transformer-based网络和动态重组卷积块（DRCB）。

Result: 在DSEC、Gen1和1Mpx数据集上表现优于现有方法，并在NVIDIA Jetson Orin NX上实现35 FPS的高帧率。

Conclusion: WD-DETR有效解决了事件相机中的噪声问题，适用于实时机器人感知。

Abstract: Previous studies on event camera sensing have demonstrated certain detection
performance using dense event representations. However, the accumulated noise
in such dense representations has received insufficient attention, which
degrades the representation quality and increases the likelihood of missed
detections. To address this challenge, we propose the Wavelet
Denoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event
cameras. In particular, a dense event representation is presented first, which
enables real-time reconstruction of events as tensors. Then, a wavelet
transform method is designed to filter noise in the event representations. Such
a method is integrated into the backbone for feature extraction. The extracted
features are subsequently fed into a transformer-based network for object
prediction. To further reduce inference time, we incorporate the Dynamic
Reorganization Convolution Block (DRCB) as a fusion module within the hybrid
encoder. The proposed method has been evaluated on three event-based object
detection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that
WD-DETR outperforms tested state-of-the-art methods. Additionally, we implement
our approach on a common onboard computer for robots, the NVIDIA Jetson Orin
NX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,
which is exceptionally well-suited for real-time perception of onboard robotic
systems.

</details>


### [2] [Hearing the Slide: Acoustic-Guided Constraint Learning for Fast Non-Prehensile Transport](https://arxiv.org/abs/2506.09169)
*Yuemin Mao,Bardienus P. Duisterhof,Moonyoung Lee,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 论文提出了一种基于声学传感学习摩擦模型的新方法，用于优化非抓取式物体运输任务，显著减少物体位移。


<details>
  <summary>Details</summary>
Motivation: 现有基于库仑摩擦模型的约束在快速运动中不精确，导致物体滑动或掉落，需要更精确的摩擦约束方法。

Method: 通过声学传感学习动态摩擦系数，结合优化运动规划器调整摩擦约束。

Result: 实验表明，学习模型比库仑模型减少物体位移高达86.0%。

Conclusion: 声学传感能有效学习真实摩擦约束，提升运输效率与安全性。

Abstract: Object transport tasks are fundamental in robotic automation, emphasizing the
importance of efficient and secure methods for moving objects. Non-prehensile
transport can significantly improve transport efficiency, as it enables
handling multiple objects simultaneously and accommodating objects unsuitable
for parallel-jaw or suction grasps. Existing approaches incorporate constraints
based on the Coulomb friction model, which is imprecise during fast motions
where inherent mechanical vibrations occur. Imprecise constraints can cause
transported objects to slide or even fall off the tray. To address this
limitation, we propose a novel method to learn a friction model using acoustic
sensing that maps a tray's motion profile to a dynamically conditioned friction
coefficient. This learned model enables an optimization-based motion planner to
adjust the friction constraint at each control step according to the planned
motion at that step. In experiments, we generate time-optimized trajectories
for a UR5e robot to transport various objects with constraints using both the
standard Coulomb friction model and the learned friction model. Results suggest
that the learned friction model reduces object displacement by up to 86.0%
compared to the baseline, highlighting the effectiveness of acoustic sensing in
learning real-world friction constraints.

</details>


### [3] [Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method](https://arxiv.org/abs/2506.09182)
*Hang Zhou,Chengyuan Ma,Shiyu Shen,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本文提出了一种新的全场景自动驾驶车辆（AV）安全评估框架，解决了现有方法在复杂环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AV安全评估方法主要针对简单操作（如跟车和变道），难以评估复杂环境中的高级自动化功能，且依赖高质量自然驾驶数据，计算复杂度高。

Method: 提出统一模型标准化多样化驾驶场景表示，并采用基于体积的评估方法量化风险场景比例，避免概率方法的局限性。

Result: 实验验证了基于体积的方法的有效性，使用现有文献中的AV行为模型和实际测试数据校准的模型。

Conclusion: 新框架显著降低了维度问题，为AV安全评估提供了更高效和可扩展的解决方案。

Abstract: With the rapid development of automated vehicles (AVs) in recent years,
commercially available AVs are increasingly demonstrating high-level automation
capabilities. However, most existing AV safety evaluation methods are primarily
designed for simple maneuvers such as car-following and lane-changing. While
suitable for basic tests, these methods are insufficient for assessing
high-level automation functions deployed in more complex environments. First,
these methods typically use crash rate as the evaluation metric, whose accuracy
heavily depends on the quality and completeness of naturalistic driving
environment data used to estimate scenario probabilities. Such data is often
difficult and expensive to collect. Second, when applied to diverse scenarios,
these methods suffer from the curse of dimensionality, making large-scale
evaluation computationally intractable. To address these challenges, this paper
proposes a novel framework for full-scenario AV safety evaluation. A unified
model is first introduced to standardize the representation of diverse driving
scenarios. This modeling approach constrains the dimension of most scenarios to
a regular highway setting with three lanes and six surrounding background
vehicles, significantly reducing dimensionality. To further avoid the
limitations of probability-based method, we propose a volume-based evaluation
method that quantifies the proportion of risky scenarios within the entire
scenario space. For car-following scenarios, we prove that the set of safe
scenarios is convex under specific settings, enabling exact volume computation.
Experimental results validate the effectiveness of the proposed volume-based
method using both AV behavior models from existing literature and six
production AV models calibrated from field-test trajectory data in the Ultra-AV
dataset. Code and data will be made publicly available upon acceptance of this
paper.

</details>


### [4] [Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule](https://arxiv.org/abs/2506.09217)
*Boyu Jiang,Liang Shi,Zhengzhi Lin,Loren Stowe,Feng Guo*

Main category: cs.RO

TL;DR: 论文提出了一种新的感知评估指标PCD，用于量化自动驾驶系统中对象检测的最远可靠距离，并结合了模型输出的不确定性。同时发布了SensorRainFall数据集，用于评估不同天气条件下的感知性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统的性能受对象距离、场景动态和天气等环境因素影响，传统静态评估指标无法捕捉置信度的动态变化。

Method: 引入PCD指标，结合SensorRainFall数据集（包含晴天和雨天场景的精确数据），通过统计分析方法量化检测置信度的变化。

Result: PCD能够捕捉不同天气条件下感知系统的可靠性差异，而传统静态指标无法做到。

Conclusion: PCD为感知性能提供了分布感知的评估方法，支持更安全的自动驾驶系统运行，SensorRainFall数据集为评估提供了基准。

Abstract: The performance of perception systems in autonomous driving systems (ADS) is
strongly influenced by object distance, scene dynamics, and environmental
conditions such as weather. AI-based perception outputs are inherently
stochastic, with variability driven by these external factors, while
traditional evaluation metrics remain static and event-independent, failing to
capture fluctuations in confidence over time. In this work, we introduce the
Perception Characteristics Distance (PCD) -- a novel evaluation metric that
quantifies the farthest distance at which an object can be reliably detected,
incorporating uncertainty in model outputs. To support this, we present the
SensorRainFall dataset, collected on the Virginia Smart Road using a
sensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear
and daylight-rain scenarios, with precise ground-truth distances to the target
objects. Statistical analysis reveals the presence of change points in the
variance of detection confidence score with distance. By averaging the PCD
values across a range of detection quality thresholds and probabilistic
thresholds, we compute the mean PCD (mPCD), which captures the overall
perception characteristics of a system with respect to detection distance.
Applying state-of-the-art perception models shows that mPCD captures meaningful
reliability differences under varying weather conditions -- differences that
static metrics overlook. PCD provides a principled, distribution-aware measure
of perception performance, supporting safer and more robust ADS operation,
while the SensorRainFall dataset offers a valuable benchmark for evaluation.
The SensorRainFall dataset is publicly available at
https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the
evaluation code is open-sourced at
https://github.com/datadrivenwheels/PCD_Python.

</details>


### [5] [Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations](https://arxiv.org/abs/2506.09383)
*Chengtian Ma,Yunyue Wei,Chenhui Zuo,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 论文提出了一种分层控制管道，通过全身肌肉骨骼系统模拟人类平衡，揭示了肌肉损伤对平衡行为的影响，并验证了髋部外骨骼辅助对平衡的改善效果。


<details>
  <summary>Details</summary>
Motivation: 动态平衡研究较多，但静态平衡和跌倒的定量理解有限，需要更深入的肌肉层面分析。

Method: 采用分层控制管道和全身肌肉骨骼系统模拟人类平衡，分析稳定站立时的时空动态及肌肉损伤影响。

Result: 揭示了平衡行为的时空动态，模拟结果与临床数据一致，外骨骼辅助可改善平衡并减少肌肉努力。

Conclusion: 研究为平衡障碍的干预措施和人形机器人系统的发展提供了肌肉层面的理论基础。

Abstract: Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.

</details>


### [6] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: UAD是一种无需人工标注的方法，通过利用基础模型从视觉和语言模型中提取知识，自动生成大规模任务条件化的视觉功能数据集，并在仿真环境中训练轻量级解码器，展现出对真实场景和人类活动的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉功能预测方法依赖人工标注或预定义任务集，限制了在开放任务指令下的机器人操作能力。

Method: UAD利用大型视觉模型和视觉语言模型的互补优势，自动标注大规模数据集，并训练任务条件化的轻量级解码器。

Result: UAD在仿真环境中训练的模型能够泛化到真实机器人场景和人类活动，且模仿学习策略在少量演示后表现出对未见对象和任务指令的泛化能力。

Conclusion: UAD提供了一种无需人工标注的高效方法，显著提升了机器人对物体功能的泛化能力。

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [7] [Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization](https://arxiv.org/abs/2506.09876)
*Jisheng Xu,Ding Lin,Pangkit Fong,Chongrong Fang,Xiaoming Duan,Jianping He*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Aucamp的水下多机器人平台，具有低成本单目摄像头感知、分布式协议和鲁棒的方向控制功能，用于定位。


<details>
  <summary>Details</summary>
Motivation: 为水下传感器网络提供广泛的海洋探索支持。

Method: 利用清晰度特征测量距离，提出单目成像模型，设计分布式更新协议实现全局定位，并提出鲁棒的方向控制框架。

Result: 平台能够覆盖更广范围，显著提高定位精度和鲁棒性，并能快速从不稳定状态恢复。

Conclusion: 该平台为水下传感器网络的海洋探索提供了新的支持。

Abstract: This paper introduces an underwater multi-robot platform, named Aucamp,
characterized by cost-effective monocular-camera-based sensing, distributed
protocol and robust orientation control for localization. We utilize the
clarity feature to measure the distance, present the monocular imaging model,
and estimate the position of the target object. We achieve global positioning
in our platform by designing a distributed update protocol. The distributed
algorithm enables the perception process to simultaneously cover a broader
range, and greatly improves the accuracy and robustness of the positioning.
Moreover, the explicit dynamics model of the robot in our platform is obtained,
based on which, we propose a robust orientation control framework. The control
system ensures that the platform maintains a balanced posture for each robot,
thereby ensuring the stability of the localization system. The platform can
swiftly recover from an forced unstable state to a stable horizontal posture.
Additionally, we conduct extensive experiments and application scenarios to
evaluate the performance of our platform. The proposed new platform may provide
support for extensive marine exploration by underwater sensor networks.

</details>


### [8] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Main category: cs.RO

TL;DR: SkillBlender是一种新颖的分层强化学习框架，通过预训练任务无关的原始技能并动态混合这些技能，实现了多功能人形机器人的运动与操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要针对每个任务进行繁琐的调整，限制了其多功能性和可扩展性。SkillBlender旨在减少任务特定的奖励工程，提升机器人在日常场景中的适应性。

Method: SkillBlender预训练目标条件的任务无关原始技能，并动态混合这些技能以完成复杂的运动与操作任务。

Result: 实验表明，SkillBlender显著优于基线方法，并避免了奖励滥用，实现了更准确和可行的运动。

Conclusion: SkillBlender为多功能人形机器人的运动与操作任务提供了一种高效且可扩展的解决方案，其代码和基准将开源。

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [9] [Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation](https://arxiv.org/abs/2506.09384)
*Chendong Xin,Mingrui Yu,Yongpeng Jiang,Zhefeng Zhang,Xiang Li*

Main category: cs.RO

TL;DR: 该论文研究了从人手到机器人手的运动重定向问题，通过实验比较分析了不同优化目标的有效性，并提出了一种综合重定向目标公式。


<details>
  <summary>Details</summary>
Motivation: 由于人手与机器人手之间的机械差异，完全复现人类动作是不可能的。现有研究缺乏对不同优化目标的实验比较，因此这些目标的重要性和有效性尚不明确。

Method: 提出了一种综合重定向目标公式，并通过实验消融研究评估了每个因素的重要性。

Result: 实验结果表明，综合目标公式在运动姿态重定向和实际遥操作任务中表现优异。

Conclusion: 研究结果为设计更准确有效的重定向算法提供了有价值的见解。

Abstract: Kinematic retargeting from human hands to robot hands is essential for
transferring dexterity from humans to robots in manipulation teleoperation and
imitation learning. However, due to mechanical differences between human and
robot hands, completely reproducing human motions on robot hands is impossible.
Existing works on retargeting incorporate various optimization objectives,
focusing on different aspects of hand configuration. However, the lack of
experimental comparative studies leaves the significance and effectiveness of
these objectives unclear. This work aims to analyze these retargeting
objectives for dexterous manipulation through extensive real-world comparative
experiments. Specifically, we propose a comprehensive retargeting objective
formulation that integrates intuitively crucial factors appearing in recent
approaches. The significance of each factor is evaluated through experimental
ablation studies on the full objective in kinematic posture retargeting and
real-world teleoperated manipulation tasks. Experimental results and
conclusions provide valuable insights for designing more accurate and effective
retargeting algorithms for real-world dexterous manipulation.

</details>


### [10] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Main category: cs.RO

TL;DR: 提出了一种四足机器人利用腿部敏捷性收集物体的框架，无需额外执行器。通过简单的铲状附件和分层策略结构，实现了动态物体操作。


<details>
  <summary>Details</summary>
Motivation: 探索四足机器人腿部在动态物体操作中的潜力，超越传统静态任务。

Method: 采用分层策略结构，包括铲取和投掷的专家策略、接近物体位置的专家策略，以及动态切换的元策略。

Result: 展示了四足机器人腿部在动态物体收集中的有效性，扩展了其功能。

Conclusion: 证明了四足机器人腿部可用于动态操作，为未来应用提供了新方向。

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [11] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Main category: cs.RO

TL;DR: 提出了时间统一的扩散策略（TUDP），通过动作识别能力构建时间统一的去噪过程，提高了机器人动作生成的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在机器人模仿学习中表现良好，但现有方法因时间变化的去噪过程导致训练复杂且动作精度不足，且生成速度慢，难以实时响应。

Method: 构建时间统一的速度场，减少策略学习难度；提出动作级训练方法，引入动作判别分支以提高去噪精度。

Result: 在RLBench上达到最高成功率（多视图82.6%，单视图83.8%），且在较少去噪迭代时表现更优。

Conclusion: TUDP能高效生成准确动作，适用于广泛的实际任务。

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [12] [Design of an innovative robotic surgical instrument for circular stapling](https://arxiv.org/abs/2506.09444)
*Paul Tucan,Nadim Al Hajjar,Calin Vaida,Alexandru Pusca,Tiberiu Antal,Corina Radu,Daniel Jucan,Adrian Pisla,Damien Chablat,Doina Pisla*

Main category: cs.RO

TL;DR: 本文提出了一种新型机器人圆形吻合器，旨在提高食管癌手术的精确性，减少术后风险。


<details>
  <summary>Details</summary>
Motivation: 传统手动吻合器在食管癌手术中存在精度不足、恢复时间长及并发症多的问题。

Method: 设计了一种集成认知机器人的吻合器，通过三个执行器实现运动，并支持75度弯曲，结合运动学分析确保同步。

Result: 该吻合器提升了在狭窄空间的操作灵活性，改善了组织对齐。

Conclusion: 新型机器人吻合器有望优化食管癌手术效果，降低术后风险。

Abstract: Esophageal cancer remains a highly aggressive malignancy with low survival
rates, requiring advanced surgical interventions like esophagectomy.
Traditional manual techniques, including circular staplers, face challenges
such as limited precision, prolonged recovery times, and complications like
leaks and tissue misalignment. This paper presents a novel robotic circular
stapler designed to enhance the dexterity in confined spaces, improve tissue
alignment, and reduce post-operative risks. Integrated with a cognitive robot
that serves as a surgeon's assistant, the surgical stapler uses three actuators
to perform anvil motion, cutter/stapler motion and allows a 75-degree bending
of the cartridge (distal tip). Kinematic analysis is used to compute the
stapler tip's position, ensuring synchronization with a robotic system.

</details>


### [13] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: Adv-BMT框架通过双向运动变换器生成多样且真实的对抗性交互，解决了自动驾驶测试中长尾安全关键场景稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏长尾安全关键场景，限制了自动驾驶系统的测试效果。

Method: 提出Adv-BMT框架，包含双向运动变换器（BMT）进行逆时序交通运动预测，无需碰撞数据预训练，分两阶段生成对抗性交互。

Result: 实验表明，Adv-BMT生成的碰撞场景质量高，训练后碰撞率降低20%。

Conclusion: Adv-BMT能有效生成多样且真实的碰撞场景，提升自动驾驶系统测试效果。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [14] [DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects](https://arxiv.org/abs/2506.09491)
*Guanghu Xie,Zhiduo Jiang,Yonglong Zhang,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: DCIRNet是一种新型多模态深度补全网络，通过融合RGB图像和深度图提升透明和反射物体的深度估计质量，显著提高了抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 透明和反射物体的独特视觉特性（如镜面反射和光线透射）导致深度传感器估计不完整或不准确，影响下游视觉任务。

Method: 提出DCIRNet，结合RGB图像和深度图，采用多模态特征融合模块和多阶段监督与深度细化策略。

Result: 在公开数据集上表现优异，抓取成功率提升44%，验证了方法的有效性和泛化能力。

Conclusion: DCIRNet能有效解决透明和反射物体的深度估计问题，提升下游任务性能。

Abstract: Transparent and reflective objects in everyday environments pose significant
challenges for depth sensors due to their unique visual properties, such as
specular reflections and light transmission. These characteristics often lead
to incomplete or inaccurate depth estimation, which severely impacts downstream
geometry-based vision tasks, including object recognition, scene
reconstruction, and robotic manipulation. To address the issue of missing depth
information in transparent and reflective objects, we propose DCIRNet, a novel
multimodal depth completion network that effectively integrates RGB images and
depth maps to enhance depth estimation quality. Our approach incorporates an
innovative multimodal feature fusion module designed to extract complementary
information between RGB images and incomplete depth maps. Furthermore, we
introduce a multi-stage supervision and depth refinement strategy that
progressively improves depth completion and effectively mitigates the issue of
blurred object boundaries. We integrate our depth completion model into
dexterous grasping frameworks and achieve a $44\%$ improvement in the grasp
success rate for transparent and reflective objects. We conduct extensive
experiments on public datasets, where DCIRNet demonstrates superior
performance. The experimental results validate the effectiveness of our
approach and confirm its strong generalization capability across various
transparent and reflective objects.

</details>


### [15] [Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications](https://arxiv.org/abs/2506.09494)
*Alberto San-Miguel-Tello,Gennaro Scarati,Alejandro Hernández,Mario Cavero-Vidal,Aakash Maroti,Néstor García*

Main category: cs.RO

TL;DR: 本文介绍了通用操作接口（UMI）的改进，用于农业场景中的机器人学习演示（LfD），重点是通过减少空闲时间和用户认知负荷，结合惯性测量和视觉标记定位，提高了任务样本采集的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决农业场景中复杂任务样本采集的挑战，减少用户负担并提高数据可靠性。

Method: 通过任务事件提取连续演示中的样本，结合惯性测量和视觉标记定位（使用EKF）生成轨迹。

Result: 在水果采摘任务中表现优于默认流程。

Conclusion: UMI改进方法有效提升了复杂农业任务中的样本采集效率和可靠性。

Abstract: This paper presents advances on the Universal Manipulation Interface (UMI), a
low-cost hand-held gripper for robot Learning from Demonstration (LfD), for
complex in-the-wild scenarios found in agricultural settings. The focus is on
improving the acquisition of suitable samples with minimal additional setup.
Firstly, idle times and user's cognitive load are reduced through the
extraction of individual samples from a continuous demonstration considering
task events. Secondly, reliability on the generation of task sample's
trajectories is increased through the combination on-board inertial
measurements and external visual marker localization usage using Extended
Kalman Filtering (EKF). Results are presented for a fruit harvesting task,
outperforming the default pipeline.

</details>


### [16] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出了一种紧耦合的LiDAR-IMU-腿里程计方法，通过在线学习腿部运动学模型和不确定性估计，在特征缺失和可变形地形等挑战性环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在特征缺失环境和可变形地形中机器人里程计的鲁棒性问题。

Method: 开发了基于神经网络的腿部运动学模型，结合触觉信息（脚部反作用力），并通过在线训练增强适应性。在统一因子图上联合优化运动学模型和里程计估计。

Result: 在沙地和校园等挑战性环境中，该方法优于现有技术。

Conclusion: 提出的方法在复杂环境中显著提升了里程计的鲁棒性和准确性。

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [17] [Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments](https://arxiv.org/abs/2506.09552)
*Fatemeh Mohammadi Amin,Darwin G. Caldwell,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: 本文提出了一种双流网络架构（FUSION），结合动态图卷积神经网络（DGCNN）和卷积神经网络（CNN），用于3D点云数据的Sim2Real域适应，显著提升了语义分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人协作（HRC）应用中，安全性和操作效率至关重要，而语义分割是实现环境精确理解的关键。然而，真实工业标注数据稀缺，因此需要一种从模拟环境到真实应用的域适应方法。

Method: 提出了一种双流网络架构（FUSION），结合DGCNN和CNN，并引入残差层，作为Sim2Real域适应算法，专门针对工业环境设计。

Result: 在真实HRC设置和模拟工业点云数据上评估，模型达到了97.76%的分割准确率，性能优于现有方法。

Conclusion: 该方法显著提升了语义分割的准确性和鲁棒性，为安全HRC提供了实用且高效的解决方案。

Abstract: The robust interpretation of 3D environments is crucial for human-robot
collaboration (HRC) applications, where safety and operational efficiency are
paramount. Semantic segmentation plays a key role in this context by enabling a
precise and detailed understanding of the environment. Considering the intense
data hunger for real-world industrial annotated data essential for effective
semantic segmentation, this paper introduces a pioneering approach in the
Sim2Real domain adaptation for semantic segmentation of 3D point cloud data,
specifically tailored for HRC. Our focus is on developing a network that
robustly transitions from simulated environments to real-world applications,
thereby enhancing its practical utility and impact on a safe HRC.
  In this work, we propose a dual-stream network architecture (FUSION)
combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional
Neural Networks (CNN) augmented with residual layers as a Sim2Real domain
adaptation algorithm for an industrial environment. The proposed model was
evaluated on real-world HRC setups and simulation industrial point clouds, it
showed increased state-of-the-art performance, achieving a segmentation
accuracy of 97.76%, and superior robustness compared to existing methods.

</details>


### [18] [Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities](https://arxiv.org/abs/2506.09581)
*Miguel Á. González-Santamarta,Francisco J. Rodríguez-Lera,David Sobrín-Hidalgo,Ángel Manuel Guerrero-Higueras,Vicente MatellÁn-Olivera*

Main category: cs.RO

TL;DR: llama_ros是一个工具，用于将量化的大型语言模型（LLMs）集成到ROS 2机器人系统中，提升自然语言理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在机器人领域的应用潜力巨大，但面临计算效率和内存限制的挑战。llama_ros旨在解决这些问题。

Method: 利用llama.cpp高效运行时引擎，部署量化LLMs，结合提示工程、知识图谱等工具增强机器人能力。

Result: llama_ros成功在资源受限环境中运行量化LLMs，提升机器人决策和交互能力。

Conclusion: llama_ros为机器人系统提供了高效的LLMs集成方案，展示了在规划和可解释性方面的应用潜力。

Abstract: Large Language Models (LLMs) have experienced great advancements in the last
year resulting in an increase of these models in several fields to face natural
language tasks. The integration of these models in robotics can also help to
improve several aspects such as human-robot interaction, navigation, planning
and decision-making. Therefore, this paper introduces llama\_ros, a tool
designed to integrate quantized Large Language Models (LLMs) into robotic
systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,
llama\_ros enables the efficient execution of quantized LLMs as edge artificial
intelligence (AI) in robotics systems with resource-constrained environments,
addressing the challenges of computational efficiency and memory limitations.
By deploying quantized LLMs, llama\_ros empowers robots to leverage the natural
language understanding and generation for enhanced decision-making and
interaction which can be paired with prompt engineering, knowledge graphs,
ontologies or other tools to improve the capabilities of autonomous robots.
Additionally, this paper provides insights into some use cases of using
llama\_ros for planning and explainability in robotics.

</details>


### [19] [VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots](https://arxiv.org/abs/2506.09583)
*Miguel Á. González-Santamarta,Francisco J. Rodríguez-Lera,Vicente Matellán-Olivera*

Main category: cs.RO

TL;DR: 本文介绍了VAULT原型，一个基于ROS 2的移动测绘系统（MMS），结合多种传感器实现室内外定位，利用GNSS、VIO、IMU和EKF生成可靠3D里程计，并通过VSLAM提高定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决农业和林业等户外环境中自主机器人实时定位和一致测绘的挑战。

Method: 结合GNSS数据、视觉惯性里程计（VIO）、IMU数据和扩展卡尔曼滤波器（EKF）生成3D里程计，并利用VSLAM创建3D点云地图。

Result: 原型系统提供了户外定位的全面解决方案，使自主移动机器人能够精确导航和测绘。

Conclusion: VAULT原型通过多传感器融合和先进算法，显著提升了户外自主机器人的定位和测绘能力。

Abstract: Localization plays a crucial role in the navigation capabilities of
autonomous robots, and while indoor environments can rely on wheel odometry and
2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,
present unique challenges that necessitate real-time localization and
consistent mapping. Addressing this need, this paper introduces the VAULT
prototype, a ROS 2-based mobile mapping system (MMS) that combines various
sensors to enable robust outdoor and indoor localization. The proposed solution
harnesses the power of Global Navigation Satellite System (GNSS) data,
visual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the
Extended Kalman Filter (EKF) to generate reliable 3D odometry. To further
enhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting
in the creation of a comprehensive 3D point cloud map. By leveraging these
sensor technologies and advanced algorithms, the prototype offers a
comprehensive solution for outdoor localization in autonomous mobile robots,
enabling them to navigate and map their surroundings with confidence and
precision.

</details>


### [20] [Attention-Based Map Encoding for Learning Generalized Legged Locomotion](https://arxiv.org/abs/2506.09588)
*Junzhe He,Chong Zhang,Fabian Jenelten,Ruben Grandia,Moritz BÄcher,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种基于注意力机制的地图编码方法，结合强化学习训练端到端控制器，以实现腿式机器人在多样化地形上的动态运动。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的控制器在复杂地形上表现良好，但对现实世界的不确定性缺乏鲁棒性；而基于学习的控制器虽鲁棒性强，但在稀疏地形上缺乏精确性。本文旨在结合两者优势，实现鲁棒且精确的运动。

Method: 通过强化学习训练一个基于注意力的地图编码器，结合机器人本体感知，形成端到端控制器。

Result: 网络学会在动态导航时关注可踩踏区域，实现了在稀疏地形上的精确运动，并展示了鲁棒性和泛化能力。

Conclusion: 该方法不仅实现了鲁棒且精确的运动，还提供了对神经网络地形感知的可解释性。

Abstract: Dynamic locomotion of legged robots is a critical yet challenging topic in
expanding the operational range of mobile robots. It requires precise planning
when possible footholds are sparse, robustness against uncertainties and
disturbances, and generalizability across diverse terrains. While traditional
model-based controllers excel at planning on complex terrains, they struggle
with real-world uncertainties. Learning-based controllers offer robustness to
such uncertainties but often lack precision on terrains with sparse steppable
areas. Hybrid methods achieve enhanced robustness on sparse terrains by
combining both methods but are computationally demanding and constrained by the
inherent limitations of model-based planners. To achieve generalized legged
locomotion on diverse terrains while preserving the robustness of
learning-based controllers, this paper proposes to learn an attention-based map
encoding conditioned on robot proprioception, which is trained as part of the
end-to-end controller using reinforcement learning. We show that the network
learns to focus on steppable areas for future footholds when the robot
dynamically navigates diverse and challenging terrains. We synthesize behaviors
that exhibit robustness against uncertainties while enabling precise and agile
traversal of sparse terrains. Additionally, our method offers a way to
interpret the topographical perception of a neural network. We have trained two
controllers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot
respectively and tested the resulting controllers in the real world under
various challenging indoor and outdoor scenarios, including ones unseen during
training.

</details>


### [21] [Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models](https://arxiv.org/abs/2506.09623)
*Lipei Xie,Yingxin Li,Huiping Zhuang*

Main category: cs.RO

TL;DR: 论文提出了一种名为Analytic Task Scheduler (ATS)的新框架，用于解决具身基础模型在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 具身基础模型在整合多模态输入以理解人类意图和控制机器人方面表现出色，但在持续学习新技能时容易遗忘旧技能。

Method: ATS包含一个任务特定模型库和一个基于递归最小二乘法（RLS）的分析调度器，用于动态选择模型并避免参数干扰。

Result: 在真实机器人平台（RM65B）上验证了ATS的抗遗忘能力和任务适应性。

Conclusion: ATS是一种高效、可扩展且可部署的解决方案，适用于复杂动态环境中的持续学习。

Abstract: Embodied foundation models are crucial for Artificial Intelligence (AI)
interacting with the physical world by integrating multi-modal inputs, such as
proprioception, vision and language, to understand human intentions and
generate actions to control robots. While these models demonstrate strong
generalization and few-shot learning capabilities, they face significant
challenges in continually acquiring new skills without forgetting previously
learned skills, a problem known as catastrophic forgetting. To address this
issue, we propose the Analytic Task Scheduler (ATS), a novel framework for
continual learning in embodied foundation models. ATS consists of a
task-specific model library, where each model is fine-tuned independently on a
single task, and an analytic scheduler trained using recursive least squares
(RLS) to learn the mapping between language instructions and task-specific
models. This architecture enables accurate task recognition and dynamic model
selection while fundamentally avoiding parameter interference across tasks. The
scheduler updates its parameters incrementally using only statistics
(autocorrelation and cross-correlation matrices), enabling forgetting-resistant
learning without the need to revisit historical data. We validate ATS on a
real-world robot platform (RM65B), demonstrating superior resistance to
forgetting and strong adaptability to task variations. The results highlight
ATS as an effective, scalable, and deployable solution for continual learning
in embodied foundation models operating in complex, dynamic environments. Our
code will be available at
https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler

</details>


### [22] [R-CARLA: High-Fidelity Sensor Simulations with Interchangeable Dynamics for Autonomous Racing](https://arxiv.org/abs/2506.09629)
*Maurice Brunner,Edoardo Ghignone,Nicolas Baumann,Michele Magno*

Main category: cs.RO

TL;DR: R-CARLA是CARLA模拟器的增强版，支持从感知到控制的全栈测试，通过整合精确的车辆动力学和传感器模拟，显著减少了模拟与现实的差距。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车开发中，车辆动力学与传感器模拟的平衡是关键，但现有工具往往难以兼顾。

Method: R-CARLA通过整合车辆动力学、传感器模拟、NPC对手模拟和数字孪生技术，提供全栈测试能力。

Result: 实验显示，R-CARLA将车辆动力学模拟的模拟与现实差距减少了42%，传感器模拟减少了82%。

Conclusion: R-CARLA为自动驾驶赛车开发提供了更真实的测试环境，显著提升了模拟的准确性。

Abstract: Autonomous racing has emerged as a crucial testbed for autonomous driving
algorithms, necessitating a simulation environment for both vehicle dynamics
and sensor behavior. Striking the right balance between vehicle dynamics and
sensor accuracy is crucial for pushing vehicles to their performance limits.
However, autonomous racing developers often face a trade-off between accurate
vehicle dynamics and high-fidelity sensor simulations. This paper introduces
R-CARLA, an enhancement of the CARLA simulator that supports holistic
full-stack testing, from perception to control, using a single system. By
seamlessly integrating accurate vehicle dynamics with sensor simulations,
opponents simulation as NPCs, and a pipeline for creating digital twins from
real-world robotic data, R-CARLA empowers researchers to push the boundaries of
autonomous racing development. Furthermore, it is developed using CARLA's rich
suite of sensor simulations. Our results indicate that incorporating the
proposed digital-twin framework into R-CARLA enables more realistic full-stack
testing, demonstrating a significant reduction in the Sim-to-Real gap of car
dynamics simulation by 42% and by 82% in the case of sensor simulation across
various testing scenarios.

</details>


### [23] [Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling](https://arxiv.org/abs/2506.09697)
*Paolo Franceschi,Andrea Bussolan,Vincenzo Pomponi,Oliver Avram,Stefano Baraldo,Anna Valente*

Main category: cs.RO

TL;DR: 论文提出了一种基于动态运动基元（DMPs）和实时速度调整的个性化轨迹生成方法，用于人机协作任务。实验表明，该方法优于现有运动规划器，并显著提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 工业对人机协作的需求增长，需要智能策略来规划机器人运动，同时考虑任务约束和人类特定因素（如身高和运动偏好）。

Method: 使用动态运动基元（DMPs）生成个性化轨迹，并结合实时速度调整技术。

Result: 实验表明，DMP生成的轨迹优于BiTRRT运动规划器，用户反馈和生理测量均支持DMP方法的优越性。

Conclusion: DMP方法显著提升了人机交互效果和用户体验，适用于工业协作任务。

Abstract: Nowadays, industries are showing a growing interest in human-robot
collaboration, particularly for shared tasks. This requires intelligent
strategies to plan a robot's motions, considering both task constraints and
human-specific factors such as height and movement preferences. This work
introduces a novel approach to generate personalized trajectories using Dynamic
Movement Primitives (DMPs), enhanced with real-time velocity scaling based on
human feedback. The method was rigorously tested in industrial-grade
experiments, focusing on the collaborative transport of an engine cowl lip
section. Comparative analysis between DMP-generated trajectories and a
state-of-the-art motion planner (BiTRRT) highlights their adaptability combined
with velocity scaling. Subjective user feedback further demonstrates a clear
preference for DMP- based interactions. Objective evaluations, including
physiological measurements from brain and skin activity, reinforce these
findings, showcasing the advantages of DMPs in enhancing human-robot
interaction and improving user experience.

</details>


### [24] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 本文提出了一种基于机器学习的框架，通过预测调整和优化吸盘选择，显著降低了仓库自动化中的拾取失败率。


<details>
  <summary>Details</summary>
Motivation: 仓库自动化对提升效率和降低成本至关重要，但现有研究多依赖启发式方法，缺乏数据驱动的优化方法。

Method: 提出了一种ML框架，用于预测调整和优化吸盘选择，以提升拾取成功率。

Result: 在超过200万次拾取测试中，该方法比启发式基线降低了20%的失败率。

Conclusion: 该框架在大规模仓库自动化场景中表现出显著效果。

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [25] [Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.09800)
*Haochen Liu,Tianyu Li,Haohan Yang,Li Chen,Caojun Wang,Ke Guo,Haochen Tian,Hongchen Li,Hongyang Li,Chen Lv*

Main category: cs.RO

TL;DR: 论文提出了一种名为R2SE的学习框架，通过强化学习和模仿学习的结合，解决端到端自动驾驶中泛化性和样本效率的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习模型在泛化到复杂场景时表现不佳，且缺乏部署后的反馈机制；强化学习虽能优化复杂场景，但容易过拟合并遗忘通用知识。

Method: R2SE包含三个关键组件：通用预训练、残差强化微调和自感知适配器扩展，通过动态识别失败案例并针对性优化，同时保持通用驾驶策略。

Result: 实验表明，R2SE在闭环仿真和真实数据中提升了泛化性、安全性和长期策略鲁棒性，优于现有端到端系统。

Conclusion: R2SE通过强化微调和策略扩展，为可扩展的自动驾驶提供了有效解决方案。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
directly mapping sensor inputs to planning maneuvers using learning-based
modular integrations. However, existing imitation learning (IL)-based models
suffer from generalization to hard cases, and a lack of corrective feedback
loop under post-deployment. While reinforcement learning (RL) offers a
potential solution to tackle hard cases with optimality, it is often hindered
by overfitting to specific driving cases, resulting in catastrophic forgetting
of generalizable knowledge and sample inefficiency. To overcome these
challenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),
a novel learning pipeline that constantly refines hard domain while keeping
generalizable driving policy for model-agnostic end-to-end driving systems.
Through reinforcement fine-tuning and policy expansion that facilitates
continuous improvement, R2SE features three key components: 1) Generalist
Pretraining with hard-case allocation trains a generalist imitation learning
(IL) driving system while dynamically identifying failure-prone cases for
targeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes
residual corrections using reinforcement learning (RL) to improve performance
in hard case domain while preserving global driving knowledge; 3) Self-aware
Adapter Expansion dynamically integrates specialist policies back into the
generalist model, enhancing continuous performance improvement. Experimental
results in closed-loop simulation and real-world datasets demonstrate
improvements in generalization, safety, and long-horizon policy robustness over
state-of-the-art E2E systems, highlighting the effectiveness of reinforce
refinement for scalable autonomous driving.

</details>


### [26] [Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints](https://arxiv.org/abs/2506.09859)
*Huajian Liu,Yixuan Feng,Wei Dong,Kunpeng Fan,Chao Wang,Yongzhuo Gao*

Main category: cs.RO

TL;DR: 提出了一种新颖的分层框架，用于动态环境中机器人导航，结合图神经网络和强化学习，通过增量动作屏蔽机制和特权学习策略实现端到端训练，性能达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人导航的复杂性和非凸优化问题，同时减少对高保真仿真环境的依赖。

Method: 使用图神经网络和强化学习估计成本，结合时空路径搜索模块生成参考轨迹，并引入增量动作屏蔽和特权学习策略。

Result: 仿真和实际实验表明，该方法在复杂动态环境中表现优异，计算效率和训练可扩展性显著提升。

Conclusion: 该方法在动态环境中实现了高效导航，性能优于现有混合方法，且代码将开源。

Abstract: In this paper, we propose a novel hierarchical framework for robot navigation
in dynamic environments with heterogeneous constraints. Our approach leverages
a graph neural network trained via reinforcement learning (RL) to efficiently
estimate the robot's cost-to-go, formulated as local goal recommendations. A
spatio-temporal path-searching module, which accounts for kinematic
constraints, is then employed to generate a reference trajectory to facilitate
solving the non-convex optimization problem used for explicit constraint
enforcement. More importantly, we introduce an incremental action-masking
mechanism and a privileged learning strategy, enabling end-to-end training of
the proposed planner. Both simulation and real-world experiments demonstrate
that the proposed method effectively addresses local planning in complex
dynamic environments, achieving state-of-the-art (SOTA) performance. Compared
with existing learning-optimization hybrid methods, our approach eliminates the
dependency on high-fidelity simulation environments, offering significant
advantages in computational efficiency and training scalability. The code will
be released as open-source upon acceptance of the paper.

</details>


### [27] [From Theory to Practice: Advancing Multi-Robot Path Planning Algorithms and Applications](https://arxiv.org/abs/2506.09914)
*Teng Guo*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的多机器人路径规划（MRPP）方法，包括理论保证和实用启发式算法，适用于密集和实际场景。


<details>
  <summary>Details</summary>
Motivation: MRPP问题在复杂性和工业应用中的重要性推动了研究，需要高效且可扩展的解决方案。

Method: 提出了Rubik Table方法用于密集2D网格MRPP，设计了结构化环境的最优布局，并扩展了Reeds-Shepp机器人的路径规划。

Result: Rubik Table方法实现了(1+δ)-最优完成时间，解决了大规模实例；实际测试验证了方法在自动驾驶和机器人运输中的有效性。

Conclusion: 本文的方法为MRPP提供了理论和实践上的突破，适用于多种实际应用场景。

Abstract: The labeled MRPP (Multi-Robot Path Planning) problem involves routing robots
from start to goal configurations efficiently while avoiding collisions.
Despite progress in solution quality and runtime, its complexity and industrial
relevance continue to drive research.
  This dissertation introduces scalable MRPP methods with provable guarantees
and practical heuristics. First, we study dense MRPP on 2D grids, relevant to
warehouse and parcel systems. We propose the Rubik Table method, achieving $(1
+ \delta)$-optimal makespan (with $\delta \in (0, 0.5]$) for up to $\frac{m_1
m_2}{2}$ robots, solving large instances efficiently and setting a new
theoretical benchmark.
  Next, we address real-world MRPP. We design optimal layouts for structured
environments (e.g., warehouses, parking systems) and propose a puzzle-based
system for dense, deadlock-free autonomous vehicle parking. We also extend MRPP
to Reeds-Shepp robots, introducing motion primitives and smoothing techniques
to ensure feasible, efficient paths under nonholonomic constraints. Simulations
and real-world tests validate the approach in urban driving and robotic
transport scenarios.

</details>


### [28] [From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models](https://arxiv.org/abs/2506.09930)
*Irving Fang,Juexiao Zhang,Shengbang Tong,Chen Feng*

Main category: cs.RO

TL;DR: 本文介绍了Vision-Language-Action (VLA) 模型的局限性，并提出了一种包含50个模拟任务的评估套件，以标准化VLA模型的性能测试。研究发现，尽管VLA模型在感知和规划方面表现良好，但在动作执行上存在不足，且微调可能损害其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前对VLA模型的评估不足，缺乏统一的基准测试，且现有研究难以复现。本文旨在填补这一空白，提供一个标准化评估工具。

Method: 引入包含50个模拟任务的评估套件，涵盖语言指令、视觉和物体操作，并系统评估多种VLA架构的泛化能力。

Result: VLA模型在感知和规划（意图）上表现良好，但在动作执行上不稳定，尤其是面对分布外数据时。微调可能削弱模型的泛化能力。

Conclusion: 本文提出的评估套件为VLA研究提供了标准化基准，并揭示了感知与动作执行之间的差距，呼吁进一步研究以弥合这一差距。

Abstract: One promise that Vision-Language-Action (VLA) models hold over traditional
imitation learning for robotics is to leverage the broad generalization
capabilities of large Vision-Language Models (VLMs) to produce versatile,
"generalist" robot policies. However, current evaluations of VLAs remain
insufficient. Traditional imitation learning benchmarks are unsuitable due to
the lack of language instructions. Emerging benchmarks for VLAs that
incorporate language often come with limited evaluation tasks and do not intend
to investigate how much VLM pretraining truly contributes to the generalization
capabilities of the downstream robotic policy. Meanwhile, much research relies
on real-world robot setups designed in isolation by different institutions,
which creates a barrier for reproducibility and accessibility. To address this
gap, we introduce a unified probing suite of 50 simulation-based tasks across
10 subcategories spanning language instruction, vision, and objects. We
systematically evaluate several state-of-the-art VLA architectures on this
suite to understand their generalization capability. Our results show that
while VLM backbones endow VLAs with robust perceptual understanding and high
level planning, which we refer to as good intentions, this does not reliably
translate into precise motor execution: when faced with out-of-distribution
observations, policies often exhibit coherent intentions, but falter in action
execution. Moreover, finetuning on action data can erode the original VLM's
generalist reasoning abilities. We release our task suite and evaluation code
to serve as a standardized benchmark for future VLAs and to drive research on
closing the perception-to-action gap. More information, including the source
code, can be found at https://ai4ce.github.io/INT-ACT/

</details>


### [29] [Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers](https://arxiv.org/abs/2506.09934)
*Jared Lawson,Rohan Chitale,Nabil Simaan*

Main category: cs.RO

TL;DR: 论文提出了一种通过定制不透射线标记物实现微导管形状和姿态同步估计的方法，以减少神经介入手术中对导管运动的感知负担。


<details>
  <summary>Details</summary>
Motivation: 当前神经介入手术中，医生需要从双平面透视图像中重建和预测导管运动，感知负担重。现有导管追踪方法局限于平面分割或笨重的传感设备，不适用于微导管。

Method: 在导管上布置定制不透射线标记物，设计标记物排列以减少对标记物追踪不确定性的敏感性。

Result: 在直径小于2mm的微导管上部署该方法，形状追踪误差小于1mm，导管滚动误差低于40度。

Conclusion: 该方法可实现导管在双平面成像下的自主导航。

Abstract: Safe navigation of steerable and robotic catheters in the cerebral
vasculature requires awareness of the catheters shape and pose. Currently, a
significant perception burden is placed on interventionalists to mentally
reconstruct and predict catheter motions from biplane fluoroscopy images.
Efforts to track these catheters are limited to planar segmentation or bulky
sensing instrumentation, which are incompatible with microcatheters used in
neurointervention. In this work, a catheter is equipped with custom radiopaque
markers arranged to enable simultaneous shape and pose estimation under biplane
fluoroscopy. A design measure is proposed to guide the arrangement of these
markers to minimize sensitivity to marker tracking uncertainty. This approach
was deployed for microcatheters smaller than 2mm OD navigating phantom
vasculature with shape tracking errors less than 1mm and catheter roll errors
below 40 degrees. This work can enable steerable catheters to autonomously
navigate under biplane imaging.

</details>


### [30] [SAFE: Multitask Failure Detection for Vision-Language-Action Models](https://arxiv.org/abs/2506.09937)
*Qiao Gu,Yuanliang Ju,Shengxiang Sun,Igor Gilitschenski,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: 本文提出了SAFE，一种用于通用机器人策略（如视觉-语言-动作模型VLA）的故障检测器，能够在未见任务和新环境中检测故障。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测器仅针对特定任务训练和测试，而VLA需要检测器在未见任务和新环境中也能泛化。

Method: 通过分析VLA特征空间，发现其具有跨任务通用的任务成功与失败知识，基于此设计SAFE，从VLA内部特征学习并预测任务失败概率。

Result: SAFE在模拟和真实环境中测试，与多种基线比较，表现出最优的故障检测性能和准确性与检测时间的最佳平衡。

Conclusion: SAFE是一种高效的通用故障检测器，适用于多种策略架构，并在未见任务中表现优异。

Abstract: While vision-language-action models (VLAs) have shown promising robotic
behaviors across a diverse set of manipulation tasks, they achieve limited
success rates when deployed on novel tasks out-of-the-box. To allow these
policies to safely interact with their environments, we need a failure detector
that gives a timely alert such that the robot can stop, backtrack, or ask for
help. However, existing failure detectors are trained and tested only on one or
a few specific tasks, while VLAs require the detector to generalize and detect
failures also in unseen tasks and novel environments. In this paper, we
introduce the multitask failure detection problem and propose SAFE, a failure
detector for generalist robot policies such as VLAs. We analyze the VLA feature
space and find that VLAs have sufficient high-level knowledge about task
success and failure, which is generic across different tasks. Based on this
insight, we design SAFE to learn from VLA internal features and predict a
single scalar indicating the likelihood of task failure. SAFE is trained on
both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is
compatible with different policy architectures. We test it on OpenVLA, $\pi_0$,
and $\pi_0$-FAST in both simulated and real-world environments extensively. We
compare SAFE with diverse baselines and show that SAFE achieves
state-of-the-art failure detection performance and the best trade-off between
accuracy and detection time using conformal prediction. More qualitative
results can be found at https://vla-safe.github.io/.

</details>


### [31] [Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control](https://arxiv.org/abs/2506.09979)
*Zachary Olkin,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种分层架构，结合无梯度和基于梯度的方法，解决了足式机器人实时控制中的非线性、混合和高维问题。


<details>
  <summary>Details</summary>
Motivation: 足式机器人的非线性、混合和高维特性导致实时稳定和最优控制困难，传统数值最优控制方法难以处理。

Method: 采用分层架构，分离离散变量选择和光滑模型预测控制（MPC），结合采样方法和经典MPC。

Result: 在四足和双足机器人上验证了方法的有效性和实时性，优于启发式方法和纯采样方法。

Conclusion: 分层架构在保证实时性的同时提高了控制的最优性和可靠性。

Abstract: Computing stabilizing and optimal control actions for legged locomotion in
real time is difficult due to the nonlinear, hybrid, and high dimensional
nature of these robots. The hybrid nature of the system introduces a
combination of discrete and continuous variables which causes issues for
numerical optimal control. To address these challenges, we propose a layered
architecture that separates the choice of discrete variables and a smooth Model
Predictive Controller (MPC). The layered formulation allows for online
flexibility and optimality without sacrificing real-time performance through a
combination of gradient-free and gradient-based methods. The architecture
leverages a sampling-based method for determining discrete variables, and a
classical smooth MPC formulation using these fixed discrete variables. We
demonstrate the results on a quadrupedal robot stepping over gaps and onto
terrain with varying heights. In simulation, we demonstrate the controller on a
humanoid robot for gap traversal. The layered approach is shown to be more
optimal and reliable than common heuristic-based approaches and faster to
compute than pure sampling methods.

</details>


### [32] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: Chain-of-Action (CoA) 是一种基于轨迹自回归建模的新型视觉运动策略范式，通过反向推理生成完整轨迹，实现了任务目标的全局到局部约束。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅正向预测下一步动作，缺乏对任务目标的全局约束。CoA 旨在通过反向推理和任务目标的显式编码，提升策略的泛化能力和性能。

Method: CoA 采用自回归结构，首先生成任务目标的关键帧动作，随后自回归生成后续动作。设计了连续动作表示、动态停止、反向时间集成和多令牌预测等机制。

Result: CoA 在60个RLBench任务和8个真实世界操作任务中实现了最先进的性能。

Conclusion: CoA 通过反向推理和全局约束，显著提升了视觉运动策略的泛化能力和任务性能。

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


### [33] [eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures](https://arxiv.org/abs/2506.09994)
*Venkatesh Pattabiraman,Zizhou Huang,Daniele Panozzo,Denis Zorin,Lerrel Pinto,Raunaq Bhirangi*

Main category: cs.RO

TL;DR: eFlesh是一种低成本、易定制、易制造的磁性触觉传感器，填补了机器人操作中缺乏通用触觉传感器的空白。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中（如家庭和办公室），机器人需要感知物理交互中的力，但现有触觉传感器缺乏通用性和可定制性，导致解决方案分散或完全忽略力的感知。

Method: eFlesh传感器由3D打印的微结构、现成磁铁和磁力计电路板组成，通过开源设计工具将CAD模型转换为可3D打印的STL文件。

Result: 实验显示，eFlesh在接触定位（RMSE 0.5 mm）和力预测（正常力RMSE 0.27 N，剪切力RMSE 0.12 N）方面表现优异，并支持95%准确率的滑动检测模型和40%性能提升的视觉触觉控制策略。

Conclusion: eFlesh为机器人操作提供了一种低成本、高定制化的触觉感知解决方案，其开源设计工具和模块化框架进一步推动了广泛应用。

Abstract: If human experience is any guide, operating effectively in unstructured
environments -- like homes and offices -- requires robots to sense the forces
during physical interaction. Yet, the lack of a versatile, accessible, and
easily customizable tactile sensor has led to fragmented, sensor-specific
solutions in robotic manipulation -- and in many cases, to force-unaware,
sensorless approaches. With eFlesh, we bridge this gap by introducing a
magnetic tactile sensor that is low-cost, easy to fabricate, and highly
customizable. Building an eFlesh sensor requires only four components: a
hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired
shape, and a magnetometer circuit board. The sensor is constructed from tiled,
parameterized microstructures, which allow for tuning the sensor's geometry and
its mechanical response. We provide an open-source design tool that converts
convex OBJ/STL files into 3D-printable STLs for fabrication. This modular
design framework enables users to create application-specific sensors, and to
adjust sensitivity depending on the task. Our sensor characterization
experiments demonstrate the capabilities of eFlesh: contact localization RMSE
of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for
shear force. We also present a learned slip detection model that generalizes to
unseen objects with 95% accuracy, and visuotactile control policies that
improve manipulation performance by 40% over vision-only baselines -- achieving
91% average success rate for four precise tasks that require sub-mm accuracy
for successful completion. All design files, code and the CAD-to-eFlesh STL
conversion tool are open-sourced and available on https://e-flesh.com.

</details>
