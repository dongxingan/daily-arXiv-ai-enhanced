{"id": "2508.02873", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02873", "abs": "https://arxiv.org/abs/2508.02873", "authors": ["Rongqian Chen", "Jun Kwon", "Kefan Wu", "Wei-Hsi Chen"], "title": "Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient Vertical Hopping Across Varying Ground Profiles", "comment": "2025 IEEE International Conference on Robotics & Automation (ICRA)", "summary": "We present the design and implementation of HASTA (Hopper with Adjustable\nStiffness for Terrain Adaptation), a vertical hopping robot with real-time\ntunable leg stiffness, aimed at optimizing energy efficiency across various\nground profiles (a pair of ground stiffness and damping conditions). By\nadjusting leg stiffness, we aim to maximize apex hopping height, a key metric\nfor energy-efficient vertical hopping. We hypothesize that softer legs perform\nbetter on soft, damped ground by minimizing penetration and energy loss, while\nstiffer legs excel on hard, less damped ground by reducing limb deformation and\nenergy dissipation. Through experimental tests and simulations, we find the\nbest leg stiffness within our selection for each combination of ground\nstiffness and damping, enabling the robot to achieve maximum steady-state\nhopping height with a constant energy input. These results support our\nhypothesis that tunable stiffness improves energy-efficient locomotion in\ncontrolled experimental conditions. In addition, the simulation provides\ninsights that could aid in the future development of controllers for selecting\nleg stiffness.", "AI": {"tldr": "HASTA是一种具有实时可调腿部刚度的垂直跳跃机器人，旨在优化不同地面条件下的能量效率。通过调整刚度，机器人能在各种地面条件下实现最大跳跃高度。", "motivation": "研究动机是通过调整腿部刚度，优化机器人在不同地面条件下的能量效率，以实现最大跳跃高度。", "method": "通过实验测试和模拟，确定最佳腿部刚度组合，以适应不同地面刚度和阻尼条件。", "result": "实验结果表明，可调刚度能显著提高机器人在控制实验条件下的能量效率。", "conclusion": "可调刚度有助于能量高效的运动，并为未来开发腿部刚度选择控制器提供了见解。"}}
{"id": "2508.03043", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03043", "abs": "https://arxiv.org/abs/2508.03043", "authors": ["Yi-Hsuan Hsiao", "Andrea Tagliabue", "Owen Matteson", "Suhan Kim", "Tong Zhao", "Jonathan P. How", "YuFeng Chen"], "title": "Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control", "comment": "27 pages, 26 supplementary pages, 6 main figures, 16 supplementary\n  figures, 1 table", "summary": "Aerial insects exhibit highly agile maneuvers such as sharp braking,\nsaccades, and body flips under disturbance. In contrast, insect-scale aerial\nrobots are limited to tracking non-aggressive trajectories with small body\nacceleration. This performance gap is contributed by a combination of low robot\ninertia, fast dynamics, uncertainty in flapping-wing aerodynamics, and high\nsusceptibility to environmental disturbance. Executing highly dynamic maneuvers\nrequires the generation of aggressive flight trajectories that push against the\nhardware limit and a high-rate feedback controller that accounts for model and\nenvironmental uncertainty. Here, through designing a deep-learned robust tube\nmodel predictive controller, we showcase insect-like flight agility and\nrobustness in a 750-millgram flapping-wing robot. Our model predictive\ncontroller can track aggressive flight trajectories under disturbance. To\nachieve a high feedback rate in a compute-constrained real-time system, we\ndesign imitation learning methods to train a two-layer, fully connected neural\nnetwork, which resembles insect flight control architecture consisting of\ncentral nervous system and motor neurons. Our robot demonstrates insect-like\nsaccade movements with lateral speed and acceleration of 197 centimeters per\nsecond and 11.7 meters per second square, representing 447$\\%$ and 255$\\%$\nimprovement over prior results. The robot can also perform saccade maneuvers\nunder 160 centimeters per second wind disturbance and large command-to-force\nmapping errors. Furthermore, it performs 10 consecutive body flips in 11\nseconds - the most challenging maneuver among sub-gram flyers. These results\nrepresent a milestone in achieving insect-scale flight agility and inspire\nfuture investigations on sensing and compute autonomy.", "AI": {"tldr": "通过设计深度学习的鲁棒管模型预测控制器，研究团队在750毫克的扑翼机器人上实现了昆虫般的飞行敏捷性和鲁棒性。", "motivation": "昆虫飞行的高机动性与现有昆虫级飞行机器人的性能差距，促使研究团队开发更高效的控制器以实现类似昆虫的飞行敏捷性。", "method": "设计了深度学习的鲁棒管模型预测控制器，并采用模仿学习方法训练神经网络，以在计算受限的实时系统中实现高反馈率。", "result": "机器人展示了类似昆虫的快速转向动作，速度和加速度分别提高了447%和255%，并能在强风干扰下完成高难度动作。", "conclusion": "该研究在昆虫级飞行敏捷性方面取得了里程碑式的进展，为未来的传感和计算自主性研究提供了启发。"}}
{"id": "2508.03428", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03428", "abs": "https://arxiv.org/abs/2508.03428", "authors": ["Bojan Derajić", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Wolfgang Hönig"], "title": "Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments", "comment": null, "summary": "In this paper, we propose a hybrid MPC local planner that uses a\nlearning-based approximation of a time-varying safe set, derived from local\nobservations and applied as the MPC terminal constraint. This set can be\nrepresented as a zero-superlevel set of the value function computed via\nHamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time.\nWe exploit the property that the HJ value function can be expressed as a\ndifference of the corresponding signed distance function (SDF) and a\nnon-negative residual function. The residual component is modeled as a neural\nnetwork with non-negative output and subtracted from the computed SDF,\nresulting in a real-time value function estimate that is at least as safe as\nthe SDF by design. Additionally, we parametrize the neural residual by a\nhypernetwork to improve real-time performance and generalization properties.\nThe proposed method is compared with three state-of-the-art methods in\nsimulations and hardware experiments, achieving up to 30\\% higher success rates\ncompared to the best baseline while requiring a similar computational effort\nand producing high-quality (low travel-time) solutions.", "AI": {"tldr": "提出了一种混合MPC局部规划器，利用学习逼近时间变化的安全集作为MPC终端约束，通过HJ可达性分析实现实时安全估计。", "motivation": "解决HJ可达性分析在实时应用中不可行的问题，同时保证安全性。", "method": "利用SDF和神经网络的非负残差函数逼近HJ值函数，并通过超网络参数化提高实时性和泛化能力。", "result": "在仿真和硬件实验中，相比三种先进方法，成功率提高30%，计算成本相近且路径质量高。", "conclusion": "该方法在实时性和安全性上表现优异，适用于复杂环境下的路径规划。"}}
{"id": "2508.02870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02870", "abs": "https://arxiv.org/abs/2508.02870", "authors": ["Mohamed Irfan Refai", "Abdulaziz Y. Alkayas", "Anup Teejo Mathew", "Federico Renda", "Thomas George Thuruthel"], "title": "Learning User Interaction Forces using Vision for a Soft Finger Exosuit", "comment": "13 pages, 9 figures", "summary": "Wearable assistive devices are increasingly becoming softer. Modelling their\ninterface with human tissue is necessary to capture transmission of dynamic\nassistance. However, their nonlinear and compliant nature makes both physical\nmodeling and embedded sensing challenging. In this paper, we develop a\nimage-based, learning-based framework to estimate distributed contact forces\nfor a finger-exosuit system. We used the SoRoSim toolbox to generate a diverse\ndataset of exosuit geometries and actuation scenarios for training. The method\naccurately estimated interaction forces across multiple contact locations from\nlow-resolution grayscale images, was able to generalize to unseen shapes and\nactuation levels, and remained robust under visual noise and contrast\nvariations. We integrated the model into a feedback controller, and found that\nthe vision-based estimator functions as a surrogate force sensor for\nclosed-loop control. This approach could be used as a non-intrusive alternative\nfor real-time force estimation for exosuits.", "AI": {"tldr": "提出了一种基于图像和学习的框架，用于估计手指外骨骼系统的分布式接触力，解决了软性可穿戴设备的建模和传感挑战。", "motivation": "软性可穿戴设备的非线性和柔顺性使得物理建模和嵌入式传感具有挑战性，需要一种新的方法来捕捉动态辅助力的传递。", "method": "使用SoRoSim工具箱生成多样化的外骨骼几何和驱动场景数据集，开发了基于图像和学习的框架，从低分辨率灰度图像中估计接触力。", "result": "方法能够准确估计多个接触点的力，泛化能力强，对视觉噪声和对比度变化具有鲁棒性，并成功集成到反馈控制器中。", "conclusion": "该框架可作为外骨骼实时力估计的非侵入性替代方案，适用于闭环控制。"}}
{"id": "2508.02898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02898", "abs": "https://arxiv.org/abs/2508.02898", "authors": ["Isobel Voysey", "Lynne Baillie", "Joanne Williams", "Michael Herrmann"], "title": "Co-designing Zoomorphic Robot Concepts for Animal Welfare Education", "comment": null, "summary": "Animal welfare education could greatly benefit from customized robots to help\nchildren learn about animals and their behavior, and thereby promote positive,\nsafe child-animal interactions. To this end, we ran Participatory Design\nworkshops with animal welfare educators and children to identify key\nrequirements for zoomorphic robots from their perspectives. Our findings\nencompass a zoomorphic robot's appearance, behavior, and features, as well as\nconcepts for a narrative surrounding the robot. Through comparing and\ncontrasting the two groups, we find the importance of: negative reactions to\nundesirable behavior from children; using the facial features and tail to\nprovide cues signaling an animal's internal state; and a natural, furry\nappearance and texture. We also contribute some novel activities for\nParticipatory Design with children, including branching storyboards inspired by\nthematic apperception tests and interactive narratives, and reflect on some of\nthe key design challenges of achieving consensus between the groups, despite\nmuch overlap in their design concepts.", "AI": {"tldr": "研究探讨了如何通过定制化动物形态机器人促进儿童动物福利教育，通过参与式设计工作坊收集了教育者和儿童的需求，总结了机器人的外观、行为和功能设计要点。", "motivation": "通过机器人帮助儿童学习动物行为，促进安全的儿童与动物互动。", "method": "采用参与式设计工作坊，结合教育者和儿童的意见，分析机器人的外观、行为及功能需求。", "result": "发现负面反应、面部和尾巴特征的重要性，以及自然外观和触感的需求。提出了新的参与式设计活动。", "conclusion": "尽管两组设计概念有重叠，达成共识仍具挑战性，但研究为动物福利教育机器人设计提供了重要参考。"}}
{"id": "2508.02919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02919", "abs": "https://arxiv.org/abs/2508.02919", "authors": ["Boyang Tian", "Weisong Shi"], "title": "Context-aware Risk Assessment and Its Application in Autonomous Driving", "comment": "ITSC 2025, 7 pages", "summary": "Ensuring safety in autonomous driving requires precise, real-time risk\nassessment and adaptive behavior. Prior work on risk estimation either outputs\ncoarse, global scene-level metrics lacking interpretability, proposes\nindicators without concrete integration into autonomous systems, or focuses\nnarrowly on specific driving scenarios. We introduce the Context-aware Risk\nIndex (CRI), a light-weight modular framework that quantifies directional risks\nbased on object kinematics and spatial relationships, dynamically adjusting\ncontrol commands in real time. CRI employs direction-aware spatial partitioning\nwithin a dynamic safety envelope using Responsibility-Sensitive Safety (RSS)\nprinciples, a hybrid probabilistic-max fusion strategy for risk aggregation,\nand an adaptive control policy for real-time behavior modulation. We evaluate\nCRI on the Bench2Drive benchmark comprising 220 safety-critical scenarios using\na state-of-the-art end-to-end model Transfuser++ on challenging routes. Our\ncollision-rate metrics show a 19\\% reduction (p = 0.003) in vehicle collisions\nper failed route, a 20\\% reduction (p = 0.004) in collisions per kilometer, a\n17\\% increase (p = 0.016) in composed driving score, and a statistically\nsignificant reduction in penalty scores (p = 0.013) with very low overhead (3.6\nms per decision cycle). These results demonstrate that CRI substantially\nimproves safety and robustness in complex, risk-intensive environments while\nmaintaining modularity and low runtime overhead.", "AI": {"tldr": "论文提出了一种轻量级模块化框架CRI，用于实时风险评估和行为调整，显著提升了自动驾驶的安全性。", "motivation": "现有风险评估方法要么缺乏可解释性，要么未具体整合到自动驾驶系统中，或仅针对特定场景。", "method": "CRI基于物体运动学和空间关系量化方向性风险，采用方向感知空间分区、混合概率-最大融合策略和自适应控制策略。", "result": "在Bench2Drive基准测试中，CRI显著减少了碰撞率并提高了驾驶评分，且运行时开销低。", "conclusion": "CRI在复杂高风险环境中显著提升了安全性和鲁棒性，同时保持了模块化和低运行时开销。"}}
{"id": "2508.02930", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02930", "abs": "https://arxiv.org/abs/2508.02930", "authors": ["Zenan Zhu", "Wenxi Chen", "Pei-Chun Kao", "Janelle Clark", "Lily Behnke", "Rebecca Kramer-Bottiglio", "Holly Yanco", "Yan Gu"], "title": "Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain Geometry Estimation with Wearable Soft Sensors", "comment": "8 pages, 5 figures", "summary": "This letter presents a model-agnostic meta-learning (MAML) based framework\nfor simultaneous and accurate estimation of human gait phase and terrain\ngeometry using a small set of fabric-based wearable soft sensors, with\nefficient adaptation to unseen subjects and strong generalization across\ndifferent subjects and terrains. Compared to rigid alternatives such as\ninertial measurement units, fabric-based soft sensors improve comfort but\nintroduce nonlinearities due to hysteresis, placement error, and fabric\ndeformation. Moreover, inter-subject and inter-terrain variability, coupled\nwith limited calibration data in real-world deployments, further complicate\naccurate estimation. To address these challenges, the proposed framework\nintegrates MAML into a deep learning architecture to learn a generalizable\nmodel initialization that captures subject- and terrain-invariant structure.\nThis initialization enables efficient adaptation (i.e., adaptation with only a\nsmall amount of calibration data and a few fine-tuning steps) to new users,\nwhile maintaining strong generalization (i.e., high estimation accuracy across\nsubjects and terrains). Experiments on nine participants walking at various\nspeeds over five terrain conditions demonstrate that the proposed framework\noutperforms baseline approaches in estimating gait phase, locomotion mode, and\nincline angle, with superior accuracy, adaptation efficiency, and\ngeneralization.", "AI": {"tldr": "提出了一种基于MAML的框架，用于通过少量织物传感器同时准确估计步态和地形，适应性强且泛化能力高。", "motivation": "解决织物传感器因非线性特性和数据不足带来的估计难题，适应不同用户和地形。", "method": "将MAML集成到深度学习架构中，学习通用模型初始化，实现高效适应和强泛化。", "result": "在九名参与者的实验中，框架在步态、运动模式和倾斜角估计上优于基线方法。", "conclusion": "该框架在准确性和适应性上表现优异，适用于实际部署。"}}
{"id": "2508.02947", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02947", "abs": "https://arxiv.org/abs/2508.02947", "authors": ["M Tanjid Hasan Tonmoy", "Rahath Malladi", "Kaustubh Singh", "Forsad Al Hossain", "Rajesh Gupta", "Andrés E. Tejada-Martínez", "Tauhidur Rahman"], "title": "AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed", "comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2025. Author Accepted Manuscript", "summary": "Indoor air quality plays an essential role in the safety and well-being of\noccupants, especially in the context of airborne diseases. This paper\nintroduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor\nair purification systems through a robotic cough emulator testbed and a\ndigital-twins-based aerosol residence time analysis. Current portable air\nfilters often overlook the concentrations of respiratory aerosols generated by\ncoughs, posing a risk, particularly in high-exposure environments like\nhealthcare facilities and public spaces. To address this gap, we present a\nrobotic dual-agent physical emulator comprising a maneuverable mannequin\nsimulating cough events and a portable air purifier autonomously responding to\naerosols. The generated data from this emulator trains a digital twins model,\ncombining a physics-based compartment model with a machine learning approach,\nusing Long Short-Term Memory (LSTM) networks and graph convolution layers.\nExperimental results demonstrate the model's ability to predict aerosol\nconcentration dynamics with a mean residence time prediction error within 35\nseconds. The proposed system's real-time intervention strategies outperform\nstatic air filter placement, showcasing its potential in mitigating airborne\npathogen risks.", "AI": {"tldr": "AeroSafe是一种通过机器人咳嗽模拟器和数字孪生技术优化室内空气净化系统的新方法，旨在解决现有便携式空气过滤器忽视咳嗽产生的气溶胶浓度的问题。", "motivation": "室内空气质量对健康至关重要，尤其是在高暴露环境中，现有空气过滤器未能有效处理咳嗽产生的气溶胶。", "method": "开发了一个机器人双代理物理模拟器，结合数字孪生模型（物理隔间模型和机器学习方法）预测气溶胶动态。", "result": "模型预测气溶胶浓度的平均停留时间误差在35秒内，实时干预策略优于静态空气过滤器。", "conclusion": "AeroSafe系统在减少空气传播病原体风险方面具有潜力。"}}
{"id": "2508.02952", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02952", "abs": "https://arxiv.org/abs/2508.02952", "authors": ["Hassan Iqbal", "Kobiny Rex", "Joseph Shirley", "Carlos Baiz", "Christian Claudel"], "title": "A novel autonomous microplastics surveying robot for beach environments", "comment": "12 pages, 11 figures", "summary": "Microplastics, defined as plastic particles smaller than 5 millimeters, have\nbecome a pervasive environmental contaminant that accumulates on beaches due to\nwind patterns and tidal forcing. Detecting microplastics and mapping their\nconcentration in the wild remains one of the primary challenges in addressing\nthis environmental issue. This paper introduces a novel robotic platform that\nautomatically detects and chemically analyzes microplastics on beach surfaces.\nThis mobile manipulator system scans areas for microplastics using a camera\nmounted on the robotic arm's end effector. The system effectively segments\ncandidate microplastic particles on sand surfaces even in the presence of\norganic matter such as leaves and clams. Once a candidate microplastic particle\nis detected, the system steers a near-infrared (NIR) spectroscopic sensor onto\nthe particle using both NIR and visual feedback to chemically analyze it in\nreal-time. Through experiments in lab and beach environments, the system is\nshown to achieve an excellent positional precision in manipulation control and\nhigh microplastic classification accuracy.", "AI": {"tldr": "本文介绍了一种新型机器人平台，用于自动检测和分析海滩表面的微塑料，结合视觉和近红外光谱技术，实现了高精度的微塑料分类。", "motivation": "微塑料已成为普遍的环境污染物，但检测和绘制其浓度分布仍是主要挑战。", "method": "开发了一种移动机械臂系统，通过摄像头和近红外光谱传感器实时检测和分析微塑料。", "result": "实验表明，该系统在操控精度和微塑料分类准确性上表现优异。", "conclusion": "该机器人平台为微塑料污染监测提供了一种高效、自动化的解决方案。"}}
{"id": "2508.02953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02953", "abs": "https://arxiv.org/abs/2508.02953", "authors": ["Adarsh Salagame", "Eric Sihite", "Alireza Ramezani"], "title": "Optimal Trajectory Planning in a Vertically Undulating Snake Locomotion using Contact-implicit Optimization", "comment": null, "summary": "Contact-rich problems, such as snake robot locomotion, offer unexplored yet\nrich opportunities for optimization-based trajectory and acyclic contact\nplanning. So far, a substantial body of control research has focused on\nemulating snake locomotion and replicating its distinctive movement patterns\nusing shape functions that either ignore the complexity of interactions or\nfocus on complex interactions with matter (e.g., burrowing movements). However,\nmodels and control frameworks that lie in between these two paradigms and are\nbased on simple, fundamental rigid body dynamics, which alleviate the\nchallenging contact and control allocation problems in snake locomotion, remain\nabsent. This work makes meaningful contributions, substantiated by simulations\nand experiments, in the following directions: 1) introducing a reduced-order\nmodel based on Moreau's stepping-forward approach from differential inclusion\nmathematics, 2) verifying model accuracy, 3) experimental validation.", "AI": {"tldr": "本文提出了一种基于简化动力学模型的蛇形机器人运动控制方法，填补了现有研究的空白，并通过仿真和实验验证了模型的有效性。", "motivation": "现有研究要么忽略蛇形机器人运动的复杂性，要么过于关注复杂交互（如挖掘运动），缺乏基于简单刚体动力学的中间模型。本文旨在填补这一空白。", "method": "采用基于Moreau微分包含数学的降阶模型，并通过仿真和实验验证模型准确性。", "result": "提出的简化模型有效解决了蛇形机器人运动中的接触和控制分配问题。", "conclusion": "本文的模型为蛇形机器人运动控制提供了新的理论基础，并通过实验验证了其可行性。"}}
{"id": "2508.02962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02962", "abs": "https://arxiv.org/abs/2508.02962", "authors": ["Peter Burke"], "title": "Robot builds a robot's brain: AI generated drone command and control station hosted in the sky", "comment": null, "summary": "Advances in artificial intelligence (AI) including large language models\n(LLMs) and hybrid reasoning models present an opportunity to reimagine how\nautonomous robots such as drones are designed, developed, and validated. Here,\nwe demonstrate a fully AI-generated drone control system: with minimal human\ninput, an artificial intelligence (AI) model authored all the code for a\nreal-time, self-hosted drone command and control platform, which was deployed\nand demonstrated on a real drone in flight as well as a simulated virtual drone\nin the cloud. The system enables real-time mapping, flight telemetry,\nautonomous mission planning and execution, and safety protocolsall orchestrated\nthrough a web interface hosted directly on the drone itself. Not a single line\nof code was written by a human. We quantitatively benchmark system performance,\ncode complexity, and development speed against prior, human-coded\narchitectures, finding that AI-generated code can deliver functionally complete\ncommand-and-control stacks at orders-of-magnitude faster development cycles,\nthough with identifiable current limitations related to specific model context\nwindow and reasoning depth. Our analysis uncovers the practical boundaries of\nAI-driven robot control code generation at current model scales, as well as\nemergent strengths and failure modes in AI-generated robotics code. This work\nsets a precedent for the autonomous creation of robot control systems and, more\nbroadly, suggests a new paradigm for robotics engineeringone in which future\nrobots may be largely co-designed, developed, and verified by artificial\nintelligence. In this initial work, a robot built a robot's brain.", "AI": {"tldr": "AI完全生成无人机控制系统，无需人工编码，性能优于传统方法，但存在模型限制。", "motivation": "探索AI在自主机器人设计和开发中的潜力，特别是通过LLMs和混合推理模型实现无人机控制系统的自动化生成。", "method": "利用AI模型生成无人机实时控制和任务规划代码，部署到真实和模拟无人机上，并通过Web界面管理。", "result": "AI生成的代码在开发速度和功能完整性上显著优于人工编码，但受限于模型上下文窗口和推理深度。", "conclusion": "AI驱动的机器人控制系统具有潜力，未来可能实现机器人主要由AI共同设计和开发。"}}
{"id": "2508.02976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02976", "abs": "https://arxiv.org/abs/2508.02976", "authors": ["Hanwen Ren", "Ruiqi Ni", "Ahmed H. Qureshi"], "title": "Physics-informed Neural Time Fields for Prehensile Object Manipulation", "comment": null, "summary": "Object manipulation skills are necessary for robots operating in various\ndaily-life scenarios, ranging from warehouses to hospitals. They allow the\nrobots to manipulate the given object to their desired arrangement in the\ncluttered environment. The existing approaches to solving object manipulations\nare either inefficient sampling based techniques, require expert\ndemonstrations, or learn by trial and error, making them less ideal for\npractical scenarios. In this paper, we propose a novel, multimodal\nphysics-informed neural network (PINN) for solving object manipulation tasks.\nOur approach efficiently learns to solve the Eikonal equation without expert\ndata and finds object manipulation trajectories fast in complex, cluttered\nenvironments. Our method is multimodal as it also reactively replans the\nrobot's grasps during manipulation to achieve the desired object poses. We\ndemonstrate our approach in both simulation and real-world scenarios and\ncompare it against state-of-the-art baseline methods. The results indicate that\nour approach is effective across various objects, has efficient training\ncompared to previous learning-based methods, and demonstrates high performance\nin planning time, trajectory length, and success rates. Our demonstration\nvideos can be found at https://youtu.be/FaQLkTV9knI.", "AI": {"tldr": "提出了一种新型多模态物理信息神经网络（PINN），用于高效解决物体操纵任务，无需专家数据，并在复杂环境中快速找到轨迹。", "motivation": "现有物体操纵方法效率低、依赖专家演示或试错学习，不适用于实际场景。", "method": "采用多模态物理信息神经网络，学习解决Eikonal方程，并动态调整抓取策略。", "result": "在仿真和实际场景中验证，性能优于现有方法，训练高效，轨迹规划快且成功率高。", "conclusion": "该方法在多种物体上表现优异，适用于复杂环境，具有实际应用潜力。"}}
{"id": "2508.02982", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02982", "abs": "https://arxiv.org/abs/2508.02982", "authors": ["Lucas Chen", "Guna Avula", "Hanwen Ren", "Zixing Wang", "Ahmed H. Qureshi"], "title": "Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects", "comment": null, "summary": "Human-robot object handover is a crucial element for assistive robots that\naim to help people in their daily lives, including elderly care, hospitals, and\nfactory floors. The existing approaches to solving these tasks rely on\npre-selected target objects and do not contextualize human implicit and\nexplicit preferences for handover, limiting natural and smooth interaction\nbetween humans and robots. These preferences can be related to the target\nobject selection from the cluttered environment and to the way the robot should\ngrasp the selected object to facilitate desirable human grasping during\nhandovers. Therefore, this paper presents a unified approach that selects\ntarget distant objects using human verbal and non-verbal commands and performs\nthe handover operation by contextualizing human implicit and explicit\npreferences to generate robot grasps and compliant handover motion sequences.\nWe evaluate our integrated framework and its components through real-world\nexperiments and user studies with arbitrary daily-life objects. The results of\nthese evaluations demonstrate the effectiveness of our proposed pipeline in\nhandling object handover tasks by understanding human preferences. Our\ndemonstration videos can be found at https://youtu.be/6z27B2INl-s.", "AI": {"tldr": "本文提出了一种统一方法，通过结合人类的言语和非言语指令选择目标物体，并考虑人类偏好生成机器人抓取和柔顺的交接动作序列。", "motivation": "现有方法依赖预选目标物体且未考虑人类偏好，限制了人机交互的自然性和流畅性。", "method": "结合人类言语和非言语指令选择目标物体，并生成考虑人类偏好的机器人抓取和交接动作序列。", "result": "通过真实实验和用户研究验证了方法的有效性，能够理解人类偏好并完成任务。", "conclusion": "提出的方法能够提升人机交接任务的自然性和流畅性。"}}
{"id": "2508.02984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02984", "abs": "https://arxiv.org/abs/2508.02984", "authors": ["Bibek Gupta", "Mintae Kim", "Albert Park", "Eric Sihite", "Koushil Sreenath", "Alireza Ramezani"], "title": "Estimation of Aerodynamics Forces in Dynamic Morphing Wing Flight", "comment": null, "summary": "Accurate estimation of aerodynamic forces is essential for advancing the\ncontrol, modeling, and design of flapping-wing aerial robots with dynamic\nmorphing capabilities. In this paper, we investigate two distinct methodologies\nfor force estimation on Aerobat, a bio-inspired flapping-wing platform designed\nto emulate the inertial and aerodynamic behaviors observed in bat flight. Our\ngoal is to quantify aerodynamic force contributions during tethered flight, a\ncrucial step toward closed-loop flight control. The first method is a\nphysics-based observer derived from Hamiltonian mechanics that leverages the\nconcept of conjugate momentum to infer external aerodynamic forces acting on\nthe robot. This observer builds on the system's reduced-order dynamic model and\nutilizes real-time sensor data to estimate forces without requiring training\ndata. The second method employs a neural network-based regression model,\nspecifically a multi-layer perceptron (MLP), to learn a mapping from joint\nkinematics, flapping frequency, and environmental parameters to aerodynamic\nforce outputs. We evaluate both estimators using a 6-axis load cell in a\nhigh-frequency data acquisition setup that enables fine-grained force\nmeasurements during periodic wingbeats. The conjugate momentum observer and the\nregression model demonstrate strong agreement across three force components\n(Fx, Fy, Fz).", "AI": {"tldr": "论文研究了两种方法（基于物理的观测器和神经网络回归模型）用于估计仿生扑翼机器人Aerobat的气动力，并验证了其准确性。", "motivation": "精确估计气动力对扑翼机器人的控制、建模和设计至关重要，尤其是实现闭环飞行控制。", "method": "1. 基于哈密顿力学的物理观测器，利用共轭动量推断气动力；2. 多层感知机（MLP）神经网络回归模型，从运动学和环境参数映射到气动力。", "result": "两种方法在三个力分量（Fx, Fy, Fz）上表现出高度一致性。", "conclusion": "两种方法均能有效估计气动力，为扑翼机器人的闭环控制提供了重要工具。"}}
{"id": "2508.02988", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02988", "abs": "https://arxiv.org/abs/2508.02988", "authors": ["Linji Wang", "Zifan Xu", "Peter Stone", "Xuesu Xiao"], "title": "GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring", "comment": "7 pages, IROS 2025", "summary": "Curriculum learning has emerged as a promising approach for training complex\nrobotics tasks, yet current applications predominantly rely on manually\ndesigned curricula, which demand significant engineering effort and can suffer\nfrom subjective and suboptimal human design choices. While automated curriculum\nlearning has shown success in simple domains like grid worlds and games where\ntask distributions can be easily specified, robotics tasks present unique\nchallenges: they require handling complex task spaces while maintaining\nrelevance to target domain distributions that are only partially known through\nlimited samples. To this end, we propose Grounded Adaptive Curriculum Learning,\na framework specifically designed for robotics curriculum learning with three\nkey innovations: (1) a task representation that consistently handles complex\nrobot task design, (2) an active performance tracking mechanism that allows\nadaptive curriculum generation appropriate for the robot's current\ncapabilities, and (3) a grounding approach that maintains target domain\nrelevance through alternating sampling between reference and synthetic tasks.\nWe validate GACL on wheeled navigation in constrained environments and\nquadruped locomotion in challenging 3D confined spaces, achieving 6.8% and 6.1%\nhigher success rates, respectively, than state-of-the-art methods in each\ndomain.", "AI": {"tldr": "提出了一种名为Grounded Adaptive Curriculum Learning（GACL）的框架，用于机器人课程学习，解决了手动设计课程和复杂任务空间的挑战，并在导航和四足机器人任务中取得了优于现有方法的结果。", "motivation": "当前机器人课程学习依赖手动设计，工程量大且可能主观或不优。自动课程学习在简单领域有效，但机器人任务复杂且目标分布部分未知，需要新方法。", "method": "GACL框架包含三个创新：1) 处理复杂任务的任务表示；2) 自适应课程生成的性能跟踪机制；3) 通过交替采样保持目标相关性的接地方法。", "result": "在受限环境中的轮式导航和3D狭窄空间中的四足机器人运动任务中，GACL分别比现有方法提高了6.8%和6.1%的成功率。", "conclusion": "GACL为机器人课程学习提供了有效的自动化框架，解决了复杂任务空间和目标分布的部分未知问题，并在实验中验证了其优越性。"}}
{"id": "2508.03003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03003", "abs": "https://arxiv.org/abs/2508.03003", "authors": ["Chenghao Wang", "Alireza Ramezani"], "title": "Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals", "comment": null, "summary": "Husky Carbon, a robot developed by Northeastern University, serves as a\nresearch platform to explore unification of posture manipulation and thrust\nvectoring. Unlike conventional quadrupeds, its joint actuators and thrusters\nenable enhanced control authority, facilitating thruster-assisted narrow-path\nwalking. While a unified Model Predictive Control (MPC) framework optimizing\nboth ground reaction forces and thruster forces could theoretically address\nthis control problem, its feasibility is limited by the low torque-control\nbandwidth of the system's lightweight actuators. To overcome this challenge, we\npropose a decoupled control architecture: a Raibert-type controller governs\nlegged locomotion using position-based control, while an MPC regulates the\nthrusters augmented by learned Contact Residual Dynamics (CRD) to account for\nleg-ground impacts. This separation bypasses the torque-control rate bottleneck\nwhile retaining the thruster MPC to explicitly account for leg-ground impact\ndynamics through learned residuals. We validate this approach through both\nsimulation and hardware experiments, showing that the decoupled control\narchitecture with CRD performs more stable behavior in terms of push recovery\nand cat-like walking gait compared to the decoupled controller without CRD.", "AI": {"tldr": "Husky Carbon机器人通过解耦控制架构（Raibert控制器和MPC结合学习接触残差动力学）解决了轻量执行器带宽不足的问题，实现了更稳定的推恢复和猫式步态。", "motivation": "研究如何结合姿态操纵和推力矢量控制，以增强机器人在狭窄路径上的行走能力。", "method": "提出解耦控制架构：Raibert控制器负责腿部运动（位置控制），MPC调节推力器，并结合学习的接触残差动力学（CRD）处理腿-地面碰撞。", "result": "仿真和硬件实验表明，带CRD的解耦控制器在推恢复和步态稳定性上优于无CRD的控制器。", "conclusion": "解耦控制架构结合CRD能有效解决轻量执行器带宽限制，提升机器人动态性能。"}}
{"id": "2508.03024", "categories": ["cs.RO", "I.2.9; C.3"], "pdf": "https://arxiv.org/pdf/2508.03024", "abs": "https://arxiv.org/abs/2508.03024", "authors": ["Jie Lin", "Hsun-Yu Lee", "Ho-Ming Li", "Fang-Jing Wu"], "title": "LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning", "comment": "6 pages, 10 figures", "summary": "Accurate and robust indoor localization is critical for smart building\napplications, yet existing Wi-Fi-based systems are often vulnerable to\nenvironmental conditions. This work presents a novel indoor localization\nsystem, called LiGen, that leverages the spectral intensity patterns of ambient\nlight as fingerprints, offering a more stable and infrastructure-free\nalternative to radio signals. To address the limited spectral data, we design a\ndata augmentation framework based on generative adversarial networks (GANs),\nfeaturing two variants: PointGAN, which generates fingerprints conditioned on\ncoordinates, and FreeGAN, which uses a weak localization model to label\nunconditioned samples. Our positioning model, leveraging a Multi-Layer\nPerceptron (MLP) architecture to train on synthesized data, achieves\nsubmeter-level accuracy, outperforming Wi-Fi-based baselines by over 50\\%.\nLiGen also demonstrates strong robustness in cluttered environments. To the\nbest of our knowledge, this is the first system to combine spectral\nfingerprints with GAN-based data augmentation for indoor localization.", "AI": {"tldr": "LiGen是一种新型室内定位系统，利用环境光的光谱强度模式作为指纹，通过GAN数据增强和MLP模型实现高精度定位。", "motivation": "现有Wi-Fi定位系统易受环境影响，LiGen提出更稳定的光指纹替代方案。", "method": "使用GAN生成增强数据（PointGAN和FreeGAN），MLP模型训练合成数据。", "result": "LiGen达到亚米级精度，优于Wi-Fi基线50%以上，且在复杂环境中表现稳健。", "conclusion": "LiGen首次结合光谱指纹与GAN数据增强，为室内定位提供了新思路。"}}
{"id": "2508.03027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03027", "abs": "https://arxiv.org/abs/2508.03027", "authors": ["Yizhuo Wang", "Haodong He", "Jingsong Liang", "Yuhong Cao", "Ritabrata Chakraborty", "Guillaume Sartoretti"], "title": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction", "comment": "Accepted for presentation at CORL 2025", "summary": "Path planning in unknown environments is a crucial yet inherently challenging\ncapability for mobile robots, which primarily encompasses two coupled tasks:\nautonomous exploration and point-goal navigation. In both cases, the robot must\nperceive the environment, update its belief, and accurately estimate potential\ninformation gain on-the-fly to guide planning. In this work, we propose\nCogniPlan, a novel path planning framework that leverages multiple plausible\nlayouts predicted by a COnditional GeNerative Inpainting model, mirroring how\nhumans rely on cognitive maps during navigation. These predictions, based on\nthe partially observed map and a set of layout conditioning vectors, enable our\nplanner to reason effectively under uncertainty. We demonstrate strong synergy\nbetween generative image-based layout prediction and graph-attention-based path\nplanning, allowing CogniPlan to combine the scalability of graph\nrepresentations with the fidelity and predictiveness of occupancy maps,\nyielding notable performance gains in both exploration and navigation. We\nextensively evaluate CogniPlan on two datasets (hundreds of maps and realistic\nfloor plans), consistently outperforming state-of-the-art planners. We further\ndeploy it in a high-fidelity simulator and on hardware, showcasing its\nhigh-quality path planning and real-world applicability.", "AI": {"tldr": "CogniPlan是一个新的路径规划框架，利用生成模型预测多种可能的布局，结合图注意力路径规划，显著提升探索和导航性能。", "motivation": "解决移动机器人在未知环境中路径规划的挑战，特别是自主探索和点目标导航的耦合任务。", "method": "使用条件生成修复模型预测多种布局，结合图注意力路径规划，在不确定性下有效推理。", "result": "在多个数据集和实际场景中表现优异，超越现有先进规划器。", "conclusion": "CogniPlan结合生成模型和图表示，实现了高效、可扩展的路径规划，具有实际应用潜力。"}}
{"id": "2508.03053", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03053", "abs": "https://arxiv.org/abs/2508.03053", "authors": ["Haojun Xu", "Jiaqi Xiang", "Wu Wei", "Jinyu Chen", "Linqing Zhong", "Linjiang Huang", "Hongyu Yang", "Si Liu"], "title": "SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps", "comment": "9 pages, 5 figures", "summary": "A typical human strategy for giving navigation guidance is to sketch route\nmaps based on the environmental layout. Inspired by this, we introduce Sketch\nmap-based visual Navigation (SkeNa), an embodied navigation task in which an\nagent must reach a goal in an unseen environment using only a hand-drawn sketch\nmap as guidance. To support research for SkeNa, we present a large-scale\ndataset named SoR, comprising 54k trajectory and sketch map pairs across 71\nindoor scenes. In SoR, we introduce two navigation validation sets with varying\nlevels of abstraction in hand-drawn sketches, categorized based on their\npreservation of spatial scales in the environment, to facilitate future\nresearch. To construct SoR, we develop an automated sketch-generation pipeline\nthat efficiently converts floor plans into hand-drawn representations. To solve\nSkeNa, we propose SkeNavigator, a navigation framework that aligns visual\nobservations with hand-drawn maps to estimate navigation targets. It employs a\nRay-based Map Descriptor (RMD) to enhance sketch map valid feature\nrepresentation using equidistant sampling points and boundary distances. To\nimprove alignment with visual observations, a Dual-Map Aligned Goal Predictor\n(DAGP) leverages the correspondence between sketch map features and on-site\nconstructed exploration map features to predict goal position and guide\nnavigation. SkeNavigator outperforms prior floor plan navigation methods by a\nlarge margin, improving SPL on the high-abstract validation set by 105%\nrelatively. Our code and dataset will be released.", "AI": {"tldr": "论文提出了一种基于手绘草图导航的任务SkeNa，并发布了大规模数据集SoR。通过自动化草图生成管道和导航框架SkeNavigator，显著提升了导航性能。", "motivation": "受人类导航策略启发，研究如何在未见环境中仅凭手绘草图实现导航。", "method": "开发自动化草图生成管道，构建数据集SoR；提出SkeNavigator框架，结合RMD和DAGP技术。", "result": "SkeNavigator在高度抽象的验证集上相对提升了105%的SPL。", "conclusion": "SkeNa任务和SoR数据集为未来研究提供了新方向，SkeNavigator框架表现出色。"}}
{"id": "2508.03068", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03068", "abs": "https://arxiv.org/abs/2508.03068", "authors": ["Sirui Chen", "Yufei Ye", "Zi-Ang Cao", "Jennifer Lew", "Pei Xu", "C. Karen Liu"], "title": "Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching", "comment": null, "summary": "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns\nnavigation, locomotion, and reaching skills for humanoids, directly from human\nmotion and vision perception data. We take a modular approach where the\nhigh-level planner commands the target position and orientation of the hands\nand eyes of the humanoid, delivered by the low-level policy that controls the\nwhole-body movements. Specifically, the low-level whole-body controller learns\nto track the three points (eyes, left hand, and right hand) from existing\nlarge-scale human motion capture data while high-level policy learns from human\ndata collected by Aria glasses. Our modular approach decouples the ego-centric\nvision perception from physical actions, promoting efficient learning and\nscalability to novel scenes. We evaluate our method both in simulation and in\nthe real-world, demonstrating humanoid's capabilities to navigate and reach in\ncomplex environments designed for humans.", "AI": {"tldr": "HEAD框架通过模块化方法，从人类运动和视觉感知数据中学习人形机器人的导航、运动和抓取技能。", "motivation": "解决人形机器人在复杂环境中导航和抓取的挑战，通过模块化设计提高学习效率和可扩展性。", "method": "采用模块化方法，高层规划器指挥手和眼的目标位置，低层策略控制全身动作，分别从人类运动捕捉数据和Aria眼镜数据中学习。", "result": "在仿真和现实环境中验证了人形机器人在复杂环境中的导航和抓取能力。", "conclusion": "HEAD框架通过模块化设计成功实现了人形机器人在复杂环境中的高效学习和任务执行。"}}
{"id": "2508.03070", "categories": ["cs.RO", "cs.AI", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.03070", "abs": "https://arxiv.org/abs/2508.03070", "authors": ["Devin Crowley", "Jeremy Dao", "Helei Duan", "Kevin Green", "Jonathan Hurst", "Alan Fern"], "title": "Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running", "comment": "7 pages, 7 figures, published by IEEE at ICRA 2023, pp. 12205-12211,\n  see https://ieeexplore.ieee.org/document/10160436", "summary": "In this paper, we explore the space of running gaits for the bipedal robot\nCassie. Our first contribution is to present an approach for optimizing gait\nefficiency across a spectrum of speeds with the aim of enabling extremely\nhigh-speed running on hardware. This raises the question of how the resulting\ngaits compare to human running mechanics, which are known to be highly\nefficient in comparison to quadrupeds. Our second contribution is to conduct\nthis comparison based on established human biomechanical studies. We find that\ndespite morphological differences between Cassie and humans, key properties of\nthe gaits are highly similar across a wide range of speeds. Finally, our third\ncontribution is to integrate the optimized running gaits into a full controller\nthat satisfies the rules of the real-world task of the 100m dash, including\nstarting and stopping from a standing position. We demonstrate this controller\non hardware to establish the Guinness World Record for Fastest 100m by a\nBipedal Robot.", "AI": {"tldr": "本文探讨了双足机器人Cassie的跑步步态优化，比较了其与人类跑步力学的相似性，并实现了100米短跑的世界纪录。", "motivation": "研究旨在优化双足机器人Cassie的跑步步态效率，以实现高速跑步，并与人类跑步力学进行比较。", "method": "提出了一种优化步态效率的方法，并与人类跑步力学进行对比分析，最终将优化步态集成到控制器中实现100米短跑。", "result": "发现Cassie与人类跑步步态在关键特性上高度相似，并成功创造了双足机器人100米短跑的世界纪录。", "conclusion": "研究证明了双足机器人跑步步态优化的可行性及其与人类跑步力学的相似性，为机器人运动控制提供了新思路。"}}
{"id": "2508.03099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03099", "abs": "https://arxiv.org/abs/2508.03099", "authors": ["Sang Min Kim", "Hyeongjun Heo", "Junho Kim", "Yonghyeon Lee", "Young Min Kim"], "title": "Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping", "comment": null, "summary": "We propose Point2Act, which directly retrieves the 3D action point relevant\nfor a contextually described task, leveraging Multimodal Large Language Models\n(MLLMs). Foundation models opened the possibility for generalist robots that\ncan perform a zero-shot task following natural language descriptions within an\nunseen environment. While the semantics obtained from large-scale image and\nlanguage datasets provide contextual understanding in 2D images, the rich yet\nnuanced features deduce blurry 2D regions and struggle to find precise 3D\nlocations for actions. Our proposed 3D relevancy fields bypass the\nhigh-dimensional features and instead efficiently imbue lightweight 2D\npoint-level guidance tailored to the task-specific action. The multi-view\naggregation effectively compensates for misalignments due to geometric\nambiguities, such as occlusion, or semantic uncertainties inherent in the\nlanguage descriptions. The output region is highly localized, reasoning\nfine-grained 3D spatial context that can directly transfer to an explicit\nposition for physical action at the on-the-fly reconstruction of the scene. Our\nfull-stack pipeline, which includes capturing, MLLM querying, 3D\nreconstruction, and grasp pose extraction, generates spatially grounded\nresponses in under 20 seconds, facilitating practical manipulation tasks.\nProject page: https://sangminkim-99.github.io/point2act/", "AI": {"tldr": "Point2Act利用多模态大语言模型（MLLMs）直接检索与任务相关的3D动作点，解决了2D语义模糊问题，并通过多视角聚合和3D相关性场实现高效定位。", "motivation": "通用机器人需在未见环境中执行零样本任务，但现有方法因2D语义模糊难以精确定位3D动作点。", "method": "提出3D相关性场，结合多视角聚合和轻量级2D点级指导，直接推理3D空间上下文。", "result": "系统能在20秒内生成空间响应，支持实际操控任务。", "conclusion": "Point2Act通过高效3D定位和快速响应，为机器人任务提供了实用解决方案。"}}
{"id": "2508.03129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03129", "abs": "https://arxiv.org/abs/2508.03129", "authors": ["Le Qiu", "Yusuf Umut Ciftci", "Somil Bansal"], "title": "Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection", "comment": null, "summary": "Imitation Learning has provided a promising approach to learning complex\nrobot behaviors from expert demonstrations. However, learned policies can make\nerrors that lead to safety violations, which limits their deployment in\nsafety-critical applications. We propose MPC-SafeGIL, a design-time approach\nthat enhances the safety of imitation learning by injecting adversarial\ndisturbances during expert demonstrations. This exposes the expert to a broader\nrange of safety-critical scenarios and allows the imitation policy to learn\nrobust recovery behaviors. Our method uses sampling-based Model Predictive\nControl (MPC) to approximate worst-case disturbances, making it scalable to\nhigh-dimensional and black-box dynamical systems. In contrast to prior work\nthat relies on analytical models or interactive experts, MPC-SafeGIL integrates\nsafety considerations directly into data collection. We validate our approach\nthrough extensive simulations including quadruped locomotion and visuomotor\nnavigation and real-world experiments on a quadrotor, demonstrating\nimprovements in both safety and task performance. See our website here:\nhttps://leqiu2003.github.io/MPCSafeGIL/", "AI": {"tldr": "MPC-SafeGIL通过在设计阶段注入对抗性扰动增强模仿学习的安全性，利用MPC近似最坏情况扰动，适用于高维和黑盒系统。", "motivation": "模仿学习在安全关键应用中因策略错误导致安全问题，需提升安全性。", "method": "在专家演示中注入对抗性扰动，利用MPC近似最坏情况扰动，直接整合安全考虑。", "result": "在四足机器人运动和视觉导航仿真及四旋翼实验中验证了安全性和任务性能的提升。", "conclusion": "MPC-SafeGIL通过扰动增强安全性，适用于复杂系统，提升了模仿学习的实用性。"}}
{"id": "2508.03138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03138", "abs": "https://arxiv.org/abs/2508.03138", "authors": ["Mintaek Oh", "Chan Kim", "Seung-Woo Seo", "Seong-Woo Kim"], "title": "Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 8 pages, 7 figures", "summary": "Robots operating in human-centric or hazardous environments must proactively\nanticipate and mitigate dangers beyond basic obstacle detection. Traditional\nnavigation systems often depend on static maps, which struggle to account for\ndynamic risks, such as a person emerging from a suddenly opening door. As a\nresult, these systems tend to be reactive rather than anticipatory when\nhandling dynamic hazards. Recent advancements in pre-trained large language\nmodels and vision-language models (VLMs) create new opportunities for proactive\nhazard avoidance. In this work, we propose a zero-shot language-as-cost mapping\nframework that leverages VLMs to interpret visual scenes, assess potential\ndynamic risks, and assign risk-aware navigation costs preemptively, enabling\nrobots to anticipate hazards before they materialize. By integrating this\nlanguage-based cost map with a geometric obstacle map, the robot not only\nidentifies existing obstacles but also anticipates and proactively plans around\npotential hazards arising from environmental dynamics. Experiments in simulated\nand diverse dynamic environments demonstrate that the proposed method\nsignificantly improves navigation success rates and reduces hazard encounters,\ncompared to reactive baseline planners. Code and supplementary materials are\navailable at https://github.com/Taekmino/LaC.", "AI": {"tldr": "提出了一种基于视觉语言模型（VLMs）的零样本语言成本映射框架，用于机器人主动预测和规避动态风险。", "motivation": "传统导航系统依赖静态地图，难以应对动态风险（如突然出现的行人），导致反应式而非预测式行为。", "method": "利用VLMs解析视觉场景，评估动态风险，并预分配风险感知导航成本，结合几何障碍地图实现主动规划。", "result": "在模拟和多样动态环境中的实验表明，该方法显著提高了导航成功率并减少了危险遭遇。", "conclusion": "语言成本映射框架为机器人提供了预测动态风险的能力，优于传统反应式规划器。"}}
{"id": "2508.03232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03232", "abs": "https://arxiv.org/abs/2508.03232", "authors": ["Muzhen Cai", "Xiubo Chen", "Yining An", "Jiaxin Zhang", "Xuesong Wang", "Wang Xu", "Weinan Zhang", "Ting Liu"], "title": "CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios", "comment": "9 pages, 5 figures", "summary": "Embodied Planning is dedicated to the goal of creating agents capable of\nexecuting long-horizon tasks in complex physical worlds. However, existing\nembodied planning benchmarks frequently feature short-horizon tasks and\ncoarse-grained action primitives. To address this challenge, we introduce\nCookBench, a benchmark for long-horizon planning in complex cooking scenarios.\nBy leveraging a high-fidelity simulation environment built upon the powerful\nUnity game engine, we define frontier AI challenges in a complex, realistic\nenvironment. The core task in CookBench is designed as a two-stage process.\nFirst, in Intention Recognition, an agent needs to accurately parse a user's\ncomplex intent. Second, in Embodied Interaction, the agent should execute the\nidentified cooking goal through a long-horizon, fine-grained sequence of\nphysical actions. Unlike existing embodied planning benchmarks, we refine the\naction granularity to a spatial level that considers crucial operational\ninformation while abstracting away low-level robotic control. Besides, We\nprovide a comprehensive toolset that encapsulates the simulator. Its unified\nAPI supports both macro-level operations, such as placing orders and purchasing\ningredients, and a rich set of fine-grained embodied actions for physical\ninteraction, enabling researchers to focus on high-level planning and\ndecision-making. Furthermore, we present an in-depth analysis of\nstate-of-the-art, closed-source Large Language Model and Vision-Language Model,\nrevealing their major shortcomings and challenges posed by complex,\nlong-horizon tasks. The full benchmark will be open-sourced to facilitate\nfuture research.", "AI": {"tldr": "CookBench是一个专注于复杂烹饪场景中长期规划任务的基准测试，通过高保真模拟环境支持细粒度动作和意图识别，旨在推动智能体在复杂物理世界中的长期任务执行能力。", "motivation": "现有基准测试多关注短期任务和粗粒度动作，无法满足复杂长期任务的需求，因此需要开发更贴近现实的长期规划基准。", "method": "利用Unity引擎构建高保真模拟环境，设计两阶段任务（意图识别和具身交互），并提供统一API工具集支持宏操作和细粒度动作。", "result": "揭示了当前大型语言模型和视觉语言模型在复杂长期任务中的主要不足，并提供了开放基准以促进研究。", "conclusion": "CookBench填补了长期规划任务的空白，为未来研究提供了实用工具和挑战方向。"}}
{"id": "2508.03246", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03246", "abs": "https://arxiv.org/abs/2508.03246", "authors": ["Zehua Fan", "Feng Gao", "Zhijun Chen", "Yunpeng Yin", "Limin Yang", "Qingxing Xi", "En Yang", "Xuefeng Luo"], "title": "Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots", "comment": null, "summary": "Guiding the visually impaired in complex environments requires real-time\ntwo-way interaction and safety assurance. We propose a Force-Compliance Model\nPredictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for\nforce-compliant navigation and obstacle avoidance in Hexapod guide robots.\nFC-MPC enables two-way interaction by estimating user-applied forces and\nmoments using the robot's dynamic model and the recursive least squares (RLS)\nmethod, and then adjusting the robot's movements accordingly, while Robot-User\nCBFs ensure the safety of both the user and the robot by handling static and\ndynamic obstacles, and employ weighted slack variables to overcome feasibility\nissues in complex dynamic environments. We also adopt an Eight-Way Connected\nDBSCAN method for obstacle clustering, reducing computational complexity from\nO(n2) to approximately O(n), enabling real-time local perception on\nresource-limited on-board robot computers. Obstacles are modeled using Minimum\nBounding Ellipses (MBEs), and their trajectories are predicted through Kalman\nfiltering. Implemented on the HexGuide robot, the system seamlessly integrates\nforce compliance, autonomous navigation, and obstacle avoidance. Experimental\nresults demonstrate the system's ability to adapt to user force commands while\nguaranteeing user and robot safety simultaneously during navigation in complex\nenvironments.", "AI": {"tldr": "提出了一种用于六足导盲机器人的力-顺应模型预测控制（FC-MPC）和机器人-用户控制屏障函数（CBFs），实现实时双向交互和安全导航。", "motivation": "为视障人士在复杂环境中提供实时、安全的导航辅助。", "method": "结合FC-MPC估计用户施加的力并调整机器人运动，使用CBFs处理障碍物，采用八向连接DBSCAN和最小包围椭圆（MBE）进行障碍物建模与预测。", "result": "实验表明系统能适应用户指令并保障安全。", "conclusion": "系统成功整合了力顺应、自主导航和避障，适用于复杂环境。"}}
{"id": "2508.03339", "categories": ["cs.RO", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03339", "abs": "https://arxiv.org/abs/2508.03339", "authors": ["Haoran Lin", "Wenrui Chen", "Xianchi Chen", "Fan Yang", "Qiang Diao", "Wenxin Xie", "Sijie Wu", "Kailun Yang", "Maojun Li", "Yaonan Wang"], "title": "UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands", "comment": "The project page is at https://haochen611.github.io/UFG", "summary": "Dexterous grasp datasets are vital for embodied intelligence, but mostly\nemphasize grasp stability, ignoring functional grasps needed for tasks like\nopening bottle caps or holding cup handles. Most rely on bulky, costly, and\nhard-to-control high-DOF Shadow Hands. Inspired by the human hand's\nunderactuated mechanism, we establish UniFucGrasp, a universal functional grasp\nannotation strategy and dataset for multiple dexterous hand types. Based on\nbiomimicry, it maps natural human motions to diverse hand structures and uses\ngeometry-based force closure to ensure functional, stable, human-like grasps.\nThis method supports low-cost, efficient collection of diverse, high-quality\nfunctional grasps. Finally, we establish the first multi-hand functional grasp\ndataset and provide a synthesis model to validate its effectiveness.\nExperiments on the UFG dataset, IsaacSim, and complex robotic tasks show that\nour method improves functional manipulation accuracy and grasp stability,\nenables efficient generalization across diverse robotic hands, and overcomes\nannotation cost and generalization challenges in dexterous grasping. The\nproject page is at https://haochen611.github.io/UFG.", "AI": {"tldr": "论文提出了一种通用的功能性抓取标注策略和数据集UniFucGrasp，解决了现有抓取数据集忽视功能性抓取的问题，并支持低成本高效收集高质量抓取数据。", "motivation": "现有灵巧抓取数据集过于关注稳定性，而忽略了功能性抓取（如开瓶盖或握杯柄），且依赖昂贵难控的高自由度Shadow Hands。", "method": "基于仿生学，将人类自然动作映射到不同手型结构，利用几何力闭合确保功能性、稳定性和类人抓取。", "result": "实验表明，该方法提高了功能性操作精度和抓取稳定性，支持跨多种机器人手的高效泛化。", "conclusion": "UniFucGrasp克服了灵巧抓取中标注成本和泛化挑战，为功能性抓取提供了新解决方案。"}}
{"id": "2508.03408", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03408", "abs": "https://arxiv.org/abs/2508.03408", "authors": ["Ivana Collado-Gonzalez", "John McConnell", "Paul Szenher", "Brendan Englot"], "title": "Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments", "comment": null, "summary": "Scene reconstruction is an essential capability for underwater robots\nnavigating in close proximity to structures. Monocular vision-based\nreconstruction methods are unreliable in turbid waters and lack depth scale\ninformation. Sonars are robust to turbid water and non-uniform lighting\nconditions, however, they have low resolution and elevation ambiguity. This\nwork proposes a real-time opti-acoustic scene reconstruction method that is\nspecially optimized to work in turbid water. Our strategy avoids having to\nidentify point features in visual data and instead identifies regions of\ninterest in the data. We then match relevant regions in the image to\ncorresponding sonar data. A reconstruction is obtained by leveraging range data\nfrom the sonar and elevation data from the camera image. Experimental\ncomparisons against other vision-based and sonar-based approaches at varying\nturbidity levels, and field tests conducted in marina environments, validate\nthe effectiveness of the proposed approach. We have made our code open-source\nto facilitate reproducibility and encourage community engagement.", "AI": {"tldr": "提出了一种实时光声场景重建方法，适用于浑浊水域，结合视觉和声纳数据，避免传统方法的局限性。", "motivation": "水下机器人在浑浊水域中导航时，单目视觉重建方法不可靠且缺乏深度信息，而声纳分辨率低且有高度模糊性。", "method": "通过识别视觉数据中的感兴趣区域，并将其与声纳数据匹配，利用声纳的距离信息和相机的高度数据进行重建。", "result": "实验验证了该方法在不同浑浊度下的有效性，并在码头环境中进行了实地测试。", "conclusion": "该方法在浑浊水域中表现优异，代码已开源以促进复现和社区参与。"}}
{"id": "2508.03514", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.03514", "abs": "https://arxiv.org/abs/2508.03514", "authors": ["Pavlos Panagiotidis", "Victor Zhi Heung Ngo", "Sean Myatt", "Roma Patel", "Rachel Ramchurn", "Alan Chamberlain", "Ayse Kucukyilmaz"], "title": "Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours", "comment": "The paper is accepted for presentation to International Conference on\n  Social Robotics + AI (https://icsr2025.eu/)", "summary": "In this paper, we propose theatre-in-the-loop, a framework for developing\nexpressive robot behaviours tailored to artistic performance through a\ndirector-guided puppeteering workflow. Leveraging theatrical methods, we use\nnarrative objectives to direct a puppeteer in generating improvised robotic\ngestures that convey specific emotions. These improvisations are captured and\ncurated to build a dataset of reusable movement templates for standalone\nplayback in future autonomous performances. Initial trials demonstrate the\nfeasibility of this approach, illustrating how the workflow enables precise\nsculpting of robotic gestures into coherent emotional arcs while revealing\nchallenges posed by the robot's mechanical constraints. We argue that this\npractice-led framework provides a model for interdisciplinary teams creating\nsocially expressive robot behaviours, contributing to (1) theatre as an\ninteractive training ground for human-robot interaction and (2) co-creation\nmethodologies between humans and machines.", "AI": {"tldr": "提出了一种名为“theatre-in-the-loop”的框架，通过导演指导的木偶工作流开发艺术表演中的机器人行为。", "motivation": "探索如何利用戏剧方法为机器人设计表达情感的姿势，以提升其在艺术表演中的表现力。", "method": "采用叙事目标指导木偶师生成即兴机器人动作，捕捉并整理为可重用的动作模板。", "result": "初步试验验证了该方法的可行性，展示了如何将机器人动作塑造成连贯的情感表达，但也揭示了机械限制带来的挑战。", "conclusion": "该框架为跨学科团队提供了一种模型，用于开发社交表达机器人行为，同时促进了人机交互的戏剧化训练和共创方法。"}}
{"id": "2508.03526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03526", "abs": "https://arxiv.org/abs/2508.03526", "authors": ["Kun Song", "Shentao Ma", "Gaoming Chen", "Ninglong Jin", "Guangbao Zhao", "Mingyu Ding", "Zhenhua Xiong", "Jia Pan"], "title": "CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation", "comment": "9 pages,5 figures", "summary": "A central research topic in robotics is how to use this system to interact\nwith the physical world. Traditional manipulation tasks primarily focus on\nsmall objects. However, in factory or home environments, there is often a need\nfor the movement of large objects, such as moving tables. These tasks typically\nrequire multi-robot systems to work collaboratively. Previous research lacks a\nframework that can scale to arbitrary sizes of robots and generalize to various\nkinds of tasks. In this work, we propose CollaBot, a generalist framework for\nsimultaneous collaborative manipulation. First, we use SEEM for scene\nsegmentation and point cloud extraction of the target object. Then, we propose\na collaborative grasping framework, which decomposes the task into local grasp\npose generation and global collaboration. Finally, we design a 2-stage planning\nmodule that can generate collision-free trajectories to achieve this task.\nExperiments show a success rate of 52% across different numbers of robots,\nobjects, and tasks, indicating the effectiveness of the proposed framework.", "AI": {"tldr": "提出了一种名为CollaBot的通用框架，用于多机器人协作搬运大型物体，通过场景分割、协作抓取和两阶段规划实现任务。", "motivation": "传统机器人操纵任务主要针对小型物体，而工厂或家庭环境中常需搬运大型物体，现有研究缺乏可扩展至任意机器人规模和任务的通用框架。", "method": "使用SEEM进行场景分割和目标物体点云提取；提出协作抓取框架，分解任务为局部抓取姿态生成和全局协作；设计两阶段规划模块生成无碰撞轨迹。", "result": "实验显示在不同机器人数量、物体和任务中成功率为52%，验证了框架的有效性。", "conclusion": "CollaBot框架在多机器人协作搬运任务中表现出潜力，为通用化协作操纵提供了可行方案。"}}
{"id": "2508.03541", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03541", "abs": "https://arxiv.org/abs/2508.03541", "authors": ["Ergi Tushe", "Bilal Farooq"], "title": "Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions", "comment": null, "summary": "The integration of Automated Delivery Robots (ADRs) into pedestrian-heavy\nurban spaces introduces unique challenges in terms of safe, efficient, and\nsocially acceptable navigation. We develop the complete pipeline for a single\nvision sensor based multi-pedestrian detection and tracking, pose estimation,\nand monocular depth perception. Leveraging the real-world MOT17 dataset\nsequences, this study demonstrates how integrating human-pose estimation and\ndepth cues enhances pedestrian trajectory prediction and identity maintenance,\neven under occlusions and dense crowds. Results show measurable improvements,\nincluding up to a 10% increase in identity preservation (IDF1), a 7%\nimprovement in multiobject tracking accuracy (MOTA), and consistently high\ndetection precision exceeding 85%, even in challenging scenarios. Notably, the\nsystem identifies vulnerable pedestrian groups supporting more socially aware\nand inclusive robot behaviour.", "AI": {"tldr": "论文提出了一种基于单视觉传感器的多行人检测与跟踪、姿态估计和深度感知的完整流程，用于提升自动送货机器人在行人密集区域的导航能力。", "motivation": "解决自动送货机器人在行人密集城市空间中安全、高效且社会可接受的导航问题。", "method": "利用MOT17数据集，结合行人姿态估计和深度信息，优化行人轨迹预测和身份保持。", "result": "身份保持（IDF1）提升10%，多目标跟踪准确率（MOTA）提高7%，检测精度超过85%。", "conclusion": "系统能识别易受伤害的行人群体，支持更具社会意识和包容性的机器人行为。"}}
{"id": "2508.03559", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03559", "abs": "https://arxiv.org/abs/2508.03559", "authors": ["Gokhan Solak", "Arash Ajoudani"], "title": "Online Learning for Vibration Suppression in Physical Robot Interaction using Power Tools", "comment": "Submitted, under review", "summary": "Vibration suppression is an important capability for collaborative robots\ndeployed in challenging environments such as construction sites. We study the\nactive suppression of vibration caused by external sources such as power tools.\nWe adopt the band-limited multiple Fourier linear combiner (BMFLC) algorithm to\nlearn the vibration online and counter it by feedforward force control. We\npropose the damped BMFLC method, extending BMFLC with a novel adaptive\nstep-size approach that improves the convergence time and noise resistance. Our\nlogistic function-based damping mechanism reduces the effect of noise and\nenables larger learning rates. We evaluate our method on extensive simulation\nexperiments with realistic time-varying multi-frequency vibration and\nreal-world physical interaction experiments. The simulation experiments show\nthat our method improves the suppression rate in comparison to the original\nBMFLC and its recursive least squares and Kalman filter-based extensions.\nFurthermore, our method is far more efficient than the latter two. We further\nvalidate the effectiveness of our method in real-world polishing experiments. A\nsupplementary video is available at https://youtu.be/ms6m-6JyVAI.", "AI": {"tldr": "论文提出了一种改进的振动抑制方法（阻尼BMFLC），通过自适应步长和噪声抑制机制，显著提高了振动抑制效果和收敛速度。", "motivation": "协作机器人在复杂环境（如建筑工地）中工作时，振动抑制是关键能力。研究旨在解决由外部源（如电动工具）引起的振动问题。", "method": "采用带限多重傅里叶线性组合器（BMFLC）算法在线学习振动，并通过前馈力控制抵消振动。提出阻尼BMFLC方法，结合自适应步长和逻辑函数阻尼机制，提升收敛速度和噪声抵抗能力。", "result": "仿真和实际实验表明，阻尼BMFLC在振动抑制率和效率上优于原始BMFLC及其递归最小二乘和卡尔曼滤波扩展方法。", "conclusion": "阻尼BMFLC方法在振动抑制中表现出色，适用于实际应用，如抛光任务。"}}
{"id": "2508.03600", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.03600", "abs": "https://arxiv.org/abs/2508.03600", "authors": ["Hamze Hammami", "Eva Denisa Barbulescu", "Talal Shaikh", "Mouayad Aldada", "Muhammad Saad Munawar"], "title": "Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control", "comment": "This work was accepted for presentation at the ALIFE 2025 Conference\n  in Kyoto, and will be published by MIT Press as part of the ALIFE 2025\n  proceedings", "summary": "Imagine a robot controller with the ability to adapt like human synapses,\ndynamically rewiring itself to overcome unforeseen challenges in real time.\nThis paper proposes a novel zero-shot adaptation mechanism for evolutionary\nrobotics, merging a standard Genetic Algorithm (GA) controller with online\nHebbian plasticity. Inspired by biological systems, the method separates\nlearning and memory, with the genotype acting as memory and Hebbian updates\nhandling learning. In our approach, the fitness function is leveraged as a live\nscaling factor for Hebbian learning, enabling the robot's neural controller to\nadjust synaptic weights on-the-fly without additional training. This adds a\ndynamic adaptive layer that activates only during runtime to handle unexpected\nenvironmental changes. After the task, the robot 'forgets' the temporary\nadjustments and reverts to the original weights, preserving core knowledge. We\nvalidate this hybrid GA-Hebbian controller on an e-puck robot in a T-maze\nnavigation task with changing light conditions and obstacles.", "AI": {"tldr": "论文提出了一种结合遗传算法和海布学习的零样本适应机制，使机器人控制器能在运行时动态调整突触权重以应对环境变化。", "motivation": "受生物系统启发，旨在开发一种能实时适应未知挑战的机器人控制器，无需额外训练。", "method": "将遗传算法控制器与在线海布可塑性结合，基因型作为记忆，海布更新处理学习，适应层仅在运行时激活。", "result": "在T型迷宫导航任务中验证了混合GA-海布控制器的有效性，成功应对光线变化和障碍。", "conclusion": "该方法为机器人提供了一种动态适应能力，同时保留了核心知识，适用于多变环境。"}}
{"id": "2508.03645", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03645", "abs": "https://arxiv.org/abs/2508.03645", "authors": ["Akshay L Chandra", "Iman Nematollahi", "Chenguang Huang", "Tim Welschehold", "Wolfram Burgard", "Abhinav Valada"], "title": "DiWA: Diffusion Policy Adaptation with World Models", "comment": "Accepted at the 2025 Conference on Robot Learning (CoRL)", "summary": "Fine-tuning diffusion policies with reinforcement learning (RL) presents\nsignificant challenges. The long denoising sequence for each action prediction\nimpedes effective reward propagation. Moreover, standard RL methods require\nmillions of real-world interactions, posing a major bottleneck for practical\nfine-tuning. Although prior work frames the denoising process in diffusion\npolicies as a Markov Decision Process to enable RL-based updates, its strong\ndependence on environment interaction remains highly inefficient. To bridge\nthis gap, we introduce DiWA, a novel framework that leverages a world model for\nfine-tuning diffusion-based robotic skills entirely offline with reinforcement\nlearning. Unlike model-free approaches that require millions of environment\ninteractions to fine-tune a repertoire of robot skills, DiWA achieves effective\nadaptation using a world model trained once on a few hundred thousand offline\nplay interactions. This results in dramatically improved sample efficiency,\nmaking the approach significantly more practical and safer for real-world robot\nlearning. On the challenging CALVIN benchmark, DiWA improves performance across\neight tasks using only offline adaptation, while requiring orders of magnitude\nfewer physical interactions than model-free baselines. To our knowledge, this\nis the first demonstration of fine-tuning diffusion policies for real-world\nrobotic skills using an offline world model. We make the code publicly\navailable at https://diwa.cs.uni-freiburg.de.", "AI": {"tldr": "DiWA框架通过离线世界模型优化扩散策略，显著提升样本效率，减少实际交互需求。", "motivation": "扩散策略在强化学习中的微调面临奖励传播困难和实际交互需求高的问题。", "method": "提出DiWA框架，利用离线世界模型进行扩散策略的强化学习微调。", "result": "在CALVIN基准测试中，DiWA仅通过离线适应就提升了八项任务的性能，且所需物理交互远少于基线方法。", "conclusion": "DiWA首次实现了基于离线世界模型的扩散策略微调，为实际机器人学习提供了更高效、更安全的方法。"}}
{"id": "2508.03672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03672", "abs": "https://arxiv.org/abs/2508.03672", "authors": ["Zhongbi Luo", "Yunjia Wang", "Jan Swevers", "Peter Slaets", "Herman Bruyninckx"], "title": "Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways", "comment": null, "summary": "Accurate geospatial information is crucial for safe, autonomous Inland\nWaterway Transport (IWT), as existing charts (IENC) lack real-time detail and\nconventional LiDAR SLAM fails in waterway environments. These challenges lead\nto vertical drift and non-semantic maps, hindering autonomous navigation.\n  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It\nuses an improved feature extraction and a water surface planar constraint to\nmitigate vertical drift. A novel pipeline transforms 3D point clouds into\nstructured 2D semantic maps using voxel-based geometric analysis, enabling\nreal-time computation of navigational parameters like bridge clearances. An\nautomated module extracts shorelines and exports them into a lightweight,\nIENC-compatible format.\n  Evaluations on a real-world dataset show Inland-LOAM achieves superior\nlocalization accuracy over state-of-the-art methods. The generated semantic\nmaps and shorelines align with real-world conditions, providing reliable data\nfor enhanced situational awareness. The code and dataset will be publicly\navailable", "AI": {"tldr": "Inland-LOAM 是一个针对内河水道的 LiDAR SLAM 框架，通过改进特征提取和水面平面约束减少垂直漂移，并生成语义地图以支持自主导航。", "motivation": "现有水道地图缺乏实时细节，传统 LiDAR SLAM 在水道环境中表现不佳，导致垂直漂移和非语义地图，阻碍自主导航。", "method": "提出 Inland-LOAM，结合改进的特征提取、水面平面约束和体素几何分析，将 3D 点云转换为结构化 2D 语义地图，并自动提取岸线。", "result": "在真实数据集上验证，Inland-LOAM 定位精度优于现有方法，生成的语义地图和岸线与实际情况一致。", "conclusion": "Inland-LOAM 为内河水道自主导航提供了可靠的地理空间信息，代码和数据集将公开。"}}
