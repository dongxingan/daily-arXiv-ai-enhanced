<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation](https://arxiv.org/abs/2506.18960)
*Siqi Shang,Mingyo Seo,Yuke Zhu,Lilly Chin*

Main category: cs.RO

TL;DR: FORTE是一种嵌入柔性夹爪的触觉传感系统，通过3D打印的鳍射线夹爪和内部空气通道提供低延迟的力和滑动反馈，实现了对脆弱物体的高成功率抓取。


<details>
  <summary>Details</summary>
Motivation: 刚性平行夹爪在处理脆弱物体时依赖视觉反馈，缺乏响应性和适应性。触觉传感和软机器人技术可以解决这一问题，但现有方法集成复杂或响应慢。

Method: FORTE采用3D打印的鳍射线夹爪，内部设计空气通道，提供低延迟的力和滑动反馈，确保抓取时施加的力适中且不损坏物体。

Result: FORTE能准确估计0-8N的抓取力（平均误差0.2N），并在100ms内检测滑动事件，抓取脆弱物体（如覆盆子和薯片）成功率达98.6%，滑动检测准确率为93%。

Conclusion: FORTE是一种实用且高效的解决方案，适用于脆弱物体的机器人操作。

Abstract: Handling delicate and fragile objects remains a major challenge for robotic
manipulation, especially for rigid parallel grippers. While the simplicity and
versatility of parallel grippers have led to widespread adoption, these
grippers are limited by their heavy reliance on visual feedback. Tactile
sensing and soft robotics can add responsiveness and compliance. However,
existing methods typically involve high integration complexity or suffer from
slow response times. In this work, we introduce FORTE, a tactile sensing system
embedded in compliant gripper fingers. FORTE uses 3D-printed fin-ray grippers
with internal air channels to provide low-latency force and slip feedback.
FORTE applies just enough force to grasp objects without damaging them, while
remaining easy to fabricate and integrate. We find that FORTE can accurately
estimate grasping forces from 0-8 N with an average error of 0.2 N, and detect
slip events within 100 ms of occurring. We demonstrate FORTE's ability to grasp
a wide range of slippery, fragile, and deformable objects. In particular, FORTE
grasps fragile objects like raspberries and potato chips with a 98.6% success
rate, and achieves 93% accuracy in detecting slip events. These results
highlight FORTE's potential as a robust and practical solution for enabling
delicate robotic manipulation. Project page: https://merge-lab.github.io/FORTE

</details>


### [2] [Faster Motion Planning via Restarts](https://arxiv.org/abs/2506.19016)
*Nancy Amato,Stav Ashur,Sariel Har-Peled%*

Main category: cs.RO

TL;DR: 论文提出通过随机重启技术改进PRM和RRT等随机运动规划算法，显著提升运行效率、路径质量和多线程性能。


<details>
  <summary>Details</summary>
Motivation: 随机运动规划算法（如PRM和RRT）在某些情况下存在性能不稳定的问题，导致简单实例也可能表现不佳。

Method: 应用随机重启技术（包括一些新技术）来加速Las Vegas算法。

Result: 实验显示新算法运行更快、路径更短，多线程性能提升显著（比直接并行实现更优）。

Conclusion: 新算法在理论和实践中均表现优异，且开源实现易于部署和使用。

Abstract: Randomized methods such as PRM and RRT are widely used in motion planning.
However, in some cases, their running-time suffers from inherent instability,
leading to ``catastrophic'' performance even for relatively simple instances.
We apply stochastic restart techniques, some of them new, for speeding up Las
Vegas algorithms, that provide dramatic speedups in practice (a factor of $3$
[or larger] in many cases).
  Our experiments demonstrate that the new algorithms have faster runtimes,
shorter paths, and greater gains from multi-threading (when compared with
straightforward parallel implementation). We prove the optimality of the new
variants. Our implementation is open source, available on github, and is easy
to deploy and use.

</details>


### [3] [Multimodal Anomaly Detection with a Mixture-of-Experts](https://arxiv.org/abs/2506.19077)
*Christoph Willibald,Daniel Sliwowski,Dongheui Lee*

Main category: cs.RO

TL;DR: 提出了一种混合专家框架，结合视觉语言模型和高斯混合回归检测器，用于机器人操作中的多模态异常检测，显著减少检测延迟并提高性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器人应用的多样化，多模态异常检测的需求增加。传统方法仅针对机器人或环境驱动的异常，无法同时处理两者。

Method: 采用混合专家框架，整合视觉语言模型（环境监测）和高斯混合回归检测器（交互力与运动偏差跟踪），并通过置信度融合机制动态选择最可靠的检测器。

Result: 在家庭和工业任务中测试，检测延迟减少60%，帧级异常检测性能优于单一检测器。

Conclusion: 提出的框架有效整合了互补检测机制，显著提升了多模态异常检测的性能和效率。

Abstract: With a growing number of robots being deployed across diverse applications,
robust multimodal anomaly detection becomes increasingly important. In robotic
manipulation, failures typically arise from (1) robot-driven anomalies due to
an insufficient task model or hardware limitations, and (2) environment-driven
anomalies caused by dynamic environmental changes or external interferences.
Conventional anomaly detection methods focus either on the first by low-level
statistical modeling of proprioceptive signals or the second by deep
learning-based visual environment observation, each with different
computational and training data requirements. To effectively capture anomalies
from both sources, we propose a mixture-of-experts framework that integrates
the complementary detection mechanisms with a visual-language model for
environment monitoring and a Gaussian-mixture regression-based detector for
tracking deviations in interaction forces and robot motions. We introduce a
confidence-based fusion mechanism that dynamically selects the most reliable
detector for each situation. We evaluate our approach on both household and
industrial tasks using two robotic systems, demonstrating a 60% reduction in
detection delay while improving frame-wise anomaly detection performance
compared to individual detectors.

</details>


### [4] [Analysis and experiments of the dissipative Twistcar: direction reversal and asymptotic approximations](https://arxiv.org/abs/2506.19112)
*Rom Levy,Ari Dantus,Zitao Yu,Yizhar Or*

Main category: cs.RO

TL;DR: 研究了一个考虑滚动摩擦的双连杆Twistcar模型，发现通过改变几何和质量属性可以反转运动方向，并设计了一个机器人原型验证这一现象。


<details>
  <summary>Details</summary>
Motivation: 以往对Twistcar的研究未考虑摩擦能量耗散，本研究填补了这一空白，并探索了运动方向反转的可能性。

Method: 建立了一个理论双连杆模型，结合滚动摩擦，分析了小振幅稳态周期动力学，并设计了一个可调节质心的机器人原型。

Result: 发现通过改变几何和质量属性可以反转运动方向，并通过实验验证了这一现象。

Conclusion: 研究揭示了摩擦对Twistcar动力学的重要影响，并展示了运动方向反转的实际可行性。

Abstract: Underactuated wheeled vehicles are commonly studied as nonholonomic systems
with periodic actuation. Twistcar is a classical example inspired by a riding
toy, which has been analyzed using a planar model of a dynamical system with
nonholonomic constraints. Most of the previous analyses did not account for
energy dissipation due to friction. In this work, we study a theoretical
two-link model of the Twistcar while incorporating dissipation due to rolling
resistance. We obtain asymptotic expressions for the system's small-amplitude
steady-state periodic dynamics, which reveals the possibility of reversing the
direction of motion upon varying the geometric and mass properties of the
vehicle. Next, we design and construct a robotic prototype of the Twistcar
whose center-of-mass position can be shifted by adding and removing a massive
block, enabling demonstration of the Twistcar's direction reversal phenomenon.
We also conduct parameter fitting for the frictional resistance in order to
improve agreement with experiments.

</details>


### [5] [Ontology Neural Network and ORTSF: A Framework for Topological Reasoning and Delay-Robust Control](https://arxiv.org/abs/2506.19277)
*Jaehong Oh*

Main category: cs.RO

TL;DR: 论文提出了一种结合ONN和ORTSF的统一架构，用于解决自主机器人系统中语义推理和控制的不足。


<details>
  <summary>Details</summary>
Motivation: 现有框架在几何推理和动态稳定性方面表现优异，但在关系语义表示、上下文推理和认知透明度方面存在不足，特别是在动态、以人为中心的环境中。

Method: 通过ONN（Ontology Neural Network）将关系语义推理形式化为动态拓扑过程，并利用ORTSF（Ontological Real-Time Semantic Fabric）将推理痕迹转化为可操作的控制命令。

Result: 实验证明，ONN + ORTSF框架能够统一语义认知和鲁棒控制，为认知机器人提供数学上严谨且实际可行的解决方案。

Conclusion: 该框架填补了现有技术的空白，为动态、人机协作环境中的机器人系统提供了更全面的能力。

Abstract: The advancement of autonomous robotic systems has led to impressive
capabilities in perception, localization, mapping, and control. Yet, a
fundamental gap remains: existing frameworks excel at geometric reasoning and
dynamic stability but fall short in representing and preserving relational
semantics, contextual reasoning, and cognitive transparency essential for
collaboration in dynamic, human-centric environments. This paper introduces a
unified architecture comprising the Ontology Neural Network (ONN) and the
Ontological Real-Time Semantic Fabric (ORTSF) to address this gap. The ONN
formalizes relational semantic reasoning as a dynamic topological process. By
embedding Forman-Ricci curvature, persistent homology, and semantic tensor
structures within a unified loss formulation, ONN ensures that relational
integrity and topological coherence are preserved as scenes evolve over time.
The ORTSF transforms reasoning traces into actionable control commands while
compensating for system delays. It integrates predictive and delay-aware
operators that ensure phase margin preservation and continuity of control
signals, even under significant latency conditions. Empirical studies
demonstrate the ONN + ORTSF framework's ability to unify semantic cognition and
robust control, providing a mathematically principled and practically viable
solution for cognitive robotics.

</details>


### [6] [CUPID: Curating Data your Robot Loves with Influence Functions](https://arxiv.org/abs/2506.19121)
*Christopher Agia,Rohan Sinha,Jingyun Yang,Rika Antonova,Marco Pavone,Haruki Nishimura,Masha Itkina,Jeannette Bohg*

Main category: cs.RO

TL;DR: CUPID是一种基于影响函数理论的机器人数据筛选方法，用于优化模仿学习策略的性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略的性能与演示数据的质量和组成密切相关，但理解单个演示对最终任务成功的影响仍具挑战性。

Method: CUPID通过估计每个训练演示对策略预期回报的影响，筛选出对性能有害的数据并选择能提升策略的新轨迹。

Result: 实验表明，CUPID能显著提升策略性能，例如仅用33%的筛选数据即可达到最佳性能，并在硬件实验中验证了其鲁棒性。

Conclusion: CUPID是一种高效的数据筛选方法，能显著提升模仿学习策略的性能和鲁棒性。

Abstract: In robot imitation learning, policy performance is tightly coupled with the
quality and composition of the demonstration data. Yet, developing a precise
understanding of how individual demonstrations contribute to downstream
outcomes - such as closed-loop task success or failure - remains a persistent
challenge. We propose CUPID, a robot data curation method based on a novel
influence function-theoretic formulation for imitation learning policies. Given
a set of evaluation rollouts, CUPID estimates the influence of each training
demonstration on the policy's expected return. This enables ranking and
selection of demonstrations according to their impact on the policy's
closed-loop performance. We use CUPID to curate data by 1) filtering out
training demonstrations that harm policy performance and 2) subselecting newly
collected trajectories that will most improve the policy. Extensive simulated
and hardware experiments show that our approach consistently identifies which
data drives test-time performance. For example, training with less than 33% of
curated data can yield state-of-the-art diffusion policies on the simulated
RoboMimic benchmark, with similar gains observed in hardware. Furthermore,
hardware experiments show that our method can identify robust strategies under
distribution shift, isolate spurious correlations, and even enhance the
post-training of generalist robot policies. Additional materials are made
available at: https://cupid-curation.github.io.

</details>


### [7] [Zero-Shot Parameter Learning of Robot Dynamics Using Bayesian Statistics and Prior Knowledge](https://arxiv.org/abs/2506.19350)
*Carsten Reiners,Minh Trinh,Lukas Gründel,Sven Tauchmann,David Bitterolf,Oliver Petrovic,Christian Brecher*

Main category: cs.RO

TL;DR: 提出了一种基于贝叶斯统计的机器人惯性参数识别方法，结合先验知识，减少测量需求，实现零样本学习。


<details>
  <summary>Details</summary>
Motivation: 传统最小二乘法或机器学习方法未利用机器人先验信息且需大量测量，需改进。

Method: 采用贝叶斯方法，结合先验知识，支持少量或无测量学习，确保物理可行性并提供置信区间。

Result: 成功识别MABI Max 100机器人的惯性、机械和基础参数，适用于6自由度串联机器人。

Conclusion: 该方法显著减少测量需求，提升泛化能力，适用于缺乏数据表或CAD模型的场景。

Abstract: Inertial parameter identification of industrial robots is an established
process, but standard methods using Least Squares or Machine Learning do not
consider prior information about the robot and require extensive measurements.
Inspired by Bayesian statistics, this paper presents an identification method
with improved generalization that incorporates prior knowledge and is able to
learn with only a few or without additional measurements (Zero-Shot Learning).
Furthermore, our method is able to correctly learn not only the inertial but
also the mechanical and base parameters of the MABI Max 100 robot while
ensuring physical feasibility and specifying the confidence intervals of the
results. We also provide different types of priors for serial robots with 6
degrees of freedom, where datasheets or CAD models are not available.

</details>


### [8] [Situated Haptic Interaction: Exploring the Role of Context in Affective Perception of Robotic Touch](https://arxiv.org/abs/2506.19179)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.RO

TL;DR: 研究探讨了情境情绪线索如何影响对机器人触觉信号的感知，发现视觉情境常主导触觉信号的效价感知，触觉反馈能增强或软化情绪交互。


<details>
  <summary>Details</summary>
Motivation: 情感交互不仅是情绪识别，而是受情境和互动共同塑造的过程。触觉反馈在动态情感交换中的作用尚未充分探索。

Method: 32名参与者观看机器人经历积极、消极或中性动作的视频，随后通过可穿戴振动袖接收机器人的触觉情绪反馈，并评估其情绪状态。

Result: 视觉情境强烈影响触觉信号的感知，消极触觉线索增强交互效价，积极线索软化效价；触觉反馈还主导了参与者对视频唤醒度的感知。

Conclusion: 研究揭示了情境触觉反馈如何丰富人机情感交互，为更细腻的机器情感通信提供了方向。

Abstract: Affective interaction is not merely about recognizing emotions; it is an
embodied, situated process shaped by context and co-created through
interaction. In affective computing, the role of haptic feedback within dynamic
emotional exchanges remains underexplored. This study investigates how
situational emotional cues influence the perception and interpretation of
haptic signals given by a robot. In a controlled experiment, 32 participants
watched video scenarios in which a robot experienced either positive actions
(such as being kissed), negative actions (such as being slapped) or neutral
actions. After each video, the robot conveyed its emotional response through
haptic communication, delivered via a wearable vibration sleeve worn by the
participant. Participants rated the robot's emotional state-its valence
(positive or negative) and arousal (intensity)-based on the video, the haptic
feedback, and the combination of the two. The study reveals a dynamic interplay
between visual context and touch. Participants' interpretation of haptic
feedback was strongly shaped by the emotional context of the video, with visual
context often overriding the perceived valence of the haptic signal. Negative
haptic cues amplified the perceived valence of the interaction, while positive
cues softened it. Furthermore, haptics override the participants' perception of
arousal of the video. Together, these results offer insights into how situated
haptic feedback can enrich affective human-robot interaction, pointing toward
more nuanced and embodied approaches to emotional communication with machines.

</details>


### [9] [Robotics Under Construction: Challenges on Job Sites](https://arxiv.org/abs/2506.19597)
*Haruki Uchiito,Akhilesh Bhat,Koji Kusaka,Xiaoya Zhang,Hiraku Kinjo,Honoka Uehara,Motoki Koyama,Shinji Natsume*

Main category: cs.RO

TL;DR: 本文提出了一种基于CD110R-3履带运输车的自主载荷运输系统，旨在解决建筑行业劳动力短缺和生产力停滞问题。系统整合了自主导航、车队管理和GNSS定位技术，初步展示了无人建筑工地的潜力。


<details>
  <summary>Details</summary>
Motivation: 建筑行业面临劳动力短缺和生产力停滞的挑战，自动化成为可持续基础设施发展的关键。

Method: 基于CD110R-3履带运输车，整合自主导航、车队管理和GNSS定位技术，初步探索外部传感器感知与建图系统。

Result: 初步结果揭示了动态地形导航、建筑环境感知和传感器优化等挑战。

Conclusion: 展望未来，协作自主代理将动态适应工地条件，优化工作流程。本文为机器人驱动的建筑自动化提供了基础性见解，并指出了关键技术发展方向。

Abstract: As labor shortages and productivity stagnation increasingly challenge the
construction industry, automation has become essential for sustainable
infrastructure development. This paper presents an autonomous payload
transportation system as an initial step toward fully unmanned construction
sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous
navigation, fleet management, and GNSS-based localization to facilitate
material transport in construction site environments. While the current system
does not yet incorporate dynamic environment adaptation algorithms, we have
begun fundamental investigations into external-sensor based perception and
mapping system. Preliminary results highlight the potential challenges,
including navigation in evolving terrain, environmental perception under
construction-specific conditions, and sensor placement optimization for
improving autonomy and efficiency. Looking forward, we envision a construction
ecosystem where collaborative autonomous agents dynamically adapt to site
conditions, optimizing workflow and reducing human intervention. This paper
provides foundational insights into the future of robotics-driven construction
automation and identifies critical areas for further technological development.

</details>


### [10] [The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors](https://arxiv.org/abs/2506.19201)
*Hanyang Zhou,Haozhe Lou,Wenhao Liu,Enyu Zhao,Yue Wang,Daniel Seita*

Main category: cs.RO

TL;DR: 本文提出了一种名为MOTIF的新型多模态机器人手，扩展了LEAP手的功能，集成了多种传感器，用于增强灵巧操作。


<details>
  <summary>Details</summary>
Motivation: 现有的多指机器人手缺乏热感和扭矩感知能力，限制了其在复杂任务中的应用。

Method: MOTIF手集成了密集触觉信息、深度传感器、热成像相机、IMU传感器和视觉传感器，设计为低成本且易于复制。

Result: 实验验证了MOTIF手在温度感知安全抓取和区分外观相同但质量不同的物体方面的能力。

Conclusion: MOTIF手通过多模态传感扩展了机器人手的应用范围，证明了其在复杂任务中的潜力。

Abstract: Advancing dexterous manipulation with multi-fingered robotic hands requires
rich sensory capabilities, while existing designs lack onboard thermal and
torque sensing. In this work, we propose the MOTIF hand, a novel multimodal and
versatile robotic hand that extends the LEAP hand by integrating: (i) dense
tactile information across the fingers, (ii) a depth sensor, (iii) a thermal
camera, (iv), IMU sensors, and (v) a visual sensor. The MOTIF hand is designed
to be relatively low-cost (under 4000 USD) and easily reproducible. We validate
our hand design through experiments that leverage its multimodal sensing for
two representative tasks. First, we integrate thermal sensing into 3D
reconstruction to guide temperature-aware, safe grasping. Second, we show how
our hand can distinguish objects with identical appearance but different masses
- a capability beyond methods that use vision only.

</details>


### [11] [Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment](https://arxiv.org/abs/2506.19620)
*Mustafa Adam,Kangfeng Ye,David A. Anisi,Ana Cavalcanti,Jim Woodcock,Robert Morris*

Main category: cs.RO

TL;DR: 本文提出了一种用于农业机器人安全保证的概率建模和风险分析框架，重点关注障碍物和人类的检测、跟踪与避障。


<details>
  <summary>Details</summary>
Motivation: 农民对农业机器人的信任依赖于其可靠性、鲁棒性和安全性，因此需要确保机器人在早期开发阶段的安全性。

Method: 通过危险识别和风险评估矩阵，使用三个状态机建模机器人平台、传感器系统及人类行为，并利用PRISM概率模型检查器分析模型。

Result: 量化了不同设计概念（如高性能物体检测系统或更复杂的警告系统）对风险缓解的影响，提供了独特的开发见解。

Conclusion: 该框架不仅适用于概念开发阶段，还可用于实施、部署和操作阶段。

Abstract: Continued adoption of agricultural robots postulates the farmer's trust in
the reliability, robustness and safety of the new technology. This motivates
our work on safety assurance of agricultural robots, particularly their ability
to detect, track and avoid obstacles and humans. This paper considers a
probabilistic modelling and risk analysis framework for use in the early
development phases. Starting off with hazard identification and a risk
assessment matrix, the behaviour of the mobile robot platform, sensor and
perception system, and any humans present are captured using three state
machines. An auto-generated probabilistic model is then solved and analysed
using the probabilistic model checker PRISM. The result provides unique insight
into fundamental development and engineering aspects by quantifying the effect
of the risk mitigation actions and risk reduction associated with distinct
design concepts. These include implications of adopting a higher performance
and more expensive Object Detection System or opting for a more elaborate
warning system to increase human awareness. Although this paper mainly focuses
on the initial concept-development phase, the proposed safety assurance
framework can also be used during implementation, and subsequent deployment and
operation phases.

</details>


### [12] [Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks](https://arxiv.org/abs/2506.19202)
*Claire Yang,Heer Patel,Max Kleiman-Weiner,Maya Cakmak*

Main category: cs.RO

TL;DR: 研究探讨用户对家用辅助机器人自主性的偏好及其对用户控制感的影响，发现自主性和第三方介入是关键因素，高风险任务中用户更倾向控制。


<details>
  <summary>Details</summary>
Motivation: 探讨用户是否偏好高度自主的辅助机器人，以及机器人自主性如何影响用户的控制感。

Method: 通过实验让参与者评估四种不同自主性水平下对机器人的控制感，并针对不同家庭任务排名偏好。

Result: 用户控制感受机器人自主性和第三方介入影响；高风险任务中用户更倾向控制；第三方信任也影响偏好。

Conclusion: 机器人设计需平衡自主性与用户控制感，高风险任务中应优先用户控制。

Abstract: Roboticists often design with the assumption that assistive robots should be
fully autonomous. However, it remains unclear whether users prefer highly
autonomous robots, as prior work in assistive robotics suggests otherwise. High
robot autonomy can reduce the user's sense of agency, which represents feeling
in control of one's environment. How much control do users, in fact, want over
the actions of robots used for in-home assistance? We investigate how robot
autonomy levels affect users' sense of agency and the autonomy level they
prefer in contexts with varying risks. Our study asked participants to rate
their sense of agency as robot users across four distinct autonomy levels and
ranked their robot preferences with respect to various household tasks. Our
findings revealed that participants' sense of agency was primarily influenced
by two factors: (1) whether the robot acts autonomously, and (2) whether a
third party is involved in the robot's programming or operation. Notably, an
end-user programmed robot highly preserved users' sense of agency, even though
it acts autonomously. However, in high-risk settings, e.g., preparing a snack
for a child with allergies, they preferred robots that prioritized their
control significantly more. Additional contextual factors, such as trust in a
third party operator, also shaped their preferences.

</details>


### [13] [A Verification Methodology for Safety Assurance of Robotic Autonomous Systems](https://arxiv.org/abs/2506.19622)
*Mustafa Adam,David A. Anisi,Pedro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了一种用于农业自主机器人的安全验证工作流程，覆盖从概念设计到运行时验证的全生命周期，确保其在动态环境中的安全性和合规性。


<details>
  <summary>Details</summary>
Motivation: 农业自主机器人需要在动态、非结构化环境中安全运行并与人类互动，因此需要严格的安全保障以满足功能可靠性和法规要求。

Method: 通过系统性的危险分析和风险评估确定潜在风险，并开发安全控制器的形式化模型，验证其满足安全属性。

Result: 该方法在农业现场机器人上得到验证，能够有效识别设计问题并验证安全关键属性。

Conclusion: 该工作流程有助于开发更安全的自主机器人系统，并促进早期设计问题的发现。

Abstract: Autonomous robots deployed in shared human environments, such as agricultural
settings, require rigorous safety assurance to meet both functional reliability
and regulatory compliance. These systems must operate in dynamic, unstructured
environments, interact safely with humans, and respond effectively to a wide
range of potential hazards. This paper presents a verification workflow for the
safety assurance of an autonomous agricultural robot, covering the entire
development life-cycle, from concept study and design to runtime verification.
The outlined methodology begins with a systematic hazard analysis and risk
assessment to identify potential risks and derive corresponding safety
requirements. A formal model of the safety controller is then developed to
capture its behaviour and verify that the controller satisfies the specified
safety properties with respect to these requirements. The proposed approach is
demonstrated on a field robot operating in an agricultural setting. The results
show that the methodology can be effectively used to verify safety-critical
properties and facilitate the early identification of design issues,
contributing to the development of safer robots and autonomous systems.

</details>


### [14] [Scaffolding Dexterous Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.19212)
*Vincent de Bakker,Joey Hejna,Tyler Ga Wei Lum,Onur Celik,Aleksandar Taranovic,Denis Blessing,Gerhard Neumann,Jeannette Bohg,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 论文提出了一种利用视觉语言模型（VLM）生成粗轨迹指导强化学习（RL）策略的方法，用于训练灵巧机器人手完成复杂任务，无需人工演示或手工设计奖励函数。


<details>
  <summary>Details</summary>
Motivation: 灵巧机器人手的训练面临数据收集和高维控制的挑战，传统RL依赖任务特定奖励函数，限制了扩展性和泛化能力。

Method: 利用现成的VLM识别任务相关关键点并合成3D轨迹，训练低层RL策略跟踪这些粗轨迹。

Result: 在模拟任务中成功学习到鲁棒的灵巧操作策略，并能迁移到真实机器人手。

Conclusion: 该方法通过结合VLM和RL，实现了无需人工干预的灵巧操作训练，具有实际应用潜力。

Abstract: Dexterous robotic hands are essential for performing complex manipulation
tasks, yet remain difficult to train due to the challenges of demonstration
collection and high-dimensional control. While reinforcement learning (RL) can
alleviate the data bottleneck by generating experience in simulation, it
typically relies on carefully designed, task-specific reward functions, which
hinder scalability and generalization. Thus, contemporary works in dexterous
manipulation have often bootstrapped from reference trajectories. These
trajectories specify target hand poses that guide the exploration of RL
policies and object poses that enable dense, task-agnostic rewards. However,
sourcing suitable trajectories - particularly for dexterous hands - remains a
significant challenge. Yet, the precise details in explicit reference
trajectories are often unnecessary, as RL ultimately refines the motion. Our
key insight is that modern vision-language models (VLMs) already encode the
commonsense spatial and semantic knowledge needed to specify tasks and guide
exploration effectively. Given a task description (e.g., "open the cabinet")
and a visual scene, our method uses an off-the-shelf VLM to first identify
task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D
trajectories for hand motion and object motion. Subsequently, we train a
low-level residual RL policy in simulation to track these coarse trajectories
or "scaffolds" with high fidelity. Across a number of simulated tasks involving
articulated objects and semantic understanding, we demonstrate that our method
is able to learn robust dexterous manipulation policies. Moreover, we showcase
that our method transfers to real-world robotic hands without any human
demonstrations or handcrafted rewards.

</details>


### [15] [Estimating Spatially-Dependent GPS Errors Using a Swarm of Robots](https://arxiv.org/abs/2506.19712)
*Praneeth Somisetty,Robert Griffin,Victor M. Baez,Miguel F. Arevalo-Castiblanco,Aaron T. Becker,Jason M. O'Kane*

Main category: cs.RO

TL;DR: 论文提出了一种通过机器人团队估计静态空间变化GPS误差的方法，结合高斯过程回归和信息路径规划算法优化数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决因城市峡谷和干扰导致的GPS误差问题，提高定位精度。

Method: 使用状态偏差估计算法（SBE）和稀疏高斯过程信息路径规划（IPP）算法，通过机器人团队收集数据并优化路径。

Result: 在仿真中验证了SBE和IPP的有效性，相比开环策略，IPP能更高效地优化数据收集。

Conclusion: 提出的方法能有效估计和优化GPS误差，适用于复杂环境中的定位问题。

Abstract: External factors, including urban canyons and adversarial interference, can
lead to Global Positioning System (GPS) inaccuracies that vary as a function of
the position in the environment. This study addresses the challenge of
estimating a static, spatially-varying error function using a team of robots.
We introduce a State Bias Estimation Algorithm (SBE) whose purpose is to
estimate the GPS biases. The central idea is to use sensed estimates of the
range and bearing to the other robots in the team to estimate changes in bias
across the environment. A set of drones moves in a 2D environment, each
sampling data from GPS, range, and bearing sensors. The biases calculated by
the SBE at estimated positions are used to train a Gaussian Process Regression
(GPR) model. We use a Sparse Gaussian process-based Informative Path Planning
(IPP) algorithm that identifies high-value regions of the environment for data
collection. The swarm plans paths that maximize information gain in each
iteration, further refining their understanding of the environment's positional
bias landscape. We evaluated SBE and IPP in simulation and compared the IPP
methodology to an open-loop strategy.

</details>


### [16] [AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation](https://arxiv.org/abs/2506.19269)
*Ziyan Zhao,Ke Fan,He-Yang Xu,Ning Qiao,Bo Peng,Wenlong Gao,Dongjiang Li,Hui Shen*

Main category: cs.RO

TL;DR: AnchorDP3是一种用于双臂机器人操作的扩散策略框架，在高度随机化环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在高度随机化环境中机器人操作的挑战，减少对人类示范的依赖。

Method: 结合模拟器监督语义分割、任务条件特征编码器和基于关键姿态的扩散策略，简化预测空间并提升效率。

Result: 在RoboTwin基准测试中达到98.7%的平均成功率。

Conclusion: AnchorDP3框架有望实现完全自主的视觉运动策略生成，无需人类示范。

Abstract: We present AnchorDP3, a diffusion policy framework for dual-arm robotic
manipulation that achieves state-of-the-art performance in highly randomized
environments. AnchorDP3 integrates three key innovations: (1)
Simulator-Supervised Semantic Segmentation, using rendered ground truth to
explicitly segment task-critical objects within the point cloud, which provides
strong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight
modules processing augmented point clouds per task, enabling efficient
multi-task learning through a shared diffusion-based action expert; (3)
Affordance-Anchored Keypose Diffusion with Full State Supervision, replacing
dense trajectory prediction with sparse, geometrically meaningful action
anchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to
affordances, drastically simplifying the prediction space; the action expert is
forced to predict both robot joint angles and end-effector poses
simultaneously, which exploits geometric consistency to accelerate convergence
and boost accuracy. Trained on large-scale, procedurally generated simulation
data, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark
across diverse tasks under extreme randomization of objects, clutter, table
height, lighting, and backgrounds. This framework, when integrated with the
RoboTwin real-to-sim pipeline, has the potential to enable fully autonomous
generation of deployable visuomotor policies from only scene and instruction,
totally eliminating human demonstrations from learning manipulation skills.

</details>


### [17] [Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference](https://arxiv.org/abs/2506.19303)
*Zexiang Guo,Hengxiang Chen,Xinheng Mai,Qiusang Qiu,Gan Ma,Zhanat Kappassov,Qiang Li,Nutan Chen*

Main category: cs.RO

TL;DR: 提出了一种新颖的跨模态感知框架，结合视觉和触觉数据，通过多模态视觉语言模型提升机器人对物体物理属性的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖视觉或触觉数据，无法全面捕捉物体属性，限制了机器人操作的适应性和安全性。

Method: 采用分层特征对齐机制和改进的提示策略，构建多模态视觉语言模型，融合视觉和触觉数据。

Result: 在35种不同物体上测试，性能优于现有基线，并表现出强大的零样本泛化能力。

Conclusion: 该框架显著提升了机器人对物体物理属性的推理能力，为自适应抓取策略提供了有效支持。

Abstract: Inferring physical properties can significantly enhance robotic manipulation
by enabling robots to handle objects safely and efficiently through adaptive
grasping strategies. Previous approaches have typically relied on either
tactile or visual data, limiting their ability to fully capture properties. We
introduce a novel cross-modal perception framework that integrates visual
observations with tactile representations within a multimodal vision-language
model. Our physical reasoning framework, which employs a hierarchical feature
alignment mechanism and a refined prompting strategy, enables our model to make
property-specific predictions that strongly correlate with ground-truth
measurements. Evaluated on 35 diverse objects, our approach outperforms
existing baselines and demonstrates strong zero-shot generalization. Keywords:
tactile perception, visual-tactile fusion, physical property inference,
multimodal integration, robot perception

</details>


### [18] [A Survey on Soft Robot Adaptability: Implementations, Applications, and Prospects](https://arxiv.org/abs/2506.19397)
*Zixi Chen,Di Wu,Qinghua Guan,David Hardman,Federico Renda,Josie Hughes,Thomas George Thuruthel,Cosimo Della Santina,Barbara Mazzolai,Huichan Zhao,Cesare Stefanini*

Main category: cs.RO

TL;DR: 本文综述了软体机器人的适应性，将其分为外部和内部适应性，总结了提升适应性的设计、传感和控制策略，并探讨了其在手术、可穿戴设备等领域的应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 软体机器人因其高自由度、柔顺性和安全性等优势应用广泛，适应性是其关键特性，本文旨在全面理解适应性及其对应用的影响。

Method: 通过分类外部和内部适应性，总结设计、传感和控制策略，并分析其在多个领域的应用效果。

Result: 适应性对软体机器人在手术、可穿戴设备等领域的应用具有显著影响，但仍存在局限性。

Conclusion: 本文为软体机器人适应性的研究提供了全面视角，指出了未来研究方向和应用潜力。

Abstract: Soft robots, compared to rigid robots, possess inherent advantages, including
higher degrees of freedom, compliance, and enhanced safety, which have
contributed to their increasing application across various fields. Among these
benefits, adaptability is particularly noteworthy. In this paper, adaptability
in soft robots is categorized into external and internal adaptability. External
adaptability refers to the robot's ability to adjust, either passively or
actively, to variations in environments, object properties, geometries, and
task dynamics. Internal adaptability refers to the robot's ability to cope with
internal variations, such as manufacturing tolerances or material aging, and to
generalize control strategies across different robots. As the field of soft
robotics continues to evolve, the significance of adaptability has become
increasingly pronounced. In this review, we summarize various approaches to
enhancing the adaptability of soft robots, including design, sensing, and
control strategies. Additionally, we assess the impact of adaptability on
applications such as surgery, wearable devices, locomotion, and manipulation.
We also discuss the limitations of soft robotics adaptability and prospective
directions for future research. By analyzing adaptability through the lenses of
implementation, application, and challenges, this paper aims to provide a
comprehensive understanding of this essential characteristic in soft robotics
and its implications for diverse applications.

</details>


### [19] [Ground-Effect-Aware Modeling and Control for Multicopters](https://arxiv.org/abs/2506.19424)
*Tiankai Yang,Kaixin Chai,Jialin Ji,Yuze Wu,Chao Xu,Fei Gao*

Main category: cs.RO

TL;DR: 本文研究了多旋翼飞行器在近地飞行中的地面效应问题，提出了结合动态逆和扰动模型的控制方法，显著降低了控制误差。


<details>
  <summary>Details</summary>
Motivation: 地面效应对多旋翼飞行器的控制带来额外升力、振荡和气流干扰等挑战，需要建立数学模型并设计补偿方法。

Method: 通过力测量平台和实际飞行数据收集分析，建立地面效应下的外部扭矩数学模型，提出动态逆与扰动模型结合的控制方法。

Result: 实验表明，该方法将控制误差（RMSE）降低了45.3%。

Conclusion: 提出的方法有效补偿了地面效应的影响，确保了高低空飞行中的控制一致性。

Abstract: The ground effect on multicopters introduces several challenges, such as
control errors caused by additional lift, oscillations that may occur during
near-ground flight due to external torques, and the influence of ground airflow
on models such as the rotor drag and the mixing matrix. This article collects
and analyzes the dynamics data of near-ground multicopter flight through
various methods, including force measurement platforms and real-world flights.
For the first time, we summarize the mathematical model of the external torque
of multicopters under ground effect. The influence of ground airflow on rotor
drag and the mixing matrix is also verified through adequate experimentation
and analysis. Through simplification and derivation, the differential flatness
of the multicopter's dynamic model under ground effect is confirmed. To
mitigate the influence of these disturbance models on control, we propose a
control method that combines dynamic inverse and disturbance models, ensuring
consistent control effectiveness at both high and low altitudes. In this
method, the additional thrust and variations in rotor drag under ground effect
are both considered and compensated through feedforward models. The leveling
torque of ground effect can be equivalently represented as variations in the
center of gravity and the moment of inertia. In this way, the leveling torque
does not explicitly appear in the dynamic model. The final experimental results
show that the method proposed in this paper reduces the control error (RMSE) by
\textbf{45.3\%}. Please check the supplementary material at:
https://github.com/ZJU-FAST-Lab/Ground-effect-controller.

</details>


### [20] [T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.19498)
*Yiteng Chen,Wenbo Li,Shiyi Wang,Huiping Zhuang,Qingyao Wu*

Main category: cs.RO

TL;DR: 论文提出了一种任务自适应的空间表示提取框架T-Rex，动态选择最适合任务需求的表示方案，提升了机器人操作的空间理解、效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的机器人方法通常采用固定的空间表示提取方案，导致表示能力不足或提取时间过长。

Method: 提出T-Rex框架，根据任务复杂度动态选择空间表示的类型和粒度。

Result: 实验表明，该方法在空间理解、效率和稳定性方面具有显著优势，且无需额外训练。

Conclusion: T-Rex通过任务自适应的空间表示提取，有效提升了机器人操作的性能。

Abstract: Building a general robotic manipulation system capable of performing a wide
variety of tasks in real-world settings is a challenging task. Vision-Language
Models (VLMs) have demonstrated remarkable potential in robotic manipulation
tasks, primarily due to the extensive world knowledge they gain from
large-scale datasets. In this process, Spatial Representations (such as points
representing object positions or vectors representing object orientations) act
as a bridge between VLMs and real-world scene, effectively grounding the
reasoning abilities of VLMs and applying them to specific task scenarios.
However, existing VLM-based robotic approaches often adopt a fixed spatial
representation extraction scheme for various tasks, resulting in insufficient
representational capability or excessive extraction time. In this work, we
introduce T-Rex, a Task-Adaptive Framework for Spatial Representation
Extraction, which dynamically selects the most appropriate spatial
representation extraction scheme for each entity based on specific task
requirements. Our key insight is that task complexity determines the types and
granularity of spatial representations, and Stronger representational
capabilities are typically associated with Higher overall system operation
costs. Through comprehensive experiments in real-world robotic environments, we
show that our approach delivers significant advantages in spatial
understanding, efficiency, and stability without additional training.

</details>


### [21] [Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects](https://arxiv.org/abs/2506.19579)
*Federico Tavella,Kathryn Mearns,Angelo Cangelosi*

Main category: cs.RO

TL;DR: 比较了机器人场景理解中不同视觉语言模型（VLMs）的标题生成策略，评估了单视角与多视角、真实物体与3D打印物体的识别效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用视觉语言模型（VLMs）生成机器人场景的自然语言描述，以提升机器人对环境的理解能力。

Method: 通过机器人手臂配备的RGB相机采集多视角图像，比较BLIP和VLMs等模型的标题生成性能，评估对象识别准确性、完整性和描述自然性。

Result: VLMs在常见物体识别中表现良好，但对新颖表示泛化能力不足。

Conclusion: 研究为实际部署基础模型于机器人场景提供了实用见解。

Abstract: Robotic scene understanding increasingly relies on vision-language models
(VLMs) to generate natural language descriptions of the environment. In this
work, we present a comparative study of captioning strategies for tabletop
scenes captured by a robotic arm equipped with an RGB camera. The robot
collects images of objects from multiple viewpoints, and we evaluate several
models that generate scene descriptions. We compare the performance of various
captioning models, like BLIP and VLMs. Our experiments examine the trade-offs
between single-view and multi-view captioning, and difference between
recognising real-world and 3D printed objects. We quantitatively evaluate
object identification accuracy, completeness, and naturalness of the generated
captions. Results show that VLMs can be used in robotic settings where common
objects need to be recognised, but fail to generalise to novel representations.
Our findings provide practical insights into deploying foundation models for
embodied agents in real-world settings.

</details>


### [22] [Soft Robotic Delivery of Coiled Anchors for Cardiac Interventions](https://arxiv.org/abs/2506.19602)
*Leonardo Zamora Yanez,Jacob Rogatinsky,Dominic Recco,Sang-Yoep Lee,Grace Matthews,Andrew P. Sabelhaus,Tommaso Ranzani*

Main category: cs.RO

TL;DR: 论文介绍了一种用于心脏内锚定线圈植入的机器人平台，解决了现有导管平台在灵活性和力量应用上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前导管平台在复杂心脏内手术中缺乏灵活性、力量应用和适应性，限制了其在高风险患者中的应用。

Method: 开发了一种机器人平台，建立了其运动-静力学模型，并利用其被动适应性和高力量输出进行多锚定植入。

Result: 实验显示该平台具有低位置误差，并在模拟器中实现了毫米级精度的锚定植入。

Conclusion: 该机器人平台为复杂心脏内手术提供了更高的灵活性和精确性，有望改善高风险患者的治疗效果。

Abstract: Trans-catheter cardiac intervention has become an increasingly available
option for high-risk patients without the complications of open heart surgery.
However, current catheterbased platforms suffer from a lack of dexterity, force
application, and compliance required to perform complex intracardiac
procedures. An exemplary task that would significantly ease minimally invasive
intracardiac procedures is the implantation of anchor coils, which can be used
to fix and implant various devices in the beating heart. We introduce a robotic
platform capable of delivering anchor coils. We develop a kineto-statics model
of the robotic platform and demonstrate low positional error. We leverage the
passive compliance and high force output of the actuator in a multi-anchor
delivery procedure against a motile in-vitro simulator with millimeter level
accuracy.

</details>


### [23] [UniTac-NV: A Unified Tactile Representation For Non-Vision-Based Tactile Sensors](https://arxiv.org/abs/2506.19699)
*Jian Hou,Xin Zhou,Qihan Yang,Adam J. Spiers*

Main category: cs.RO

TL;DR: 提出了一种编码器-解码器架构，用于统一非视觉触觉传感器的数据，实现跨传感器数据转换和下游应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注光学触觉传感器的跨传感器转换，而忽略了非光学触觉传感器。本文旨在填补这一空白。

Method: 采用传感器特定编码器构建传感器无关的潜在空间，通过联合自编码器训练实现数据对齐。实验使用两种商业触觉传感器（Xela uSkin uSPa 46和Contactile PapillArray）进行验证。

Result: 模型在潜在空间中对齐数据，实现了低误差的跨传感器转换，并成功应用于接触几何估计任务。

Conclusion: 该方法为非光学触觉传感器的通用化算法提供了有效解决方案，展示了潜在空间对齐的实用价值。

Abstract: Generalizable algorithms for tactile sensing remain underexplored, primarily
due to the diversity of sensor modalities. Recently, many methods for
cross-sensor transfer between optical (vision-based) tactile sensors have been
investigated, yet little work focus on non-optical tactile sensors. To address
this gap, we propose an encoder-decoder architecture to unify tactile data
across non-vision-based sensors. By leveraging sensor-specific encoders, the
framework creates a latent space that is sensor-agnostic, enabling cross-sensor
data transfer with low errors and direct use in downstream applications. We
leverage this network to unify tactile data from two commercial tactile
sensors: the Xela uSkin uSPa 46 and the Contactile PapillArray. Both were
mounted on a UR5e robotic arm, performing force-controlled pressing sequences
against distinct object shapes (circular, square, and hexagonal prisms) and two
materials (rigid PLA and flexible TPU). Another more complex unseen object was
also included to investigate the model's generalization capabilities. We show
that alignment in latent space can be implicitly learned from joint autoencoder
training with matching contacts collected via different sensors. We further
demonstrate the practical utility of our approach through contact geometry
estimation, where downstream models trained on one sensor's latent
representation can be directly applied to another without retraining.

</details>


### [24] [The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication](https://arxiv.org/abs/2506.19781)
*Boyi Liu,Qianyi Zhang,Qiang Yang,Jianhao Jiao,Jagmohan Chauhan,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 论文介绍了Starlink Robot，首个配备Starlink卫星互联网的移动机器人平台，用于研究运动和环境遮挡下的卫星通信性能。


<details>
  <summary>Details</summary>
Motivation: 卫星通信在移动设备中的集成是一个重大转变，但其在运动和环境遮挡下的性能特性尚未充分理解。

Method: 开发了配备Starlink卫星互联网和多模态传感器（如摄像头、LiDAR和IMU）的移动机器人平台，收集同步通信指标、运动动态、天空可见性和3D环境数据。

Result: 生成了一个多模态数据集，涵盖稳态运动、变速和不同遮挡条件下的通信性能。

Conclusion: 该平台和数据集为开发运动感知通信协议、预测连接中断以及优化卫星通信提供了基础，适用于智能手机到自动驾驶车辆等新兴移动应用。

Abstract: The integration of satellite communication into mobile devices represents a
paradigm shift in connectivity, yet the performance characteristics under
motion and environmental occlusion remain poorly understood. We present the
Starlink Robot, the first mobile robotic platform equipped with Starlink
satellite internet, comprehensive sensor suite including upward-facing camera,
LiDAR, and IMU, designed to systematically study satellite communication
performance during movement. Our multi-modal dataset captures synchronized
communication metrics, motion dynamics, sky visibility, and 3D environmental
context across diverse scenarios including steady-state motion, variable
speeds, and different occlusion conditions. This platform and dataset enable
researchers to develop motion-aware communication protocols, predict
connectivity disruptions, and optimize satellite communication for emerging
mobile applications from smartphones to autonomous vehicles. The project is
available at https://github.com/StarlinkRobot.

</details>


### [25] [ReactEMG: Zero-Shot, Low-Latency Intent Detection via sEMG](https://arxiv.org/abs/2506.19815)
*Runsheng Wang,Xinyue Zhu,Ava Chen,Jingxi Xu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 提出了一种基于表面肌电信号（sEMG）的意图检测框架，通过分段策略和掩码建模实现快速意图识别，适用于康复和假肢领域。


<details>
  <summary>Details</summary>
Motivation: 解决现有sEMG系统在不同用户间快速、可靠识别意图且无需耗时校准的挑战。

Method: 采用分段策略实时标记意图，并引入掩码建模对齐肌肉激活与用户意图，实现快速检测和稳定跟踪。

Result: 在零样本迁移条件下，方法在准确性和稳定性上优于现有技术。

Conclusion: 该框架在可穿戴机器人和下一代假肢系统中具有应用潜力。

Abstract: Surface electromyography (sEMG) signals show promise for effective
human-computer interfaces, particularly in rehabilitation and prosthetics.
However, challenges remain in developing systems that respond quickly and
reliably to user intent, across different subjects and without requiring
time-consuming calibration. In this work, we propose a framework for EMG-based
intent detection that addresses these challenges. Unlike traditional gesture
recognition models that wait until a gesture is completed before classifying
it, our approach uses a segmentation strategy to assign intent labels at every
timestep as the gesture unfolds. We introduce a novel masked modeling strategy
that aligns muscle activations with their corresponding user intents, enabling
rapid onset detection and stable tracking of ongoing gestures. In evaluations
against baseline methods, considering both accuracy and stability for device
control, our approach surpasses state-of-the-art performance in zero-shot
transfer conditions, demonstrating its potential for wearable robotics and
next-generation prosthetic systems. Our project page is available at:
https://reactemg.github.io

</details>


### [26] [CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation](https://arxiv.org/abs/2506.19816)
*Hao Li,Shuai Yang,Yilun Chen,Yang Tian,Xiaoda Yang,Xinyi Chen,Hanqing Wang,Tai Wang,Feng Zhao,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: CronusVLA扩展了单帧视觉语言动作模型至多帧范式，通过高效的后训练阶段提升性能，减少计算冗余。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作模型受限于单帧观察，无法充分利用多帧历史信息，计算成本高。

Method: 1. 单帧预训练；2. 多帧编码；3. 跨帧解码。通过特征分块和动作适应机制优化。

Result: 在SimperEnv上成功率70.9%，LIBERO上比OpenVLA提升12.7%，真实机器人实验表现优异。

Conclusion: CronusVLA通过多帧处理和高效推理，显著提升了性能与鲁棒性。

Abstract: Recent vision-language-action (VLA) models built on pretrained
vision-language models (VLMs) have demonstrated strong generalization across
manipulation tasks. However, they remain constrained by a single-frame
observation paradigm and cannot fully benefit from the motion information
offered by aggregated multi-frame historical observations, as the large
vision-language backbone introduces substantial computational cost and
inference latency. We propose CronusVLA, a unified framework that extends
single-frame VLA models to the multi-frame paradigm through an efficient
post-training stage. CronusVLA comprises three key components: (1) single-frame
pretraining on large-scale embodied datasets with autoregressive action tokens
prediction, which establishes an embodied vision-language foundation; (2)
multi-frame encoding, adapting the prediction of vision-language backbones from
discrete action tokens to motion features during post-training, and aggregating
motion features from historical frames into a feature chunking; (3) cross-frame
decoding, which maps the feature chunking to accurate actions via a shared
decoder with cross-attention. By reducing redundant token computation and
caching past motion features, CronusVLA achieves efficient inference. As an
application of motion features, we further propose an action adaptation
mechanism based on feature-action retrieval to improve model performance during
finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with
70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world
Franka experiments also show the strong performance and robustness.

</details>


### [27] [Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital Maps for GNSS-Challenged Environments](https://arxiv.org/abs/2506.19827)
*Ola Elmaghraby,Eslam Mounier,Paulo Ricardo Marques de Araujo,Aboelmagd Noureldin*

Main category: cs.RO

TL;DR: 论文提出了一种低成本、基于视觉的多传感器导航系统，用于GNSS受限环境中的车辆定位，结合单目深度估计、语义过滤和视觉地图注册，实现了高精度定位。


<details>
  <summary>Details</summary>
Motivation: 在GNSS受限环境（如室内停车场或密集城市峡谷）中，实现准确且鲁棒的车辆定位是一个重大挑战。

Method: 提出了一种结合单目深度估计、语义过滤和视觉地图注册（VMR）的多传感器导航系统，并与3D数字地图集成。

Result: 在真实驾驶场景中测试，室内定位精度达到92%的亚米级，室外超过80%，水平定位和航向平均均方根误差分别为0.98米和1.25度。与基线相比，定位精度平均提高了88%。

Conclusion: 该研究表明低成本单目视觉系统结合3D地图在陆地车辆导航中具有潜力，可实现不依赖GNSS的可扩展导航。

Abstract: In Global Navigation Satellite System (GNSS)-denied environments such as
indoor parking structures or dense urban canyons, achieving accurate and robust
vehicle positioning remains a significant challenge. This paper proposes a
cost-effective, vision-based multi-sensor navigation system that integrates
monocular depth estimation, semantic filtering, and visual map registration
(VMR) with 3-D digital maps. Extensive testing in real-world indoor and outdoor
driving scenarios demonstrates the effectiveness of the proposed system,
achieving sub-meter accuracy of 92% indoors and more than 80% outdoors, with
consistent horizontal positioning and heading average root mean-square errors
of approximately 0.98 m and 1.25 {\deg}, respectively. Compared to the
baselines examined, the proposed solution significantly reduced drift and
improved robustness under various conditions, achieving positioning accuracy
improvements of approximately 88% on average. This work highlights the
potential of cost-effective monocular vision systems combined with 3D maps for
scalable, GNSS-independent navigation in land vehicles.

</details>


### [28] [ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model](https://arxiv.org/abs/2506.19842)
*Tengbo Yu,Guanxing Lu,Zaijia Yang,Haoyuan Deng,Season Si Chen,Jiwen Lu,Wenbo Ding,Guoqiang Hu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: ManiGaussian++扩展了ManiGaussian框架，通过分层高斯世界模型改进多任务双手机器人操作，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作任务复杂，现有方法ManiGaussian忽略多体交互，导致性能下降。

Method: 提出分层高斯世界模型，通过任务导向的高斯泼溅和领导者-跟随者架构建模多体时空动态。

Result: 在10个模拟任务中性能提升20.2%，在9个真实任务中平均成功率60%。

Conclusion: ManiGaussian++有效解决了双手机器人操作中的多体动态建模问题，性能显著优于现有技术。

Abstract: Multi-task robotic bimanual manipulation is becoming increasingly popular as
it enables sophisticated tasks that require diverse dual-arm collaboration
patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to
understanding the multi-body spatiotemporal dynamics. An existing method
ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual
representation via Gaussian world model for single-arm settings, which ignores
the interaction of multiple embodiments for dual-arm systems with significant
performance drop. In this paper, we propose ManiGaussian++, an extension of
ManiGaussian framework that improves multi-task bimanual manipulation by
digesting multi-body scene dynamics through a hierarchical Gaussian world
model. To be specific, we first generate task-oriented Gaussian Splatting from
intermediate visual features, which aims to differentiate acting and
stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build
a hierarchical Gaussian world model with the leader-follower architecture,
where the multi-body spatiotemporal dynamics is mined for intermediate visual
representation via future scene prediction. The leader predicts Gaussian
Splatting deformation caused by motions of the stabilizing arm, through which
the follower generates the physical consequences resulted from the movement of
the acting arm. As a result, our method significantly outperforms the current
state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in
10 simulated tasks, and achieves 60% success rate on average in 9 challenging
real-world tasks. Our code is available at
https://github.com/April-Yz/ManiGaussian_Bimanual.

</details>
