{"id": "2508.18397", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18397", "abs": "https://arxiv.org/abs/2508.18397", "authors": ["Antonio Guillen-Perez"], "title": "Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning", "comment": null, "summary": "Offline Reinforcement Learning (RL) presents a promising paradigm for\ntraining autonomous vehicle (AV) planning policies from large-scale, real-world\ndriving logs. However, the extreme data imbalance in these logs, where mundane\nscenarios vastly outnumber rare \"long-tail\" events, leads to brittle and unsafe\npolicies when using standard uniform data sampling. In this work, we address\nthis challenge through a systematic, large-scale comparative study of data\ncuration strategies designed to focus the learning process on information-rich\nsamples. We investigate six distinct criticality weighting schemes which are\ncategorized into three families: heuristic-based, uncertainty-based, and\nbehavior-based. These are evaluated at two temporal scales, the individual\ntimestep and the complete scenario. We train seven goal-conditioned\nConservative Q-Learning (CQL) agents with a state-of-the-art, attention-based\narchitecture and evaluate them in the high-fidelity Waymax simulator. Our\nresults demonstrate that all data curation methods significantly outperform the\nbaseline. Notably, data-driven curation using model uncertainty as a signal\nachieves the most significant safety improvements, reducing the collision rate\nby nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear\ntrade-off where timestep-level weighting excels at reactive safety while\nscenario-level weighting improves long-horizon planning. Our work provides a\ncomprehensive framework for data curation in Offline RL and underscores that\nintelligent, non-uniform sampling is a critical component for building safe and\nreliable autonomous agents.", "AI": {"tldr": "本文通过系统比较六种数据筛选策略，解决了离线强化学习中数据不平衡问题，发现基于模型不确定性的方法能显著提升自动驾驶安全性，碰撞率降低近三倍。", "motivation": "自动驾驶离线强化学习面临数据极度不平衡的挑战，普通驾驶场景远多于罕见的长尾事件，导致标准均匀采样训练出的策略脆弱且不安全。", "method": "研究六种关键性加权方案（启发式、不确定性、行为式三类），在两个时间尺度（单时间步和完整场景）评估，使用目标条件CQL和注意力架构训练七个代理，在Waymax模拟器中测试。", "result": "所有数据筛选方法均显著优于基线，基于模型不确定性的方法安全性提升最显著，碰撞率从16.0%降至5.5%。时间步级加权擅长反应性安全，场景级加权改善长期规划。", "conclusion": "智能非均匀采样是构建安全可靠自动驾驶代理的关键组件，为离线RL数据筛选提供了全面框架。"}}
{"id": "2508.18399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18399", "abs": "https://arxiv.org/abs/2508.18399", "authors": ["Christian Friedrich", "Ralf Gulde", "Armin Lechler", "Alexander Verl"], "title": "Maintenance automation: methods for robotics manipulation planning and execution", "comment": "11 pages, 12 figures", "summary": "Automating complex tasks using robotic systems requires skills for planning,\ncontrol and execution. This paper proposes a complete robotic system for\nmaintenance automation, which can automate disassembly and assembly operations\nunder environmental uncertainties (e.g. deviations between prior plan\ninformation). The cognition of the robotic system is based on a planning\napproach (using CAD and RGBD data) and includes a method to interpret a\nsymbolic plan and transform it to a set of executable robot instructions. The\ncomplete system is experimentally evaluated using real-world applications. This\nwork shows the first step to transfer these theoretical results into a\npractical robotic solution.", "AI": {"tldr": "提出基于CAD和RGBD数据的机器人维护自动化系统，能够在环境不确定性下执行拆卸和组装操作", "motivation": "自动化复杂维护任务需要解决规划、控制和执行的集成问题，特别是在存在环境偏差等不确定性的情况下", "method": "采用基于CAD和RGBD数据的规划方法，将符号化计划转换为可执行的机器人指令", "result": "通过真实应用场景进行实验评估，验证了系统的可行性", "conclusion": "这是将理论成果转化为实际机器人解决方案的重要第一步"}}
{"id": "2508.18400", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18400", "abs": "https://arxiv.org/abs/2508.18400", "authors": ["Christian Friedrich", "Akos Csiszar", "Armin Lechler", "Alexander Verl"], "title": "Efficient task and path planning for maintenance automation using a robot system", "comment": "10 pages, 10 figures", "summary": "The research and development of intelligent automation solutions is a\nground-breaking point for the factory of the future. A promising and\nchallenging mission is the use of autonomous robot systems to automate tasks in\nthe field of maintenance. For this purpose, the robot system must be able to\nplan autonomously the different manipulation tasks and the corresponding paths.\nBasic requirements are the development of algorithms with a low computational\ncomplexity and the possibility to deal with environmental uncertainties. In\nthis work, an approach is presented, which is especially suited to solve the\nproblem of maintenance automation. For this purpose, offline data from CAD is\ncombined with online data from an RGBD vision system via a probabilistic\nfilter, to compensate uncertainties from offline data. For planning the\ndifferent tasks, a method is explained, which use a symbolic description,\nfounded on a novel sampling-based method to compute the disassembly space. For\npath planning we use global state-of-the art algorithms with a method that\nallows the adaption of the exploration stepsize in order to reduce the planning\ntime. Every method is experimentally validated and discussed.", "AI": {"tldr": "本文提出了一种用于维护自动化的自主机器人系统方案，结合CAD离线数据和RGBD视觉系统在线数据，通过概率滤波器补偿不确定性，并使用符号描述和采样基方法进行任务规划和路径规划。", "motivation": "工厂未来的智能自动化需要自主机器人系统来自动化维护任务，这需要算法具有低计算复杂度并能处理环境不确定性。", "method": "结合CAD离线数据和RGBD视觉系统在线数据，使用概率滤波器补偿不确定性；采用符号描述和新题采样基方法计算解体空间进行任务规划；使用全局先进路径规划算法并通过调整探索步长来减少规划时间。", "result": "所有方法都通过实验进行了验证和讨论，证明了方案的可行性和效果。", "conclusion": "该方法特别适合解决维护自动化问题，能够有效处理环境不确定性并提高规划效率。"}}
{"id": "2508.18443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18443", "abs": "https://arxiv.org/abs/2508.18443", "authors": ["Ruohan Zhang", "Uksang Yoo", "Yichen Li", "Arpit Argawal", "Wenzhen Yuan"], "title": "PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing", "comment": "16 pages, 12 figures, International Journal of Robotics Research\n  (accepted), 2025", "summary": "Soft pneumatic robot manipulators are popular in industrial and\nhuman-interactive applications due to their compliance and flexibility.\nHowever, deploying them in real-world scenarios requires advanced sensing for\ntactile feedback and proprioception. Our work presents a novel vision-based\napproach for sensorizing soft robots. We demonstrate our approach on\nPneuGelSight, a pioneering pneumatic manipulator featuring high-resolution\nproprioception and tactile sensing via an embedded camera. To optimize the\nsensor's performance, we introduce a comprehensive pipeline that accurately\nsimulates its optical and dynamic properties, facilitating a zero-shot\nknowledge transition from simulation to real-world applications. PneuGelSight\nand our sim-to-real pipeline provide a novel, easily implementable, and robust\nsensing methodology for soft robots, paving the way for the development of more\nadvanced soft robots with enhanced sensory capabilities.", "AI": {"tldr": "这篇论文提出了一种新题的视觉基于感知方法PneuGelSight，通过嵌入摄像头实现软体机器人的高分辨率体感和触觉感知，并使用模拟到实际的经验迁移流程优化性能。", "motivation": "软体气动机器人虽然具有优秀的程度和灵活性，但在实际应用中需要充分的触觉反馈和体感能力。需要一种简单易行、稳健的感知方案来推动软体机器人的实际部署。", "method": "设计了PneuGelSight软体机器人，通过嵌入摄像头实现高分辨率体感和触觉感知。开发了一个综合性的模拟到实际迁移流程，准确模拟传感器的光学和动力学特性，支持零样本知识迁移。", "result": "PneuGelSight成功实现了高分辨率的体感和触觉感知能力，模拟到实际迁移流程有效优化了传感器性能，无需实际调试即可直接应用于实际场景。", "conclusion": "该研究提供了一种新题、简单易行且稳健的软体机器人感知方案，为开发具有更强感官能力的进阶软体机器人掌平了道路。"}}
{"id": "2508.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18460", "abs": "https://arxiv.org/abs/2508.18460", "authors": ["Tianze Liu", "Md Abu Bakr Siddique", "Hongyu An"], "title": "Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models", "comment": null, "summary": "Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable\nprowess across various cognitive tasks using extensive training data. However,\nthe reliance on large datasets and neural networks presents challenges such as\nhighpower consumption and limited adaptability, particularly in\nSWaP-constrained applications like planetary exploration. To address these\nissues, we propose enhancing the autonomous capabilities of intelligent robots\nby emulating the associative learning observed in animals. Associative learning\nenables animals to adapt to their environment by memorizing concurrent events.\nBy replicating this mechanism, neuromorphic robots can navigate dynamic\nenvironments autonomously, learning from interactions to optimize performance.\nThis paper explores the emulation of associative learning in rodents using\nneuromorphic robots within open-field maze environments, leveraging insights\nfrom spatial cells such as place and grid cells. By integrating these models,\nwe aim to enable online associative learning for spatial tasks in real-time\nscenarios, bridging the gap between biological spatial cognition and robotics\nfor advancements in autonomous systems.", "AI": {"tldr": "本文提出通过模拟动物联想学习机制来增强神经形态机器人的自主能力，以解决传统AI方法在功耗和适应性方面的限制", "motivation": "传统数据驱动的AI方法依赖大数据集和神经网络，存在高功耗和适应性有限的问题，特别是在SWaP受限的应用中如行星探索", "method": "模拟啮齿类动物的联想学习机制，在开放场地迷宫环境中使用神经形态机器人，整合位置细胞和网格细胞等空间细胞模型", "result": "实现了在线联想学习机制，使机器人能够在动态环境中自主导航并通过交互学习优化性能", "conclusion": "该方法成功将生物空间认知与机器人技术相结合，为自主系统的进步提供了新途径，特别是在资源受限的环境中"}}
{"id": "2508.18694", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18694", "abs": "https://arxiv.org/abs/2508.18694", "authors": ["Jaehwan Jeong", "Tuan-Anh Vu", "Mohammad Jony", "Shahab Ahmad", "Md. Mukhlesur Rahman", "Sangpil Kim", "M. Khalid Jawed"], "title": "AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot", "comment": null, "summary": "Existing datasets for precision agriculture have primarily been collected in\nstatic or controlled environments such as indoor labs or greenhouses, often\nwith limited sensor diversity and restricted temporal span. These conditions\nfail to reflect the dynamic nature of real farmland, including illumination\nchanges, crop growth variation, and natural disturbances. As a result, models\ntrained on such data often lack robustness and generalization when applied to\nreal-world field scenarios. In this paper, we present AgriChrono, a novel\nrobotic data collection platform and multi-modal dataset designed to capture\nthe dynamic conditions of real-world agricultural environments. Our platform\nintegrates multiple sensors and enables remote, time-synchronized acquisition\nof RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable\nlong-term data collection across varying illumination and crop growth stages.\nWe benchmark a range of state-of-the-art 3D reconstruction models on the\nAgriChrono dataset, highlighting the difficulty of reconstruction in real-world\nfield environments and demonstrating its value as a research asset for\nadvancing model generalization under dynamic conditions. The code and dataset\nare publicly available at: https://github.com/StructuresComp/agri-chrono", "AI": {"tldr": "AgriChrono是一个新颖的机器人数据收集平台和多模态数据集，专门设计用于捕捉真实农业环境的动态条件，解决了现有数据集在静态环境中收集、传感器多样性有限和缺乏时间跨度的问题。", "motivation": "现有精准农业数据集主要在静态或受控环境中收集，无法反映真实农田的动态特性（如光照变化、作物生长变化和自然干扰），导致训练出的模型在真实场景中缺乏鲁棒性和泛化能力。", "method": "开发了一个集成多传感器的机器人数据收集平台，支持远程、时间同步采集RGB、深度、LiDAR和IMU数据，能够在不同光照和作物生长阶段进行高效、可重复的长期数据收集。", "result": "在AgriChrono数据集上对多种最先进的3D重建模型进行了基准测试，证明了在真实农田环境中进行重建的困难性，并展示了该数据集作为研究资产的价值。", "conclusion": "AgriChrono数据集和平台为在动态条件下推进模型泛化研究提供了重要资源，代码和数据集已公开可用。"}}
{"id": "2508.18606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18606", "abs": "https://arxiv.org/abs/2508.18606", "authors": ["Nicky Zimmerman", "Joel Loo", "Ayush Agrawal", "David Hsu"], "title": "SignLoc: Robust Localization using Navigation Signs and Public Maps", "comment": "Under submission for Robotics and Automation Letters (RA-L)", "summary": "Navigation signs and maps, such as floor plans and street maps, are widely\navailable and serve as ubiquitous aids for way-finding in human environments.\nYet, they are rarely used by robot systems. This paper presents SignLoc, a\nglobal localization method that leverages navigation signs to localize the\nrobot on publicly available maps -- specifically floor plans and OpenStreetMap\n(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a\nnavigation graph from the input map. It then employs a probabilistic\nobservation model to match directional and locational cues from the detected\nsigns to the graph, enabling robust topo-semantic localization within a Monte\nCarlo framework. We evaluated SignLoc in diverse large-scale environments: part\nof a university campus, a shopping mall, and a hospital complex. Experimental\nresults show that SignLoc reliably localizes the robot after observing only one\nto two signs.", "AI": {"tldr": "SignLoc是一种利用导航标志进行全局定位的方法，无需先验传感器建图，仅需观察1-2个标志即可在公开地图上实现鲁棒的拓扑语义定位", "motivation": "导航标志和地图（如平面图和街道地图）在人类环境中广泛可用，但很少被机器人系统使用。本文旨在利用这些现成的导航标志来实现机器人的全局定位", "method": "首先从输入地图中提取导航图，然后使用概率观测模型将检测到的标志的方向和位置线索与图进行匹配，在蒙特卡洛框架内实现鲁棒的拓扑语义定位", "result": "在多种大规模环境（大学校园、购物中心、医院综合体）中进行评估，实验结果显示SignLoc在仅观察1-2个标志后就能可靠地定位机器人", "conclusion": "SignLoc方法成功证明了利用现成导航标志和公开地图进行机器人全局定位的可行性，为机器人导航提供了一种新颖且实用的解决方案"}}
{"id": "2508.19164", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19164", "abs": "https://arxiv.org/abs/2508.19164", "authors": ["Morokot Sakal", "George Nehma", "Camilo Riano-Rios", "Madhur Tiwari"], "title": "Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform", "comment": "15 pages, 10 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite\nattitude control system with reaction wheel health estimation capabilities.\nPrevious simulations and Software-in-the-Loop testing have prompted further\nexperiments to explore the validity of the controller with real momentum\nexchange devices in the loop. This work is a step toward a comprehensive\ntesting framework for validation of spacecraft attitude control algorithms. The\nproposed HIL testbed includes brushless DC motors and drivers that communicate\nusing a CAN bus, an embedded computer that executes control and adaptation\nlaws, and a satellite simulator that produces simulated sensor data, estimated\nattitude states, and responds to actions of the external actuators. We propose\nmethods to artificially induce failures on the reaction wheels, and present\nrelated issues and lessons learned.", "AI": {"tldr": "确定卫星姿态控制系统的硬件在环测试方案，包括反应轮健康估计和故障模拟方法", "motivation": "之前的仿真和软件在环测试推动了进一步实验，以验证控制器在真实动量交换设备环境下的有效性", "method": "构建硬件在环测试平台，包含CAN总线通信的无刷直流电机、执行控制算法的嵌入式计算机、以及生成仿真传感器数据的卫星模拟器", "result": "提出了人工引发反应轮故障的方法，并总结了相关问题和经验教训", "conclusion": "这项工作是建立完整太空船姿态控制算法验证框架的重要步骤"}}
{"id": "2508.18627", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18627", "abs": "https://arxiv.org/abs/2508.18627", "authors": ["Ziyuan Jiao", "Yida Niu", "Zeyu Zhang", "Yangyang Wu", "Yao Su", "Yixin Zhu", "Hangxin Liu", "Song-Chun Zhu"], "title": "Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning", "comment": "20 pages, 13 figures; accepted by Transactions on Robotics", "summary": "We present a Sequential Mobile Manipulation Planning (SMMP) framework that\ncan solve long-horizon multi-step mobile manipulation tasks with coordinated\nwhole-body motion, even when interacting with articulated objects. By\nabstracting environmental structures as kinematic models and integrating them\nwith the robot's kinematics, we construct an Augmented Configuration Apace\n(A-Space) that unifies the previously separate task constraints for navigation\nand manipulation, while accounting for the joint reachability of the robot\nbase, arm, and manipulated objects. This integration facilitates efficient\nplanning within a tri-level framework: a task planner generates symbolic action\nsequences to model the evolution of A-Space, an optimization-based motion\nplanner computes continuous trajectories within A-Space to achieve desired\nconfigurations for both the robot and scene elements, and an intermediate plan\nrefinement stage selects action goals that ensure long-horizon feasibility. Our\nsimulation studies first confirm that planning in A-Space achieves an 84.6\\%\nhigher task success rate compared to baseline methods. Validation on real\nrobotic systems demonstrates fluid mobile manipulation involving (i) seven\ntypes of rigid and articulated objects across 17 distinct contexts, and (ii)\nlong-horizon tasks of up to 14 sequential steps. Our results highlight the\nsignificance of modeling scene kinematics into planning entities, rather than\nencoding task-specific constraints, offering a scalable and generalizable\napproach to complex robotic manipulation.", "AI": {"tldr": "提出了一种序列移动操作规划框架(SMMP)，通过构建增强配置空间(A-Space)来统一导航和操作任务约束，实现长期望多步骤移动操作任务的高效规划。", "motivation": "解决长期望多步骤移动操作任务中导航和操作约束分离的问题，提高机器人在处理关节式物体时的整体运动协调性和任务成功率。", "method": "构建增强配置空间(A-Space)，将环境结构抽象为运动学模型并与机器人运动学集成，采用三层框架：任务规划器生成符号动作序列，优化基于运动规划器计算连续轨迹，中间规划精细阶段选择保证长期望可行性的动作目标。", "result": "模拟实验显示在A-Space中规划的任务成功率比基线方法提高84.6%，真实机器人系统验证了在17个不同场景中处理7种类型的刚体和关节式物体，完成了达到14个序列步骤的长期望任务。", "conclusion": "将场景运动学模型化为规划实体，而非编码任务特定约束，提供了一种可扩展和通用的处理复杂机器人操作任务的方法。"}}
{"id": "2508.18662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18662", "abs": "https://arxiv.org/abs/2508.18662", "authors": ["Stefan Ramdhan", "Winnie Trandinh", "Istvan David", "Vera Pantelic", "Mark Lawford"], "title": "Engineering Automotive Digital Twins on Standardized Architectures: A Case Study", "comment": "7 pages, 6 figures. Submitted to EDTconf 2025", "summary": "Digital twin (DT) technology has become of interest in the automotive\nindustry. There is a growing need for smarter services that utilize the unique\ncapabilities of DTs, ranging from computer-aided remote control to cloud-based\nfleet coordination. Developing such services starts with the software\narchitecture. However, the scarcity of DT architectural guidelines poses a\nchallenge for engineering automotive DTs. Currently, the only DT architectural\nstandard is the one defined in ISO 23247. Though not developed for automotive\nsystems, it is one of the few feasible starting points for automotive DTs. In\nthis work, we investigate the suitability of the ISO 23247 reference\narchitecture for developing automotive DTs. Through the case study of\ndeveloping an Adaptive Cruise Control DT for a 1/10\\textsuperscript{th}-scale\nautonomous vehicle, we identify some strengths and limitations of the reference\narchitecture and begin distilling future directions for researchers,\npractitioners, and standard developers.", "AI": {"tldr": "本研究评估ISO 23247数字孪生参考架构在汽车领域的适用性，通过自适应巡航控制数字孪生案例研究，识别该架构的优势与局限", "motivation": "汽车行业对数字孪生技术需求增长，但缺乏专门的架构指导。ISO 23247是当前唯一的数字孪生架构标准，虽非针对汽车系统开发，却是可行的起点", "method": "通过为1/10比例自动驾驶车辆开发自适应巡航控制数字孪生的案例研究，分析ISO 23247参考架构的适用性", "result": "识别了该参考架构在汽车数字孪生开发中的优势与局限性", "conclusion": "为研究人员、从业者和标准制定者提炼了未来发展方向，指出了汽车数字孪生架构需要改进的方向"}}
{"id": "2508.18691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18691", "abs": "https://arxiv.org/abs/2508.18691", "authors": ["Himanshu Gaurav Singh", "Pieter Abbeel", "Jitendra Malik", "Antonio Loquercio"], "title": "Deep Sensorimotor Control by Imitating Predictive Models of Human Motion", "comment": "Blog Post: https://hgaurav2k.github.io/trackr/", "summary": "As the embodiment gap between a robot and a human narrows, new opportunities\narise to leverage datasets of humans interacting with their surroundings for\nrobot learning. We propose a novel technique for training sensorimotor policies\nwith reinforcement learning by imitating predictive models of human motions.\nOur key insight is that the motion of keypoints on human-inspired robot\nend-effectors closely mirrors the motion of corresponding human body keypoints.\nThis enables us to use a model trained to predict future motion on human data\n\\emph{zero-shot} on robot data. We train sensorimotor policies to track the\npredictions of such a model, conditioned on a history of past robot states,\nwhile optimizing a relatively sparse task reward. This approach entirely\nbypasses gradient-based kinematic retargeting and adversarial losses, which\nlimit existing methods from fully leveraging the scale and diversity of modern\nhuman-scene interaction datasets. Empirically, we find that our approach can\nwork across robots and tasks, outperforming existing baselines by a large\nmargin. In addition, we find that tracking a human motion model can substitute\nfor carefully designed dense rewards and curricula in manipulation tasks. Code,\ndata and qualitative results available at\nhttps://jirl-upenn.github.io/track_reward/.", "AI": {"tldr": "提出一种通过模仿人类运动预测模型来训练机器人传感器运动策略的新方法，无需梯度运动重定向或对抗损失，直接在机器人上零样本应用人类数据训练的模型。", "motivation": "随着机器人与人类之间的体现差距缩小，可以利用人类与环境交互的大规模数据集进行机器人学习，但现有方法受限于运动重定向和对抗损失，无法充分利用人类数据的规模和多样性。", "method": "利用人体关键点与机器人末端执行器关键点运动的相似性，使用人类数据训练的预测模型在机器人数据上零样本应用，训练传感器运动策略来跟踪模型预测，同时优化稀疏任务奖励。", "result": "该方法在不同机器人和任务上都能有效工作，大幅超越现有基线方法，并且能够替代精心设计的密集奖励和课程学习。", "conclusion": "通过直接利用人类运动预测模型，可以绕过传统限制，充分利用大规模人类交互数据集，为机器人学习提供新的有效途径。"}}
{"id": "2508.18705", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18705", "abs": "https://arxiv.org/abs/2508.18705", "authors": ["Santosh Thoduka", "Sebastian Houben", "Juergen Gall", "Paul G. Plöger"], "title": "Enhancing Video-Based Robot Failure Detection Using Task Knowledge", "comment": "Accepted at ECMR 2025", "summary": "Robust robotic task execution hinges on the reliable detection of execution\nfailures in order to trigger safe operation modes, recovery strategies, or task\nreplanning. However, many failure detection methods struggle to provide\nmeaningful performance when applied to a variety of real-world scenarios. In\nthis paper, we propose a video-based failure detection approach that uses\nspatio-temporal knowledge in the form of the actions the robot performs and\ntask-relevant objects within the field of view. Both pieces of information are\navailable in most robotic scenarios and can thus be readily obtained. We\ndemonstrate the effectiveness of our approach on three datasets that we amend,\nin part, with additional annotations of the aforementioned task-relevant\nknowledge. In light of the results, we also propose a data augmentation method\nthat improves performance by applying variable frame rates to different parts\nof the video. We observe an improvement from 77.9 to 80.0 in F1 score on the\nARMBench dataset without additional computational expense and an additional\nincrease to 81.4 with test-time augmentation. The results emphasize the\nimportance of spatio-temporal information during failure detection and suggest\nfurther investigation of suitable heuristics in future implementations. Code\nand annotations are available.", "AI": {"tldr": "提出了一种基于视频的机器人执行失败检测方法，利用动作信息和任务相关物体的时空知识，通过数据增强技术提升检测性能", "motivation": "现有失败检测方法在真实场景中性能有限，需要更可靠的检测机制来触发安全操作模式和恢复策略", "method": "使用机器人执行的动作和视野中任务相关物体的时空信息，提出可变帧率的数据增强方法", "result": "在ARMBench数据集上F1分数从77.9提升到80.0，测试时增强后达到81.4，无需额外计算开销", "conclusion": "时空信息对失败检测至关重要，未来应进一步研究合适的启发式方法"}}
{"id": "2508.18802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18802", "abs": "https://arxiv.org/abs/2508.18802", "authors": ["Li Sun", "Jiefeng Wu", "Feng Chen", "Ruizhe Liu", "Yanchao Yang"], "title": "HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation", "comment": null, "summary": "Effective policy learning for robotic manipulation requires scene\nrepresentations that selectively capture task-relevant environmental features.\nCurrent approaches typically employ task-agnostic representation extraction,\nfailing to emulate the dynamic perceptual adaptation observed in human\ncognition. We present HyperTASR, a hypernetwork-driven framework that modulates\nscene representations based on both task objectives and the execution phase.\nOur architecture dynamically generates representation transformation parameters\nconditioned on task specifications and progression state, enabling\nrepresentations to evolve contextually throughout task execution. This approach\nmaintains architectural compatibility with existing policy learning frameworks\nwhile fundamentally reconfiguring how visual features are processed. Unlike\nmethods that simply concatenate or fuse task embeddings with task-agnostic\nrepresentations, HyperTASR establishes computational separation between\ntask-contextual and state-dependent processing paths, enhancing learning\nefficiency and representational quality. Comprehensive evaluations in both\nsimulation and real-world environments demonstrate substantial performance\nimprovements across different representation paradigms. Through ablation\nstudies and attention visualization, we confirm that our approach selectively\nprioritizes task-relevant scene information, closely mirroring human adaptive\nperception during manipulation tasks. The project website is at\n\\href{https://lisunphil.github.io/HyperTASR_projectpage/}{lisunphil.github.io/HyperTASR\\_projectpage}.", "AI": {"tldr": "HyperTASR是一个超网络驱动的框架，通过任务目标和执行阶段动态调制场景表示，提升机器人操作任务中的策略学习效果。", "motivation": "当前方法使用任务无关的表示提取，无法模拟人类认知中的动态感知适应，需要能够根据任务上下文选择性捕捉相关环境特征的表示方法。", "method": "采用超网络架构，根据任务规范和进展状态动态生成表示变换参数，使表示在任务执行过程中上下文演化，同时保持与现有策略学习框架的架构兼容性。", "result": "在仿真和真实环境中的综合评估显示，该方法在不同表示范式中均取得显著性能提升，通过消融研究和注意力可视化证实了其选择性关注任务相关信息的能力。", "conclusion": "HyperTASR通过计算分离任务上下文和状态依赖处理路径，提高了学习效率和表示质量，更接近人类在操作任务中的自适应感知模式。"}}
{"id": "2508.18817", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18817", "abs": "https://arxiv.org/abs/2508.18817", "authors": ["Colin Merk", "Ismail Geles", "Jiaxu Xing", "Angel Romero", "Giorgia Ramponi", "Davide Scaramuzza"], "title": "Learning Real-World Acrobatic Flight from Human Preferences", "comment": "8 pages, 7 figures", "summary": "Preference-based reinforcement learning (PbRL) enables agents to learn\ncontrol policies without requiring manually designed reward functions, making\nit well-suited for tasks where objectives are difficult to formalize or\ninherently subjective. Acrobatic flight poses a particularly challenging\nproblem due to its complex dynamics, rapid movements, and the importance of\nprecise execution. In this work, we explore the use of PbRL for agile drone\ncontrol, focusing on the execution of dynamic maneuvers such as powerloops.\nBuilding on Preference-based Proximal Policy Optimization (Preference PPO), we\npropose Reward Ensemble under Confidence (REC), an extension to the reward\nlearning objective that improves preference modeling and learning stability.\nOur method achieves 88.4% of the shaped reward performance, compared to 55.2%\nwith standard Preference PPO. We train policies in simulation and successfully\ntransfer them to real-world drones, demonstrating multiple acrobatic maneuvers\nwhere human preferences emphasize stylistic qualities of motion. Furthermore,\nwe demonstrate the applicability of our probabilistic reward model in a\nrepresentative MuJoCo environment for continuous control. Finally, we highlight\nthe limitations of manually designed rewards, observing only 60.7% agreement\nwith human preferences. These results underscore the effectiveness of PbRL in\ncapturing complex, human-centered objectives across both physical and simulated\ndomains.", "AI": {"tldr": "基于偏好的强化学习用于无人机特技飞行控制，提出Reward Ensemble under Confidence方法提升偏好建模和学习稳定性，在仿真和真实无人机上成功实现特技动作", "motivation": "特技飞行具有复杂动力学和快速运动特性，传统手动设计奖励函数难以捕捉人类偏好的风格化运动质量，需要基于偏好的学习方法", "method": "基于Preference PPO提出Reward Ensemble under Confidence扩展，改进奖励学习目标，提升偏好建模和学习稳定性", "result": "达到88.4%的成型奖励性能（标准方法仅55.2%），成功将仿真训练的策略迁移到真实无人机，实现多种特技动作，手动设计奖励仅60.7%符合人类偏好", "conclusion": "基于偏好的强化学习能有效捕捉复杂的人类中心目标，在物理和仿真领域都表现出色，证明了其在难以形式化任务中的有效性"}}
{"id": "2508.18820", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.18820", "abs": "https://arxiv.org/abs/2508.18820", "authors": ["Christian Henkel", "Marco Lampacrescia", "Michaela Klauck", "Matteo Morelli"], "title": "AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy", "comment": "Accepted at IROS2025", "summary": "Designing robotic systems to act autonomously in unforeseen environments is a\nchallenging task. This work presents a novel approach to use formal\nverification, specifically Statistical Model Checking (SMC), to verify system\nproperties of autonomous robots at design-time. We introduce an extension of\nthe SCXML format, designed to model system components including both Robot\nOperating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we\ncontribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the\nfull system model into JANI. The use of JANI, a standard format for\nquantitative model checking, enables verification of system properties with\noff-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both\nin terms of applicability to real-world autonomous robotic control systems, and\nin terms of verification runtime scaling. We provide a case study, where we\nsuccessfully identify problems in a ROS 2-based robotic manipulation use case\nthat is verifiable in less than one second using consumer hardware.\nAdditionally, we compare to the state of the art and demonstrate that our\nmethod is more comprehensive in system feature support, and that the\nverification runtime scales linearly with the size of the model, instead of\nexponentially.", "AI": {"tldr": "提出AS2FM工具，将自主机器人系统模型转换为JANI格式，使用统计模型检验(SMC)在设计时验证系统属性，相比现有方法支持更全面的系统特性且验证时间线性增长。", "motivation": "自主机器人在不可预见环境中运行时面临挑战，需要设计时验证系统属性以确保安全性和可靠性。", "method": "扩展SCXML格式建模ROS 2和行为树组件，开发AS2FM工具将系统模型转换为标准JANI格式，利用现成SMC工具进行验证。", "result": "成功识别ROS 2机械臂用例中的问题，验证时间少于1秒，验证运行时间随模型大小线性增长而非指数增长。", "conclusion": "AS2FM工具实用性强，能有效验证真实自主机器人控制系统，验证效率高且可扩展性好。"}}
{"id": "2508.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18937", "abs": "https://arxiv.org/abs/2508.18937", "authors": ["Wang Jiayin", "Wei Yanran", "Jiang Lei", "Guo Xiaoyu", "Zheng Ayong", "Zhao Weidong", "Li Zhongkui"], "title": "VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery", "comment": "8 pages, 6 figures", "summary": "Autonomous control of the laparoscope in robot-assisted Minimally Invasive\nSurgery (MIS) has received considerable research interest due to its potential\nto improve surgical safety. Despite progress in pixel-level Image-Based Visual\nServoing (IBVS) control, the requirement of continuous visibility and the\nexistence of complex disturbances, such as parameterization error, measurement\nnoise, and uncertainties of payloads, could degrade the surgeon's visual\nexperience and compromise procedural safety. To address these limitations, this\npaper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and\nuncertainty-adaptive framework for autonomous laparoscope control that\nguarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian\nProcess Regression (GPR) is utilized to perform hybrid (deterministic +\nstochastic) quantification of operational uncertainties including residual\nmodel uncertainties, stochastic uncertainties, and external disturbances. Based\non uncertainty quantification, a novel safety aware trajectory optimization\nframework with probabilistic guarantees is proposed, where a\nuncertainty-adaptive safety Control Barrier Function (CBF) condition is given\nbased on uncertainty propagation, and chance constraints are simultaneously\nformulated based on probabilistic approximation. This uncertainty aware\nformulation enables adaptive control effort allocation, minimizing unnecessary\ncamera motion while maintaining robustness. The proposed method is validated\nthrough comparative simulations and experiments on a commercial surgical robot\nplatform (MicroPort MedBot Toumai) performing a sequential multi-target lymph\nnode dissection. Compared with baseline methods, the framework maintains\nnear-perfect target visibility (>99.9%), reduces tracking e", "AI": {"tldr": "提出VisionSafeEnhanced VPC框架，用于机器人辅助微创手术中腹腔镜的自主控制，通过高斯过程回归量化不确定性并保证视野安全", "motivation": "解决现有基于图像的视觉伺服控制中连续可见性要求和复杂干扰（参数化误差、测量噪声、负载不确定性）导致外科医生视觉体验下降和手术安全性受损的问题", "method": "使用高斯过程回归进行混合（确定性+随机性）不确定性量化，提出具有概率保证的安全感知轨迹优化框架，包括基于不确定性传播的自适应安全控制屏障函数条件和机会约束", "result": "在商业手术机器人平台上验证，相比基线方法保持接近完美的目标可见性（>99.9%），减少跟踪误差", "conclusion": "该框架能够在不确定性下保证视野安全，实现自适应控制努力分配，在保持鲁棒性的同时最小化不必要的相机运动"}}
{"id": "2508.18967", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18967", "abs": "https://arxiv.org/abs/2508.18967", "authors": ["Hichem Cheriet", "Khellat Kihel Badra", "Chouraqui Samira"], "title": "Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm", "comment": "Accepted for publication in JAMRIS Journal", "summary": "Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical\nfor various applications, including combat support, package delivery and Search\nand Rescue Operations. This paper introduces the Tangent Intersection Guidance\n(TIG) algorithm, an advanced approach for UAV path planning in both static and\ndynamic environments. The algorithm uses the elliptic tangent intersection\nmethod to generate feasible paths. It generates two sub-paths for each threat,\nselects the optimal route based on a heuristic rule, and iteratively refines\nthe path until the target is reached. Considering the UAV kinematic and dynamic\nconstraints, a modified smoothing technique based on quadratic B\\'ezier curves\nis adopted to generate a smooth and efficient route. Experimental results show\nthat the TIG algorithm can generate the shortest path in less time, starting\nfrom 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent\nGraph, and Static APPATT algorithms in static environments. Furthermore, in\ncompletely unknown and partially known environments, TIG demonstrates efficient\nreal-time path planning capabilities for collision avoidance, outperforming APF\nand Dynamic APPATT algorithms.", "AI": {"tldr": "提出Tangent Intersection Guidance (TIG)算法，用于无人机在静态和动态环境中的高效路径规划，通过椭圆切线交点方法生成可行路径，在时间和路径质量上优于现有算法。", "motivation": "无人机的高效安全导航对于作战支持、包裹投递和搜救行动等应用至关重要，需要一种能够在静态和动态环境中进行实时路径规划的先进算法。", "method": "使用椭圆切线交点方法生成可行路径，为每个威胁生成两条子路径，基于启发式规则选择最优路线，并迭代优化路径。采用基于二次贝塞尔曲线的改进平滑技术来满足无人机运动学约束。", "result": "实验结果显示，TIG算法在静态环境中能在0.01秒内生成最短路径，转弯角度更少，性能优于A*、PRM、RRT*等算法。在未知和部分已知环境中，实时避障能力优于APF和Dynamic APPATT算法。", "conclusion": "TIG算法是一种高效的无人机路径规划方法，在静态和动态环境中都能提供实时、平滑且最优的路径规划解决方案。"}}
{"id": "2508.19002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19002", "abs": "https://arxiv.org/abs/2508.19002", "authors": ["Shipeng Lyu", "Fangyuan Wang", "Weiwei Lin", "Luhao Zhu", "David Navarro-Alarcon", "Guodong Guo"], "title": "HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots", "comment": "8 pages, 8 figures,4 tables", "summary": "Achieving both behavioral similarity and appropriateness in human-like motion\ngeneration for humanoid robot remains an open challenge, further compounded by\nthe lack of cross-embodiment adaptability. To address this problem, we propose\nHuBE, a bi-level closed-loop framework that integrates robot state, goal poses,\nand contextual situations to generate human-like behaviors, ensuring both\nbehavioral similarity and appropriateness, and eliminating structural\nmismatches between motion generation and execution. To support this framework,\nwe construct HPose, a context-enriched dataset featuring fine-grained\nsituational annotations. Furthermore, we introduce a bone scaling-based data\naugmentation strategy that ensures millimeter-level compatibility across\nheterogeneous humanoid robots. Comprehensive evaluations on multiple commercial\nplatforms demonstrate that HuBE significantly improves motion similarity,\nbehavioral appropriateness, and computational efficiency over state-of-the-art\nbaselines, establishing a solid foundation for transferable and human-like\nbehavior execution across diverse humanoid robots.", "AI": {"tldr": "HuBE是一个双层闭环框架，通过整合机器人状态、目标姿态和情境信息来生成类人行为，解决了人形机器人运动生成中行为相似性和适当性的挑战，并实现了跨异构机器人的毫米级兼容性。", "motivation": "解决人形机器人运动生成中同时实现行为相似性和适当性的开放挑战，以及缺乏跨体现适应性的问题。", "method": "提出HuBE双层闭环框架，整合机器人状态、目标姿态和情境信息；构建HPose情境丰富数据集；引入基于骨骼缩放的数据增强策略确保跨异构人形机器人的毫米级兼容性。", "result": "在多个商业平台上的综合评估显示，HuBE在运动相似性、行为适当性和计算效率方面显著优于现有最先进基线方法。", "conclusion": "HuBE为跨多样人形机器人的可转移和类人行为执行奠定了坚实基础。"}}
{"id": "2508.19074", "categories": ["cs.RO", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.19074", "abs": "https://arxiv.org/abs/2508.19074", "authors": ["ZhenDong Chen", "ZhanShang Nie", "ShiXing Wan", "JunYi Li", "YongTian Cheng", "Shuai Zhao"], "title": "An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees", "comment": null, "summary": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs.", "AI": {"tldr": "提出NRTrans框架，通过机器人技能语言(RSL)和编译器验证机制，为LLM生成的机器人控制程序提供正确性保证和反馈优化", "motivation": "现有方法直接让LLM从自然语言生成可执行程序，但由于LLM不一致性和任务复杂性，经常产生大量编程错误，特别是在轻量级LLM应用中效果不佳", "method": "提出机器人技能语言(RSL)抽象控制程序细节，构建RSL编译器和调试器验证LLM生成的程序并提供错误反馈，通过反馈微调优化LLM输出", "result": "实验表明NRTrans在多种LLM和任务下优于现有方法，对轻量级LLM实现了高成功率", "conclusion": "该框架为LLM生成的程序提供执行前正确性保证，显著提升了LLM驱动的机器人应用效果"}}
{"id": "2508.19114", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19114", "abs": "https://arxiv.org/abs/2508.19114", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Alexander Derrico", "Philip Dames"], "title": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning", "comment": "Submission under review at the 2026 IEEE/SICE International Symposium\n  on System Integration (SII 2026)", "summary": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration.", "AI": {"tldr": "DELIVER是一个基于自然语言指令的多机器人协作拾取配送框架，通过语言理解、空间分解、中继规划和运动执行实现可扩展的无碰撞协调", "motivation": "为了解决多机器人系统在现实环境中基于自然语言指令进行协作拾取配送的挑战，需要开发一个集成框架来实现可扩展、无碰撞的协调", "method": "使用LLaMA3进行自然语言理解提取位置信息，Voronoi图进行空间分区定义机器人操作区域，计算最优中继点协调交接，有限状态机控制机器人行为", "result": "在MultiTRAIL仿真平台和TurtleBot3机器人上验证，相比单机器人系统减少55%的单体工作量，团队规模增大时活跃中继机器人数量保持低位", "conclusion": "DELIVER展示了模块化和可扩展的架构，推动了语言引导的多机器人协调和网络物理系统集成的发展"}}
{"id": "2508.19131", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19131", "abs": "https://arxiv.org/abs/2508.19131", "authors": ["Shreya Gummadi", "Mateus V. Gasparino", "Gianluca Capezzuto", "Marcelo Becker", "Girish Chowdhary"], "title": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments", "comment": null, "summary": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal.", "AI": {"tldr": "ZeST利用大型语言模型的视觉推理能力，实现零样本地形可通行性预测，无需将机器人置于危险环境中即可创建实时可通行性地图。", "motivation": "传统的地形可通行性预测方法需要将机器人置于危险环境中收集数据，存在设备损坏和安全风险。需要一种更安全、成本效益更高的解决方案。", "method": "利用大型语言模型（LLMs）的视觉推理能力，通过零样本学习方式实时生成地形可通行性地图，避免实际环境中的风险数据收集。", "result": "在受控室内和非结构化室外环境中的导航实验表明，该方法相比其他最先进方法提供更安全的导航，能够持续到达最终目标。", "conclusion": "ZeST方法为零样本地形可通行性预测提供了一种安全、成本效益高且可扩展的解决方案，加速了先进导航系统的开发。"}}
{"id": "2508.19150", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19150", "abs": "https://arxiv.org/abs/2508.19150", "authors": ["Juan Carlos Saborío", "Marc Vinci", "Oscar Lima", "Sebastian Stock", "Lennart Niecksch", "Martin Günther", "Alexander Sung", "Joachim Hertzberg", "Martin Atzmüller"], "title": "Uncertainty-Resilient Active Intention Recognition for Robotic Assistants", "comment": "(To appear) In Proceedings of ECMR 2025", "summary": "Purposeful behavior in robotic assistants requires the integration of\nmultiple components and technological advances. Often, the problem is reduced\nto recognizing explicit prompts, which limits autonomy, or is oversimplified\nthrough assumptions such as near-perfect information. We argue that a critical\ngap remains unaddressed -- specifically, the challenge of reasoning about the\nuncertain outcomes and perception errors inherent to human intention\nrecognition. In response, we present a framework designed to be resilient to\nuncertainty and sensor noise, integrating real-time sensor data with a\ncombination of planners. Centered around an intention-recognition POMDP, our\napproach addresses cooperative planning and acting under uncertainty. Our\nintegrated framework has been successfully tested on a physical robot with\npromising results.", "AI": {"tldr": "提出了一个基于POMDP的意图识别框架，用于处理机器人助手中的不确定性和感知错误，实现协作规划与行动。", "motivation": "现有机器人助手系统要么依赖显式提示限制了自主性，要么基于完美信息假设过于简化，未能解决人类意图识别中的不确定结果和感知错误这一关键挑战。", "method": "开发了一个以意图识别POMDP为中心的框架，整合实时传感器数据和多种规划器，旨在对不确定性和传感器噪声具有鲁棒性。", "result": "该集成框架已在物理机器人上成功测试，并取得了有希望的结果。", "conclusion": "该研究为解决机器人助手中人类意图识别的不确定性问题提供了一个有效的POMDP-based框架，在实际机器人应用中展现出良好潜力。"}}
{"id": "2508.19153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19153", "abs": "https://arxiv.org/abs/2508.19153", "authors": ["Allen Wang", "Gavin Tao"], "title": "QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning", "comment": "14pages, 9 figures, Journal paper", "summary": "We address vision-guided quadruped motion control with reinforcement learning\n(RL) and highlight the necessity of combining proprioception with vision for\nrobust control. We propose QuadKAN, a spline-parameterized cross-modal policy\ninstantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates\na spline encoder for proprioception and a spline fusion head for\nproprioception-vision inputs. This structured function class aligns the\nstate-to-action mapping with the piecewise-smooth nature of gait, improving\nsample efficiency, reducing action jitter and energy consumption, and providing\ninterpretable posture-action sensitivities. We adopt Multi-Modal Delay\nRandomization (MMDR) and perform end-to-end training with Proximal Policy\nOptimization (PPO). Evaluations across diverse terrains, including both even\nand uneven surfaces and scenarios with static or dynamic obstacles, demonstrate\nthat QuadKAN achieves consistently higher returns, greater distances, and fewer\ncollisions than state-of-the-art (SOTA) baselines. These results show that\nspline-parameterized policies offer a simple, effective, and interpretable\nalternative for robust vision-guided locomotion. A repository will be made\navailable upon acceptance.", "AI": {"tldr": "提出了QuadKAN框架，使用样条参数化的KAN网络结合本体感觉和视觉输入，通过强化学习实现四足机器人鲁棒运动控制，在多种地形和障碍物场景下优于现有方法。", "motivation": "解决视觉引导的四足运动控制问题，强调需要结合本体感觉和视觉信息来实现鲁棒控制，现有方法在样本效率、动作抖动和能耗方面存在不足。", "method": "提出QuadKAN框架，使用样条参数化的Kolmogorov-Arnold Networks（KANs），包含样条编码器处理本体感觉输入和样条融合头处理多模态输入，采用多模态延迟随机化和PPO算法进行端到端训练。", "result": "在包括平坦/不平坦地形和静态/动态障碍物的多种场景评估中，QuadKAN相比最先进基线方法获得了更高的回报、更长的移动距离和更少的碰撞。", "conclusion": "样条参数化策略为鲁棒的视觉引导运动控制提供了一种简单、有效且可解释的替代方案，能够改善样本效率、减少动作抖动和能耗，并提供可解释的姿态-动作敏感性。"}}
{"id": "2508.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19168", "abs": "https://arxiv.org/abs/2508.19168", "authors": ["Liding Zhang", "Kejia Chen", "Kuanqi Cai", "Yu Zhang", "Yixuan Dang", "Yansong Wu", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic", "comment": "7 pages, 5 figures, 2025 IEEE International Conference on Robotics\n  and Automation (ICRA)", "summary": "Optimal path planning requires finding a series of feasible states from the\nstarting point to the goal to optimize objectives. Popular path planning\nalgorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to\nguide the search. Effective heuristics are accurate and computationally\nefficient, but achieving both can be challenging due to their conflicting\nnature. This paper proposes Direction Informed Trees (DIT*), a sampling-based\nplanner that focuses on optimizing the search direction for each edge,\nresulting in goal bias during exploration. We define edges as generalized\nvectors and integrate similarity indexes to establish a directional filter that\nselects the nearest neighbors and estimates direction costs. The estimated\ndirection cost heuristics are utilized in edge evaluation. This strategy allows\nthe exploration to share directional information efficiently. DIT* convergence\nfaster than existing single-query, sampling-based planners on tested problems\nin R^4 to R^16 and has been demonstrated in real-world environments with\nvarious planning tasks. A video showcasing our experimental results is\navailable at: https://youtu.be/2SX6QT2NOek", "AI": {"tldr": "DIT*是一种基于采样的路径规划算法，通过优化搜索方向和方向成本启发式，比现有算法在R^4到R^16空间中收敛更快", "motivation": "现有路径规划算法（如EIT*）使用effort启发式来指导搜索，但准确性和计算效率往往难以兼顾，需要更好的启发式方法", "method": "将边定义为广义向量，集成相似性索引建立方向过滤器来选择最近邻和估计方向成本，在边评估中使用方向成本启发式", "result": "DIT*在R^4到R^16的测试问题上比现有单查询采样规划器收敛更快，并在各种实际规划任务中得到验证", "conclusion": "通过优化搜索方向和有效共享方向信息，DIT*实现了更快的收敛性能，适用于高维空间的路径规划问题"}}
{"id": "2508.19172", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19172", "abs": "https://arxiv.org/abs/2508.19172", "authors": ["Luca Grillotti", "Lisa Coiffard", "Oscar Pang", "Maxence Faldor", "Antoine Cully"], "title": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity", "comment": "Accepted at CoRL 2025", "summary": "Autonomous skill discovery aims to enable robots to acquire diverse behaviors\nwithout explicit supervision. Learning such behaviors directly on physical\nhardware remains challenging due to safety and data efficiency constraints.\nExisting methods, including Quality-Diversity Actor-Critic (QDAC), require\nmanually defined skill spaces and carefully tuned heuristics, limiting\nreal-world applicability. We propose Unsupervised Real-world Skill Acquisition\n(URSA), an extension of QDAC that enables robots to autonomously discover and\nmaster diverse, high-performing skills directly in the real world. We\ndemonstrate that URSA successfully discovers diverse locomotion skills on a\nUnitree A1 quadruped in both simulation and the real world. Our approach\nsupports both heuristic-driven skill discovery and fully unsupervised settings.\nWe also show that the learned skill repertoire can be reused for downstream\ntasks such as real-world damage adaptation, where URSA outperforms all\nbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.\nOur results establish a new framework for real-world robot learning that\nenables continuous skill discovery with limited human intervention,\nrepresenting a significant step toward more autonomous and adaptable robotic\nsystems. Demonstration videos are available at\nhttp://adaptive-intelligent-robotics.github.io/URSA .", "AI": {"tldr": "URSA是一种无监督真实世界技能获取方法，扩展了QDAC算法，使机器人能够在真实环境中自主发现和掌握多样化高性能技能，无需手动定义技能空间或精心调整启发式方法。", "motivation": "现有方法如QDAC需要手动定义技能空间和精心调整启发式方法，限制了在真实世界中的适用性。学习直接在物理硬件上的行为由于安全和数据效率限制仍然具有挑战性。", "method": "提出URSA方法，扩展QDAC算法，支持启发式驱动技能发现和完全无监督设置，使机器人能够在真实世界中自主发现多样化技能。", "result": "在Unitree A1四足机器人上成功发现多样化运动技能，在仿真和真实世界中都有效。在9个仿真损坏场景中5个、5个真实世界损坏场景中3个都优于所有基线方法。", "conclusion": "URSA为真实世界机器人学习建立了新框架，实现了有限人工干预下的持续技能发现，是迈向更自主和适应性机器人系统的重要一步。"}}
{"id": "2508.19186", "categories": ["cs.RO", "cs.AI", "cs.FL", "I.2.9; I.2; D.2.4"], "pdf": "https://arxiv.org/pdf/2508.19186", "abs": "https://arxiv.org/abs/2508.19186", "authors": ["Christopher Chandler", "Bernd Porr", "Giulia Lafratta", "Alice Miller"], "title": "Real-Time Model Checking for Closed-Loop Robot Reactive Planning", "comment": "30 pages excluding references, 18 figures, submitted to Formal\n  Aspects of Computing", "summary": "We present a new application of model checking which achieves real-time\nmulti-step planning and obstacle avoidance on a real autonomous robot. We have\ndeveloped a small, purpose-built model checking algorithm which generates plans\nin situ based on \"core\" knowledge and attention as found in biological agents.\nThis is achieved in real-time using no pre-computed data on a low-powered\ndevice. Our approach is based on chaining temporary control systems which are\nspawned to counteract disturbances in the local environment that disrupt an\nautonomous agent from its preferred action (or resting state). A novel\ndiscretization of 2D LiDAR data sensitive to bounded variations in the local\nenvironment is used. Multi-step planning using model checking by forward\ndepth-first search is applied to cul-de-sac and playground scenarios. Both\nempirical results and informal proofs of two fundamental properties of our\napproach demonstrate that model checking can be used to create efficient\nmulti-step plans for local obstacle avoidance, improving on the performance of\na reactive agent which can only plan one step. Our approach is an instructional\ncase study for the development of safe, reliable and explainable planning in\nthe context of autonomous vehicles.", "AI": {"tldr": "基于模型检查的实时多步规划技术，在低力量设备上实现自主机器人的障碍物避免和多步规划", "motivation": "仿照生物代理的\"6838心\"知识和注意力机制，开发能够在实时环境中进行多步规划的方法，改善只能做单步规划的反应式算法", "method": "使用专门设计的小型模型检查算法，基于链式临时控制系统来对抗局部环境干扰；采用对局部环境有界变化敏感的2D LiDAR数据离散化方法；通过前向深度优先搜索进行多步规划", "result": "在小均上和游戏场场景中验证了方法的有效性，证明模型检查能够为局部障碍物避免创建高效的多步计划，性能超过只能进行单步规划的反应式代理", "conclusion": "该方法为自主车辆领域开发安全、可靠和可解释的规划系统提供了教学案例研究，证明了模型检查在实时多步规划中的应用价值"}}
{"id": "2508.19191", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19191", "abs": "https://arxiv.org/abs/2508.19191", "authors": ["Yue Wang", "Wenjie Deng", "Haotian Xue", "Di Cui", "Yiqi Chen", "Mingchuan Zhou", "Haochao Ying", "Jian Wu"], "title": "AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot", "comment": null, "summary": "Intraocular foreign body removal demands millimeter-level precision in\nconfined intraocular spaces, yet existing robotic systems predominantly rely on\nmanual teleoperation with steep learning curves. To address the challenges of\nautonomous manipulation (particularly kinematic uncertainties from variable\nmotion scaling and variation of the Remote Center of Motion (RCM) point), we\npropose AutoRing, an imitation learning framework for autonomous intraocular\nforeign body ring manipulation. Our approach integrates dynamic RCM calibration\nto resolve coordinate-system inconsistencies caused by intraocular instrument\nvariation and introduces the RCM-ACT architecture, which combines\naction-chunking transformers with real-time kinematic realignment. Trained\nsolely on stereo visual data and instrument kinematics from expert\ndemonstrations in a biomimetic eye model, AutoRing successfully completes ring\ngrasping and positioning tasks without explicit depth sensing. Experimental\nvalidation demonstrates end-to-end autonomy under uncalibrated microscopy\nconditions. The results provide a viable framework for developing intelligent\neye-surgical systems capable of complex intraocular procedures.", "AI": {"tldr": "AutoRing是一个基于模仿学习的自主眼内异物环操作框架，通过动态RCM校准和RCM-ACT架构解决运动缩放和RCM点变化带来的运动学不确定性，仅使用立体视觉数据和专家演示即可完成精确操作。", "motivation": "眼内异物移除需要在有限空间内实现毫米级精度，现有机器人系统主要依赖手动遥操作，学习曲线陡峭，需要解决自主操作中的运动学不确定性挑战。", "method": "提出AutoRing模仿学习框架，整合动态RCM校准解决坐标系不一致问题，引入结合动作分块变换器和实时运动学重对齐的RCM-ACT架构，仅使用立体视觉数据和专家演示的仪器运动学数据进行训练。", "result": "在未经校准的显微镜条件下实现了端到端自主操作，成功完成了环抓取和定位任务，无需显式深度感知。", "conclusion": "该研究为开发能够执行复杂眼内手术的智能眼外科系统提供了可行框架。"}}
{"id": "2508.19199", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19199", "abs": "https://arxiv.org/abs/2508.19199", "authors": ["Alex LaGrassa", "Zixuan Huang", "Dmitry Berenson", "Oliver Kroemer"], "title": "Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation", "comment": "9 pages, 7 figures", "summary": "Efficient planning in high-dimensional spaces, such as those involving\ndeformable objects, requires computationally tractable yet sufficiently\nexpressive dynamics models. This paper introduces a method that automatically\ngenerates task-specific, spatially adaptive dynamics models by learning which\nregions of the object require high-resolution modeling to achieve good task\nperformance for a given planning query. Task performance depends on the complex\ninterplay between the dynamics model, world dynamics, control, and task\nrequirements. Our proposed diffusion-based model generator predicts per-region\nmodel resolutions based on start and goal pointclouds that define the planning\nquery. To efficiently collect the data for learning this mapping, a two-stage\nprocess optimizes resolution using predictive dynamics as a prior before\ndirectly optimizing using closed-loop performance. On a tree-manipulation task,\nour method doubles planning speed with only a small decrease in task\nperformance over using a full-resolution model. This approach informs a path\ntowards using previous planning and control data to generate computationally\nefficient yet sufficiently expressive dynamics models for new tasks.", "AI": {"tldr": "这篇论文提出了一种自动生成任务特定、空间适配性动力学模型的方法，通过学习识别对象哪些区域需要高分辨率建模来提高规划效率。该方法在树林操纵任务上实现了规划速度倍增且任务性能仅有轻微下降。", "motivation": "在高维度空间（如可变形对象）中进行高效规划需要计算可处理但充分表达的动力学模型。传统方法使用固定分辨率的模型导致计算资源浪费或性能不足。", "method": "提出基于涵渗模型的模型生成器，根据规划查询的起始和目标点云预测各区域的模型分辨率。采用两阶段数据收集过程：首先使用预测动力学作为先验优化分辨率，然后直接使用闭环性能进行优化。", "result": "在树林操纵任务上，该方法实现了规划速度倍增，而任务性能仅比使用全分辨率模型有轻微下降。", "conclusion": "这种方法为利用历史规划和控制数据生成计算高效但充分表达的动力学模型提供了新的路径，适用于新任务。"}}
{"id": "2508.19236", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19236", "abs": "https://arxiv.org/abs/2508.19236", "authors": ["Hao Shi", "Bin Xie", "Yingfei Liu", "Lin Sun", "Fengrong Liu", "Tiancai Wang", "Erjin Zhou", "Haoqiang Fan", "Xiangyu Zhang", "Gao Huang"], "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation", "comment": "The project is available at https://shihao1895.github.io/MemoryVLA", "summary": "Temporal context is essential for robotic manipulation because such tasks are\ninherently non-Markovian, yet mainstream VLA models typically overlook it and\nstruggle with long-horizon, temporally dependent tasks. Cognitive science\nsuggests that humans rely on working memory to buffer short-lived\nrepresentations for immediate control, while the hippocampal system preserves\nverbatim episodic details and semantic gist of past experience for long-term\nmemory. Inspired by these mechanisms, we propose MemoryVLA, a\nCognition-Memory-Action framework for long-horizon robotic manipulation. A\npretrained VLM encodes the observation into perceptual and cognitive tokens\nthat form working memory, while a Perceptual-Cognitive Memory Bank stores\nlow-level details and high-level semantics consolidated from it. Working memory\nretrieves decision-relevant entries from the bank, adaptively fuses them with\ncurrent tokens, and updates the bank by merging redundancies. Using these\ntokens, a memory-conditioned diffusion action expert yields temporally aware\naction sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks\nacross three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it\nachieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming\nstate-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on\nBridge. On 12 real-world tasks spanning general skills and long-horizon\ntemporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon\ntasks showing a +26 improvement over state-of-the-art baseline. Project Page:\nhttps://shihao1895.github.io/MemoryVLA", "AI": {"tldr": "MemoryVLA是一个受人类记忆机制启发的机器人操作框架，通过工作记忆和长期记忆系统处理时序上下文，在长时程任务中显著优于现有方法", "motivation": "主流VLA模型忽视时序上下文，难以处理长时程、时序依赖的任务。受认知科学启发，人类利用工作记忆和长期记忆系统来处理时序信息", "method": "提出Cognition-Memory-Action框架：预训练VLM编码观测为感知和认知token形成工作记忆，感知-认知记忆库存储低层细节和高层语义，工作记忆从库中检索相关条目并与当前token融合，记忆条件扩散动作专家生成时序感知动作序列", "result": "在150+仿真和真实任务中验证：SimpletEnv-Bridge(71.9%)、Fractal(72.7%)、LIBERO-5(96.5%)，均优于CogACT和pi-0，Bridge任务提升14.6%；真实世界12个任务达到84.0%成功率，长时程任务提升26%", "conclusion": "MemoryVLA通过模拟人类记忆机制有效处理机器人操作中的时序上下文问题，在长时程任务中表现出色，为时序感知的机器人控制提供了新思路"}}
