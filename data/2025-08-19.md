<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: 本文探讨了如何通过大语言模型提升自主机器人的自然语言理解能力，以实现人机协作的远景。作者提出了一种整合认知代理、物理机器人和LLM的方案，并通过ChatGPT进行了概念验证实验。


<details>
  <summary>Details</summary>
Motivation: 传统交互式任务学习系统的语言理解能力有限，而大语言模型的出现为提升机器人语言理解能力提供了机会，但需要解决LLM与物理世界操作机器人的集成挑战。

Method: 提出一种以认知代理为核心的AI系统方案，该代理控制物理机器人、与人类和LLM交互、通过经验积累情境知识。使用ChatGPT进行了三个具体挑战的概念验证实验。

Result: 通过简单的概念验证实验，证明了使用LLM提升机器人自然语言理解的可行性，为构建集成化语言协作机器人助手系统奠定了基础。

Conclusion: 虽然目前只是概念验证，但本文提出的方案为实现具有健壮语言能力的人机协作机器人助手指明了方向，需要进一步研究如何将这些简单实验转化为可操作的集成系统。

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [2] [Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots](https://arxiv.org/abs/2508.11802)
*Luigi Penco,Beomyeong Park,Stefan Fasano,Nehar Poddar,Stephen McCrory,Nicholas Kitchel,Tomasz Bialek,Dexton Anderson,Duncan Calvert,Robert Griffin*

Main category: cs.RO

TL;DR: 通过预测用户步伐并重定向到机器人步伐位置，实现了高速任务中用户与机器人运动的实时同步，而非直接复制脚部姿态，从而保持机器人平衡性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决高速任务中用户与机器人运动同步的挑战，特别是在环境不匹配的情况下保证机器人的平衡和稳定性。

Method: 预测用户步伐并重定向到机器人步伐位置，让机器人利用自身动力学进行移动；持续适应步伐估计以进入测量用户参考；自主调整机器人步伐以适应周围地形。

Result: 在人型机器人Nadia上的实验结果证明了所提系统的有效性。

Conclusion: 该方法能够有效解决高速任务中的同步问题，保证机器人在不平坦地形上的平衡和稳定性，为遥播操作提供了可靠的解决方案。

Abstract: Achieving seamless synchronization between user and robot motion in
teleoperation, particularly during high-speed tasks, remains a significant
challenge. In this work, we propose a novel approach for transferring stepping
motions from the user to the robot in real-time. Instead of directly
replicating user foot poses, we retarget user steps to robot footstep
locations, allowing the robot to utilize its own dynamics for locomotion,
ensuring better balance and stability. Our method anticipates user footsteps to
minimize delays between when the user initiates and completes a step and when
the robot does it. The step estimates are continuously adapted to converge with
the measured user references. Additionally, the system autonomously adjusts the
robot's steps to account for its surrounding terrain, overcoming challenges
posed by environmental mismatches between the user's flat-ground setup and the
robot's uneven terrain. Experimental results on the humanoid robot Nadia
demonstrate the effectiveness of the proposed system.

</details>


### [3] [LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2508.11849)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: LocoMamba是一个基于Mamba选择性状态空间模型的视觉驱动跨模态深度强化学习框架，能够实现近线性时间序列建模，有效捕获长距离依赖关系，并在具有挑战性的模拟环境中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法在序列建模中的计算效率问题，同时需要有效处理长距离依赖和跨模态信息融合，特别是在机器人导航任务中需要处理视觉和本体感知信息。

Method: 1) 使用多层感知机嵌入本体感知状态，用轻量级CNN处理深度图像生成紧凑token；2) 堆叠Mamba层通过近线性时间选择性扫描融合token；3) 使用PPO算法在随机化地形和外观下进行端到端策略训练，采用障碍物密度课程学习和紧凑状态中心奖励函数。

Result: 在具有静态和动态障碍物以及不平坦地形的挑战性模拟环境中，相比最先进基线方法，LocoMamba获得更高的回报和成功率，碰撞更少，对未见地形和障碍物密度表现出更强的泛化能力，在相同计算预算下以更少的更新次数收敛。

Conclusion: LocoMamba框架通过选择性状态空间模型有效解决了长序列建模的计算效率问题，在机器人导航任务中表现出优异的性能和训练效率，为跨模态深度强化学习提供了新的解决方案。

Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.

</details>


### [4] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: 本文研究自动驾驶中的数据偏移问题，通过数据偏移检测和CycleGAN数据增帽技术优化YOLOv5模型，在BDD100K数据集上达到更优性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的机器学习模型对训练和测试数据的IID假设敏感，实际应用中季节、天气等因素导致的数据分布变化会引发数据偏移问题，影响目标检测性能。

Method: 系统分析数据偏移的复杂性和多样性，综述数据偏移检测方法，进行数据集分类和平衡处理。基于YOLOv5构建目标检测模型，结合CycleGAN数据增帽技术进行模型优化。

Result: 在BDD100K数据集上的实验结果显示，该方法比基线模型表现更优。

Conclusion: 通过数据偏移检测和数据增帽技术的结合，可有效提升自动驾驶目标检测模型在实际应用中的性能和适应性。

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [5] [Bioinspired underwater soft robots: from biology to robotics and back](https://arxiv.org/abs/2508.11883)
*Lei Li,Boyang Qin,Wenzhuo Gao,Yanyu Li,Yiyuan Zhang,Bo Wang,Shihan Kong,Jian Wang,Dekui He,Junzhi Yu*

Main category: cs.RO

TL;DR: 这篇论文提出了一种双向软体机器人研究框架，将生物学原理、机器人实现和生物验证相结合，以推动海洋探索和科学发现。


<details>
  <summary>Details</summary>
Motivation: 当前海洋软体机器人研究多为单向影响（生物学引导机器人），缺乏从机器人到生物学的反馈循环。需要一种更全面的双向框架来深化科学发现和推动技术进步。

Method: 提出了一种整体性的双向框架，让软体机器人作为实验工具来探索生物功能和验证进化假说。同时提出"生物普适源机器人学"新范式，跨越特定物种模仿，寻找不同物种间的聚合原理来设计更适应性强的机器人。

Result: 软体机器人在非结构化环境中表现出比硬质系统更优异的性能，支撑了海洋探索、操作和医疗等应用领域。通过双向框架，机器人技术不仅能够模仿生物，还能反过来验证生物学理论和假说。

Conclusion: 软体机器人通过联合生物学和工程学科，能够推动海洋探索和深化科学发现。虽然面临材料耐用性、驱动效率、自主性和智能化等挑战，但双向框架和生物普适源机器人学的新范式为未来研究指明了方向。

Abstract: The ocean vast unexplored regions and diverse soft-bodied marine organisms
have spurred interest in bio-inspired underwater soft robotics. Recent advances
have enabled new capabilities in underwater movement, sensing, and interaction.
However, these efforts are largely unidirectional, with biology guiding
robotics while insights from robotics rarely feed back into biology. Here we
propose a holistic, bidirectional framework that integrates biological
principles, robotic implementation, and biological validation. We show that
soft robots can serve as experimental tools to probe biological functions and
even test evolutionary hypotheses. Their inherent compliance also allows them
to outperform rigid systems in unstructured environments, supporting
applications in marine exploration, manipulation, and medicine. Looking
forward, we introduce bio-universal-inspired robotics, a paradigm that
transcends species-specific mimicry by identifying convergent principles across
species to inspire more adaptable designs. Despite rapid progress, challenges
persist in material robustness, actuation efficiency, autonomy, and
intelligence. By uniting biology and engineering, soft robots can advance ocean
exploration and deepen scientific discovery.

</details>


### [6] [From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics](https://arxiv.org/abs/2508.11884)
*Havel Liu,Mingzhang Zhu,Arturo Moises Flores Alvarez,Yuan Hung Lo,Conrad Ku,Federico Parres,Justin Quan,Colin Togashi,Aditya Navghare,Quanyou Wang,Dennis W. Hong*

Main category: cs.RO

TL;DR: Kid Cosmo是一个儿童尺寸的娱乐人形机器人平台，专为稳健运动和拟人动作生成而设计，模仿了Netflix电影中的角色形象，展示了娱乐导向人形机器人的可行性


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人主要关注功能性设计，而娱乐领域更注重视觉效果和形态，娱乐人形机器人的潜力尚未被充分探索

Method: 开发了1.45米高、25公斤重的28自由度人形机器人Kid Cosmo，使用本体感受执行器实现扭矩控制行走和逼真动作生成

Result: 通过全球展示验证了系统架构，解决了功能性娱乐机器人的挑战，展示了上下半身同时运动时的稳定性

Conclusion: 证明了同时注重角色体现和技术功能的表演导向人形机器人的可行性

Abstract: Humanoid robots represent the cutting edge of robotics research, yet their
potential in entertainment remains largely unexplored. Entertainment as a field
prioritizes visuals and form, a principle that contrasts with the purely
functional designs of most contemporary humanoid robots. Designing
entertainment humanoid robots capable of fluid movement presents a number of
unique challenges. In this paper, we present Kid Cosmo, a research platform
designed for robust locomotion and life-like motion generation while imitating
the look and mannerisms of its namesake character from Netflix's movie The
Electric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall
and weighing 25 kg. It contains 28 degrees of freedom and primarily uses
proprioceptive actuators, enabling torque-control walking and lifelike motion
generation. Following worldwide showcases as part of the movie's press tour, we
present the system architecture, challenges of a functional entertainment robot
and unique solutions, and our initial findings on stability during simultaneous
upper and lower body movement. We demonstrate the viability of
performance-oriented humanoid robots that prioritize both character embodiment
and technical functionality.

</details>


### [7] [Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System](https://arxiv.org/abs/2508.11885)
*Haixin Gong,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 开发了一种新的可变形脚部模型，通过两阶段策略训练自然步态，在动态学、动力学和步态稳定性方面显著改善了传统粗糕模型的误差


<details>
  <summary>Details</summary>
Motivation: 现有肌骨驱动模型对脚地接触力学过于简化，限制了模拟人类步态动态的准确性

Method: 构建了一个接触丰富的可变形脚部模型，并集成到完整的肌骨驱动系统中，采用两阶段策略训练方法来学习自然步态

Result: 与传统粗糕模型相比，在运动学、力学和步态稳定性指标上都显著改善，通过人体实验数据验证了模拟结果的准确性

Conclusion: 这项工作推进了人体肌骨驱动系统的接触丰富界面模型技术，为需要精确脚地交互控制的人形机器人应用建立了稳健框架

Abstract: The human foot serves as the critical interface between the body and
environment during locomotion. Existing musculoskeletal models typically
oversimplify foot-ground contact mechanics, limiting their ability to
accurately simulate human gait dynamics. We developed a novel contact-rich and
deformable model of the human foot integrated within a complete musculoskeletal
system that captures the complex biomechanical interactions during walking. To
overcome the control challenges inherent in modeling multi-point contacts and
deformable material, we developed a two-stage policy training strategy to learn
natural walking patterns for this interface-enhanced model. Comparative
analysis between our approach and conventional rigid musculoskeletal models
demonstrated improvements in kinematic, kinetic, and gait stability metrics.
Validation against human subject data confirmed that our simulation closely
reproduced real-world biomechanical measurements. This work advances
contact-rich interface modeling for human musculoskeletal systems and
establishes a robust framework that can be extended to humanoid robotics
applications requiring precise foot-ground interaction control.

</details>


### [8] [Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing](https://arxiv.org/abs/2508.12166)
*Gokul Puthumanaillam,Aditya Penumarti,Manav Vora,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Jane Shin,Melkior Ornik*

Main category: cs.RO

TL;DR: B-COD是一种基于扩散模型的规划器，通过单次前向传播即可生成轨迹和定位误差代理，结合强化学习在线选择最小传感器子集，在保证任务完成的同时显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要持续开启所有传感器（浪费能源），要么依赖启发式传感器切换和昂贵的协方差传播计算。需要一种能够在保持足够状态不确定性的前提下，选择最小传感器子集的方法。

Method: 提出Belief-Conditioned One-Step Diffusion (B-COD)规划器，将位姿信念栅格和传感器掩码作为条件输入扩散模型，通过去噪轨迹的扩散程度获得定位误差的校准代理。结合soft-actor-critic在线选择传感器。

Result: 在无人水面艇的真实海洋试验中，B-COD在匹配全开传感器基线性能的同时，显著降低了传感能耗，单次前向传播仅需10毫秒。

Conclusion: B-COD首次实现了在单次前向传播中同时生成轨迹、方差估计和定位误差代理，为在线传感器选择提供了高效解决方案，在真实环境中验证了其有效性和能效优势。

Abstract: Robots equipped with rich sensor suites can localize reliably in
partially-observable environments, but powering every sensor continuously is
wasteful and often infeasible. Belief-space planners address this by
propagating pose-belief covariance through analytic models and switching
sensors heuristically--a brittle, runtime-expensive approach. Data-driven
approaches--including diffusion models--learn multi-modal trajectories from
demonstrations, but presuppose an accurate, always-on state estimate. We
address the largely open problem: for a given task in a mapped environment,
which \textit{minimal sensor subset} must be active at each location to
maintain state uncertainty \textit{just low enough} to complete the task? Our
key insight is that when a diffusion planner is explicitly conditioned on a
pose-belief raster and a sensor mask, the spread of its denoising trajectories
yields a calibrated, differentiable proxy for the expected localisation error.
Building on this insight, we present Belief-Conditioned One-Step Diffusion
(B-COD), the first planner that, in a 10 ms forward pass, returns a
short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for
localisation error--eliminating external covariance rollouts. We show that this
single proxy suffices for a soft-actor-critic to choose sensors online,
optimising energy while bounding pose-covariance growth. We deploy B-COD in
real-time marine trials on an unmanned surface vehicle and show that it reduces
sensing energy consumption while matching the goal-reach performance of an
always-on baseline.

</details>


### [9] [Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards](https://arxiv.org/abs/2508.11887)
*Yousra Shleibik,Jordan Sinclair,Kerstin Haring*

Main category: cs.RO

TL;DR: 论文提出了一种通过视觉和听觉线索进行注意力重定向的框架，以增强半自动驾驶场景中的情境意识，帮助驾驶员在接管过程中更好地应对潜在危险。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术向更高自主性发展，需要在无法识别场景元素时进行人工干预。情境意识对于降低接管过程中的风险至关重要，需要保持驾驶员注意力以避免碰撞并确保平稳过渡。

Method: 提出了一个概念框架，结合实时视线追踪、上下文感知的显著性分析和同步的视觉听觉警报，通过目标视觉和听觉线索进行注视操纵来实现注意力重定向。

Result: 该框架旨在帮助驾驶员保持对突发危险的关注，减少目标固定现象，增强情境意识，主动应对潜在危险，促进人与自主系统的有效协作。

Conclusion: 注意力重定向技术对于提升半自动驾驶安全性具有重要意义，提出的集成框架能够有效支持人类在决策过程中的参与，确保自动驾驶系统向更高自主性发展的平稳过渡。

Abstract: The advent of autonomous driving systems promises to transform transportation
by enhancing safety, efficiency, and comfort. As these technologies evolve
toward higher levels of autonomy, the need for integrated systems that
seamlessly support human involvement in decision-making becomes increasingly
critical. Certain scenarios necessitate human involvement, including those
where the vehicle is unable to identify an object or element in the scene, and
as such cannot take independent action. Therefore, situational awareness is
essential to mitigate potential risks during a takeover, where a driver must
assume control and autonomy from the vehicle. The need for driver attention is
important to avoid collisions with external agents and ensure a smooth
transition during takeover operations. This paper explores the integration of
attention redirection techniques, such as gaze manipulation through targeted
visual and auditory cues, to help drivers maintain focus on emerging hazards
and reduce target fixation in semi-autonomous driving scenarios. We propose a
conceptual framework that combines real-time gaze tracking, context-aware
saliency analysis, and synchronized visual and auditory alerts to enhance
situational awareness, proactively address potential hazards, and foster
effective collaboration between humans and autonomous systems.

</details>


### [10] [Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control](https://arxiv.org/abs/2508.12335)
*Yunfan Gao,Florian Messerer,Niels van Duijkeren,Rashmi Dabir,Moritz Diehl*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新题的最优控制和模型预测控制中的碰撞避免方法，通过半无穷规划和主动集方法高效处理无穷约束，并在实际机器人上实现了20Hz的快速碰撞免遮导航。


<details>
  <summary>Details</summary>
Motivation: 解决最优控制和模型预测控制中的碰撞避免问题，特别是当环境由大量点体表示而机器人由多边形组成时，需要处理无穷多个约束的技术挑战。

Method: 采用半无穷规划(SIP)方法，结合局部约化和外部主动集算法，迭代识别最近的障碍点、确定可行机器人形状参数的距离最小化器，并求解有限约束子问题。对于状态不确定性，通过局部约化处理平移不确定性，通过后退重构处理旋转不确定性。

Result: 在实际机器人上实现了20Hz的控制器，能够在窄窄空间中实现快速的碰撞免遮导航。同时在模拟中也展示了3D碰撞避免的应用效果。

Conclusion: 该方法能够高效处理最优控制问题中的无穷约束挑战，并在存在状态不确定性的情况下实现稳健的碰撞避免，为机器人在复杂环境中的安全导航提供了有效解决方案。

Abstract: This paper presents a novel approach for collision avoidance in optimal and
model predictive control, in which the environment is represented by a large
number of points and the robot as a union of padded polygons. The conditions
that none of the points shall collide with the robot can be written in terms of
an infinite number of constraints per obstacle point. We show that the
resulting semi-infinite programming (SIP) optimal control problem (OCP) can be
efficiently tackled through a combination of two methods: local reduction and
an external active-set method. Specifically, this involves iteratively
identifying the closest point obstacles, determining the lower-level distance
minimizer among all feasible robot shape parameters, and solving the
upper-level finitely-constrained subproblems.
  In addition, this paper addresses robust collision avoidance in the presence
of ellipsoidal state uncertainties. Enforcing constraint satisfaction over all
possible uncertainty realizations extends the dimension of constraint
infiniteness. The infinitely many constraints arising from translational
uncertainty are handled by local reduction together with the robot shape
parameterization, while rotational uncertainty is addressed via a backoff
reformulation.
  A controller implemented based on the proposed method is demonstrated on a
real-world robot running at 20Hz, enabling fast and collision-free navigation
in tight spaces. An application to 3D collision avoidance is also demonstrated
in simulation.

</details>


### [11] [Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation](https://arxiv.org/abs/2508.11890)
*Sangwoo Jeon,Juchul Shin,YeonJe Cho,Gyeong-Tae Kim,Seongwoo Kim*

Main category: cs.RO

TL;DR: 基于AMAD-SRL框架，通过符号强化学习整合结构化规划与适应性决策，在软件在环测试中将无人机任务效率提升75%，为复杂任务提供了可靠解决方案。


<details>
  <summary>Details</summary>
Motivation: 现代无人机任务需要能够无缝整合结构化符号规划与适应性强化学习的框架，以应对动态复杂环境中的适应性符号规划需求，提高决策的可靠性和安全性。

Method: 提出AMAD-SRL框架，作为AMAD认知多代理架构的扩展和精化版本，采用基于PDDL语言的符号强化学习技术，在软件在环测试环境中进行验证。

Result: 实验结果显示模块集成稳定、互操作性良好，能够成功在BDI驱动和符号RL驱动规划阶段之间转换，任务效率按行程距离计算提高了75%。

Conclusion: 该研究为处理复杂无人机任务打下了坚实基础，并提出了进一步提升和验证的方向。

Abstract: Modern autonomous drone missions increasingly require software frameworks
capable of seamlessly integrating structured symbolic planning with adaptive
reinforcement learning (RL). Although traditional rule-based architectures
offer robust structured reasoning for drone autonomy, their capabilities fall
short in dynamically complex operational environments that require adaptive
symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition
Language (PDDL), explicitly integrates domain-specific knowledge and
operational constraints, significantly improving the reliability and safety of
unmanned aerial vehicle (UAV) decision making. In this study, we propose the
AMAD-SRL framework, an extended and refined version of the Autonomous Mission
Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with
symbolic reinforcement learning for dynamic mission planning and execution. We
validated our framework in a Software-in-the-Loop (SIL) environment structured
identically to an intended Hardware-In-the-Loop Simulation (HILS) platform,
ensuring seamless transition to real hardware. Experimental results demonstrate
stable integration and interoperability of modules, successful transitions
between BDI-driven and symbolic RL-driven planning phases, and consistent
mission performance. Specifically, we evaluate a target acquisition scenario in
which the UAV plans a surveillance path followed by a dynamic reentry path to
secure the target while avoiding threat zones. In this SIL evaluation, mission
efficiency improved by approximately 75% over a coverage-based baseline,
measured by travel distance reduction. This study establishes a robust
foundation for handling complex UAV missions and discusses directions for
further enhancement and validation.

</details>


### [12] [PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting](https://arxiv.org/abs/2508.12395)
*Zihan Wang*

Main category: cs.RO

TL;DR: 这篇论文提出了一种采用等离子风推进的超静音氯气航天器(PUB)，通过模块化推进系统和闭环滑移控制实现了高机动性和超低噪音的飞行。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统旋翼推进噪音大、结构复杂的问题，提出一种无机械推进器的超静音氯气航天器方案，适用于噪音敏感、封闭式和近空间应用场景。

Method: 采用氯气提供升力平台，四层环形非对称电容器生成离子风推力，模块化推进单元支持灵活配置，两度自由度头部实现推力向量控制，集成闭环滑移控制方案保证稳定机动。

Result: 飞行实验证明系统具备全包线能力，包括起飞、爬升、悬停、降落和平滑着陆，验证了等离子向量推进的可行性、向量控制的有效性以及控制系统的稳定性。

Conclusion: PUB系统通过等离子推进技术实现了超低噪音、结构简单和高机动性，在噪音敏感、封闭环境和近空间应用中具有重要价值和应用潜力。

Abstract: This study presents the design and control of a Plasma-propelled
Ultra-silence Blimp (PUB), a novel aerial robot employing plasma vector
propulsion for ultra-quiet flight without mechanical propellers. The system
utilizes a helium-lift platform for extended endurance and a four-layer ring
asymmetric capacitor to generate ionic wind thrust. The modular propulsion
units allow flexible configuration to meet mission-specific requirements, while
a two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop
slip control scheme is implemented for stable maneuvering. Flight experiments
demonstrate full-envelope capability, including take-off, climb, hover,
descent, and smooth landing, confirming the feasibility of plasma vector
propulsion, the effectiveness of DOF vector control, and the stability of the
control system. Owing to its low acoustic signature, structural simplicity, and
high maneuverability, PUB is well suited for noise-sensitive, enclosed, and
near-space applications.

</details>


### [13] [OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation](https://arxiv.org/abs/2508.11898)
*Jilei Mao,Jiarui Guan,Yingjuan Tang,Qirui Hu,Zhihang Li,Junjie Yu,Yongjie Mao,Yunzhe Sun,Shuang Liu,Xiaozhu Ju*

Main category: cs.RO

TL;DR: OmniD是一个多视角融合框架，通过可变形注意力机制将多视角图像合成为统一的鸟瞰图表示，显著提升了视觉运动策略在分布内外和少样本场景下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉运动策略容易在训练数据上过拟合（如固定相机位置和背景），导致在分布外场景泛化能力差，且难以有效融合多视角信息生成3D表示。

Method: 提出Omni-Vision Diffusion Policy (OmniD)框架，使用基于可变形注意力的Omni-Feature Generator (OFG)选择性提取任务相关特征，抑制视角特定噪声和背景干扰，合成统一的鸟瞰图表示。

Result: 在分布内、分布外和少样本实验中，OmniD相比最佳基线模型分别平均提升了11%、17%和84%的性能。

Conclusion: OmniD通过有效的多视角融合和特征选择机制，显著提升了视觉运动策略的泛化能力，为解决过拟合和多视角信息融合问题提供了有效方案。

Abstract: The visuomotor policy can easily overfit to its training datasets, such as
fixed camera positions and backgrounds. This overfitting makes the policy
perform well in the in-distribution scenarios but underperform in the
out-of-distribution generalization. Additionally, the existing methods also
have difficulty fusing multi-view information to generate an effective 3D
representation. To tackle these issues, we propose Omni-Vision Diffusion Policy
(OmniD), a multi-view fusion framework that synthesizes image observations into
a unified bird's-eye view (BEV) representation. We introduce a deformable
attention-based Omni-Feature Generator (OFG) to selectively abstract
task-relevant features while suppressing view-specific noise and background
distractions. OmniD achieves 11\%, 17\%, and 84\% average improvement over the
best baseline model for in-distribution, out-of-distribution, and few-shot
experiments, respectively. Training code and simulation benchmark are
available: https://github.com/1mather/omnid.git

</details>


### [14] [MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA](https://arxiv.org/abs/2508.12729)
*Junhao Ye,Cheng Hu,Yiqin Wang,Weizhan Huang,Nicolas Baumann,Jie He,Meixun Qu,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: MCTR算法通过曲率校正移动平均提高轨迹平滑度，并在CARLA模拟器中实现数字孪生系统，解决了DTR算法轨迹不平滑和F1TENTH模拟器缺乏3D LiDAR支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Follow-The-Gap和DTR算法在自主赛车中存在轨迹不够平滑的问题，且常用F1TENTH模拟器缺乏3D LiDAR感知支持，限制了算法的真实环境测试效果。

Method: 提出MCTR算法，使用曲率校正移动平均(CCMA)改进轨迹平滑度，并在CARLA模拟器中构建数字孪生系统来验证3D LiDAR感知下的算法鲁棒性。

Result: 算法通过仿真和真实车辆实验得到充分验证，证明了其在3D LiDAR感知环境下的有效性和鲁棒性。

Conclusion: MCTR算法成功解决了自主赛车中轨迹平滑度和3D感知验证的问题，为反应式控制器提供了更好的性能表现。

Abstract: In autonomous racing, reactive controllers eliminate the computational burden
of the full See-Think-Act autonomy stack by directly mapping sensor inputs to
control actions. This bypasses the need for explicit localization and
trajectory planning. A widely adopted baseline in this category is the
Follow-The-Gap method, which performs trajectory planning using LiDAR data.
Building on FTG, the Delaunay Triangulation-based Racing algorithm introduces
further enhancements. However, DTR's use of circumcircles for trajectory
generation often results in insufficiently smooth paths, ultimately degrading
performance. Additionally, the commonly used F1TENTH-simulator for autonomous
racing competitions lacks support for 3D LiDAR perception, limiting its
effectiveness in realistic testing. To address these challenges, this work
proposes the MCTR algorithm. MCTR improves trajectory smoothness through the
use of Curvature Corrected Moving Average and implements a digital twin system
within the CARLA simulator to validate the algorithm's robustness under 3D
LiDAR perception. The proposed algorithm has been thoroughly validated through
both simulation and real-world vehicle experiments.

</details>


### [15] [Control of Legged Robots using Model Predictive Optimized Path Integral](https://arxiv.org/abs/2508.11917)
*Hossein Keshavarz,Alejandro Ramirez-Serrano,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出MPOPI算法，结合MPPI、CE和CMA方法，用于四足机器人实时全身运动控制，相比传统MPPI算法具有更好的样本效率和运动性能


<details>
  <summary>Details</summary>
Motivation: 腿式机器人在复杂非结构化环境中具有独特优势，但目前性能仍不及自然系统。采样预测控制器显示出良好前景，需要提高样本效率和实时控制能力

Method: 采用基于采样的模型预测策略，结合模型预测路径积分(MPPI)、交叉熵(CE)和协方差矩阵自适应(CMA)方法，开发MPOPI算法

Result: MPOPI算法表现出更高的样本效率，能用更少样本获得优于传统MPPI算法的运动效果。在四足机器人多种场景的仿真实验中证明可作为随时控制策略

Conclusion: MPOPI结合MPPI、CE和CMA的优势，能够有效提升腿式机器人的运动能力，每次迭代都能提高运动性能，是一种有效的实时控制方法

Abstract: Legged robots possess a unique ability to traverse rough terrains and
navigate cluttered environments, making them well-suited for complex,
real-world unstructured scenarios. However, such robots have not yet achieved
the same level as seen in natural systems. Recently, sampling-based predictive
controllers have demonstrated particularly promising results. This paper
investigates a sampling-based model predictive strategy combining model
predictive path integral (MPPI) with cross-entropy (CE) and covariance matrix
adaptation (CMA) methods to generate real-time whole-body motions for legged
robots across multiple scenarios. The results show that combining the benefits
of MPPI, CE and CMA, namely using model predictive optimized path integral
(MPOPI), demonstrates greater sample efficiency, enabling robots to attain
superior locomotion results using fewer samples when compared to typical MPPI
algorithms. Extensive simulation experiments in multiple scenarios on a
quadruped robot show that MPOPI can be used as an anytime control strategy,
increasing locomotion capabilities at each iteration.

</details>


### [16] [Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors](https://arxiv.org/abs/2508.13151)
*Yuying Zhang,Joni Pajarinen*

Main category: cs.RO

TL;DR: 提出基于强化学习的方法解决移动机器人在动态环境中需要先操纵障碍物再导航的问题，结合可操纵性先验和功能映射来选择高质量操纵动作，在模拟和真实Spot机器人上验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统方法将导航和操纵作为独立任务处理，在需要先清除障碍物才能导航的'操纵以导航'场景中经常失败，需要主动与环境交互来清理障碍物同时确保足够的移动空间

Method: 强化学习方法结合可操纵性先验（关注高可操纵性体位）和功能映射（选择高质量操纵动作），减少不必要的探索，更有效地学习操纵策略

Result: 在两个新的模拟任务（Reach和Door）中验证方法有效性，并将学习到的策略成功迁移到真实Boston Dynamics Spot机器人上执行Reach任务

Conclusion: 该方法使机器人能够有效与动态环境交互并穿越，解决了操纵以导航的问题

Abstract: Mobile manipulation in dynamic environments is challenging due to movable
obstacles blocking the robot's path. Traditional methods, which treat
navigation and manipulation as separate tasks, often fail in such
'manipulate-to-navigate' scenarios, as obstacles must be removed before
navigation. In these cases, active interaction with the environment is required
to clear obstacles while ensuring sufficient space for movement. To address the
manipulate-to-navigate problem, we propose a reinforcement learning-based
approach for learning manipulation actions that facilitate subsequent
navigation. Our method combines manipulability priors to focus the robot on
high manipulability body positions with affordance maps for selecting
high-quality manipulation actions. By focusing on feasible and meaningful
actions, our approach reduces unnecessary exploration and allows the robot to
learn manipulation strategies more effectively. We present two new
manipulate-to-navigate simulation tasks called Reach and Door with the Boston
Dynamics Spot robot. The first task tests whether the robot can select a good
hand position in the target area such that the robot base can move effectively
forward while keeping the end effector position fixed. The second task requires
the robot to move a door aside in order to clear the navigation path. Both of
these tasks need first manipulation and then navigating the base forward.
Results show that our method allows a robot to effectively interact with and
traverse dynamic environments. Finally, we transfer the learned policy to a
real Boston Dynamics Spot robot, which successfully performs the Reach task.

</details>


### [17] [ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models](https://arxiv.org/abs/2508.11918)
*Zhichen Lou,Kechun Xu,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: ExploreVLM是一个基于视觉语言模型的闭环任务规划框架，通过逐步反馈机制实现实时计划调整和交互式探索，在探索型任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能的发展，机器人需要理解高级指令、规划任务并在动态环境中感知适应。现有VLM方法在交互探索、精确感知和实时计划调整方面存在不足。

Method: 提出双阶段任务规划器（带自反思机制）、对象中心空间关系图提供结构化场景表示、执行验证器形成闭环系统，支持实时重新规划。

Result: 大量真实世界实验表明，ExploreVLM显著优于最先进基线方法，特别是在探索型任务中表现突出。消融研究验证了反思规划器和结构化感知的关键作用。

Conclusion: ExploreVLM通过闭环反馈机制和结构化感知表示，有效解决了VLM在机器人任务规划中的交互探索和实时适应挑战，实现了鲁棒高效的任务执行。

Abstract: The advancement of embodied intelligence is accelerating the integration of
robots into daily life as human assistants. This evolution requires robots to
not only interpret high-level instructions and plan tasks but also perceive and
adapt within dynamic environments. Vision-Language Models (VLMs) present a
promising solution by combining visual understanding and language reasoning.
However, existing VLM-based methods struggle with interactive exploration,
accurate perception, and real-time plan adaptation. To address these
challenges, we propose ExploreVLM, a novel closed-loop task planning framework
powered by Vision-Language Models (VLMs). The framework is built around a
step-wise feedback mechanism that enables real-time plan adjustment and
supports interactive exploration. At its core is a dual-stage task planner with
self-reflection, enhanced by an object-centric spatial relation graph that
provides structured, language-grounded scene representations to guide
perception and planning. An execution validator supports the closed loop by
verifying each action and triggering re-planning. Extensive real-world
experiments demonstrate that ExploreVLM significantly outperforms
state-of-the-art baselines, particularly in exploration-centric tasks. Ablation
studies further validate the critical role of the reflective planner and
structured perception in achieving robust and efficient task execution.

</details>


### [18] [No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain](https://arxiv.org/abs/2508.11929)
*Mohitvishnu S. Gadde,Pranay Dugar,Ashish Malik,Alan Fern*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的全向双足运动学习框架，通过结合盲控制器和教师策略来训练学生策略，避免了仿真渲染的高计算成本，实现了高效的全向地形适应运动。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中实现有效的双足运动需要全向地形感知和相应的控制器，但传统仿真到现实的强化学习方法因全向深度图像渲染计算成本过高而不实用。

Method: 结合鲁棒的盲控制器和教师策略来监督基于视觉的学生策略，使用噪声增强的地形数据进行训练，避免RL过程中的渲染成本，并引入数据增强技术加速训练。

Result: 在仿真和真实世界测试中验证了框架有效性，展示了全向运动能力，对多样化地形具有良好的适应性，训练速度比传统方法快10倍。

Conclusion: 这是首个基于视觉的全向双足运动演示，展示了在最小化昂贵渲染依赖的情况下实现有效全向运动的能力，为动态环境中的双足机器人运动提供了实用解决方案。

Abstract: Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.

</details>


### [19] [Toward General Physical Intelligence for Resilient Agile Manufacturing Automation](https://arxiv.org/abs/2508.11960)
*Sandeep Kanta,Mehrdad Tavassoli,Varun Teja Chirkuri,Venkata Akhil Kumar,Santhi Bharath Punati,Praveen Damacharla,Sunny Katyara*

Main category: cs.RO

TL;DR: 这篇实践性综述系统性分析了基础模型在普遍物理智能(GPI)中的最新进展，尤其是视觉语言动作(VLA)模型在灵活制造中的应用潜力和挑战


<details>
  <summary>Details</summary>
Motivation: 尽管普遍物理智能(GPI)已在理论中描述，但其在当代灵活制造过程中的实际应用和发展角色仍未得到充分探索，需要桥接这一空白

Method: 通过系统性综述最近VLA模型在GPI背景下的进展，进行全面的比较分析，并通过结构化的消融研究评估其工业部署准备度

Result: 将最新技术进展组织为五个主题架构：多感知表征学习、模拟到实际转移、规划与控制、不确定性与安全措施、以及基准测试，识别了相关技术的工业应用潜力

Conclusion: 提出了开放性研究挑战，并建议了方向来更好地将GPI集成到符合工业5.0的下一代工业生态系统中

Abstract: Agile and human-centric manufacturing stipulates resilient robotic solutions
capable of contextual reasoning and safe interaction in unstructured
environments. Foundation models particularly the Vision Language Action (VLA)
models have emerged to fuse multimodal perception, reasoning and physically
grounded action across varied embodiments into unified representation, termed
as General Physical Intelligence (GPI). While GPI has already been described in
the literature but its practical application and evolving role in contemporary
agile manufacturing processes have yet to be duly explored. To bridge this gap,
this practical review systematically surveys recent advancements in VLA models
within GPI context, performs comprehensive comparative analysis of leading
implementations and evaluates their readiness for industrial deployment through
structured ablation study. Our analysis has organized state-of-the-art into
five thematic pillars including multisensory representation learning, sim2real
transfer, planning and control, uncertainty and safety measures and
benchmarking. Finally, we articulate open research challenges and propose
directions to better integrate GPI into next-generation industrial ecosystems
in line with Industry 5.0.

</details>


### [20] [Fully Spiking Actor-Critic Neural Network for Robotic Manipulation](https://arxiv.org/abs/2508.12038)
*Liwen Zhang,Heng Deng,Guanghui Sun*

Main category: cs.RO

TL;DR: 基于全空间刷新神经网络的混合课程强化学习框架，用于9自由度机器手的目标到达和抓取任务，具有低网络复杂度、低能耗和高效率特点


<details>
  <summary>Details</summary>
Motivation: 解决传统人工神经网络在机器手控制中的高能耗问题，利用空间刷新神经网络的低能耗特性来实现高效能的动态操控任务

Method: 简化SNN网络结构仅包含输入输出层，集成时间进度分区课程策略与PPO算法，引入能耗模型框架和动态两阶段奖励调整机制，优化观测空间

Result: 在Isaac Gym模拟平台上达到了优异性能，进行了与传统PPO和ANN基线的对比评估，验证了方法在动态机器人操控任务中的可扩展性和能量效率

Conclusion: 该混合课程强化学习框架通过简化SNN结构和优化策略，在保持高性能的同时显著降低了能耗，为资源受限环境下的机器人控制提供了有效解决方案

Abstract: This study proposes a hybrid curriculum reinforcement learning (CRL)
framework based on a fully spiking neural network (SNN) for 9-degree-of-freedom
robotic arms performing target reaching and grasping tasks. To reduce network
complexity and inference latency, the SNN architecture is simplified to include
only an input and an output layer, which shows strong potential for
resource-constrained environments. Building on the advantages of SNNs-high
inference speed, low energy consumption, and spike-based biological
plausibility, a temporal progress-partitioned curriculum strategy is integrated
with the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy
consumption modeling framework is introduced to quantitatively compare the
theoretical energy consumption between SNNs and conventional Artificial Neural
Networks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized
observation space further improve learning efficiency and policy accuracy.
Experiments on the Isaac Gym simulation platform demonstrate that the proposed
method achieves superior performance under realistic physical constraints.
Comparative evaluations with conventional PPO and ANN baselines validate the
scalability and energy efficiency of the proposed approach in dynamic robotic
manipulation tasks.

</details>


### [21] [Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs](https://arxiv.org/abs/2508.12043)
*Fei Lin,Tengchao Zhang,Qinghua Ni,Jun Huang,Siji Ma,Yonglin Tian,Yisheng Lv,Naiqi Wu*

Main category: cs.RO

TL;DR: 论文探索LLM驱动的无人机群在带宽受限条件下实现语义压缩通信的可行性，通过构建不同复杂度的2D仿真场景，评估9种主流LLM的语义压缩性能。


<details>
  <summary>Details</summary>
Motivation: 无人机群采用大语言模型后语义理解和自主任务执行能力显著提升，但有限通信带宽和高频交互需求对语义信息传输构成严峻挑战。

Method: 构建四种不同环境复杂度的2D仿真场景，设计集成系统提示和任务指令提示的通信-执行流水线，系统评估9种主流LLM在不同场景下的语义压缩性能。

Result: 实验结果表明，基于LLM的无人机群有潜力在带宽受限和多跳链路条件下实现高效的协作通信。

Conclusion: LLM驱动的无人机群能够通过语义压缩有效减少通信负载，同时保持关键任务语义，在复杂环境下表现出良好的适应性和稳定性。

Abstract: The rapid adoption of Large Language Models (LLMs) in unmanned systems has
significantly enhanced the semantic understanding and autonomous task execution
capabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited
communication bandwidth and the need for high-frequency interactions pose
severe challenges to semantic information transmission within the swarm. This
paper explores the feasibility of LLM-driven UAV swarms for autonomous semantic
compression communication, aiming to reduce communication load while preserving
critical task semantics. To this end, we construct four types of 2D simulation
scenarios with different levels of environmental complexity and design a
communication-execution pipeline that integrates system prompts with task
instruction prompts. On this basis, we systematically evaluate the semantic
compression performance of nine mainstream LLMs in different scenarios and
analyze their adaptability and stability through ablation studies on
environmental complexity and swarm size. Experimental results demonstrate that
LLM-based UAV swarms have the potential to achieve efficient collaborative
communication under bandwidth-constrained and multi-hop link conditions.

</details>


### [22] [OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments](https://arxiv.org/abs/2508.12071)
*Amy Phung,Richard Camilli*

Main category: cs.RO

TL;DR: OASIS是一种实时水下3D重建方法，通过融合光学相机和声纳数据，结合体素雕刻技术，实现非结构化水下工作空间的实时三维重建


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注离线重建，而实时空间感知对于自主和有人驾驶水下车辆操作至关重要，需要开发实时重建技术

Method: 采用"眼在手"配置，利用机器人机械臂的灵活性在短基线上捕获多个工作空间视图，融合光学图像和声纳数据，结合体素雕刻技术

Result: 通过水箱实验验证，提供了定性和定量结果，证明其对水下操作任务的有效性

Conclusion: OASIS方法能够实现实时水下3D重建，为水下操作任务提供了实用的解决方案

Abstract: High resolution underwater 3D scene reconstruction is crucial for various
applications, including construction, infrastructure maintenance, monitoring,
exploration, and scientific investigation. Prior work has leveraged the
complementary sensing modalities of imaging sonars and optical cameras for
opti-acoustic 3D scene reconstruction, demonstrating improved results over
methods which rely solely on either sensor. However, while most existing
approaches focus on offline reconstruction, real-time spatial awareness is
essential for both autonomous and piloted underwater vehicle operations. This
paper presents OASIS, an opti-acoustic fusion method that integrates data from
optical images with voxel carving techniques to achieve real-time 3D
reconstruction unstructured underwater workspaces. Our approach utilizes an
"eye-in-hand" configuration, which leverages the dexterity of robotic
manipulator arms to capture multiple workspace views across a short baseline.
We validate OASIS through tank-based experiments and present qualitative and
quantitative results that highlight its utility for underwater manipulation
tasks.

</details>


### [23] [Into the Wild: When Robots Are Not Welcome](https://arxiv.org/abs/2508.12075)
*Shaul Ashkenazi,Gabriel Skantze,Jane Stuart-Smith,Mary Ellen Foster*

Main category: cs.RO

TL;DR: 社交机器人在公共空间部署遇到的困难和反对声音，但最终通过建立信任关系成功完成部署


<details>
  <summary>Details</summary>
Motivation: 研究社交机器人在公共空间部署时遇到的挑战，包括技术难题、意外用户语言和相关方反对

Method: 在两个公共场景中部署社交机器人：1学生服务中心；2雷渌湾和寻求宵报者临时服务点

Result: 尽管遇到困难，最终成功获得员工信任并建立良好关系，完成机器人部署和研究

Conclusion: 在公共空间部署社交机器人需要先获得相关方的信任和支持，建立良好关系是成功部署的关键因素

Abstract: Social robots are increasingly being deployed in public spaces, where they
face not only technological difficulties and unexpected user utterances, but
also objections from stakeholders who may not be comfortable with introducing a
robot into those spaces. We describe our difficulties with deploying a social
robot in two different public settings: 1) Student services center; 2) Refugees
and asylum seekers drop-in service. Although this is a failure report, in each
use case we eventually managed to earn the trust of the staff and form a
relationship with them, allowing us to deploy our robot and conduct our
studies.

</details>


### [24] [Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)](https://arxiv.org/abs/2508.12170)
*Aryan Gupta*

Main category: cs.RO

TL;DR: 这是一份2020-2024年软件层面机器人能源效率研究的系统性文献综述，分析了79份研究，发现运动/轨迹优化是主要技术，电机/执行器是主要能源消耗者，并提出了最小报告检查单和研究机遇。


<details>
  <summary>Details</summary>
Motivation: 更新和扩展2020年之前的证据，系统评估软件层面的机器人能源效率技术，以提高研究可比性和指导未来研究方向。

Method: 采用自动化但审计的流水线，结合Google Scholar种子、向后/向前雪球投放、以及大语言模型辅助进行筛选和数据提取，每个自动步骤有10%的人工审计。

Result: 分析了79份同行审查研究，发现工业设置占主导(31.6%)，电机/执行器是主要能源消耗者(68.4%)，运动和轨迹优化是主要技术(69.6%)，但报告标准异质性限制了可比性。

Conclusion: 研究提出了最小报告检查单（包括总能消耗、平均功率、任务标准化指标和清晰基准），并强调了跨层设计和量化非性能交易的机遇。

Abstract: This study presents a systematic literature review of software-level
approaches to energy efficiency in robotics published from 2020 through 2024,
updating and extending pre-2020 evidence. An automated-but-audited pipeline
combined Google Scholar seeding, backward/forward snowballing, and
large-language-model (LLM) assistance for screening and data extraction, with
~10% human audits at each automated step and consensus-with-tie-breaks for
full-text decisions. The final corpus comprises 79 peer-reviewed studies
analyzed across application domain, metrics, evaluation type, energy models,
major energy consumers, software technique families, and energy-quality
trade-offs. Industrial settings dominate (31.6%) followed by exploration
(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of
studies, with computing/controllers a distant second (13.9%). Simulation-only
evaluations remain most common (51.9%), though hybrid evaluations are frequent
(25.3%). Representational (physics-grounded) energy models predominate (87.3%).
Motion and trajectory optimization is the leading technique family (69.6%),
often paired with learning/prediction (40.5%) and computation
allocation/scheduling (26.6%); power management/idle control (11.4%) and
communication/data efficiency (3.8%) are comparatively underexplored. Reporting
is heterogeneous: composite objectives that include energy are most common,
while task-normalized and performance-per-energy metrics appear less often,
limiting cross-paper comparability. The review offers a minimal reporting
checklist (e.g., total energy and average power plus a task-normalized metric
and clear baselines) and highlights opportunities in cross-layer designs and in
quantifying non-performance trade-offs (accuracy, stability). A replication
package with code, prompts, and frozen datasets accompanies the review.

</details>


### [25] [Humanoid Motion Scripting with Postural Synergies](https://arxiv.org/abs/2508.12184)
*Rhea Malhotra,William Chong,Catie Cuan,Oussama Khatib*

Main category: cs.RO

TL;DR: SynSculptor是一个基于姿态协同的人形机器人运动分析与编辑框架，通过提取人体运动的主要协同模式，实现无需训练的人形运动生成。


<details>
  <summary>Details</summary>
Motivation: 为人形机器人生成类人运动序列面临参考运动采集分析困难、新运动合成复杂以及运动映射到机器人上的挑战，需要一种无需训练的运动生成方法。

Method: 收集3+小时20人的运动捕捉数据，使用PCA提取速度轨迹的主要姿态协同，构建风格条件协同库，通过运动-语言变换器实现基于协同的运动适应。

Result: 开发了基于脚滑动比、总动量和动能偏差的运动平滑度评估指标，生成的运动与参考运动进行比较验证。

Conclusion: SynSculptor框架成功利用姿态协同实现了人形机器人的类人运动生成和编辑，为实时运动控制提供了有效解决方案。

Abstract: Generating sequences of human-like motions for humanoid robots presents
challenges in collecting and analyzing reference human motions, synthesizing
new motions based on these reference motions, and mapping the generated motion
onto humanoid robots. To address these issues, we introduce SynSculptor, a
humanoid motion analysis and editing framework that leverages postural
synergies for training-free human-like motion scripting. To analyze human
motion, we collect 3+ hours of motion capture data across 20 individuals where
a real-time operational space controller mimics human motion on a simulated
humanoid robot. The major postural synergies are extracted using principal
component analysis (PCA) for velocity trajectories segmented by changes in
robot momentum, constructing a style-conditioned synergy library for free-space
motion generation. To evaluate generated motions using the synergy library, the
foot-sliding ratio and proposed metrics for motion smoothness involving total
momentum and kinetic energy deviations are computed for each generated motion,
and compared with reference motions. Finally, we leverage the synergies with a
motion-language transformer, where the humanoid, during execution of motion
tasks with its end-effectors, adapts its posture based on the chosen synergy.
Supplementary material, code, and videos are available at
https://rhea-mal.github.io/humanoidsynergies.io.

</details>


### [26] [Self-Guided Action Diffusion](https://arxiv.org/abs/2508.12189)
*Rhea Malhotra,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 自我导向动作液散方法，通过在每个液散步骤使用之前决策来导向建议分布，以更高效地提升液散策略的一致性和反应性。


<details>
  <summary>Details</summary>
Motivation: 虽然双向解码方法在提升液散策略的一致性和反应性方面显示了潜力，但随着动作采样多样性的增加，该方法计算成本过高。需要一种更高效的推理时搜索方法。

Method: 提出自我导向动作液散方法，在每个液散步骤中，基于之前的决策来导向建议分布，从而减少计算开销。

Result: 在模拟任务中，该方法在可忽略的推理成本下实现了近优性能。在严格的采样预算下，在具有挑战性的动态任务上比现有方法获得了较高成功率。

Conclusion: 自我导向动作液散方法能够在保持高性能的同时显著降低计算成本，为液散基于策略的推理时优化提供了一种高效的解决方案。

Abstract: Recent works have shown the promise of inference-time search over action
samples for improving generative robot policies. In particular, optimizing
cross-chunk coherence via bidirectional decoding has proven effective in
boosting the consistency and reactivity of diffusion policies. However, this
approach remains computationally expensive as the diversity of sampled actions
grows. In this paper, we introduce self-guided action diffusion, a more
efficient variant of bidirectional decoding tailored for diffusion-based
policies. At the core of our method is to guide the proposal distribution at
each diffusion step based on the prior decision. Experiments in simulation
tasks show that the proposed self-guidance enables near-optimal performance at
negligible inference cost. Notably, under a tight sampling budget, our method
achieves up to 70% higher success rates than existing counterparts on
challenging dynamic tasks. See project website at
https://rhea-mal.github.io/selfgad.github.io.

</details>


### [27] [Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search](https://arxiv.org/abs/2508.12211)
*Cyrus Neary,Omar G. Younis,Artur Kuramshin,Ozgur Aslan,Glen Berseth*

Main category: cs.RO

TL;DR: VLAPS是一个将基于模型的搜索嵌入预训练VLA模型推理过程的新框架，通过结合蒙特卡洛树搜索和环境模型来提升机器人任务性能，显著优于纯VLA基线方法。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉-语言-动作(VLA)模型在零样本部署时容易产生脆弱行为或不安全故障，特别是在分布外场景中。需要一种方法来提高VLA模型在机器人任务中的鲁棒性和性能。

Method: 提出VLAPS框架，将改进的蒙特卡洛树搜索(MCTS)算法嵌入VLA策略推理过程。使用环境模型进行搜索，并用VLA策略定义的动作先验来引导搜索过程。

Result: 在所有实验中，VLAPS显著优于纯VLA基线方法，在语言指定任务上的成功率最高提升了67个百分点，解决了原本对无信息搜索算法不可行的问题。

Conclusion: VLAPS提供了一个原则性框架，可以控制VLA模型的测试时计算、利用机器人环境的先验知识，并将成熟的规划和强化学习技术集成到VLA推理过程中。

Abstract: Pre-trained vision-language-action (VLA) models offer a promising foundation
for generalist robot policies, but often produce brittle behaviours or unsafe
failures when deployed zero-shot in out-of-distribution scenarios. We present
Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and
accompanying algorithms that embed model-based search into the inference
procedure of pre-trained VLA policies to improve their performance on robotic
tasks. Specifically, our method biases a modified Monte Carlo Tree Search
(MCTS) algorithm -- run using a model of the target environment -- using action
priors defined by the VLA policy. By using VLA-derived abstractions and priors
in model-based search, VLAPS efficiently explores language-conditioned robotics
tasks whose search spaces would otherwise be intractably large. Conversely, by
integrating model-based search with the VLA policy's inference procedure, VLAPS
yields behaviours that are more performant than those obtained by directly
following the VLA policy's action predictions. VLAPS offers a principled
framework to: i) control test-time compute in VLA models, ii) leverage a priori
knowledge of the robotic environment, and iii) integrate established planning
and reinforcement learning techniques into the VLA inference process. Across
all experiments, VLAPS significantly outperforms VLA-only baselines on
language-specified tasks that would otherwise be intractable for uninformed
search algorithms, increasing success rates by as much as 67 percentage points.

</details>


### [28] [Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids](https://arxiv.org/abs/2508.12252)
*Kaizhe Hu,Haochen Shi,Yao He,Weizhuo Wang,C. Karen Liu,Shuran Song*

Main category: cs.RO

TL;DR: 提出了Robot-Trains-Robot (RTR)框架，通过机械臂教师主动指导人形机器人学生，实现高效的长时真实世界人形机器人训练，并提出了新的RL管道来促进和稳定sim-to-real迁移。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界人形机器人强化学习面临的安全性、奖励设计和学习效率等挑战，克服sim-to-real差距，充分发挥人形机器人的潜力。

Method: RTR框架：机械臂教师提供保护、学习计划、奖励、扰动、故障检测和自动重置；提出新的RL管道，通过优化真实世界中的单个动力学编码潜在变量来促进sim-to-real迁移。

Result: 在两个具有挑战性的真实世界人形任务中验证了方法：精确速度跟踪的行走策略微调，以及从零开始学习人形摆动任务。

Conclusion: RTR系统展示了实现真实世界人形机器人学习的有前景能力，为克服sim-to-real差距提供了有效解决方案。

Abstract: Simulation-based reinforcement learning (RL) has significantly advanced
humanoid locomotion tasks, yet direct real-world RL from scratch or adapting
from pretrained policies remains rare, limiting the full potential of humanoid
robots. Real-world learning, despite being crucial for overcoming the
sim-to-real gap, faces substantial challenges related to safety, reward design,
and learning efficiency. To address these limitations, we propose
Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher
actively supports and guides a humanoid robot student. The RTR system provides
protection, learning schedule, reward, perturbation, failure detection, and
automatic resets. It enables efficient long-term real-world humanoid training
with minimal human intervention. Furthermore, we propose a novel RL pipeline
that facilitates and stabilizes sim-to-real transfer by optimizing a single
dynamics-encoded latent variable in the real world. We validate our method
through two challenging real-world humanoid tasks: fine-tuning a walking policy
for precise speed tracking and learning a humanoid swing-up task from scratch,
illustrating the promising capabilities of real-world humanoid learning
realized by RTR-style systems. See https://robot-trains-robot.github.io/ for
more info.

</details>


### [29] [Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments](https://arxiv.org/abs/2508.12274)
*Jian Zhao,Yunlong Lian,Andy M Tyrrell,Michael Gienger,Jihong Zhu*

Main category: cs.RO

TL;DR: 提出了一种适用于穿紧身服装的双手机器人穿衣策略，通过球坐标系和模建方法实现了适应不同人手臂姿势的穿衣轨迹生成


<details>
  <summary>Details</summary>
Motivation: 当前机器人辅助穿衣研究主要集中在松身服装，对紧身服装关注少，单手机器人穿紧身服装容易失败

Method: 建立球坐标系统作为穿衣任务的表示，使用方位角作为双手操作的任务相关特征，采用GMM和GMR进行仿真学习生成适应性穿衣轨迹

Result: 通过多种实验验证了所提方法的有效性

Conclusion: 该双手穿衣策略能够有效解决紧身服装穿着的挑战，为机器人辅助穿衣领域提供了新的解决方案

Abstract: Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.

</details>


### [30] [A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts](https://arxiv.org/abs/2508.12296)
*Bin Wang,Jiwen Zhang,Song Wang,Dan Wu*

Main category: cs.RO

TL;DR: 提出了一种基于力-视觉融合控制器驱动的强化学习方法(FVFC-MTRL)和多任务强化学习训练方法，用于解决机器人批量精密装配中配合类型和配合量不确定的问题，通过多教师策略蒸馏整合多个控制策略，实现鲁棒装配控制。


<details>
  <summary>Details</summary>
Motivation: 在高精度工业应用中，机器人执行精密装配任务时，由于加工误差导致销孔配合类型（间隙配合或过盈配合）和配合量存在不确定性，需要开发鲁棒的顺应性装配控制策略。

Method: 将批量精密装配任务分解为多个确定性子任务，采用力-视觉融合控制器驱动的强化学习方法和多任务强化学习训练方法(FVFC-MTRL)联合学习多个顺应控制策略，然后通过多教师策略蒸馏方法将这些策略整合到统一的学生网络中。

Result: 实验表明该方法成功构建了适用于不同配合类型和配合量的鲁棒控制策略，MTRL框架显著提高了训练效率，最终的控制策略在力顺应性和成功率方面优于现有方法。

Conclusion: 提出的FVFC-MTRL方法能够有效处理装配任务中的不确定性，实现高性能的鲁棒精密装配控制，为工业机器人精密装配提供了有效的解决方案。

Abstract: In some high-precision industrial applications, robots are deployed to
perform precision assembly tasks on mass batches of manufactured pegs and
holes. If the peg and hole are designed with transition fit, machining errors
may lead to either a clearance or an interference fit for a specific pair of
components, with uncertain fit amounts. This paper focuses on the robotic batch
precision assembly task involving components with uncertain fit types and fit
amounts, and proposes an efficient methodology to construct the robust and
compliant assembly control strategy. Specifically, the batch precision assembly
task is decomposed into multiple deterministic subtasks, and a force-vision
fusion controller-driven reinforcement learning method and a multi-task
reinforcement learning training method (FVFC-MTRL) are proposed to jointly
learn multiple compliance control strategies for these subtasks. Subsequently,
the multi-teacher policy distillation approach is designed to integrate
multiple trained strategies into a unified student network, thereby
establishing a robust control strategy. Real-world experiments demonstrate that
the proposed method successfully constructs the robust control strategy for
high-precision assembly task with different fit types and fit amounts.
Moreover, the MTRL framework significantly improves training efficiency, and
the final developed control strategy achieves superior force compliance and
higher success rate compared with many existing methods.

</details>


### [31] [Implementation and evaluation of a prediction algorithm for an autonomous vehicle](https://arxiv.org/abs/2508.12312)
*Marco Leon Rapp*

Main category: cs.RO

TL;DR: 提出了一种基于动态自行车模型的车辆轨迹预测算法，每5毫秒预测一次，比运动学模型精度高82.6%，位置偏差仅为每米1.25厘米


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶车辆开发高精度的实时轨迹预测算法，解决传统运动学模型在高速下精度不足的问题

Method: 使用动态自行车模型，通过实验确定车辆参数（质量、重心、转动惯量、侧偏刚度），采用光学位置跟踪测量侧偏刚度，将模型集成到扩展卡尔曼滤波器中，并在ROS中用C++实现

Result: 算法在整个测试过程中位置偏差仅为每米1.25厘米，比运动学模型精度提高82.6%，在高速下动态模型表现更优

Conclusion: 动态自行车模型比运动学模型更适合自动驾驶车辆的轨迹预测，特别是在高速情况下，提出的测量方法和算法实现能够实现高精度的实时预测

Abstract: This paper presents a prediction algorithm that estimates the vehicle
trajectory every five milliseconds for an autonomous vehicle. A kinematic and a
dynamic bicycle model are compared, with the dynamic model exhibiting superior
accuracy at higher speeds. Vehicle parameters such as mass, center of gravity,
moment of inertia, and cornering stiffness are determined experimentally. For
cornering stiffness, a novel measurement procedure using optical position
tracking is introduced. The model is incorporated into an extended Kalman
filter and implemented in a ROS node in C++. The algorithm achieves a
positional deviation of only 1.25 cm per meter over the entire test drive and
is up to 82.6% more precise than the kinematic model.

</details>


### [32] [SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning](https://arxiv.org/abs/2508.12394)
*Zichen Yan,Rui Huang,Lei He,Shao Guo,Lin Zhao*

Main category: cs.RO

TL;DR: 通过视觉强化学习和辅助任务训练，实现了无人机的图像目标导航，支持自主探索、障碍避免和目标寻找，无需全局定位或显式地图建模。


<details>
  <summary>Details</summary>
Motivation: 现有无人机导航方法主要关注参考跟踪或障碍避免，而图像目标导航对于无人机更具挑战性，需要高频率控制和全局定位来维持稳定飞行。

Method: 提出了一种模拟到实际的框架，利用视觉强化学习训练视觉背榜，包括图像批变和未来过渡预测等辅助任务。集成了深度基于的安全模块用于实时障碍避免。

Result: 算法能够实现端到端的图像目标导航，直接通过速度控制，无需外部定位系统。在杂乱环境中也能安全导航。

Conclusion: 该框架为无人机提供了全面的导航能力，支持自主探索、障碍避免和图像目标寻找，而无需显式的全局地图建模，促进了无人机在复杂环境中的自主导航发展。

Abstract: Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an
unknown environment and reaching a location that visually matches a given
target image. While prior works primarily study ImageNav for ground robots,
enabling this capability for autonomous drones is substantially more
challenging due to their need for high-frequency feedback control and global
localization for stable flight. In this paper, we propose a novel sim-to-real
framework that leverages visual reinforcement learning (RL) to achieve ImageNav
for drones. To enhance visual representation ability, our approach trains the
vision backbone with auxiliary tasks, including image perturbations and future
transition prediction, which results in more effective policy training. The
proposed algorithm enables end-to-end ImageNav with direct velocity control,
eliminating the need for external localization. Furthermore, we integrate a
depth-based safety module for real-time obstacle avoidance, allowing the drone
to safely navigate in cluttered environments. Unlike most existing drone
navigation methods that focus solely on reference tracking or obstacle
avoidance, our framework supports comprehensive navigation
behaviors--autonomous exploration, obstacle avoidance, and image-goal
seeking--without requiring explicit global mapping. Code and model checkpoints
will be released upon acceptance.

</details>


### [33] [Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots](https://arxiv.org/abs/2508.12435)
*Deqing Song,Weimin Yang,Maryam Rezayati,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: 本文探索仅使用机器人内置关节传感器的深度学习手势识别方法，无需外部传感器，通过CNN架构和频谱图表示实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 在HRC领域，传统手势识别依赖视觉或机器人皮肤等外部传感器，成本高且部署复杂。本研究旨在探索仅利用机器人内置关节传感器的解决方案，实现更经济、可扩展的触觉识别。

Method: 评估多种CNN架构，收集两个数据集研究数据表示和模型架构的影响。采用频谱图表示方法，开发STFT2DCNN和STT3DCNN模型，在Franka Emika Research机器人上实现。

Result: 频谱图表示显著提高识别精度（95%以上），模型架构影响较小。在机器人新姿态泛化测试中，频谱图模型表现更优。成功实现接触检测和手势分类。

Conclusion: 证明了无需外部传感器的触觉识别可行性，为HRC提供了成本效益高、可扩展的解决方案，推动了该领域的进一步研究。

Abstract: While gesture recognition using vision or robot skins is an active research
area in Human-Robot Collaboration (HRC), this paper explores deep learning
methods relying solely on a robot's built-in joint sensors, eliminating the
need for external sensors. We evaluated various convolutional neural network
(CNN) architectures and collected two datasets to study the impact of data
representation and model architecture on the recognition accuracy. Our results
show that spectrogram-based representations significantly improve accuracy,
while model architecture plays a smaller role. We also tested generalization to
new robot poses, where spectrogram-based models performed better. Implemented
on a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,
achieved over 95% accuracy in contact detection and gesture classification.
These findings demonstrate the feasibility of external-sensor-free tactile
recognition and promote further research toward cost-effective, scalable
solutions for HRC.

</details>


### [34] [Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2508.12439)
*Sunyu Wang,Arjun S. Lakshmipathy,Jean Oh,Nancy S. Pollard*

Main category: cs.RO

TL;DR: 扩展滚动滑动接触模型到流形网格，通过海线迹踪积分方案在网格上直接时间积分，支持高保真度的手势操控规划


<details>
  <summary>Details</summary>
Motivation: 现有滚动滑动接触研究主要集中在连续可微形状上，需要扩展到流形网格以支持更精细的真实几何表示

Method: 基于海线迹踪的积分方案，在网格上进行一阶时间积分，使用最小二乘优化器规划多指手势操控运动

Result: 在五个对象的手势操推模拟中，方法在准确性和精确度方面都超过了碰撞检测基准和基础形状基准，甚至在粗糕网格上也表现优异

Conclusion: 方法能够在网格上实现高保真度的表面接触模型，未来工作将重点研究多重接触和接触力的集成，以实现更准确和稳健的模型

Abstract: Reasoning about rolling and sliding contact, or roll-slide contact for short,
is critical for dexterous manipulation tasks that involve intricate geometries.
But existing works on roll-slide contact mostly focus on continuous shapes with
differentiable parametrizations. This work extends roll-slide contact modeling
to manifold meshes. Specifically, we present an integration scheme based on
geodesic tracing to first-order time-integrate roll-slide contact directly on
meshes, enabling dexterous manipulation to reason over high-fidelity discrete
representations of an object's true geometry. Using our method, we planned
dexterous motions of a multi-finger robotic hand manipulating five objects
in-hand in simulation. The planning was achieved with a least-squares optimizer
that strives to maintain the most stable instantaneous grasp by minimizing
contact sliding and spinning. Then, we evaluated our method against a baseline
using collision detection and a baseline using primitive shapes. The results
show that our method performed the best in accuracy and precision, even for
coarse meshes. We conclude with a future work discussion on incorporating
multiple contacts and contact forces to achieve accurate and robust mesh-based
surface contact modeling.

</details>


### [35] [Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics](https://arxiv.org/abs/2508.12456)
*Hadas C. Kuzmenko,David Ehevich,Oren Gal*

Main category: cs.RO

TL;DR: 基于MOOS-IvP平台的多机器人群智能系统结合液体时间常数神经网络(LTCNs)，实现油漆澄渍轨迹的高精度实时预测和动态响应


<details>
  <summary>Details</summary>
Motivation: 海洋油漆澄渍造成严重环境和经济风险，需要准确的实时轨迹预测和协调减少影响，但因风、流、温度等多因素交互而极其复杂

Method: 集成多机器人群智能系统(MOOS-IvP平台)与液体时间常数神经网络(LTCNs)，结合自适应机器学习与自主海洋机器人技术

Result: 在Deepwater Horizon漆澄数据上，LTC-RK4模型达到0.96空间准确度，超过LSTM方法23%，显著提升了预测精度、灵活性和操作可扩展性

Conclusion: 该研究通过先进神经建模与自主协同机器人技术的集成，大大提高了油漆澄渍管理的预测精度和响应协调能力，推进了可持续自主环境保护技术的发展

Abstract: Marine oil spills pose grave environmental and economic risks, threatening
marine ecosystems, coastlines, and dependent industries. Predicting and
managing oil spill trajectories is highly complex, due to the interplay of
physical, chemical, and environmental factors such as wind, currents, and
temperature, which makes timely and effective response challenging. Accurate
real-time trajectory forecasting and coordinated mitigation are vital for
minimizing the impact of these disasters. This study introduces an integrated
framework combining a multi-agent swarm robotics system built on the MOOS-IvP
platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system
fuses adaptive machine learning with autonomous marine robotics, enabling
real-time prediction, dynamic tracking, and rapid response to evolving oil
spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent
processes--the framework achieves real-time, high-accuracy forecasts of spill
movement. Swarm intelligence enables decentralized, scalable, and resilient
decision-making among robot agents, enhancing collective monitoring and
containment efforts. Our approach was validated using data from the Deepwater
Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,
surpassing LSTM approaches by 23%. The integration of advanced neural modeling
with autonomous, coordinated robotics demonstrates substantial improvements in
prediction precision, flexibility, and operational scalability. Ultimately,
this research advances the state-of-the-art for sustainable, autonomous oil
spill management and environmental protection by enhancing both trajectory
prediction and response coordination.

</details>


### [36] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: 基于YOLOv8实时检测和Kociemba算法的麻将魔方自动解法系统，通过三个步进电机实现物理操控，平均解法时间2.2分钟


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动检测麻将魔方状态并自动解决的系统，通过统一的GUI界面实时显示魔方状态和解法过程

Method: 使用YOLOv8模型进行实时魔斶状态检测（精度0.98443，回归0.98419），通过Kociemba算法求解，使用三个步进电机组合实现单自由度的物理操控

Result: 系统成功实现了麻将魔方的自动检测和解法，平均解法时间约2.2分钟，YOLOv8模型表现优异（Box Loss 0.42051, Class Loss 0.2611）

Conclusion: 该系统成功展示了基于计算机视觉和机械控制的麻将魔方自动解法方案，具有高精度检测和较快解法速度，Unity GUI提供了便捷的用户交互体验

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [37] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: PROD是一种通过触觉交互重建可变形物体形状和力学特性的新方法，使用弹性静力学SDF和力控表面探测来估计软材料的静态和动态响应


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖纯几何或视觉数据，无法有效估计可变形物体的力学特性，需要一种能够整合触觉交互信息的方法

Method: 将物体变形建模为弹性静力学过程，推导控制泊松方程从稀疏位姿和力测量估计SDF，结合稳态弹性动力学假设恢复未变形SDF

Result: PROD能够处理位姿误差、非垂直力施加和曲率误差，在模拟软体交互中表现出鲁棒性，并能通过分析位移响应估计材料刚度

Conclusion: PROD是重建可变形物体的强大工具，适用于机器人操作、医学成像和触觉反馈系统等应用

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [38] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: 事件相机多传感器系统的无标定物时间和旋转外参检定方法，通过正常流观测估计角速度，结合CCA初始化和连续时间SO(3)优化，达到了与标定物方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 事件相机作为一种低延迟传感器，在多传感器融合中具有重要价值，但相关外参检定方法研究较少。需要一种无需专门标定物的检定方法来支持事件相机为中心的多传感器系统。

Method: 提出基于运动的时间和旋转检定框架：1)通过正常流观测估计角速度，避免事件-帧转换；2)利用动力学相关性通过CCA方法初始化时间偏移和旋转外参；3)使用连续时间SO(3)参数化进行非线性优化精细调整所有参数。

Result: 在公开和自收数据集上的广泛评估显示，该方法达到了与基于标定物方法相当的检定精度，同时比纯CCA方法更稳定，并体现了高精度、验实性和灵活性。

Conclusion: 成功开发了一种无需标定物的事件相机多传感器系统外参检定方法，通过正常流角速度估计和两步优化流程，实现了高精度和稳定性。该方法为事件相机多传感器融合提供了可靠的检定解决方案，并将开源代码以促进相关研究。

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [39] [Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory](https://arxiv.org/abs/2508.12681)
*Johann Licher,Max Bartholdt,Henrik Krauss,Tim-Lukas Habich,Thomas Seel,Moritz Schappler*

Main category: cs.RO

TL;DR: 基于域解耦物理信息神经网络(DD-PINN)的非线性模型预测控制框架，实现了软连续体机器人的高速实时动态控制，稳定性和准确性显著提升。


<details>
  <summary>Details</summary>
Motivation: 软连续体机器人(SCRs)的动态控制具有广阔应用前景，但精确动态模型的高计算要求使其面临挑战。现有的数据驱动方法缺乏适应性且无法完整描述机器人形状，限制了实际应用。

Method: 提出基于域解耦物理信息神经网络(DD-PINN)的非线性模型预测控制框架。DD-PINN作为动态Cosserat柱模型的代理模型，速度提升44000倍，并在无味卡尔滤波器中用于估计模型状态和弯曲顺度。在GPU上实现频70Hz运行的非线性进化MPC。

Result: 在模拟中，控制器展现出准确的动态轨迹跟踪能力，终端执行器位置误差低于3mm(活动器长度的2.3%)。在实际实验中，控制器达到了相似的精度，并实现了超过3.55m/s²的加速度。

Conclusion: 该框架成功解决了SCRs动态控制的计算挑战，通过DD-PINN代理模型实现了高效计算，并在实时性和控制精度方面取得了显著成效，为软连续体机器人的应用扩展提供了可靠技术支撑。

Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for
expanding their applications, but remains a challenging problem due to the high
computational demands of accurate dynamic models. While data-driven approaches
like Koopman-operator-based methods have been proposed, they typically lack
adaptability and cannot capture the full robot shape, limiting their
applicability. This work introduces a real-time-capable nonlinear
model-predictive control (MPC) framework for SCRs based on a domain-decoupled
physics-informed neural network (DD-PINN) with adaptable bending stiffness. The
DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a
speed-up factor of 44000. It is also used within an unscented Kalman filter for
estimating the model states and bending compliance from end-effector position
measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the
GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories
and setpoint control with end-effector position errors below 3 mm (2.3% of the
actuator's length). In real-world experiments, the controller achieves similar
accuracy and accelerations up to 3.55 m/s2.

</details>


### [40] [RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph](https://arxiv.org/abs/2508.12916)
*Hecheng Wang,Jiankun Ren,Jia Yu,Lizhe Qi,Yunquan Sun*

Main category: cs.RO

TL;DR: RoboRetriever是一个仅使用单个手腕安装RGB-D相机和自然语言指令的机器人对象检索框架，通过动态层次场景图和主动感知实现真实世界的物体检索。


<details>
  <summary>Details</summary>
Motivation: 人类能够在杂乱、部分可观察的环境中轻松检索物体，而现有机器人系统通常需要多摄像头设置和完整场景可见性，限制了适应性和增加了硬件成本。

Method: 构建动态层次场景图编码对象语义、几何和关系；使用监督模块推理任务指令；集成主动感知、交互感知和操作；引入视觉提示方案利用大型视觉语言模型确定6-DoF相机位姿。

Result: 在多样化真实世界物体检索任务中表现出强大的适应性和鲁棒性，包括有人类干预的场景，仅使用单个RGB-D相机。

Conclusion: RoboRetriever展示了仅使用单个相机和自然语言指令就能在杂乱环境中有效检索物体的能力，为机器人系统提供了更灵活和低成本的解决方案。

Abstract: Humans effortlessly retrieve objects in cluttered, partially observable
environments by combining visual reasoning, active viewpoint adjustment, and
physical interaction-with only a single pair of eyes. In contrast, most
existing robotic systems rely on carefully positioned fixed or multi-camera
setups with complete scene visibility, which limits adaptability and incurs
high hardware costs. We present \textbf{RoboRetriever}, a novel framework for
real-world object retrieval that operates using only a \textbf{single}
wrist-mounted RGB-D camera and free-form natural language instructions.
RoboRetriever grounds visual observations to build and update a \textbf{dynamic
hierarchical scene graph} that encodes object semantics, geometry, and
inter-object relations over time. The supervisor module reasons over this
memory and task instruction to infer the target object and coordinate an
integrated action module combining \textbf{active perception},
\textbf{interactive perception}, and \textbf{manipulation}. To enable
task-aware scene-grounded active perception, we introduce a novel visual
prompting scheme that leverages large reasoning vision-language models to
determine 6-DoF camera poses aligned with the semantic task goal and geometry
scene context. We evaluate RoboRetriever on diverse real-world object retrieval
tasks, including scenarios with human intervention, demonstrating strong
adaptability and robustness in cluttered scenes with only one RGB-D camera.

</details>


### [41] [Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users](https://arxiv.org/abs/2508.12925)
*Eetu Laukka,Evan G. Center,Timo Ojala,Steven M. LaValle,Matti Pouke*

Main category: cs.RO

TL;DR: 使用光流技术在500ms延迟情况下为移动传真机器人创建自我运动幻觉，但未显著提升性能或准确性，反而可能增加VR舔晕


<details>
  <summary>Details</summary>
Motivation: 解决360度摄像头流媒体影音导致的高延迟问题，提升移动传真机器人的实时控制能力

Method: 利用光流技术在用户发送运动命令到看到实际运动的延迟期间创建自我运动幻觉

Result: 在500ms延迟下未发现任务完成时间和碰撞等性能指标的显著改善，反而可能增加了VR舔晕

Conclusion: 该方法需要进一步调整才能成为可行的解决方案

Abstract: Mobile telepresence robots allow users to feel present and explore remote
environments using technology. Traditionally, these systems are implemented
using a camera onboard a mobile robot that can be controlled. Although
high-immersion technologies, such as 360-degree cameras, can increase
situational awareness and presence, they also introduce significant challenges.
Additional processing and bandwidth requirements often result in latencies of
up to seconds. The current delay with a 360-degree camera streaming over the
internet makes real-time control of these systems difficult. Working with
high-latency systems requires some form of assistance to the users.
  This study presents a novel way to utilize optical flow to create an illusion
of self-motion to the user during the latency period between user sending
motion commands to the robot and seeing the actual motion through the
360-camera stream. We find no significant benefit of using the self-motion
illusion to performance or accuracy of controlling a telepresence robot with a
latency of 500 ms, as measured by the task completion time and collisions into
objects. Some evidence is shown that the method might increase virtual reality
(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We
conclude that further adjustments are necessary in order to render the method
viable.

</details>


### [42] [Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion](https://arxiv.org/abs/2508.12928)
*Victor Dhédin,Haizhou Zhao,Majid Khadiv*

Main category: cs.RO

TL;DR: 基于MCTS和全身轨迹优化的方法，通过同时解决接触序列和接触点选择问题，实现了四足和人形机器人在极端环境中的灵活移动规划


<details>
  <summary>Details</summary>
Motivation: 四足机器人在高度约束环境中进行灵活机动时，需要解决混合了连续和离散决策变量的高难度优化问题

Method: 采用蒙特卡洛树搜索(MCTS)算法结合全身轨迹优化(TO)，实现同时的接触序列和接触点选择

Result: 在高难度环境中快速找到多样化的动态一致规划，并成功将规划转移到真实四足机器人，同时支持复杂的非周期性人形机动

Conclusion: 这是首次在四足机器人全身动力学基础上实现同时接触序列和接触点选择的非周期性多接触移动规划案例

Abstract: Legged robots have the potential to traverse highly constrained environments
with agile maneuvers. However, planning such motions requires solving a highly
challenging optimization problem with a mixture of continuous and discrete
decision variables. In this paper, we present a full pipeline based on
Monte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to
perform simultaneous contact sequence and patch selection on highly challenging
environments. Through extensive simulation experiments, we show that our
framework can quickly find a diverse set of dynamically consistent plans. We
experimentally show that these plans are transferable to a real quadruped
robot. We further show that the same framework can find highly complex acyclic
humanoid maneuvers. To the best of our knowledge, this is the first
demonstration of simultaneous contact sequence and patch selection for acyclic
multi-contact locomotion using the whole-body dynamics of a quadruped.

</details>


### [43] [Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade](https://arxiv.org/abs/2508.12946)
*Ann-Sophie Schenk,Stefan Schiffer,Heqiu Song*

Main category: cs.RO

TL;DR: 对六年级计算机科学课堂使用社交机器人的教师和学生访谈初步研究，探讨需求和应用可能性，发现双方都持开放态度但需求存在差异


<details>
  <summary>Details</summary>
Motivation: 了解教师和学习者对在计算机科学课堂中使用社交机器人的需求和潜在应用，特别关注两个群体对机器人使用方式和功能需求的不同视角

Method: 通过对教师和学生进行访谈，收集关于社交机器人在六年级计算机科学课堂中应用的看法和需求

Result: 教师和学生都对在课堂中使用机器人持非常开放的态度，但两个群体的需求存在部分异质性

Conclusion: 研究揭示了复杂的设计挑战，需要在机器人设计中平衡教师和学生的不同需求

Abstract: In this paper we report on first insights from interviews with teachers and
students on using social robots in computer science class in sixth grade. Our
focus is on learning about requirements and potential applications. We are
particularly interested in getting both perspectives, the teachers' and the
learners' view on how robots could be used and what features they should or
should not have. Results show that teachers as well as students are very open
to robots in the classroom. However, requirements are partially quite
heterogeneous among the groups. This leads to complex design challenges which
we discuss at the end of this paper.

</details>


### [44] [Scaling Whole-body Multi-contact Manipulation with Contact Optimization](https://arxiv.org/abs/2508.12980)
*Victor Levé,João Moura,Sachiya Fujita,Tamon Miyake,Steve Tonneau,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 提出了一种新的表面表示方法和成本设计，用于人形机器人的全身操纵规划，解决了现有方法在连续接触表面处理上的挑战


<details>
  <summary>Details</summary>
Motivation: 人类日常任务需要使用全身操纵物体，但现有的人形机器人规划方法主要依赖离散采样，在处理连续接触表面时程度性不佳

Method: 提出(i)一种能够进行闭式计算近似点的机器人和物体表面表示方法，(ii)一种有效指导全身操纵规划的成本设计

Result: 实验结果显示该框架能解决现有方法无法处理的问题，并比最先进方法规划时间提高了77%，在真实硬件上通过人形机器人操纵箱子进行了验证

Conclusion: 该研究提供了一种高效的全身操纵规划解决方案，通过新的表面表示和成本设计，有效解决了连续接触表面处理的挑战

Abstract: Daily tasks require us to use our whole body to manipulate objects, for
instance when our hands are unavailable. We consider the issue of providing
humanoid robots with the ability to autonomously perform similar whole-body
manipulation tasks. In this context, the infinite possibilities for where and
how contact can occur on the robot and object surfaces hinder the scalability
of existing planning methods, which predominantly rely on discrete sampling.
Given the continuous nature of contact surfaces, gradient-based optimization
offers a more suitable approach for finding solutions. However, a key remaining
challenge is the lack of an efficient representation of robot surfaces. In this
work, we propose (i) a representation of robot and object surfaces that enables
closed-form computation of proximity points, and (ii) a cost design that
effectively guides whole-body manipulation planning. Our experiments
demonstrate that the proposed framework can solve problems unaddressed by
existing methods, and achieves a 77% improvement in planning time over the
state of the art. We also validate the suitability of our approach on real
hardware through the whole-body manipulation of boxes by a humanoid robot.

</details>


### [45] [BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments](https://arxiv.org/abs/2508.13052)
*Sourav Raxit,Abdullah Al Redwan Newaz,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: BOW规划器是一种基于约束贝叶斯优化的可扩展运动规划算法，通过高效采样控制输入来处理动力学约束和安全要求，实现了迅速、安全的轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法在处理速度、加速度等动力学约束时遇到困难，需要一种能够高效处理高维目标函数和严格安全约束的新方法。

Method: 算法采用约束贝叶斯优化(CBO)技术，通过聚焦于可到达速度的规划窗口来高效采样控制输入，以最少的采样数量管理复杂约束。

Result: 理论分析证明算法进行近优解的趋势收敛，在杂乱和约束环境中的评估显示在计算时间、轨迹长度和解决时间方面都有显著改善，实际部署也证明了其高样本效率、安全优化和快速规划能力。

Conclusion: BOW规划器作为一种高效、安全的运动规划工具，对推动机器人应用发展具有重要价值，已作为开源包发布。

Abstract: This paper introduces the BOW Planner, a scalable motion planning algorithm
designed to navigate robots through complex environments using constrained
Bayesian optimization (CBO). Unlike traditional methods, which often struggle
with kinodynamic constraints such as velocity and acceleration limits, the BOW
Planner excels by concentrating on a planning window of reachable velocities
and employing CBO to sample control inputs efficiently. This approach enables
the planner to manage high-dimensional objective functions and stringent safety
constraints with minimal sampling, ensuring rapid and secure trajectory
generation. Theoretical analysis confirms the algorithm's asymptotic
convergence to near-optimal solutions, while extensive evaluations in cluttered
and constrained settings reveal substantial improvements in computation times,
trajectory lengths, and solution times compared to existing techniques.
Successfully deployed across various real-world robotic systems, the BOW
Planner demonstrates its practical significance through exceptional sample
efficiency, safety-aware optimization, and rapid planning capabilities, making
it a valuable tool for advancing robotic applications. The BOW Planner is
released as an open-source package and videos of real-world and simulated
experiments are available at https://bow-web.github.io.

</details>


### [46] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: 这是一份关于基于大型视觉-语言模型的视觉-语言-动作模型在机器人操纵领域的系统性调研报告，包含架构分析、领域整合、特征综述和未来方向的全面评论。


<details>
  <summary>Details</summary>
Motivation: 传统的规则基础方法无法在非结构化新环境中扩展或泛化，而基于大型视觉-语言模型的VLA模型成为了变革性的解决方案。本调研旨在系统整理该领域的研究进展，解决现有分类不一致和研究分散问题。

Method: 采用系统化的分类学方法，首先定义大型VLM基础的VLA模型，并划分为两种主要架构范式：单一系统模型（包含不同集成程度的单系统和双系统设计）和层次模型（通过可解释的中间表示显式解耦规划与执行）。进行深入分析包括与各领域的整合、特征综合和未来方向识别。

Result: 本调研提供了对大型VLM基础VLA模型的全面评估，包括它们的架构特征、运作优势以及支持其开发的数据集和测试标准。完整地综合了大型VLMs与机器人操纵交叉领域的研究成果，填补了该领域的关键空白。

Conclusion: 大型VLM基础的VLA模型为机器人操纵领域带来了变革性的可能性，能够在复杂非结构化环境中实现更好的泛化能力。本调研为该领域提供了统一的分类框架和系统化的知识整理，并指明了包括内存机制、4D感知、高效适应、多代理合作等在内的未来研究方向。

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.

</details>


### [47] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: OC-VLA框架通过将动作预测直接建立在相机观测空间中，解决了VLA模型在观察和动作空间不一致的问题，显著提高了模型对相机视角变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在处理真实世界环境时面临观察空间和动作空间不一致的挑战，特别是在不同相机视角下预测末端执行器位姿时存在空间不一致问题。

Method: 利用相机外参标定矩阵，将末端执行器位姿从机器人基坐标系转换到相机坐标系，在观测空间中统一预测目标，这是一个轻量级的即插即用策略。

Result: 在仿真和真实世界机器人操作任务上的综合评估表明，OC-VLA加速了收敛速度，提高了任务成功率，并改善了跨视角泛化能力。

Conclusion: OC-VLA框架能够有效统一感知和动作之间的对齐，提高模型对相机视角变化的鲁棒性，且与现有VLA架构兼容，无需重大修改。

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>
