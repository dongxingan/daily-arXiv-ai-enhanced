{"id": "2506.19968", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19968", "abs": "https://arxiv.org/abs/2506.19968", "authors": ["Sahand Farghdani", "Robin Chhabra"], "title": "Evolutionary Gait Reconfiguration in Damaged Legged Robots", "comment": null, "summary": "Multi-legged robots deployed in complex missions are susceptible to physical\ndamage in their legs, impairing task performance and potentially compromising\nmission success. This letter presents a rapid, training-free damage recovery\nalgorithm for legged robots subject to partial or complete loss of functional\nlegs. The proposed method first stabilizes locomotion by generating a new gait\nsequence and subsequently optimally reconfigures leg gaits via a developed\ndifferential evolution algorithm to maximize forward progression while\nminimizing body rotation and lateral drift. The algorithm successfully restores\nlocomotion in a 24-degree-of-freedom hexapod within one hour, demonstrating\nboth high efficiency and robustness to structural damage.", "AI": {"tldr": "提出一种无需训练的快速损伤恢复算法，用于多足机器人部分或完全失去功能腿时的运动恢复。", "motivation": "多足机器人在复杂任务中容易腿部受损，影响任务完成和任务成功率。", "method": "首先生成新的步态序列稳定运动，随后通过差分进化算法优化步态配置，最大化前进并减少旋转和侧移。", "result": "算法在24自由度六足机器人上1小时内成功恢复运动，高效且鲁棒。", "conclusion": "该方法能快速恢复受损机器人的运动能力，适用于复杂任务。"}}
{"id": "2506.19984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19984", "abs": "https://arxiv.org/abs/2506.19984", "authors": ["Sahand Farghdani", "Mili Patel", "Robin Chhabra"], "title": "Robust Embodied Self-Identification of Morphology in Damaged Multi-Legged Robots", "comment": null, "summary": "Multi-legged robots (MLRs) are vulnerable to leg damage during complex\nmissions, which can impair their performance. This paper presents a\nself-modeling and damage identification algorithm that enables autonomous\nadaptation to partial or complete leg loss using only data from a low-cost IMU.\nA novel FFT-based filter is introduced to address time-inconsistent signals,\nimproving damage detection by comparing body orientation between the robot and\nits model. The proposed method identifies damaged legs and updates the robot's\nmodel for integration into its control system. Experiments on uneven terrain\nvalidate its robustness and computational efficiency.", "AI": {"tldr": "提出了一种基于低成本IMU的自建模与损伤识别算法，帮助多足机器人自主适应腿部损伤。", "motivation": "多足机器人在复杂任务中易受腿部损伤影响性能，需自主适应能力。", "method": "引入FFT滤波器处理时间不一致信号，通过比较机器人与模型的身体方向检测损伤，更新模型并集成到控制系统。", "result": "在崎岖地形实验中验证了算法的鲁棒性和计算效率。", "conclusion": "该方法能有效识别损伤并自主适应，提升多足机器人的可靠性。"}}
{"id": "2506.20036", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20036", "abs": "https://arxiv.org/abs/2506.20036", "authors": ["Jeremiah Coholich", "Muhammad Ali Murtaza", "Seth Hutchinson", "Zsolt Kira"], "title": "Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion", "comment": null, "summary": "We propose a novel hierarchical reinforcement learning framework for\nquadruped locomotion over challenging terrain. Our approach incorporates a\ntwo-layer hierarchy in which a high-level policy (HLP) selects optimal goals\nfor a low-level policy (LLP). The LLP is trained using an on-policy\nactor-critic RL algorithm and is given footstep placements as goals. We propose\nan HLP that does not require any additional training or environment samples and\ninstead operates via an online optimization process over the learned value\nfunction of the LLP. We demonstrate the benefits of this framework by comparing\nit with an end-to-end reinforcement learning (RL) approach. We observe\nimprovements in its ability to achieve higher rewards with fewer collisions\nacross an array of different terrains, including terrains more difficult than\nany encountered during training.", "AI": {"tldr": "提出了一种新颖的分层强化学习框架，用于四足机器人在复杂地形上的运动。高层策略（HLP）选择目标，低层策略（LLP）执行。HLP无需额外训练，通过在线优化LLP的价值函数实现。相比端到端RL，该框架在奖励和碰撞减少方面表现更优。", "motivation": "解决四足机器人在复杂地形上的运动问题，提高其适应性和效率。", "method": "采用两层分层强化学习框架：HLP选择目标，LLP执行。LLP通过on-policy actor-critic RL算法训练，HLP通过在线优化LLP的价值函数实现。", "result": "相比端到端RL，该框架在奖励更高、碰撞更少方面表现更优，尤其在训练未涉及的地形上。", "conclusion": "分层强化学习框架显著提升了四足机器人在复杂地形上的运动性能。"}}
{"id": "2506.20045", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20045", "abs": "https://arxiv.org/abs/2506.20045", "authors": ["Eric C. Joyce", "Qianwen Zhao", "Nathaniel Burgdorfer", "Long Wang", "Philippos Mordohai"], "title": "Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception", "comment": null, "summary": "Deep object pose estimators are notoriously overconfident. A grasping agent\nthat both estimates the 6-DoF pose of a target object and predicts the\nuncertainty of its own estimate could avoid task failure by choosing not to act\nunder high uncertainty. Even though object pose estimation improves and\nuncertainty quantification research continues to make strides, few studies have\nconnected them to the downstream task of robotic grasping. We propose a method\nfor training lightweight, deep networks to predict whether a grasp guided by an\nimage-based pose estimate will succeed before that grasp is attempted. We\ngenerate training data for our networks via object pose estimation on real\nimages and simulated grasping. We also find that, despite high object\nvariability in grasping trials, networks benefit from training on all objects\njointly, suggesting that a diverse variety of objects can nevertheless\ncontribute to the same goal.", "AI": {"tldr": "提出了一种训练轻量级深度网络的方法，用于预测基于图像姿态估计的抓取是否成功。", "motivation": "现有深度物体姿态估计器过于自信，导致抓取任务失败。通过预测不确定性，可以避免高不确定性下的行动。", "method": "通过真实图像的物体姿态估计和模拟抓取生成训练数据，训练网络预测抓取成功率。", "result": "发现尽管抓取试验中物体差异大，但联合训练所有物体对网络有益，表明多样性物体有助于同一目标。", "conclusion": "提出的方法能有效预测抓取成功率，联合训练提升性能。"}}
{"id": "2506.20311", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.20311", "abs": "https://arxiv.org/abs/2506.20311", "authors": ["Jingwen Wei"], "title": "Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles", "comment": null, "summary": "The growing use of mobile robots in sectors such as automotive, agriculture,\nand rescue operations reflects progress in robotics and autonomy. In unmanned\naerial vehicles (UAVs), most research emphasizes visual SLAM, sensor fusion,\nand path planning. However, applying UAVs to search and rescue missions in\ndisaster zones remains underexplored, especially for autonomous navigation.\n  This report develops methods for real-time and secure UAV maneuvering in\ncomplex 3D environments, crucial during forest fires. Building upon past\nresearch, it focuses on designing navigation algorithms for unfamiliar and\nhazardous environments, aiming to improve rescue efficiency and safety through\nUAV-based early warning and rapid response.\n  The work unfolds in phases. First, a 2D fusion navigation strategy is\nexplored, initially for mobile robots, enabling safe movement in dynamic\nsettings. This sets the stage for advanced features such as adaptive obstacle\nhandling and decision-making enhancements. Next, a novel 3D reactive navigation\nstrategy is introduced for collision-free movement in forest fire simulations,\naddressing the unique challenges of UAV operations in such scenarios.\n  Finally, the report proposes a unified control approach that integrates UAVs\nand unmanned ground vehicles (UGVs) for coordinated rescue missions in forest\nenvironments. Each phase presents challenges, proposes control models, and\nvalidates them with mathematical and simulation-based evidence. The study\noffers practical value and academic insights for improving the role of UAVs in\nnatural disaster rescue operations.", "AI": {"tldr": "该论文探讨了无人机在复杂3D环境中的实时安全导航方法，特别是在森林火灾等灾害救援中的应用，提出了一种统一的控制方法，整合无人机和地面无人车进行协同救援。", "motivation": "无人机在灾害救援中的应用尚未充分探索，尤其是在自主导航方面。研究旨在通过改进导航算法，提高救援效率和安全性。", "method": "分阶段开发：首先探索2D融合导航策略，随后提出3D反应式导航策略，最后整合无人机和地面无人车进行协同控制。", "result": "通过数学和仿真验证，提出的方法能够实现无人机在复杂环境中的安全导航和协同救援。", "conclusion": "该研究为无人机在自然灾害救援中的实际应用提供了有价值的解决方案和学术见解。"}}
{"id": "2506.20049", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20049", "abs": "https://arxiv.org/abs/2506.20049", "authors": ["Lorin Achey", "Alec Reed", "Brendan Crowe", "Bradley Hayes", "Christoffer Heckman"], "title": "Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis", "comment": "arXiv admin note: text overlap with arXiv:2409.10681", "summary": "We present a novel approach for enhancing robotic exploration by using\ngenerative occupancy mapping. We introduce SceneSense, a diffusion model\ndesigned and trained for predicting 3D occupancy maps given partial\nobservations. Our proposed approach probabilistically fuses these predictions\ninto a running occupancy map in real-time, resulting in significant\nimprovements in map quality and traversability. We implement SceneSense onboard\na quadruped robot and validate its performance with real-world experiments to\ndemonstrate the effectiveness of the model. In these experiments, we show that\noccupancy maps enhanced with SceneSense predictions better represent our fully\nobserved ground truth data (24.44% FID improvement around the robot and 75.59%\nimprovement at range). We additionally show that integrating\nSceneSense-enhanced maps into our robotic exploration stack as a \"drop-in\" map\nimprovement, utilizing an existing off-the-shelf planner, results in\nimprovements in robustness and traversability time. Finally we show results of\nfull exploration evaluations with our proposed system in two dissimilar\nenvironments and find that locally enhanced maps provide more consistent\nexploration results than maps constructed only from direct sensor measurements.", "AI": {"tldr": "提出了一种基于生成占用映射的机器人探索新方法SceneSense，显著提升了地图质量和可通行性。", "motivation": "解决机器人探索中部分观测导致的地图质量不足问题。", "method": "使用扩散模型SceneSense预测3D占用地图，并实时融合到运行地图中。", "result": "实验显示地图质量显著提升（FID改进24.44%和75.59%），探索稳健性和时间效率提高。", "conclusion": "SceneSense增强的地图在探索任务中表现优于仅依赖传感器数据的地图。"}}
{"id": "2506.20097", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20097", "abs": "https://arxiv.org/abs/2506.20097", "authors": ["Wang Bill Zhu", "Miaosen Chai", "Ishika Singh", "Robin Jia", "Jesse Thomason"], "title": "PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models", "comment": null, "summary": "We propose PSALM-V, the first autonomous neuro-symbolic learning system able\nto induce symbolic action semantics (i.e., pre- and post-conditions) in visual\nenvironments through interaction. PSALM-V bootstraps reliable symbolic planning\nwithout expert action definitions, using LLMs to generate heuristic plans and\ncandidate symbolic semantics. Previous work has explored using large language\nmodels to generate action semantics for Planning Domain Definition Language\n(PDDL)-based symbolic planners. However, these approaches have primarily\nfocused on text-based domains or relied on unrealistic assumptions, such as\naccess to a predefined problem file, full observability, or explicit error\nmessages. By contrast, PSALM-V dynamically infers PDDL problem files and domain\naction semantics by analyzing execution outcomes and synthesizing possible\nerror explanations. The system iteratively generates and executes plans while\nmaintaining a tree-structured belief over possible action semantics for each\naction, iteratively refining these beliefs until a goal state is reached.\nSimulated experiments of task completion in ALFRED demonstrate that PSALM-V\nincreases the plan success rate from 37% (Claude-3.7) to 74% in partially\nobserved setups. Results on two 2D game environments, RTFM and Overcooked-AI,\nshow that PSALM-V improves step efficiency and succeeds in domain induction in\nmulti-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions\nfor real-world robot BlocksWorld tasks, despite low-level manipulation failures\nfrom the robot.", "AI": {"tldr": "PSALM-V是一种自主神经符号学习系统，通过交互在视觉环境中推导符号动作语义，无需专家定义，显著提升了规划成功率。", "motivation": "解决现有方法在文本领域或依赖不现实假设（如预定义问题文件或完全可观察性）的局限性，实现动态推断符号语义。", "method": "利用LLM生成启发式计划和候选符号语义，动态推断PDDL问题文件和动作语义，通过迭代执行和信念树优化。", "result": "在ALFRED任务中，规划成功率从37%提升至74%；在RTFM和Overcooked-AI中提升步骤效率；在机器人任务中成功推导PDDL条件。", "conclusion": "PSALM-V在视觉环境和多智能体设置中有效，展示了神经符号学习的潜力。"}}
{"id": "2506.20212", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20212", "abs": "https://arxiv.org/abs/2506.20212", "authors": ["Andrea Bussolan", "Oliver Avram", "Andrea Pignata", "Gianvito Urgese", "Stefano Baraldo", "Anna Valente"], "title": "Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning", "comment": null, "summary": "With the advent of Industry 5.0, manufacturers are increasingly prioritizing\nworker well-being alongside mass customization. Stress-aware Human-Robot\nCollaboration (HRC) plays a crucial role in this paradigm, where robots must\nadapt their behavior to human mental states to improve collaboration fluency\nand safety. This paper presents a novel framework that integrates Federated\nLearning (FL) to enable personalized mental state evaluation while preserving\nuser privacy. By leveraging physiological signals, including EEG, ECG, EDA,\nEMG, and respiration, a multimodal model predicts an operator's stress level,\nfacilitating real-time robot adaptation. The FL-based approach allows\ndistributed on-device training, ensuring data confidentiality while improving\nmodel generalization and individual customization. Results demonstrate that the\ndeployment of an FL approach results in a global model with performance in\nstress prediction accuracy comparable to a centralized training approach.\nMoreover, FL allows for enhancing personalization, thereby optimizing\nhuman-robot interaction in industrial settings, while preserving data privacy.\nThe proposed framework advances privacy-preserving, adaptive robotics to\nenhance workforce well-being in smart manufacturing.", "AI": {"tldr": "论文提出了一种基于联邦学习的多模态框架，用于实时评估操作员的压力水平，并优化人机协作，同时保护用户隐私。", "motivation": "随着工业5.0的发展，制造商越来越重视工人福祉与大规模定制的结合。压力感知的人机协作（HRC）在此背景下至关重要，机器人需根据人类心理状态调整行为以提高协作流畅性和安全性。", "method": "论文提出了一种集成联邦学习（FL）的框架，利用EEG、ECG、EDA、EMG和呼吸等多模态生理信号预测操作员的压力水平，并通过分布式设备端训练保护数据隐私。", "result": "实验结果表明，FL方法的全局模型在压力预测准确性上与集中式训练方法相当，同时提升了个性化能力，优化了工业环境中的人机交互。", "conclusion": "该框架推动了隐私保护的适应性机器人技术，提升了智能制造中的劳动力福祉。"}}
{"id": "2506.20259", "categories": ["cs.RO", "cs.AI", "68T40, 93C85, 70E60", "I.2.9"], "pdf": "https://arxiv.org/pdf/2506.20259", "abs": "https://arxiv.org/abs/2506.20259", "authors": ["Andrej Lúčny", "Matilde Antonj", "Carlo Mazzola", "Hana Hornáčková", "Igor Farkaš"], "title": "Generating and Customizing Robotic Arm Trajectories using Neural Networks", "comment": "The code is released at\n  https://github.com/andylucny/nico2/tree/main/generate", "summary": "We introduce a neural network approach for generating and customizing the\ntrajectory of a robotic arm, that guarantees precision and repeatability. To\nhighlight the potential of this novel method, we describe the design and\nimplementation of the technique and show its application in an experimental\nsetting of cognitive robotics. In this scenario, the NICO robot was\ncharacterized by the ability to point to specific points in space with precise\nlinear movements, increasing the predictability of the robotic action during\nits interaction with humans. To achieve this goal, the neural network computes\nthe forward kinematics of the robot arm. By integrating it with a generator of\njoint angles, another neural network was developed and trained on an artificial\ndataset created from suitable start and end poses of the robotic arm. Through\nthe computation of angular velocities, the robot was characterized by its\nability to perform the movement, and the quality of its action was evaluated in\nterms of shape and accuracy. Thanks to its broad applicability, our approach\nsuccessfully generates precise trajectories that could be customized in their\nshape and adapted to different settings.", "AI": {"tldr": "提出了一种神经网络方法，用于生成和定制机械臂的轨迹，确保精度和可重复性。", "motivation": "提高机械臂在认知机器人实验中的可预测性，特别是在与人类互动时。", "method": "结合神经网络计算机械臂的正向运动学，并通过生成关节角度的神经网络训练人工数据集。", "result": "成功生成精确且可定制的轨迹，适用于不同场景。", "conclusion": "该方法具有广泛适用性，能够生成高精度且可定制的机械臂轨迹。"}}
{"id": "2506.20268", "categories": ["cs.RO", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20268", "abs": "https://arxiv.org/abs/2506.20268", "authors": ["Ruben Janssens", "Jens De Bock", "Sofie Labat", "Eva Verhelst", "Veronique Hoste", "Tony Belpaeme"], "title": "Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue", "comment": "Accepted at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN 2025)", "summary": "Detecting miscommunication in human-robot interaction is a critical function\nfor maintaining user engagement and trust. While humans effortlessly detect\ncommunication errors in conversations through both verbal and non-verbal cues,\nrobots face significant challenges in interpreting non-verbal feedback, despite\nadvances in computer vision for recognizing affective expressions. This\nresearch evaluates the effectiveness of machine learning models in detecting\nmiscommunications in robot dialogue. Using a multi-modal dataset of 240\nhuman-robot conversations, where four distinct types of conversational failures\nwere systematically introduced, we assess the performance of state-of-the-art\ncomputer vision models. After each conversational turn, users provided feedback\non whether they perceived an error, enabling an analysis of the models' ability\nto accurately detect robot mistakes. Despite using state-of-the-art models, the\nperformance barely exceeds random chance in identifying miscommunication, while\non a dataset with more expressive emotional content, they successfully\nidentified confused states. To explore the underlying cause, we asked human\nraters to do the same. They could also only identify around half of the induced\nmiscommunications, similarly to our model. These results uncover a fundamental\nlimitation in identifying robot miscommunications in dialogue: even when users\nperceive the induced miscommunication as such, they often do not communicate\nthis to their robotic conversation partner. This knowledge can shape\nexpectations of the performance of computer vision models and can help\nresearchers to design better human-robot conversations by deliberately\neliciting feedback where needed.", "AI": {"tldr": "研究评估机器学习模型在检测人机对话中的沟通错误时的表现，发现即使使用先进模型，识别效果仅略优于随机猜测，且用户反馈不足是主要限制。", "motivation": "人机交互中检测沟通错误对维持用户信任至关重要，但机器人难以通过非语言反馈识别错误。", "method": "使用包含240段人机对话的多模态数据集，引入四种对话失败类型，评估计算机视觉模型的性能。", "result": "模型识别沟通错误的效果不佳，仅略优于随机猜测；在情感表达更丰富的数据集上表现较好。", "conclusion": "用户反馈不足是识别机器人沟通错误的根本限制，需设计更好的对话机制以主动获取反馈。"}}
{"id": "2506.20314", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20314", "abs": "https://arxiv.org/abs/2506.20314", "authors": ["Marc-Philip Ecker", "Bernhard Bischof", "Minh Nhat Vu", "Christoph Fröhlich", "Tobias Glück", "Wolfgang Kemmetmüller"], "title": "Near Time-Optimal Hybrid Motion Planning for Timber Cranes", "comment": "Accepted at ICRA 2025", "summary": "Efficient, collision-free motion planning is essential for automating\nlarge-scale manipulators like timber cranes. They come with unique challenges\nsuch as hydraulic actuation constraints and passive joints-factors that are\nseldom addressed by current motion planning methods. This paper introduces a\nnovel approach for time-optimal, collision-free hybrid motion planning for a\nhydraulically actuated timber crane with passive joints. We enhance the\nvia-point-based stochastic trajectory optimization (VP-STO) algorithm to\ninclude pump flow rate constraints and develop a novel collision cost\nformulation to improve robustness. The effectiveness of the enhanced VP-STO as\nan optimal single-query global planner is validated by comparison with an\ninformed RRT* algorithm using a time-optimal path parameterization (TOPP). The\noverall hybrid motion planning is formed by combination with a gradient-based\nlocal planner that is designed to follow the global planner's reference and to\nsystematically consider the passive joint dynamics for both collision avoidance\nand sway damping.", "AI": {"tldr": "提出了一种针对液压驱动木材起重机的时间最优、无碰撞混合运动规划方法，改进了VP-STO算法，并验证了其优于RRT*算法的性能。", "motivation": "大型机械臂（如木材起重机）的运动规划面临液压驱动约束和被动关节等独特挑战，现有方法未充分解决。", "method": "增强VP-STO算法以包含泵流量约束，并提出新的碰撞成本公式；结合梯度局部规划器形成混合规划。", "result": "改进的VP-STO作为全局规划器优于RRT*算法，混合规划有效考虑了被动关节动力学。", "conclusion": "该方法为液压驱动机械臂提供了高效、鲁棒的运动规划解决方案。"}}
{"id": "2506.20315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20315", "abs": "https://arxiv.org/abs/2506.20315", "authors": ["Matías Mattamala", "Nived Chebrolu", "Jonas Frey", "Leonard Freißmuth", "Haedam Oh", "Benoit Casseau", "Marco Hutter", "Maurice Fallon"], "title": "Building Forest Inventories with Autonomous Legged Robots -- System, Lessons, and Challenges Ahead", "comment": "20 pages, 13 figures. Pre-print version of the accepted paper for\n  IEEE Transactions on Field Robotics (T-FR)", "summary": "Legged robots are increasingly being adopted in industries such as oil, gas,\nmining, nuclear, and agriculture. However, new challenges exist when moving\ninto natural, less-structured environments, such as forestry applications. This\npaper presents a prototype system for autonomous, under-canopy forest inventory\nwith legged platforms. Motivated by the robustness and mobility of modern\nlegged robots, we introduce a system architecture which enabled a quadruped\nplatform to autonomously navigate and map forest plots. Our solution involves a\ncomplete navigation stack for state estimation, mission planning, and tree\ndetection and trait estimation. We report the performance of the system from\ntrials executed over one and a half years in forests in three European\ncountries. Our results with the ANYmal robot demonstrate that we can survey\nplots up to 1 ha plot under 30 min, while also identifying trees with typical\nDBH accuracy of 2cm. The findings of this project are presented as five lessons\nand challenges. Particularly, we discuss the maturity of hardware development,\nstate estimation limitations, open problems in forest navigation, future\navenues for robotic forest inventory, and more general challenges to assess\nautonomous systems. By sharing these lessons and challenges, we offer insight\nand new directions for future research on legged robots, navigation systems,\nand applications in natural environments. Additional videos can be found in\nhttps://dynamic.robots.ox.ac.uk/projects/legged-robots", "AI": {"tldr": "本文介绍了一种用于森林自主测绘的四足机器人系统，展示了其在自然环境中导航和测绘的能力，并总结了相关挑战和未来研究方向。", "motivation": "现代四足机器人在复杂自然环境中的鲁棒性和移动性激发了开发自主森林测绘系统的需求。", "method": "提出了一种完整的导航系统架构，包括状态估计、任务规划、树木检测和特征估计。", "result": "ANYmal机器人在欧洲三国森林中测试，30分钟内可测绘1公顷区域，树木直径测量精度达2厘米。", "conclusion": "总结了硬件成熟度、状态估计限制等五方面挑战，为未来四足机器人和自然环境中导航系统研究提供了方向。"}}
{"id": "2506.20320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20320", "abs": "https://arxiv.org/abs/2506.20320", "authors": ["Malte Probst", "Raphael Wenzel", "Tim Puphal", "Monica Dasi", "Nico A. Steinhardt", "Sango Matsuzaki", "Misa Komuro"], "title": "Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation", "comment": null, "summary": "In Social Robot Navigation, autonomous agents need to resolve many sequential\ninteractions with other agents. State-of-the art planners can efficiently\nresolve the next, imminent interaction cooperatively and do not focus on longer\nplanning horizons. This makes it hard to maneuver scenarios where the agent\nneeds to select a good strategy to find gaps or channels in the crowd. We\npropose to decompose trajectory planning into two separate steps: Conflict\navoidance for finding good, macroscopic trajectories, and cooperative collision\navoidance (CCA) for resolving the next interaction optimally. We propose the\nProbabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies\nan established probabilistic collision risk model to include a general\nassumption of cooperativity. PGP biases the short-term CCA planner to head\ntowards gaps in the crowd. In extensive simulations with crowds of varying\ndensity, we show that using PGP in addition to state-of-the-art CCA planners\nimproves the agents' performance: On average, agents keep more space to others,\ncreate less tension, and cause fewer collisions. This typically comes at the\nexpense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot\nby Honda R&D.", "AI": {"tldr": "论文提出了一种分解轨迹规划的方法，结合冲突避免和协作碰撞避免，通过Probabilistic Gap Planner（PGP）提升社交机器人导航性能。", "motivation": "现有规划器仅关注短期交互，难以处理复杂场景，如寻找人群中的空隙或通道。", "method": "将轨迹规划分解为冲突避免（宏观轨迹）和协作碰撞避免（微观交互），提出PGP作为冲突避免规划器。", "result": "在模拟实验中，PGP结合现有规划器显著提升了性能：减少碰撞、增加空间、降低紧张感，但路径略长。", "conclusion": "PGP方法有效提升了社交机器人导航的长期规划能力，适用于实时应用。"}}
{"id": "2506.20343", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20343", "abs": "https://arxiv.org/abs/2506.20343", "authors": ["Kento Kawaharazuka", "Takahiro Hattori", "Keita Yoneda", "Kei Okada"], "title": "PIMBS: Efficient Body Schema Learning for Musculoskeletal Humanoids with Physics-Informed Neural Networks", "comment": "Accepted at IEEE Robotics and Automation Letters", "summary": "Musculoskeletal humanoids are robots that closely mimic the human\nmusculoskeletal system, offering various advantages such as variable stiffness\ncontrol, redundancy, and flexibility. However, their body structure is complex,\nand muscle paths often significantly deviate from geometric models. To address\nthis, numerous studies have been conducted to learn body schema, particularly\nthe relationships among joint angles, muscle tension, and muscle length. These\nstudies typically rely solely on data collected from the actual robot, but this\ndata collection process is labor-intensive, and learning becomes difficult when\nthe amount of data is limited. Therefore, in this study, we propose a method\nthat applies the concept of Physics-Informed Neural Networks (PINNs) to the\nlearning of body schema in musculoskeletal humanoids, enabling high-accuracy\nlearning even with a small amount of data. By utilizing not only data obtained\nfrom the actual robot but also the physical laws governing the relationship\nbetween torque and muscle tension under the assumption of correct joint\nstructure, more efficient learning becomes possible. We apply the proposed\nmethod to both simulation and an actual musculoskeletal humanoid and discuss\nits effectiveness and characteristics.", "AI": {"tldr": "提出了一种基于物理信息神经网络（PINNs）的方法，用于学习肌肉骨骼人形机器人的身体模式，即使在数据有限的情况下也能实现高精度学习。", "motivation": "肌肉骨骼人形机器人的身体结构复杂，肌肉路径与几何模型偏差大，传统方法依赖大量实际数据且学习困难。", "method": "结合实际机器人数据和物理规律（扭矩与肌肉张力关系），应用PINNs进行高效学习。", "result": "在仿真和实际机器人上验证了方法的有效性和特点。", "conclusion": "该方法能够显著减少数据需求，提高学习效率，适用于肌肉骨骼人形机器人的身体模式学习。"}}
{"id": "2506.20373", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20373", "abs": "https://arxiv.org/abs/2506.20373", "authors": ["Joerg Deigmoeller", "Stephan Hasler", "Nakul Agarwal", "Daniel Tanneberg", "Anna Belardinelli", "Reza Ghoddoosian", "Chao Wang", "Felix Ocker", "Fan Zhang", "Behzad Dariush", "Michael Gienger"], "title": "CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition", "comment": null, "summary": "We introduce CARMA, a system for situational grounding in human-robot group\ninteractions. Effective collaboration in such group settings requires\nsituational awareness based on a consistent representation of present persons\nand objects coupled with an episodic abstraction of events regarding actors and\nmanipulated objects. This calls for a clear and consistent assignment of\ninstances, ensuring that robots correctly recognize and track actors, objects,\nand their interactions over time. To achieve this, CARMA uniquely identifies\nphysical instances of such entities in the real world and organizes them into\ngrounded triplets of actors, objects, and actions.\n  To validate our approach, we conducted three experiments, where multiple\nhumans and a robot interact: collaborative pouring, handovers, and sorting.\nThese scenarios allow the assessment of the system's capabilities as to role\ndistinction, multi-actor awareness, and consistent instance identification. Our\nexperiments demonstrate that the system can reliably generate accurate\nactor-action-object triplets, providing a structured and robust foundation for\napplications requiring spatiotemporal reasoning and situated decision-making in\ncollaborative settings.", "AI": {"tldr": "CARMA系统通过情境感知和实例识别，支持人机群组交互中的协作。", "motivation": "在群组交互中，机器人需要情境感知和一致的实例识别以实现有效协作。", "method": "CARMA通过唯一标识实体并组织为行动者-对象-动作三元组来实现情境感知。", "result": "实验表明，CARMA能可靠生成准确的三元组，支持时空推理和决策。", "conclusion": "CARMA为协作场景提供了结构化且鲁棒的情境感知基础。"}}
{"id": "2506.20376", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20376", "abs": "https://arxiv.org/abs/2506.20376", "authors": ["Lingyun Chen", "Xinrui Zhao", "Marcos P. S. Campanha", "Alexander Wegener", "Abdeldjallil Naceri", "Abdalla Swikir", "Sami Haddadin"], "title": "Enhanced Robotic Navigation in Deformable Environments using Learning from Demonstration and Dynamic Modulation", "comment": "Accepted to IROS 2025", "summary": "This paper presents a novel approach for robot navigation in environments\ncontaining deformable obstacles. By integrating Learning from Demonstration\n(LfD) with Dynamical Systems (DS), we enable adaptive and efficient navigation\nin complex environments where obstacles consist of both soft and hard regions.\nWe introduce a dynamic modulation matrix within the DS framework, allowing the\nsystem to distinguish between traversable soft regions and impassable hard\nareas in real-time, ensuring safe and flexible trajectory planning. We validate\nour method through extensive simulations and robot experiments, demonstrating\nits ability to navigate deformable environments. Additionally, the approach\nprovides control over both trajectory and velocity when interacting with\ndeformable objects, including at intersections, while maintaining adherence to\nthe original DS trajectory and dynamically adapting to obstacles for smooth and\nreliable navigation.", "AI": {"tldr": "提出了一种结合LfD和DS的机器人导航方法，用于在包含可变形障碍物的环境中实现自适应高效导航。", "motivation": "解决在复杂环境中（包含软硬障碍物）机器人导航的灵活性和安全性问题。", "method": "在DS框架中引入动态调制矩阵，实时区分可穿越软区域与不可穿越硬区域，实现安全轨迹规划。", "result": "通过仿真和实验验证了方法在可变形环境中的导航能力，并实现了对轨迹和速度的控制。", "conclusion": "该方法能够动态适应障碍物，确保平滑可靠的导航，同时保持原始DS轨迹。"}}
{"id": "2506.20394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20394", "abs": "https://arxiv.org/abs/2506.20394", "authors": ["Mimo Shirasaka", "Yuya Ikeda", "Tatsuya Matsushima", "Yutaka Matsuo", "Yusuke Iwasawa"], "title": "SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning", "comment": null, "summary": "The ability to update information acquired through various means online\nduring task execution is crucial for a general-purpose service robot. This\ninformation includes geometric and semantic data. While SLAM handles geometric\nupdates on 2D maps or 3D point clouds, online updates of semantic information\nremain unexplored. We attribute the challenge to the online scene graph\nrepresentation, for its utility and scalability. Building on previous works\nregarding offline scene graph representations, we study online graph\nrepresentations of semantic information in this work. We introduce SPARK:\nSpatial Perception and Robot Knowledge Integration. This framework extracts\nsemantic information from environment-embedded cues and updates the scene graph\naccordingly, which is then used for subsequent task planning. We demonstrate\nthat graph representations of spatial relationships enhance the robot system's\nability to perform tasks in dynamic environments and adapt to unconventional\nspatial cues, like gestures.", "AI": {"tldr": "论文提出SPARK框架，用于在线更新语义信息，提升机器人在动态环境中的任务执行能力。", "motivation": "通用服务机器人需要在线更新几何和语义信息，但现有SLAM技术仅处理几何更新，语义信息的在线更新尚未解决。", "method": "基于离线场景图表示，提出SPARK框架，从环境线索中提取语义信息并更新场景图，用于任务规划。", "result": "实验表明，空间关系的图表示能增强机器人在动态环境中的任务执行能力，适应非常规空间线索（如手势）。", "conclusion": "SPARK框架有效解决了语义信息在线更新的问题，提升了机器人在动态环境中的适应性。"}}
{"id": "2506.20399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20399", "abs": "https://arxiv.org/abs/2506.20399", "authors": ["Hatem Fakhruldeen", "Arvind Raveendran Nambiar", "Satheeshkumar Veeramani", "Bonilkumar Vijaykumar Tailor", "Hadi Beyzaee Juneghani", "Gabriella Pizzuto", "Andrew Ian Cooper"], "title": "Multimodal Behaviour Trees for Robotic Laboratory Task Automation", "comment": "7 pages, 5 figures, accepted and presented in ICRA 2025", "summary": "Laboratory robotics offer the capability to conduct experiments with a high\ndegree of precision and reproducibility, with the potential to transform\nscientific research. Trivial and repeatable tasks; e.g., sample transportation\nfor analysis and vial capping are well-suited for robots; if done successfully\nand reliably, chemists could contribute their efforts towards more critical\nresearch activities. Currently, robots can perform these tasks faster than\nchemists, but how reliable are they? Improper capping could result in human\nexposure to toxic chemicals which could be fatal. To ensure that robots perform\nthese tasks as accurately as humans, sensory feedback is required to assess the\nprogress of task execution. To address this, we propose a novel methodology\nbased on behaviour trees with multimodal perception. Along with automating\nrobotic tasks, this methodology also verifies the successful execution of the\ntask, a fundamental requirement in safety-critical environments. The\nexperimental evaluation was conducted on two lab tasks: sample vial capping and\nlaboratory rack insertion. The results show high success rate, i.e., 88% for\ncapping and 92% for insertion, along with strong error detection capabilities.\nThis ultimately proves the robustness and reliability of our approach and that\nusing multimodal behaviour trees should pave the way towards the next\ngeneration of robotic chemists.", "AI": {"tldr": "论文提出了一种基于行为树和多模态感知的新方法，用于提高实验室机器人在执行任务时的可靠性和安全性，实验结果显示高成功率和强错误检测能力。", "motivation": "实验室机器人虽能高效执行重复任务，但其可靠性不足可能导致安全隐患（如毒物泄漏），因此需要一种方法确保任务执行的准确性。", "method": "采用行为树结合多模态感知的方法，自动化任务执行并验证任务完成情况。", "result": "实验在样本瓶盖封和实验室架插入任务中分别达到88%和92%的成功率，并具备强错误检测能力。", "conclusion": "该方法证明了其鲁棒性和可靠性，为下一代机器人化学家的研发奠定了基础。"}}
{"id": "2506.20445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20445", "abs": "https://arxiv.org/abs/2506.20445", "authors": ["Dongkun Wang", "Junkai Zhao", "Yunfei Teng", "Jieyang Peng", "Wenjing Xue", "Xiaoming Tao"], "title": "Learn to Position -- A Novel Meta Method for Robotic Positioning", "comment": null, "summary": "Absolute positioning accuracy is a vital specification for robots. Achieving\nhigh position precision can be challenging due to the presence of various\nsources of errors. Meanwhile, accurately depicting these errors is difficult\ndue to their stochastic nature. Vision-based methods are commonly integrated to\nguide robotic positioning, but their performance can be highly impacted by\ninevitable occlusions or adverse lighting conditions. Drawing on the\naforementioned considerations, a vision-free, model-agnostic meta-method for\ncompensating robotic position errors is proposed, which maximizes the\nprobability of accurate robotic position via interactive feedback. Meanwhile,\nthe proposed method endows the robot with the capability to learn and adapt to\nvarious position errors, which is inspired by the human's instinct for grasping\nunder uncertainties. Furthermore, it is a self-learning and self-adaptive\nmethod able to accelerate the robotic positioning process as more examples are\nincorporated and learned. Empirical studies validate the effectiveness of the\nproposed method. As of the writing of this paper, the proposed meta search\nmethod has already been implemented in a robotic-based assembly line for\nodd-form electronic components.", "AI": {"tldr": "提出了一种无视觉、模型无关的元方法，通过交互反馈补偿机器人位置误差，提高定位精度。", "motivation": "机器人绝对定位精度至关重要，但误差来源复杂且随机，视觉方法易受遮挡和光照影响。", "method": "基于交互反馈的无视觉元方法，具备学习和自适应能力，灵感来自人类在不确定条件下的抓取本能。", "result": "实证研究验证了方法的有效性，已在电子元件装配线中应用。", "conclusion": "该方法能自适应学习并加速定位过程，适用于复杂环境。"}}
{"id": "2506.20447", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20447", "abs": "https://arxiv.org/abs/2506.20447", "authors": ["James Fant-Male", "Roel Pieters"], "title": "A Review of Personalisation in Human-Robot Collaboration and Future Perspectives Towards Industry 5.0", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "The shift in research focus from Industry 4.0 to Industry 5.0 (I5.0) promises\na human-centric workplace, with social and well-being values at the centre of\ntechnological implementation. Human-Robot Collaboration (HRC) is a core aspect\nof I5.0 development, with an increase in adaptive and personalised interactions\nand behaviours. This review investigates recent advancements towards\npersonalised HRC, where user-centric adaption is key. There is a growing trend\nfor adaptable HRC research, however there lacks a consistent and unified\napproach. The review highlights key research trends on which personal factors\nare considered, workcell and interaction design, and adaptive task completion.\nThis raises various key considerations for future developments, particularly\naround the ethical and regulatory development of personalised systems, which\nare discussed in detail.", "AI": {"tldr": "论文探讨了从工业4.0到工业5.0的转变，强调以人为中心的工作环境，并研究了人机协作（HRC）的个性化发展。", "motivation": "研究动机在于工业5.0背景下，如何通过个性化的人机协作提升社会福祉和工作效率。", "method": "方法包括综述近期关于个性化HRC的研究，分析其适应性、交互设计和任务完成方式。", "result": "研究发现个性化HRC缺乏统一方法，但揭示了关键研究趋势和未来发展的伦理与监管问题。", "conclusion": "结论指出未来需关注个性化系统的伦理和监管发展，以实现更高效的人机协作。"}}
{"id": "2506.20485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20485", "abs": "https://arxiv.org/abs/2506.20485", "authors": ["Tian Liu", "Han Liu", "Boyang Li", "Long Chen", "Kai Huang"], "title": "EANS: Reducing Energy Consumption for UAV with an Environmental Adaptive Navigation Strategy", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVS) are limited by the onboard energy. Refinement\nof the navigation strategy directly affects both the flight velocity and the\ntrajectory based on the adjustment of key parameters in the UAVS pipeline, thus\nreducing energy consumption. However, existing techniques tend to adopt static\nand conservative strategies in dynamic scenarios, leading to inefficient energy\nreduction. Dynamically adjusting the navigation strategy requires overcoming\nthe challenges including the task pipeline interdependencies, the\nenvironmental-strategy correlations, and the selecting parameters. To solve the\naforementioned problems, this paper proposes a method to dynamically adjust the\nnavigation strategy of the UAVS by analyzing its dynamic characteristics and\nthe temporal characteristics of the autonomous navigation pipeline, thereby\nreducing UAVS energy consumption in response to environmental changes. We\ncompare our method with the baseline through hardware-in-the-loop (HIL)\nsimulation and real-world experiments, showing our method 3.2X and 2.6X\nimprovements in mission time, 2.4X and 1.6X improvements in energy,\nrespectively.", "AI": {"tldr": "本文提出一种动态调整无人机导航策略的方法，通过分析动态特性和时间特性，减少能耗，实验显示显著提升任务时间和能源效率。", "motivation": "无人机能源有限，现有静态导航策略在动态场景中效率低下，需解决任务管道依赖、环境-策略关联和参数选择等挑战。", "method": "动态调整导航策略，分析无人机动态特性和自主导航管道的时间特性。", "result": "硬件在环仿真和实际实验显示，任务时间提升3.2倍和2.6倍，能源效率提升2.4倍和1.6倍。", "conclusion": "动态导航策略能有效减少无人机能耗，适应环境变化，显著提升性能。"}}
{"id": "2506.20487", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20487", "abs": "https://arxiv.org/abs/2506.20487", "authors": ["Mingqi Yuan", "Tao Yu", "Wenqi Ge", "Xiuyong Yao", "Dapeng Li", "Huijiang Wang", "Jiayu Chen", "Xin Jin", "Bo Li", "Hua Chen", "Wei Zhang", "Wenjun Zeng"], "title": "Behavior Foundation Model: Towards Next-Generation Whole-Body Control System of Humanoid Robots", "comment": "19 pages, 8 figures", "summary": "Humanoid robots are drawing significant attention as versatile platforms for\ncomplex motor control, human-robot interaction, and general-purpose physical\nintelligence. However, achieving efficient whole-body control (WBC) in\nhumanoids remains a fundamental challenge due to sophisticated dynamics,\nunderactuation, and diverse task requirements. While learning-based controllers\nhave shown promise for complex tasks, their reliance on labor-intensive and\ncostly retraining for new scenarios limits real-world applicability. To address\nthese limitations, behavior(al) foundation models (BFMs) have emerged as a new\nparadigm that leverages large-scale pretraining to learn reusable primitive\nskills and behavioral priors, enabling zero-shot or rapid adaptation to a wide\nrange of downstream tasks. In this paper, we present a comprehensive overview\nof BFMs for humanoid WBC, tracing their development across diverse pre-training\npipelines. Furthermore, we discuss real-world applications, current\nlimitations, urgent challenges, and future opportunities, positioning BFMs as a\nkey approach toward scalable and general-purpose humanoid intelligence.\nFinally, we provide a curated and long-term list of BFM papers and projects to\nfacilitate more subsequent research, which is available at\nhttps://github.com/yuanmingqi/awesome-bfm-papers.", "AI": {"tldr": "本文综述了行为基础模型（BFMs）在人形机器人全身控制（WBC）中的应用，探讨了其发展、实际应用、局限性和未来机会。", "motivation": "人形机器人在复杂运动控制和通用物理智能方面潜力巨大，但传统学习方法依赖高成本重训练，限制了实际应用。BFMs通过大规模预训练学习可重用技能，有望解决这一问题。", "method": "BFMs利用大规模预训练学习原始技能和行为先验，支持零样本或快速适应下游任务。", "result": "BFMs为可扩展和通用的人形智能提供了关键方法，但仍面临实际应用中的挑战。", "conclusion": "BFMs是人形机器人WBC的重要研究方向，未来需解决其局限性以推动实际应用。"}}
{"id": "2506.20496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20496", "abs": "https://arxiv.org/abs/2506.20496", "authors": ["Jonathan Wang", "Hisashi Ishida", "David Usevitch", "Kesavan Venkatesh", "Yi Wang", "Mehran Armand", "Rachel Bronheim", "Amit Jain", "Adnan Munawar"], "title": "Critical Anatomy-Preserving & Terrain-Augmenting Navigation (CAPTAiN): Application to Laminectomy Surgical Education", "comment": null, "summary": "Surgical training remains a crucial milestone in modern medicine, with\nprocedures such as laminectomy exemplifying the high risks involved.\nLaminectomy drilling requires precise manual control to mill bony tissue while\npreserving spinal segment integrity and avoiding breaches in the dura: the\nprotective membrane surrounding the spinal cord. Despite unintended tears\noccurring in up to 11.3% of cases, no assistive tools are currently utilized to\nreduce this risk. Variability in patient anatomy further complicates learning\nfor novice surgeons. This study introduces CAPTAiN, a critical\nanatomy-preserving and terrain-augmenting navigation system that provides\nlayered, color-coded voxel guidance to enhance anatomical awareness during\nspinal drilling. CAPTAiN was evaluated against a standard non-navigated\napproach through 110 virtual laminectomies performed by 11 orthopedic residents\nand medical students. CAPTAiN significantly improved surgical completion rates\nof target anatomy (87.99% vs. 74.42%) and reduced cognitive load across\nmultiple NASA-TLX domains. It also minimized performance gaps across experience\nlevels, enabling novices to perform on par with advanced trainees. These\nfindings highlight CAPTAiN's potential to optimize surgical execution and\nsupport skill development across experience levels. Beyond laminectomy, it\ndemonstrates potential for broader applications across various surgical and\ndrilling procedures, including those in neurosurgery, otolaryngology, and other\nmedical fields.", "AI": {"tldr": "CAPTAiN系统通过分层彩色体素引导，显著提高椎板切除术的完成率并降低认知负荷，缩小了不同经验水平外科医生的表现差距。", "motivation": "椎板切除术中意外撕裂硬膜的风险高达11.3%，且缺乏辅助工具；患者解剖结构差异增加了新手学习难度。", "method": "开发CAPTAiN系统，提供分层彩色体素引导，通过110次虚拟椎板切除术评估其效果。", "result": "CAPTAiN显著提高了目标解剖结构的完成率（87.99% vs. 74.42%），降低了认知负荷，并使新手表现接近高级学员。", "conclusion": "CAPTAiN有望优化手术执行并支持技能发展，适用于多种外科和钻孔手术。"}}
{"id": "2506.20553", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20553", "abs": "https://arxiv.org/abs/2506.20553", "authors": ["Rachel Luo", "Heng Yang", "Michael Watson", "Apoorva Sharma", "Sushant Veer", "Edward Schmerling", "Marco Pavone"], "title": "Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation", "comment": null, "summary": "Learning-based robotic systems demand rigorous validation to assure reliable\nperformance, but extensive real-world testing is often prohibitively expensive,\nand if conducted may still yield insufficient data for high-confidence\nguarantees. In this work, we introduce a general estimation framework that\nleverages paired data across test platforms, e.g., paired simulation and\nreal-world observations, to achieve better estimates of real-world metrics via\nthe method of control variates. By incorporating cheap and abundant auxiliary\nmeasurements (for example, simulator outputs) as control variates for costly\nreal-world samples, our method provably reduces the variance of Monte Carlo\nestimates and thus requires significantly fewer real-world samples to attain a\nspecified confidence bound on the mean performance. We provide theoretical\nanalysis characterizing the variance and sample-efficiency improvement, and\ndemonstrate empirically in autonomous driving and quadruped robotics settings\nthat our approach achieves high-probability bounds with markedly improved\nsample efficiency. Our technique can lower the real-world testing burden for\nvalidating the performance of the stack, thereby enabling more efficient and\ncost-effective experimental evaluation of robotic systems.", "AI": {"tldr": "提出一种利用配对数据（如仿真和现实观测）的控制变量方法，减少机器人系统验证所需的现实样本数量。", "motivation": "现实测试成本高且数据不足，需更高效验证方法。", "method": "利用控制变量法结合仿真数据，降低蒙特卡洛估计方差。", "result": "理论分析和实验表明，显著减少现实样本需求。", "conclusion": "该方法降低测试负担，提升机器人系统验证效率。"}}
{"id": "2506.20566", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20566", "abs": "https://arxiv.org/abs/2506.20566", "authors": ["Zhonghao Shi", "Enyu Zhao", "Nathaniel Dennler", "Jingzhen Wang", "Xinyang Xu", "Kaleen Shrestha", "Mengxue Fu", "Daniel Seita", "Maja Matarić"], "title": "HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction", "comment": "Accepted to the 19th International Symposium on Experimental Robotics\n  (ISER 2025)", "summary": "Real-time human perception is crucial for effective human-robot interaction\n(HRI). Large vision-language models (VLMs) offer promising generalizable\nperceptual capabilities but often suffer from high latency, which negatively\nimpacts user experience and limits VLM applicability in real-world scenarios.\nTo systematically study VLM capabilities in human perception for HRI and\nperformance-latency trade-offs, we introduce HRIBench, a visual\nquestion-answering (VQA) benchmark designed to evaluate VLMs across a diverse\nset of human perceptual tasks critical for HRI. HRIBench covers five key\ndomains: (1) non-verbal cue understanding, (2) verbal instruction\nunderstanding, (3) human-robot object relationship understanding, (4) social\nnavigation, and (5) person identification. To construct HRIBench, we collected\ndata from real-world HRI environments to curate questions for non-verbal cue\nunderstanding, and leveraged publicly available datasets for the remaining four\ndomains. We curated 200 VQA questions for each domain, resulting in a total of\n1000 questions for HRIBench. We then conducted a comprehensive evaluation of\nboth state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.\nOur results show that, despite their generalizability, current VLMs still\nstruggle with core perceptual capabilities essential for HRI. Moreover, none of\nthe models within our experiments demonstrated a satisfactory\nperformance-latency trade-off suitable for real-time deployment, underscoring\nthe need for future research on developing smaller, low-latency VLMs with\nimproved human perception capabilities. HRIBench and our results can be found\nin this Github repository: https://github.com/interaction-lab/HRIBench.", "AI": {"tldr": "HRIBench是一个用于评估视觉语言模型（VLMs）在人类感知任务中性能与延迟权衡的基准测试，涵盖五个关键领域，结果显示当前VLMs在实时人机交互（HRI）中仍存在不足。", "motivation": "研究VLMs在HRI中的人类感知能力及其性能与延迟的权衡，填补现有研究的空白。", "method": "构建HRIBench基准测试，包含1000个视觉问答（VQA）问题，覆盖五个HRI关键领域，并评估11种VLMs。", "result": "当前VLMs在核心感知能力上表现不足，且未能在性能与延迟之间达到满意的平衡。", "conclusion": "未来需开发更小、低延迟且具备更强人类感知能力的VLMs，HRIBench为此提供了评估工具。"}}
{"id": "2506.20579", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20579", "abs": "https://arxiv.org/abs/2506.20579", "authors": ["Ali Reza Pedram", "Evangelos Psomiadis", "Dipankar Maity", "Panagiotis Tsiotras"], "title": "Communication-Aware Map Compression for Online Path-Planning: A Rate-Distortion Approach", "comment": null, "summary": "This paper addresses the problem of collaborative navigation in an unknown\nenvironment, where two robots, referred to in the sequel as the Seeker and the\nSupporter, traverse the space simultaneously. The Supporter assists the Seeker\nby transmitting a compressed representation of its local map under bandwidth\nconstraints to support the Seeker's path-planning task. We introduce a bit-rate\nmetric based on the expected binary codeword length to quantify communication\ncost. Using this metric, we formulate the compression design problem as a\nrate-distortion optimization problem that determines when to communicate, which\nregions of the map should be included in the compressed representation, and at\nwhat resolution (i.e., quantization level) they should be encoded. Our\nformulation allows different map regions to be encoded at varying quantization\nlevels based on their relevance to the Seeker's path-planning task. We\ndemonstrate that the resulting optimization problem is convex, and admits a\nclosed-form solution known in the information theory literature as reverse\nwater-filling, enabling efficient, low-computation, and real-time\nimplementation. Additionally, we show that the Seeker can infer the compression\ndecisions of the Supporter independently, requiring only the encoded map\ncontent and not the encoding policy itself to be transmitted, thereby reducing\ncommunication overhead. Simulation results indicate that our method effectively\nconstructs compressed, task-relevant map representations, both in content and\nresolution, that guide the Seeker's planning decisions even under tight\nbandwidth limitations.", "AI": {"tldr": "论文提出了一种在带宽限制下，支持者机器人通过压缩地图信息辅助寻求者机器人导航的方法，优化了通信成本和任务相关性。", "motivation": "解决在未知环境中，两机器人协作导航时的带宽限制问题，优化地图信息的压缩和传输。", "method": "引入基于二进制码字长度的比特率度量，将压缩设计问题建模为率失真优化问题，采用反向注水法求解。", "result": "仿真结果表明，该方法能在带宽限制下高效生成任务相关的地图压缩表示，指导寻求者机器人的路径规划。", "conclusion": "提出的方法在低计算量和实时性要求下，有效解决了协作导航中的通信优化问题。"}}
{"id": "2506.20636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20636", "abs": "https://arxiv.org/abs/2506.20636", "authors": ["Venkat Karramreddy", "Rangarajan Ramanujam"], "title": "A Computationally Aware Multi Objective Framework for Camera LiDAR Calibration", "comment": "16 pages, 10 figures", "summary": "Accurate extrinsic calibration between LiDAR and camera sensors is important\nfor reliable perception in autonomous systems. In this paper, we present a\nnovel multi-objective optimization framework that jointly minimizes the\ngeometric alignment error and computational cost associated with camera-LiDAR\ncalibration. We optimize two objectives: (1) error between projected LiDAR\npoints and ground-truth image edges, and (2) a composite metric for\ncomputational cost reflecting runtime and resource usage. Using the NSGA-II\n\\cite{deb2002nsga2} evolutionary algorithm, we explore the parameter space\ndefined by 6-DoF transformations and point sampling rates, yielding a\nwell-characterized Pareto frontier that exposes trade-offs between calibration\nfidelity and resource efficiency. Evaluations are conducted on the KITTI\ndataset using its ground-truth extrinsic parameters for validation, with\nresults verified through both multi-objective and constrained single-objective\nbaselines. Compared to existing gradient-based and learned calibration methods,\nour approach demonstrates interpretable, tunable performance with lower\ndeployment overhead. Pareto-optimal configurations are further analyzed for\nparameter sensitivity and innovation insights. A preference-based\ndecision-making strategy selects solutions from the Pareto knee region to suit\nthe constraints of the embedded system. The robustness of calibration is tested\nacross variable edge-intensity weighting schemes, highlighting optimal balance\npoints. Although real-time deployment on embedded platforms is deferred to\nfuture work, this framework establishes a scalable and transparent method for\ncalibration under realistic misalignment and resource-limited conditions,\ncritical for long-term autonomy, particularly in SAE L3+ vehicles receiving OTA\nupdates.", "AI": {"tldr": "提出一种多目标优化框架，用于联合优化LiDAR与相机的外参标定误差和计算成本，通过NSGA-II算法探索参数空间，并在KITTI数据集上验证性能。", "motivation": "LiDAR与相机的外参标定对自动驾驶系统的可靠感知至关重要，但现有方法在计算成本和标定精度之间缺乏平衡。", "method": "采用多目标优化框架，结合几何对齐误差和计算成本作为优化目标，使用NSGA-II算法探索6-DoF变换和点采样率的参数空间。", "result": "在KITTI数据集上验证，相比现有方法，该方法在标定精度和资源效率之间提供了可调的性能，且部署开销更低。", "conclusion": "该框架为资源受限条件下的标定提供了可扩展且透明的方法，适用于长期自主运行的L3+车辆。"}}
{"id": "2506.20668", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20668", "abs": "https://arxiv.org/abs/2506.20668", "authors": ["Sungjae Park", "Homanga Bharadhwaj", "Shubham Tulsiani"], "title": "DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy", "comment": "Preprint(17 pages). Under Review", "summary": "We propose DemoDiffusion, a simple and scalable method for enabling robots to\nperform manipulation tasks in natural environments by imitating a single human\ndemonstration. Our approach is based on two key insights. First, the hand\nmotion in a human demonstration provides a useful prior for the robot's\nend-effector trajectory, which we can convert into a rough open-loop robot\nmotion trajectory via kinematic retargeting. Second, while this retargeted\nmotion captures the overall structure of the task, it may not align well with\nplausible robot actions in-context. To address this, we leverage a pre-trained\ngeneralist diffusion policy to modify the trajectory, ensuring it both follows\nthe human motion and remains within the distribution of plausible robot\nactions. Our approach avoids the need for online reinforcement learning or\npaired human-robot data, enabling robust adaptation to new tasks and scenes\nwith minimal manual effort. Experiments in both simulation and real-world\nsettings show that DemoDiffusion outperforms both the base policy and the\nretargeted trajectory, enabling the robot to succeed even on tasks where the\npre-trained generalist policy fails entirely. Project page:\nhttps://demodiffusion.github.io/", "AI": {"tldr": "DemoDiffusion是一种通过模仿单个人类演示使机器人在自然环境中执行操作任务的简单可扩展方法。", "motivation": "解决机器人通过单次人类演示学习任务的问题，避免在线强化学习或配对数据的需求。", "method": "结合人类手部运动的先验信息（通过运动学重定向）和预训练的扩散策略，生成符合机器人动作分布的轨迹。", "result": "在仿真和实际环境中，DemoDiffusion优于基础策略和重定向轨迹，能完成预训练策略失败的任务。", "conclusion": "DemoDiffusion是一种高效且鲁棒的方法，适用于新任务和场景的快速适应。"}}
