{"id": "2507.10968", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10968", "abs": "https://arxiv.org/abs/2507.10968", "authors": ["Toktam Mohammadnejad", "Jovin D'sa", "Behdad Chalaki", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi-Pari"], "title": "SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging", "comment": "Accepted at IEEE ITSC 2025", "summary": "Merging onto a highway is a complex driving task that requires identifying a\nsafe gap, adjusting speed, often interactions to create a merging gap, and\ncompleting the merge maneuver within a limited time window while maintaining\nsafety and driving comfort. In this paper, we introduce a Safe Merging and\nReal-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed\nto facilitate safe and comfortable forced merging. By deliberately adapting\ncost terms to the unique challenges of forced merging and introducing a desired\nspeed heuristic, SMART-Merge planner enables the ego vehicle to merge\nsuccessfully while minimizing the merge time. We verify the efficiency and\neffectiveness of the proposed merge planner through high-fidelity CarMaker\nsimulations on hundreds of highway merge scenarios. Our proposed planner\nachieves the success rate of 100% as well as completes the merge maneuver in\nthe shortest amount of time compared with the baselines, demonstrating our\nplanner's capability to handle complex forced merge tasks and provide a\nreliable and robust solution for autonomous highway merge. The simulation\nresult videos are available at\nhttps://sites.google.com/view/smart-merge-planner/home.", "AI": {"tldr": "SMART-Merge是一种基于格点的运动规划器，旨在实现安全舒适的强制并道，通过优化成本项和引入期望速度启发式，显著提高了并道成功率和效率。", "motivation": "高速公路并道是一项复杂的驾驶任务，涉及识别安全间隙、调整速度以及与其他车辆交互，需要确保安全和舒适。", "method": "采用基于格点的运动规划方法，优化成本项并引入期望速度启发式，以处理强制并道的独特挑战。", "result": "在高保真CarMaker模拟中，SMART-Merge在数百种高速公路并道场景中实现了100%的成功率，并道时间最短。", "conclusion": "SMART-Merge能够高效处理复杂的强制并道任务，为自动驾驶高速公路并道提供了可靠且鲁棒的解决方案。"}}
{"id": "2507.11211", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11211", "abs": "https://arxiv.org/abs/2507.11211", "authors": ["Chen Cai", "Ernesto Dickel Saraiva", "Ya-jun Pan", "Steven Liu"], "title": "MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments", "comment": "10 pages, 5 figures, submitted to IEEE Robotics and Automation\n  Letters (RA-L)", "summary": "This letter presents a novel coarse-to-fine motion planning framework for\nrobotic manipulation in cluttered, unmodeled environments. The system\nintegrates a dual-camera perception setup with a B-spline-based model\npredictive control (MPC) scheme. Initially, the planner generates feasible\nglobal trajectories from partial and uncertain observations. As new visual data\nare incrementally fused, both the environment model and motion planning are\nprogressively refined. A vision-based cost function promotes target-driven\nexploration, while a refined kernel-perceptron collision detector enables\nefficient constraint updates for real-time planning. The framework accommodates\nclosed-chain kinematics and supports dynamic replanning. Experiments on a\nmulti-arm platform validate its robustness and adaptability under uncertainties\nand clutter.", "AI": {"tldr": "提出了一种新颖的从粗到细的运动规划框架，用于机器人在杂乱、未建模环境中的操作。", "motivation": "解决机器人在复杂、不确定环境中高效运动规划的挑战。", "method": "结合双摄像头感知与基于B样条的模型预测控制（MPC），逐步优化环境模型和运动规划。", "result": "实验验证了框架在不确定性和杂乱环境中的鲁棒性和适应性。", "conclusion": "该框架为机器人在复杂环境中的实时运动规划提供了有效解决方案。"}}
{"id": "2507.11283", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11283", "abs": "https://arxiv.org/abs/2507.11283", "authors": ["Weiyi Liu", "Jingzehua Xu", "Guanwen Xie", "Yi Li"], "title": "Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks", "comment": null, "summary": "This paper presents a diffusion-augmented reinforcement learning (RL)\napproach for robust autonomous underwater vehicle (AUV) control, addressing key\nchallenges in underwater trajectory planning and dynamic environment\nadaptation. The proposed method integrates three core innovations: (1) A\ndiffusion-based trajectory generation framework that produces physically\nfeasible multi-step trajectories, enhanced by a high-dimensional state encoding\nmechanism combining current observations with historical states and actions\nthrough a novel diffusion U-Net architecture, significantly improving\nlong-horizon planning. (2) A sample-efficient hybrid learning architecture that\nsynergizes diffusion-guided exploration with RL policy optimization, where the\ndiffusion model generates diverse candidate actions and the RL critic selects\noptimal actions, achieving higher exploration efficiency and policy stability\nin dynamic underwater environments. Extensive simulation experiments validating\nthe method's superior robustness and flexibility, outperforms conventional\ncontrol methods in challenging marine conditions, offering enhanced\nadaptability and reliability for AUV operations in the underwater tasks.", "AI": {"tldr": "提出了一种扩散增强的强化学习方法，用于自主水下车辆（AUV）的鲁棒控制，解决了水下轨迹规划和动态环境适应的关键挑战。", "motivation": "解决水下环境中AUV轨迹规划和动态适应的挑战，提高长期规划和探索效率。", "method": "结合扩散模型和强化学习，通过扩散U-Net架构生成多步轨迹，并利用RL策略优化选择最优动作。", "result": "在仿真实验中表现出优越的鲁棒性和灵活性，优于传统控制方法。", "conclusion": "该方法为水下任务提供了更高的适应性和可靠性。"}}
{"id": "2507.11447", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11447", "abs": "https://arxiv.org/abs/2507.11447", "authors": ["Shuo Yang", "John Z. Zhang", "Ibrahima Sory Sow", "Zachary Manchester"], "title": "Multi-IMU Sensor Fusion for Legged Robots", "comment": "16 pages", "summary": "This paper presents a state-estimation solution for legged robots that uses a\nset of low-cost, compact, and lightweight sensors to achieve low-drift pose and\nvelocity estimation under challenging locomotion conditions. The key idea is to\nleverage multiple inertial measurement units on different links of the robot to\ncorrect a major error source in standard proprioceptive odometry. We fuse the\ninertial sensor information and joint encoder measurements in an extended\nKalman filter, then combine the velocity estimate from this filter with camera\ndata in a factor-graph-based sliding-window estimator to form a\nvisual-inertial-leg odometry method. We validate our state estimator through\ncomprehensive theoretical analysis and hardware experiments performed using\nreal-world robot data collected during a variety of challenging locomotion\ntasks. Our algorithm consistently achieves minimal position deviation, even in\nscenarios involving substantial ground impact, foot slippage, and sudden body\nrotations. A C++ implementation, along with a large-scale dataset, is available\nat https://github.com/ShuoYangRobotics/Cerberus2.0.", "AI": {"tldr": "提出了一种基于低成本传感器的腿式机器人状态估计方法，通过多惯性测量单元和视觉数据融合，实现低漂移的位姿和速度估计。", "motivation": "解决腿式机器人在复杂运动条件下标准本体感知里程计的误差问题。", "method": "使用扩展卡尔曼滤波融合惯性传感器和关节编码器数据，再结合视觉数据在滑动窗口估计器中形成视觉-惯性-腿里程计。", "result": "在多种挑战性运动任务中，算法表现出低位置偏差，适应地面冲击、脚滑和快速旋转。", "conclusion": "该方法有效提升了腿式机器人在复杂环境中的状态估计性能，开源实现和数据集可供使用。"}}
{"id": "2507.10602", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10602", "abs": "https://arxiv.org/abs/2507.10602", "authors": ["Maximilian Stölzle", "T. Konstantin Rusch", "Zach J. Patterson", "Rodrigo Pérez-Dattari", "Francesco Stella", "Josie Hughes", "Cosimo Della Santina", "Daniela Rus"], "title": "Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees", "comment": "73 pages", "summary": "Learning from demonstration provides a sample-efficient approach to acquiring\ncomplex behaviors, enabling robots to move robustly, compliantly, and with\nfluidity. In this context, Dynamic Motion Primitives offer built - in stability\nand robustness to disturbances but often struggle to capture complex periodic\nbehaviors. Moreover, they are limited in their ability to interpolate between\ndifferent tasks. These shortcomings substantially narrow their applicability,\nexcluding a wide class of practically meaningful tasks such as locomotion and\nrhythmic tool use. In this work, we introduce Orbitally Stable Motion\nPrimitives (OSMPs) - a framework that combines a learned diffeomorphic encoder\nwith a supercritical Hopf bifurcation in latent space, enabling the accurate\nacquisition of periodic motions from demonstrations while ensuring formal\nguarantees of orbital stability and transverse contraction. Furthermore, by\nconditioning the bijective encoder on the task, we enable a single learned\npolicy to represent multiple motion objectives, yielding consistent zero-shot\ngeneralization to unseen motion objectives within the training distribution. We\nvalidate the proposed approach through extensive simulation and real-world\nexperiments across a diverse range of robotic platforms - from collaborative\narms and soft manipulators to a bio-inspired rigid-soft turtle robot -\ndemonstrating its versatility and effectiveness in consistently outperforming\nstate-of-the-art baselines such as diffusion policies, among others.", "AI": {"tldr": "论文提出了一种名为OSMPs的新框架，用于从演示中学习周期性运动，同时保证轨道稳定性和横向收缩性。", "motivation": "Dynamic Motion Primitives在捕捉复杂周期性行为和任务间插值方面存在局限性，限制了其实际应用范围。", "method": "结合学习到的微分同胚编码器和超临界Hopf分岔，实现了周期性运动的准确学习，并通过任务条件化的双射编码器支持多任务表示。", "result": "在多种机器人平台上验证了OSMPs的优越性，表现优于现有基线方法。", "conclusion": "OSMPs为周期性运动学习提供了稳定且通用的解决方案，扩展了机器人学习的应用范围。"}}
{"id": "2507.10672", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10672", "abs": "https://arxiv.org/abs/2507.10672", "authors": ["Muhayy Ud Din", "Waseem Akram", "Lyes Saad Saoud", "Jan Rosell", "Irfan Hussain"], "title": "Vision Language Action Models in Robotic Manipulation: A Systematic Review", "comment": "submitted to annual review in control", "summary": "Vision Language Action (VLA) models represent a transformative shift in\nrobotics, with the aim of unifying visual perception, natural language\nunderstanding, and embodied control within a single learning framework. This\nreview presents a comprehensive and forward-looking synthesis of the VLA\nparadigm, with a particular emphasis on robotic manipulation and\ninstruction-driven autonomy. We comprehensively analyze 102 VLA models, 26\nfoundational datasets, and 12 simulation platforms that collectively shape the\ndevelopment and evaluation of VLAs models. These models are categorized into\nkey architectural paradigms, each reflecting distinct strategies for\nintegrating vision, language, and control in robotic systems. Foundational\ndatasets are evaluated using a novel criterion based on task complexity,\nvariety of modalities, and dataset scale, allowing a comparative analysis of\ntheir suitability for generalist policy learning. We introduce a\ntwo-dimensional characterization framework that organizes these datasets based\non semantic richness and multimodal alignment, showing underexplored regions in\nthe current data landscape. Simulation environments are evaluated for their\neffectiveness in generating large-scale data, as well as their ability to\nfacilitate transfer from simulation to real-world settings and the variety of\nsupported tasks. Using both academic and industrial contributions, we recognize\nongoing challenges and outline strategic directions such as scalable\npretraining protocols, modular architectural design, and robust multimodal\nalignment strategies. This review serves as both a technical reference and a\nconceptual roadmap for advancing embodiment and robotic control, providing\ninsights that span from dataset generation to real world deployment of\ngeneralist robotic agents.", "AI": {"tldr": "本文综述了视觉语言动作（VLA）模型在机器人领域的应用，分析了102个模型、26个数据集和12个仿真平台，提出了分类框架和未来发展方向。", "motivation": "统一视觉感知、自然语言理解和机器人控制，推动通用机器人代理的发展。", "method": "通过分类VLA模型的架构范式、评估数据集和仿真平台，提出二维框架分析语义丰富性和多模态对齐。", "result": "揭示了当前数据集的不足，提出了可扩展预训练协议、模块化设计和多模态对齐策略等方向。", "conclusion": "VLA模型为机器人控制提供了新范式，但仍需解决数据集和仿真平台的挑战以实现实际部署。"}}
{"id": "2507.10694", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10694", "abs": "https://arxiv.org/abs/2507.10694", "authors": ["Francesco Fuentes", "Serigne Diagne", "Zachary Kingston", "Laura H. Blumenschein"], "title": "Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots", "comment": "22 pages, 21 figures, submitted to journal for potential publication", "summary": "Passive deformation due to compliance is a commonly used benefit of soft\nrobots, providing opportunities to achieve robust actuation with few active\ndegrees of freedom. Soft growing robots in particular have shown promise in\nnavigation of unstructured environments due to their passive deformation. If\ntheir collisions and subsequent deformations can be better understood, soft\nrobots could be used to understand the structure of the environment from direct\ntactile measurements. In this work, we propose the use of soft growing robots\nas mapping and exploration tools. We do this by first characterizing collision\nbehavior during discrete turns, then leveraging this model to develop a\ngeometry-based simulator that models robot trajectories in 2D environments.\nFinally, we demonstrate the model and simulator validity by mapping unknown\nenvironments using Monte Carlo sampling to estimate the optimal next deployment\ngiven current knowledge. Over both uniform and non-uniform environments, this\nselection method rapidly approaches ideal actions, showing the potential for\nsoft growing robots in unstructured environment exploration and mapping.", "AI": {"tldr": "利用软生长机器人作为探索和地图绘制工具，通过碰撞行为建模和几何模拟器，结合蒙特卡洛采样优化部署策略。", "motivation": "软机器人因其被动变形特性在非结构化环境中表现出色，若能更好地理解其碰撞和变形行为，可通过触觉测量探索环境结构。", "method": "首先表征离散转向中的碰撞行为，开发基于几何的2D环境模拟器，利用蒙特卡洛采样估计最优部署策略。", "result": "在均匀和非均匀环境中，该方法快速接近理想行动，验证了软生长机器人在环境探索中的潜力。", "conclusion": "软生长机器人可作为有效的非结构化环境探索和地图绘制工具。"}}
{"id": "2507.10749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10749", "abs": "https://arxiv.org/abs/2507.10749", "authors": ["Benjamin Stoler", "Juliet Yang", "Jonathan Francis", "Jean Oh"], "title": "RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding", "comment": null, "summary": "Safety-critical scenarios are essential for training and evaluating\nautonomous driving (AD) systems, yet remain extremely rare in real-world\ndriving datasets. To address this, we propose Real-world Crash Grounding (RCG),\na scenario generation framework that integrates crash-informed semantics into\nadversarial perturbation pipelines. We construct a safety-aware behavior\nrepresentation through contrastive pre-training on large-scale driving logs,\nfollowed by fine-tuning on a small, crash-rich dataset with approximate\ntrajectory annotations extracted from video. This embedding captures semantic\nstructure aligned with real-world accident behaviors and supports selection of\nadversary trajectories that are both high-risk and behaviorally realistic. We\nincorporate the resulting selection mechanism into two prior scenario\ngeneration pipelines, replacing their handcrafted scoring objectives with an\nembedding-based criterion. Experimental results show that ego agents trained\nagainst these generated scenarios achieve consistently higher downstream\nsuccess rates, with an average improvement of 9.2% across seven evaluation\nsettings. Qualitative and quantitative analyses further demonstrate that our\napproach produces more plausible and nuanced adversary behaviors, enabling more\neffective and realistic stress testing of AD systems. Code and tools will be\nreleased publicly.", "AI": {"tldr": "提出了一种名为RCG的场景生成框架，通过整合碰撞语义到对抗扰动流程中，生成更真实的高风险驾驶场景，提升自动驾驶系统的测试效果。", "motivation": "现实驾驶数据中安全关键场景稀缺，难以有效训练和评估自动驾驶系统。", "method": "结合对比预训练和微调，构建安全感知行为表示，嵌入基于真实事故行为的语义结构，改进现有场景生成流程。", "result": "实验显示，使用生成场景训练的自动驾驶系统下游成功率平均提升9.2%，且生成的对抗行为更真实。", "conclusion": "RCG框架能生成更真实的高风险场景，有效提升自动驾驶系统的压力测试效果。"}}
{"id": "2507.10776", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10776", "abs": "https://arxiv.org/abs/2507.10776", "authors": ["Howard H. Qian", "Yiting Chen", "Gaotian Wang", "Podshara Chanrungmaneekul", "Kaiyu Hang"], "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding", "comment": "8 pages, IROS 2025, Interactive Perception, Segmentation, Robotics,\n  Computer Vision", "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.", "AI": {"tldr": "提出了一种实时交互感知框架rt-RISeg，通过机器人交互和设计的体帧不变特征（BFIF）连续分割未见物体，无需学习分割模型，性能优于现有方法。", "motivation": "现有未见物体实例分割（UOIS）方法依赖大规模数据集训练，容易过拟合静态视觉特征，泛化性能差。", "method": "基于视觉交互性原理，提出rt-RISeg框架，利用机器人交互和体帧不变特征（BFIF）实时分割物体。", "result": "平均分割准确率比现有UOIS方法高27.5%，且生成的分割掩码可作为视觉基础模型的提示提升性能。", "conclusion": "rt-RISeg通过交互感知显著提升了未见物体的分割性能，且具有独立性和扩展性。"}}
{"id": "2507.10814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10814", "abs": "https://arxiv.org/abs/2507.10814", "authors": ["Huiyi Wang", "Fahim Shahriar", "Alireza Azimi", "Gautham Vasan", "Rupam Mahmood", "Colin Bellinger"], "title": "Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection", "comment": "8 pages, 4 figures, 3 tables", "summary": "General-purpose robotic manipulation, including reach and grasp, is essential\nfor deployment into households and workspaces involving diverse and evolving\ntasks. Recent advances propose using large pre-trained models, such as Large\nLanguage Models and object detectors, to boost robotic perception in\nreinforcement learning. These models, trained on large datasets via\nself-supervised learning, can process text prompts and identify diverse objects\nin scenes, an invaluable skill in RL where learning object interaction is\nresource-intensive. This study demonstrates how to integrate such models into\nGoal-Conditioned Reinforcement Learning to enable general and versatile robotic\nreach and grasp capabilities. We use a pre-trained object detection model to\nenable the agent to identify the object from a text prompt and generate a mask\nfor goal conditioning. Mask-based goal conditioning provides object-agnostic\ncues, improving feature sharing and generalization. The effectiveness of the\nproposed framework is demonstrated in a simulated reach-and-grasp task, where\nthe mask-based goal conditioning consistently maintains a $\\sim$90\\% success\nrate in grasping both in and out-of-distribution objects, while also ensuring\nfaster convergence to higher returns.", "AI": {"tldr": "该论文提出了一种将预训练模型（如大型语言模型和物体检测器）整合到目标条件强化学习中，以实现通用机器人抓取能力的方法。通过基于掩码的目标条件，提高了特征共享和泛化能力。", "motivation": "通用机器人操作（如抓取）在家庭和工作场所的多样化任务中至关重要。预训练模型可以提升机器人感知能力，减少强化学习中对象交互的资源消耗。", "method": "使用预训练的物体检测模型，根据文本提示识别物体并生成掩码，用于目标条件强化学习。掩码提供对象无关的提示，优化特征共享和泛化。", "result": "在模拟抓取任务中，掩码目标条件方法实现了约90%的成功率，包括分布内和分布外物体，同时加速了收敛。", "conclusion": "该方法通过预训练模型和目标条件强化学习的结合，显著提升了机器人抓取的通用性和效率。"}}
{"id": "2507.10878", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10878", "abs": "https://arxiv.org/abs/2507.10878", "authors": ["Savva Morozov", "Tobia Marcucci", "Bernhard Paus Graesdal", "Alexandre Amice", "Pablo A. Parrilo", "Russ Tedrake"], "title": "Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets", "comment": "10 pages", "summary": "We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A\nGCS is a graph where each vertex is paired with a convex program, and each edge\ncouples adjacent programs via additional costs and constraints. A walk in a GCS\nis a sequence of vertices connected by edges, where vertices may be repeated.\nThe length of a walk is given by the cumulative optimal value of the\ncorresponding convex programs. To solve the SWP in GCS, we first synthesize a\npiecewise-quadratic lower bound on the problem's cost-to-go function using\nsemidefinite programming. Then we use this lower bound to guide an\nincremental-search algorithm that yields an approximate shortest walk. We show\nthat the SWP in GCS is a natural language for many mixed discrete-continuous\nplanning problems in robotics, unifying problems that typically require\nspecialized solutions while delivering high performance and computational\nefficiency. We demonstrate this through experiments in collision-free motion\nplanning, skill chaining, and optimal control of hybrid systems.", "AI": {"tldr": "研究了在凸集图（GCS）中的最短路径问题（SWP），提出了一种基于半定规划和增量搜索的近似解法，并展示了其在机器人混合离散-连续规划问题中的广泛应用。", "motivation": "凸集图（GCS）为混合离散-连续规划问题提供了一种统一的建模语言，但缺乏高效的求解方法。", "method": "通过半定规划合成成本函数的二次下界，并利用增量搜索算法近似求解最短路径。", "result": "实验验证了该方法在碰撞避免运动规划、技能链和混合系统最优控制中的高效性和通用性。", "conclusion": "GCS中的SWP为多种机器人规划问题提供了统一且高效的解决方案。"}}
{"id": "2507.10899", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10899", "abs": "https://arxiv.org/abs/2507.10899", "authors": ["Wang Zhicheng", "Satoshi Yagi", "Satoshi Yamamori", "Jun Morimoto"], "title": "Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning", "comment": null, "summary": "Imitation learning for mobile manipulation is a key challenge in the field of\nrobotic manipulation. However, current mobile manipulation frameworks typically\ndecouple navigation and manipulation, executing manipulation only after\nreaching a certain location. This can lead to performance degradation when\nnavigation is imprecise, especially due to misalignment in approach angles. To\nenable a mobile manipulator to perform the same task from diverse orientations,\nan essential capability for building general-purpose robotic models, we propose\nan object-centric method based on SAM2, a foundation model towards solving\npromptable visual segmentation in images, which incorporates manipulation\norientation information into our model. Our approach enables consistent\nunderstanding of the same task from different orientations. We deploy the model\non a custom-built mobile manipulator and evaluate it on a pick-and-place task\nunder varied orientation angles. Compared to Action Chunking Transformer, our\nmodel maintains superior generalization when trained with demonstrations from\nvaried approach angles. This work significantly enhances the generalization and\nrobustness of imitation learning-based mobile manipulation systems.", "AI": {"tldr": "提出了一种基于SAM2的对象中心方法，用于提升移动机械臂在不同方向下的任务一致性，显著增强了模仿学习系统的泛化性和鲁棒性。", "motivation": "当前移动机械臂框架通常将导航和操作解耦，导致导航不精确时性能下降，尤其是在角度偏差情况下。为提升任务的多方向执行能力，提出了新方法。", "method": "基于SAM2的对象中心方法，将操作方向信息融入模型，实现任务在不同方向下的一致性理解。", "result": "在自定义移动机械臂上部署模型，通过拾取放置任务测试，相比Action Chunking Transformer，新模型在多样化角度下表现更优。", "conclusion": "该方法显著提升了模仿学习在移动机械臂任务中的泛化性和鲁棒性。"}}
{"id": "2507.10914", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10914", "abs": "https://arxiv.org/abs/2507.10914", "authors": ["James A. Preiss", "Fengze Xie", "Yiheng Lin", "Adam Wierman", "Yisong Yue"], "title": "Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization", "comment": "11 pages, 9 figures", "summary": "We study online algorithms to tune the parameters of a robot controller in a\nsetting where the dynamics, policy class, and optimality objective are all\ntime-varying. The system follows a single trajectory without episodes or state\nresets, and the time-varying information is not known in advance. Focusing on\nnonlinear geometric quadrotor controllers as a test case, we propose a\npractical implementation of a single-trajectory model-based online policy\noptimization algorithm, M-GAPS,along with reparameterizations of the quadrotor\nstate space and policy class to improve the optimization landscape. In hardware\nexperiments,we compare to model-based and model-free baselines that impose\nartificial episodes. We show that M-GAPS finds near-optimal parameters more\nquickly, especially when the episode length is not favorable. We also show that\nM-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and\nachieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our\nresults demonstrate the hardware practicality of this emerging class of online\npolicy optimization that offers significantly more flexibility than classic\nadaptive control, while being more stable and data-efficient than model-free\nreinforcement learning.", "AI": {"tldr": "论文提出了一种单轨迹模型在线策略优化算法M-GAPS，用于动态调整机器人控制器参数，适用于非线性几何四旋翼控制器，并在硬件实验中验证了其高效性和适应性。", "motivation": "研究动机在于解决机器人控制器参数在动态变化环境中的在线优化问题，尤其是在无状态重置和未知时间变化信息的情况下。", "method": "方法包括提出M-GAPS算法，重新参数化四旋翼状态空间和策略类以优化搜索空间，并与基于模型和无模型的基线方法进行比较。", "result": "实验结果表明，M-GAPS能更快找到接近最优的参数，尤其在不利的片段长度下，并能快速适应未建模的风和负载扰动。", "conclusion": "结论表明M-GAPS在硬件实践中具有高效性和灵活性，优于经典自适应控制和模型自由强化学习。"}}
{"id": "2507.10950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10950", "abs": "https://arxiv.org/abs/2507.10950", "authors": ["Zhiwei Wu", "Jiahao Luo", "Siyi Wei", "Jinhui Zhang"], "title": "Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances", "comment": null, "summary": "This paper presents a unified modeling and optimization framework to enhance\nthe kinematic performance of multi-magnet embedded soft continuum robots\n(MeSCRs). To this end, we establish a differentiable system formulation based\non an extended pseudo-rigid-body model. This formulation enables analysis of\nthe equilibrium well-posedness and the geometry of the induced configuration\nunder magnetic actuation. In particular, we show that the maximum controllable\ndegrees of freedom of a MeSCR equal twice the number of embedded magnets. We\nsubsequently develop a structural optimization framework based on differential\ngeometry that links classical kinematic measures (e.g., manipulability and\ndexterity) to the configuration of embedded magnets. The resulting optimization\ncondition reveals that improving local performance requires structurally\nmodulating the spectrum of the configuration space metric to counteract its\ndistortion. Closed-form solutions for optimal magnet configurations are derived\nunder representative conditions, and a gradient-based numerical method is\nproposed for general design scenarios. Simulation studies validate the\neffectiveness of the proposed framework.", "AI": {"tldr": "本文提出了一种统一建模与优化框架，用于提升多磁体嵌入式软连续体机器人（MeSCRs）的运动性能。通过建立基于扩展伪刚体模型的可微分系统公式，分析了磁驱动下的平衡适定性和诱导构型的几何特性。优化框架基于微分几何，将经典运动学指标与磁体配置关联，揭示了局部性能提升的条件。仿真验证了框架的有效性。", "motivation": "提升多磁体嵌入式软连续体机器人的运动性能，解决其运动学优化问题。", "method": "建立基于扩展伪刚体模型的可微分系统公式，开发基于微分几何的结构优化框架，将运动学指标与磁体配置关联。", "result": "最大可控自由度等于嵌入磁体数量的两倍；优化条件揭示了局部性能提升的结构调制需求；提出了闭式解和梯度数值方法。", "conclusion": "提出的框架有效提升了MeSCRs的运动性能，优化方法适用于多种设计场景。"}}
{"id": "2507.10960", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10960", "abs": "https://arxiv.org/abs/2507.10960", "authors": ["He Zhu", "Ryo Miyoshi", "Yuki Okafuji"], "title": "Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction", "comment": null, "summary": "Prior human-robot interaction (HRI) research has primarily focused on\nsingle-user interactions, where robots do not need to consider the timing or\nrecipient of their responses. However, in multi-party interactions, such as at\nmalls and hospitals, social robots must understand the context and decide both\nwhen and to whom they should respond. In this paper, we propose a\nTransformer-based multi-task learning framework to improve the decision-making\nprocess of social robots, particularly in multi-user environments. Considering\nthe characteristics of HRI, we propose two novel loss functions: one that\nenforces constraints on active speakers to improve scene modeling, and another\nthat guides response selection towards utterances specifically directed at the\nrobot. Additionally, we construct a novel multi-party HRI dataset that captures\nreal-world complexities, such as gaze misalignment. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in respond\ndecisions, outperforming existing heuristic-based and single-task approaches.\nOur findings contribute to the development of socially intelligent social\nrobots capable of engaging in natural and context-aware multi-party\ninteractions.", "AI": {"tldr": "提出了一种基于Transformer的多任务学习框架，用于提升社交机器人在多用户环境中的决策能力，通过两种新的损失函数和新的数据集，实现了最先进的响应决策性能。", "motivation": "多用户环境中，社交机器人需要理解上下文并决定何时及向谁响应，而现有研究主要关注单用户交互。", "method": "采用Transformer多任务学习框架，提出两种新损失函数：一种约束主动说话者以改进场景建模，另一种引导响应选择针对机器人的话语。构建了新的多用户HRI数据集。", "result": "实验表明，该模型在响应决策上优于现有的启发式和单任务方法，达到最先进性能。", "conclusion": "研究为开发具有社交智能的机器人提供了支持，使其能够进行自然且上下文感知的多方交互。"}}
{"id": "2507.10961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10961", "abs": "https://arxiv.org/abs/2507.10961", "authors": ["Joohwan Seo", "Arvind Kruthiventy", "Soomi Lee", "Megan Teng", "Xiang Zhang", "Seoyeon Choi", "Jongeun Choi", "Roberto Horowitz"], "title": "EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks", "comment": "Submitted to RA-L", "summary": "This paper presents a framework for learning vision-based robotic policies\nfor contact-rich manipulation tasks that generalize spatially across task\nconfigurations. We focus on achieving robust spatial generalization of the\npolicy for the peg-in-hole (PiH) task trained from a small number of\ndemonstrations. We propose EquiContact, a hierarchical policy composed of a\nhigh-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)\nand a novel low-level compliant visuomotor policy (Geometric Compliant ACT,\nG-CompACT). G-CompACT operates using only localized observations (geometrically\nconsistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB\nimages) and produces actions defined in the end-effector frame. Through these\ndesign choices, we show that the entire EquiContact pipeline is\nSE(3)-equivariant, from perception to force control. We also outline three key\ncomponents for spatially generalizable contact-rich policies: compliance,\nlocalized policies, and induced equivariance. Real-world experiments on PiH\ntasks demonstrate a near-perfect success rate and robust generalization to\nunseen spatial configurations, validating the proposed framework and\nprinciples. The experimental videos can be found on the project website:\nhttps://sites.google.com/berkeley.edu/equicontact", "AI": {"tldr": "提出了一种名为EquiContact的分层策略框架，用于学习视觉驱动的机器人策略，实现接触密集型任务的空间泛化。", "motivation": "解决接触密集型任务（如peg-in-hole）在少量演示下实现空间泛化的挑战。", "method": "采用分层策略：高层视觉规划器（Diff-EDF）和低层顺应性视觉运动策略（G-CompACT），结合局部观测和SE(3)-等变性设计。", "result": "在真实PiH任务中实现接近完美的成功率，并对未见空间配置表现出鲁棒泛化能力。", "conclusion": "EquiContact框架及其设计原则（顺应性、局部策略和等变性）有效提升了接触密集型任务的空间泛化性能。"}}
{"id": "2507.10991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10991", "abs": "https://arxiv.org/abs/2507.10991", "authors": ["Abhimanyu Bhowmik", "Mohit Singh", "Madhushree Sannigrahi", "Martin Ludvigsen", "Kostas Alexis"], "title": "Uncertainty Aware Mapping for Vision-Based Underwater Robots", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Vision-based underwater robots can be useful in inspecting and exploring\nconfined spaces where traditional sensors and preplanned paths cannot be\nfollowed. Sensor noise and situational change can cause significant uncertainty\nin environmental representation. Thus, this paper explores how to represent\nmapping inconsistency in vision-based sensing and incorporate depth estimation\nconfidence into the mapping framework. The scene depth and the confidence are\nestimated using the RAFT-Stereo model and are integrated into a voxel-based\nmapping framework, Voxblox. Improvements in the existing Voxblox weight\ncalculation and update mechanism are also proposed. Finally, a qualitative\nanalysis of the proposed method is performed in a confined pool and in a pier\nin the Trondheim fjord. Experiments using an underwater robot demonstrated the\nchange in uncertainty in the visualization.", "AI": {"tldr": "本文探讨了如何在基于视觉的水下机器人中表示地图不一致性，并将深度估计置信度融入体素地图框架Voxblox中，改进了权重计算和更新机制。", "motivation": "水下机器人在受限空间中的检测和探索需要应对传感器噪声和环境变化带来的不确定性。", "method": "使用RAFT-Stereo模型估计场景深度和置信度，并集成到Voxblox框架中，改进了权重计算和更新机制。", "result": "在受限水池和Trondheim峡湾的码头进行了实验，展示了不确定性在可视化中的变化。", "conclusion": "提出的方法有效提升了水下机器人环境表示的不确定性管理能力。"}}
{"id": "2507.11000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11000", "abs": "https://arxiv.org/abs/2507.11000", "authors": ["Minwoo Cho", "Jaehwi Jang", "Daehyung Park"], "title": "ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations", "comment": "8 pages, 6 figures", "summary": "We aim to solve the problem of temporal-constraint learning from\ndemonstrations to reproduce demonstration-like logic-constrained behaviors.\nLearning logic constraints is challenging due to the combinatorially large\nspace of possible specifications and the ill-posed nature of non-Markovian\nconstraints. To figure it out, we introduce a novel temporal-constraint\nlearning method, which we call inverse logic-constraint learning (ILCL). Our\nmethod frames ICL as a two-player zero-sum game between 1) a genetic\nalgorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained\nreinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax\ntrees for parameterized truncated linear temporal logic (TLTL) without\npredefined templates. Subsequently, Logic-CRL finds a policy that maximizes\ntask rewards under the constructed TLTL constraints via a novel constraint\nredistribution scheme. Our evaluations show ILCL outperforms state-of-the-art\nbaselines in learning and transferring TL constraints on four temporally\nconstrained tasks. We also demonstrate successful transfer to real-world\npeg-in-shallow-hole tasks.", "AI": {"tldr": "提出了一种名为ILCL的新方法，通过遗传算法和逻辑约束强化学习解决时间约束学习问题，并在实验中优于现有方法。", "motivation": "解决从演示中学习时间约束以复现逻辑约束行为的挑战性问题。", "method": "结合遗传算法的时间逻辑挖掘（GA-TL-Mining）和逻辑约束强化学习（Logic-CRL），通过零和博弈框架学习时间约束。", "result": "在四个时间约束任务上优于现有方法，并成功迁移到现实任务中。", "conclusion": "ILCL方法有效解决了时间约束学习问题，具有实际应用潜力。"}}
{"id": "2507.11001", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11001", "abs": "https://arxiv.org/abs/2507.11001", "authors": ["Yanbo Wang", "Zipeng Fang", "Lei Zhao", "Weidong Chen"], "title": "Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation", "comment": null, "summary": "Service robots are increasingly deployed in diverse and dynamic environments,\nwhere both physical layouts and social contexts change over time and across\nlocations. In these unstructured settings, conventional navigation systems that\nrely on fixed parameters often fail to generalize across scenarios, resulting\nin degraded performance and reduced social acceptance. Although recent\napproaches have leveraged reinforcement learning to enhance traditional\nplanners, these methods often fail in real-world deployments due to poor\ngeneralization and limited simulation diversity, which hampers effective\nsim-to-real transfer. To tackle these issues, we present LE-Nav, an\ninterpretable and scene-aware navigation framework that leverages multi-modal\nlarge language model reasoning and conditional variational autoencoders to\nadaptively tune planner hyperparameters. To achieve zero-shot scene\nunderstanding, we utilize one-shot exemplars and chain-of-thought prompting\nstrategies. Additionally, a conditional variational autoencoder captures the\nmapping between natural language instructions and navigation hyperparameters,\nenabling expert-level tuning. Experiments show that LE-Nav can generate\nhyperparameters achieving human-level tuning across diverse planners and\nscenarios. Real-world navigation trials and a user study on a smart wheelchair\nplatform demonstrate that it outperforms state-of-the-art methods on\nquantitative metrics such as success rate, efficiency, safety, and comfort,\nwhile receiving higher subjective scores for perceived safety and social\nacceptance. Code is available at https://github.com/Cavendish518/LE-Nav.", "AI": {"tldr": "LE-Nav是一个基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，实现零样本场景理解和人类级调优。", "motivation": "传统导航系统在动态和非结构化环境中泛化能力差，导致性能下降和社会接受度降低。", "method": "利用多模态大语言模型推理和条件变分自编码器，结合单样本示例和思维链提示策略，实现超参数自适应调优。", "result": "实验表明LE-Nav在多样场景和规划器中实现人类级调优，实际导航试验和用户研究显示其在成功率、效率、安全性和舒适性上优于现有方法。", "conclusion": "LE-Nav通过场景感知和自适应调优，显著提升了导航性能和社会接受度。"}}
{"id": "2507.11006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11006", "abs": "https://arxiv.org/abs/2507.11006", "authors": ["Ashutosh Mishra", "Shreya Santra", "Hazal Gozbasi", "Kentaro Uno", "Kazuya Yoshida"], "title": "Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments", "comment": "6 pages, 7 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)", "summary": "This study presents an advanced approach to enhance robotic manipulation in\nuncertain and challenging environments, with a focus on autonomous operations\naugmented by human-in-the-loop (HITL) control for lunar missions. By\nintegrating human decision-making with autonomous robotic functions, the\nresearch improves task reliability and efficiency for space applications. The\nkey task addressed is the autonomous deployment of flexible solar panels using\nan extendable ladder-like structure and a robotic manipulator with real-time\nfeedback for precision. The manipulator relays position and force-torque data,\nenabling dynamic error detection and adaptive control during deployment. To\nmitigate the effects of sinkage, variable payload, and low-lighting conditions,\nefficient motion planning strategies are employed, supplemented by human\ncontrol that allows operators to intervene in ambiguous scenarios. Digital twin\nsimulation enhances system robustness by enabling continuous feedback,\niterative task refinement, and seamless integration with the deployment\npipeline. The system has been tested to validate its performance in simulated\nlunar conditions and ensure reliability in extreme lighting, variable terrain,\nchanging payloads, and sensor limitations.", "AI": {"tldr": "研究提出了一种结合人机协同（HITL）的先进方法，用于增强机器人在月球任务中的操作能力，重点解决柔性太阳能板自主部署问题。", "motivation": "提升机器人在不确定和挑战性环境中的操作可靠性，特别是在空间任务中，结合人类决策与自主功能以提高效率。", "method": "通过实时反馈的机械臂和可扩展梯状结构实现精确部署，结合数字孪生仿真和人类干预策略。", "result": "系统在模拟月球环境中验证了性能，能够应对极端光照、多变地形和传感器限制。", "conclusion": "该方法显著提高了任务可靠性和效率，为未来空间任务提供了可行的解决方案。"}}
{"id": "2507.11069", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11069", "abs": "https://arxiv.org/abs/2507.11069", "authors": ["Jeongyun Kim", "Seunghoon Jeong", "Giseop Kim", "Myung-Hwan Jeon", "Eunji Jun", "Ayoung Kim"], "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update", "comment": null, "summary": "Understanding the 3D geometry of transparent objects from RGB images is\nchallenging due to their inherent physical properties, such as reflection and\nrefraction. To address these difficulties, especially in scenarios with sparse\nviews and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian\nSplatting-based depth reconstruction method for transparent objects. Our key\ninsight lies in separating transparent objects from the background, enabling\nfocused optimization of Gaussians corresponding to the object. We mitigate\nartifacts with an object-aware loss that places Gaussians in obscured regions,\nensuring coverage of invisible surfaces while reducing overfitting.\nFurthermore, we incorporate a physics-based simulation that refines the\nreconstruction in just a few seconds, effectively handling object removal and\nchain-reaction movement of remaining objects without the need for rescanning.\nTRAN-D is evaluated on both synthetic and real-world sequences, and it\nconsistently demonstrated robust improvements over existing GS-based\nstate-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean\nabsolute error by over 39% for the synthetic TRansPose sequences. Furthermore,\ndespite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm\naccuracy of 48.46%, over 1.5 times that of baselines, which uses six images.\nCode and more results are available at https://jeongyun0609.github.io/TRAN-D/.", "AI": {"tldr": "TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景并优化高斯分布，显著提升了稀疏视图和动态环境下的3D几何重建效果。", "motivation": "透明物体的3D几何重建因反射和折射等物理特性而具有挑战性，尤其在稀疏视图和动态环境中。", "method": "TRAN-D通过分离透明物体与背景，优化对应高斯分布，并结合物体感知损失和物理模拟，减少伪影并提升重建精度。", "result": "在合成和真实场景中，TRAN-D比现有方法平均绝对误差降低39%，单图像更新时精度提升1.5倍。", "conclusion": "TRAN-D在透明物体深度重建中表现出色，尤其在稀疏视图和动态环境下具有显著优势。"}}
{"id": "2507.11076", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.11076", "abs": "https://arxiv.org/abs/2507.11076", "authors": ["Andreas Mueller", "Shivesh Kumar"], "title": "Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems", "comment": null, "summary": "Derivatives of equations of motion(EOM) describing the dynamics of rigid body\nsystems are becoming increasingly relevant for the robotics community and find\nmany applications in design and control of robotic systems. Controlling robots,\nand multibody systems comprising elastic components in particular, not only\nrequires smooth trajectories but also the time derivatives of the control\nforces/torques, hence of the EOM. This paper presents the time derivatives of\nthe EOM in closed form up to second-order as an alternative formulation to the\nexisting recursive algorithms for this purpose, which provides a direct insight\ninto the structure of the derivatives. The Lie group formulation for rigid body\nsystems is used giving rise to very compact and easily parameterized equations.", "AI": {"tldr": "论文提出了刚性体系统运动方程的二阶时间导数闭式解，替代现有递归算法，为机器人控制提供直接洞察。", "motivation": "机器人控制需要平滑轨迹及控制力/力矩的时间导数，现有递归算法缺乏直观性。", "method": "采用李群理论，推导出紧凑且易参数化的二阶时间导数闭式解。", "result": "提供了运动方程二阶导数的直接解析形式，结构清晰。", "conclusion": "闭式解为机器人设计和控制提供了更高效且直观的工具。"}}
{"id": "2507.11133", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11133", "abs": "https://arxiv.org/abs/2507.11133", "authors": ["Luca Beber", "Edoardo Lamon", "Giacomo Moretti", "Matteo Saveriano", "Luca Fambri", "Luigi Palopoli", "Daniele Fontanelli"], "title": "Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm", "comment": null, "summary": "Diagnostic activities, such as ultrasound scans and palpation, are relatively\nlow-cost. They play a crucial role in the early detection of health problems\nand in assessing their progression. However, they are also error-prone\nactivities, which require highly skilled medical staff. The use of robotic\nsolutions can be key to decreasing the inherent subjectivity of the results and\nreducing the waiting list. For a robot to perform palpation or ultrasound\nscans, it must effectively manage physical interactions with the human body,\nwhich greatly benefits from precise estimation of the patient's tissue\nbiomechanical properties. This paper assesses the accuracy and precision of a\nrobotic system in estimating the viscoelastic parameters of various materials,\nincluding some tests on ex vivo tissues as a preliminary proof-of-concept\ndemonstration of the method's applicability to biological samples. The\nmeasurements are compared against a ground truth derived from silicone\nspecimens with different viscoelastic properties, characterised using a\nhigh-precision instrument. Experimental results show that the robotic system's\naccuracy closely matches the ground truth, increasing confidence in the\npotential use of robots for such clinical applications.", "AI": {"tldr": "论文探讨了机器人系统在估计材料粘弹性参数方面的准确性，初步验证了其在生物样本中的应用潜力。", "motivation": "诊断活动（如超声扫描和触诊）虽然成本低，但易出错且依赖高技能医疗人员。机器人解决方案可减少结果的主观性并缩短等待时间。", "method": "使用机器人系统估计不同材料（包括离体组织）的粘弹性参数，并与高精度仪器测得的硅胶标本基准数据对比。", "result": "实验结果显示机器人系统的准确性接近基准数据，增强了其在临床应用中的可信度。", "conclusion": "机器人系统在估计组织生物力学特性方面表现出潜力，为未来临床诊断应用提供了支持。"}}
{"id": "2507.11170", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11170", "abs": "https://arxiv.org/abs/2507.11170", "authors": ["Giulio Giacomuzzo", "Mohamed Abdelwahab", "Marco Calì", "Alberto Dalla Libera", "Ruggero Carli"], "title": "A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty", "comment": null, "summary": "In this paper, we propose a novel learning-based robust feedback\nlinearization strategy to ensure precise trajectory tracking for an important\nfamily of Lagrangian systems. We assume a nominal knowledge of the dynamics is\ngiven but no a-priori bounds on the model mismatch are available. In our\napproach, the key ingredient is the adoption of a regression framework based on\nGaussian Processes (GPR) to estimate the model mismatch. This estimate is added\nto the outer loop of a classical feedback linearization scheme based on the\nnominal knowledge available. Then, to compensate for the residual uncertainty,\nwe robustify the controller including an additional term whose size is designed\nbased on the variance provided by the GPR framework. We proved that, with high\nprobability, the proposed scheme is able to guarantee asymptotic tracking of a\ndesired trajectory. We tested numerically our strategy on a 2 degrees of\nfreedom planar robot.", "AI": {"tldr": "提出一种基于学习的新型鲁棒反馈线性化策略，用于拉格朗日系统的精确轨迹跟踪。", "motivation": "解决拉格朗日系统在模型不匹配且无先验界的情况下实现精确轨迹跟踪的问题。", "method": "采用高斯过程回归（GPR）估计模型不匹配，并将其加入基于名义动力学的反馈线性化外环，同时通过GPR方差设计鲁棒补偿项。", "result": "证明了方案能以高概率保证渐近跟踪目标轨迹，并在2自由度平面机器人上进行了数值验证。", "conclusion": "提出的策略有效解决了模型不匹配问题，实现了高精度的轨迹跟踪。"}}
{"id": "2507.11241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11241", "abs": "https://arxiv.org/abs/2507.11241", "authors": ["Tobias Kern", "Leon Tolksdorf", "Christian Birkner"], "title": "Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors", "comment": null, "summary": "Physically reduced-scale vehicles are emerging to accelerate the development\nof advanced automated driving functions. In this paper, we investigate the\neffects of scaling on self-localization accuracy with visual and\nvisual-inertial algorithms using cameras and an inertial measurement unit\n(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms\nare selected, and datasets are chosen as a baseline for real-sized vehicles. A\ntest drive is conducted to record data of reduced-scale vehicles. We compare\nthe selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in\nterms of their pose accuracy against the ground-truth and against data from\nreal-sized vehicles. When comparing the implementation of the selected\nlocalization algorithms to real-sized vehicles, OpenVINS has the lowest average\nlocalization error. Although all selected localization algorithms have\noverlapping error ranges, OpenVINS also performs best when applied to a\nreduced-scale vehicle. When reduced-scale vehicles were compared to real-sized\nvehicles, minor differences were found in translational vehicle motion\nestimation accuracy. However, no significant differences were found when\ncomparing the estimation accuracy of rotational vehicle motion, allowing RSVRs\nto be used as testing platforms for self-localization algorithms.", "AI": {"tldr": "研究探讨了物理缩小比例车辆对视觉和视觉-惯性自定位算法精度的影响，发现OpenVINS在缩小比例和真实尺寸车辆中表现最佳，且缩小比例车辆可作为自定位算法的测试平台。", "motivation": "加速高级自动驾驶功能的开发，需验证缩小比例车辆对自定位算法的影响。", "method": "选择ROS2兼容的视觉和视觉-惯性算法（OpenVINS、VINS-Fusion、RTAB-Map），基于真实尺寸车辆数据对比缩小比例车辆的测试数据。", "result": "OpenVINS平均定位误差最低，缩小比例与真实尺寸车辆在旋转运动估计精度上无显著差异。", "conclusion": "缩小比例车辆可作为自定位算法的有效测试平台，OpenVINS表现最优。"}}
{"id": "2507.11270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11270", "abs": "https://arxiv.org/abs/2507.11270", "authors": ["Ting-Wei Ou", "Jia-Hao Jiang", "Guan-Lin Huang", "Kuu-Young Young"], "title": "Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection", "comment": "Accepted to the IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) 2025", "summary": "The COVID-19 pandemic has severely affected public health, healthcare\nsystems, and daily life, especially amid resource shortages and limited\nworkers. This crisis has underscored the urgent need for automation in hospital\nenvironments, particularly disinfection, which is crucial to controlling virus\ntransmission and improving the safety of healthcare personnel and patients.\nUltraviolet (UV) light disinfection, known for its high efficiency, has been\nwidely adopted in hospital settings. However, most existing research focuses on\nmaximizing UV coverage while paying little attention to the impact of human\nactivity on virus distribution. To address this issue, we propose a mobile\nrobotic system for UV disinfection focusing on the virus hotspot. The system\nprioritizes disinfection in high-risk areas and employs an approach for\noptimized UV dosage to ensure that all surfaces receive an adequate level of UV\nexposure while significantly reducing disinfection time. It not only improves\ndisinfection efficiency but also minimizes unnecessary exposure in low-risk\nareas. In two representative hospital scenarios, our method achieves the same\ndisinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,\nrespectively. The video of the experiment is available at:\nhttps://youtu.be/wHcWzOcoMPM.", "AI": {"tldr": "提出一种针对病毒热点区域的移动机器人紫外线消毒系统，优化消毒效率并减少时间。", "motivation": "COVID-19疫情凸显医院消毒自动化的紧迫性，现有紫外线消毒研究忽视人类活动对病毒分布的影响。", "method": "设计移动机器人系统，优先消毒高风险区域，优化紫外线剂量分配。", "result": "在两医院场景中，消毒时间分别减少30.7%和31.9%，效果相同。", "conclusion": "该系统显著提升消毒效率，减少低风险区域不必要的紫外线暴露。"}}
{"id": "2507.11296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11296", "abs": "https://arxiv.org/abs/2507.11296", "authors": ["Huilin Xu", "Jian Ding", "Jiakun Xu", "Ruixiang Wang", "Jun Chen", "Jinjie Mai", "Yanwei Fu", "Bernard Ghanem", "Feng Xu", "Mohamed Elhoseiny"], "title": "Diffusion-Based Imaginative Coordination for Bimanual Manipulation", "comment": "15 pages, including 10 figures and 16 tables. Accepted at ICCV 2025", "summary": "Bimanual manipulation is crucial in robotics, enabling complex tasks in\nindustrial automation and household services. However, it poses significant\nchallenges due to the high-dimensional action space and intricate coordination\nrequirements. While video prediction has been recently studied for\nrepresentation learning and control, leveraging its ability to capture rich\ndynamic and behavioral information, its potential for enhancing bimanual\ncoordination remains underexplored. To bridge this gap, we propose a unified\ndiffusion-based framework for the joint optimization of video and action\nprediction. Specifically, we propose a multi-frame latent prediction strategy\nthat encodes future states in a compressed latent space, preserving\ntask-relevant features. Furthermore, we introduce a unidirectional attention\nmechanism where video prediction is conditioned on the action, while action\nprediction remains independent of video prediction. This design allows us to\nomit video prediction during inference, significantly enhancing efficiency.\nExperiments on two simulated benchmarks and a real-world setting demonstrate a\nsignificant improvement in the success rate over the strong baseline ACT using\nour method, achieving a \\textbf{24.9\\%} increase on ALOHA, an \\textbf{11.1\\%}\nincrease on RoboTwin, and a \\textbf{32.5\\%} increase in real-world experiments.\nOur models and code are publicly available at\nhttps://github.com/return-sleep/Diffusion_based_imaginative_Coordination.", "AI": {"tldr": "提出了一种基于扩散的统一框架，通过联合优化视频和动作预测来提升双手机器人操作的协调性，实验表明性能显著提升。", "motivation": "双手机器人操作在工业自动化和家庭服务中至关重要，但由于高维动作空间和复杂协调需求，存在显著挑战。视频预测在表示学习和控制中的应用潜力尚未充分探索。", "method": "提出了一种多帧潜在预测策略，在压缩潜在空间中编码未来状态，并引入单向注意力机制，视频预测依赖于动作，而动作预测独立于视频预测。", "result": "在两个模拟基准和真实环境中，相比基线ACT，成功率显著提升：ALOHA提高24.9%，RoboTwin提高11.1%，真实实验提高32.5%。", "conclusion": "该框架通过联合优化视频和动作预测，显著提升了双手机器人操作的协调性和效率，代码和模型已开源。"}}
{"id": "2507.11302", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11302", "abs": "https://arxiv.org/abs/2507.11302", "authors": ["Jesse J. Hagenaars", "Stein Stroobants", "Sander M. Bohte", "Guido C. H. E. De Croon"], "title": "All Eyes, no IMU: Learning Flight Attitude from Vision Alone", "comment": null, "summary": "Vision is an essential part of attitude control for many flying animals, some\nof which have no dedicated sense of gravity. Flying robots, on the other hand,\ntypically depend heavily on accelerometers and gyroscopes for attitude\nstabilization. In this work, we present the first vision-only approach to\nflight control for use in generic environments. We show that a quadrotor drone\nequipped with a downward-facing event camera can estimate its attitude and\nrotation rate from just the event stream, enabling flight control without\ninertial sensors. Our approach uses a small recurrent convolutional neural\nnetwork trained through supervised learning. Real-world flight tests\ndemonstrate that our combination of event camera and low-latency neural network\nis capable of replacing the inertial measurement unit in a traditional flight\ncontrol loop. Furthermore, we investigate the network's generalization across\ndifferent environments, and the impact of memory and different fields of view.\nWhile networks with memory and access to horizon-like visual cues achieve best\nperformance, variants with a narrower field of view achieve better relative\ngeneralization. Our work showcases vision-only flight control as a promising\ncandidate for enabling autonomous, insect-scale flying robots.", "AI": {"tldr": "提出了一种仅依赖视觉的飞行控制方法，使用事件相机和神经网络替代传统惯性传感器，实现无人机姿态控制。", "motivation": "许多飞行生物依赖视觉而非惯性传感器控制姿态，而现有飞行机器人通常依赖加速度计和陀螺仪。本研究旨在探索仅依赖视觉的飞行控制方法。", "method": "采用向下事件相机和递归卷积神经网络，通过监督学习训练，从事件流中估计姿态和旋转速率。", "result": "实验表明，该方法能替代传统惯性测量单元，且网络在泛化性和视野范围方面表现良好。", "conclusion": "视觉飞行控制是实现小型自主飞行机器人的有前景方案。"}}
{"id": "2507.11345", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11345", "abs": "https://arxiv.org/abs/2507.11345", "authors": ["Oscar Lima", "Marc Vinci", "Sunandita Patra", "Sebastian Stock", "Joachim Hertzberg", "Martin Atzmueller", "Malik Ghallab", "Dana Nau", "Paolo Traverso"], "title": "Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM", "comment": "Accepted in ECMR 2025 conference", "summary": "Robotic task execution faces challenges due to the inconsistency between\nsymbolic planner models and the rich control structures actually running on the\nrobot. In this paper, we present the first physical deployment of an integrated\nactor-planner system that shares hierarchical operational models for both\nacting and planning, interleaving the Reactive Acting Engine (RAE) with an\nanytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile\nmanipulator in a real-world deployment for an object collection task. Our\nexperiments demonstrate robust task execution under action failures and sensor\nnoise, and provide empirical insights into the interleaved acting-and-planning\ndecision making process.", "AI": {"tldr": "论文提出了一种集成执行器-规划器系统（RAE+UPOM），通过共享层次化操作模型，实现了在机器人任务执行中的鲁棒性。", "motivation": "解决符号规划器模型与实际机器人控制结构之间的不一致性问题。", "method": "结合反应式执行引擎（RAE）和蒙特卡洛规划器（UPOM），在移动机械臂上实现物体收集任务。", "result": "实验表明系统在动作失败和传感器噪声下仍能鲁棒执行任务。", "conclusion": "RAE+UPOM系统为实时决策提供了有效框架。"}}
{"id": "2507.11402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11402", "abs": "https://arxiv.org/abs/2507.11402", "authors": ["Supun Dissanayaka", "Alexander Ferrein", "Till Hofmann", "Kosuke Nakajima", "Mario Sanz-Lopez", "Jesus Savage", "Daniel Swoboda", "Matteo Tschesche", "Wataru Uemura", "Tarik Viehmann", "Shohei Yasuda"], "title": "From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League", "comment": "RoboCup Symposium 2025", "summary": "The RoboCup Logistics League is a RoboCup competition in a smart factory\nscenario that has focused on task planning, job scheduling, and multi-agent\ncoordination. The focus on production logistics allowed teams to develop highly\ncompetitive strategies, but also meant that some recent developments in the\ncontext of smart manufacturing are not reflected in the competition, weakening\nits relevance over the years. In this paper, we describe the vision for the\nRoboCup Smart Manufacturing League, a new competition designed as a larger\nsmart manufacturing scenario, reflecting all the major aspects of a modern\nfactory. It will consist of several tracks that are initially independent but\ngradually combined into one smart manufacturing scenario. The new tracks will\ncover industrial robotics challenges such as assembly, human-robot\ncollaboration, and humanoid robotics, but also retain a focus on production\nlogistics. We expect the reenvisioned competition to be more attractive to\nnewcomers and well-tried teams, while also shifting the focus to current and\nfuture challenges of industrial robotics.", "AI": {"tldr": "RoboCup Logistics League的局限性促使了RoboCup Smart Manufacturing League的提出，后者旨在涵盖现代工厂的更多方面，包括工业机器人挑战和生产物流。", "motivation": "RoboCup Logistics League未能反映智能制造的近期发展，削弱了其相关性，因此需要一个新的竞赛来涵盖现代工厂的所有主要方面。", "method": "设计一个更大的智能制造场景，包含多个独立的轨道，逐步整合为一个完整的智能制造场景，涵盖工业机器人挑战和生产物流。", "result": "新竞赛预计将更具吸引力，并聚焦于工业机器人的当前和未来挑战。", "conclusion": "RoboCup Smart Manufacturing League将提升竞赛的相关性和吸引力，同时应对工业机器人的新挑战。"}}
{"id": "2507.11460", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11460", "abs": "https://arxiv.org/abs/2507.11460", "authors": ["Jacinto Colan", "Ana Davila", "Yutaro Yamada", "Yasuhisa Hasegawa"], "title": "Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Human-robot collaboration in surgery represents a significant area of\nresearch, driven by the increasing capability of autonomous robotic systems to\nassist surgeons in complex procedures. This systematic review examines the\nadvancements and persistent challenges in the development of autonomous\nsurgical robotic assistants (ASARs), focusing specifically on scenarios where\nrobots provide meaningful and active support to human surgeons. Adhering to the\nPRISMA guidelines, a comprehensive literature search was conducted across the\nIEEE Xplore, Scopus, and Web of Science databases, resulting in the selection\nof 32 studies for detailed analysis. Two primary collaborative setups were\nidentified: teleoperation-based assistance and direct hands-on interaction. The\nfindings reveal a growing research emphasis on ASARs, with predominant\napplications currently in endoscope guidance, alongside emerging progress in\nautonomous tool manipulation. Several key challenges hinder wider adoption,\nincluding the alignment of robotic actions with human surgeon preferences, the\nnecessity for procedural awareness within autonomous systems, the establishment\nof seamless human-robot information exchange, and the complexities of skill\nacquisition in shared workspaces. This review synthesizes current trends,\nidentifies critical limitations, and outlines future research directions\nessential to improve the reliability, safety, and effectiveness of human-robot\ncollaboration in surgical environments.", "AI": {"tldr": "系统综述探讨了自主手术机器人助手（ASARs）的研究进展与挑战，重点分析了人机协作的两种模式及其应用与问题。", "motivation": "随着自主机器人系统在复杂手术中辅助能力的提升，研究人机协作的需求日益增长。", "method": "遵循PRISMA指南，对IEEE Xplore、Scopus和Web of Science数据库的32项研究进行了分析。", "result": "研究发现ASARs在手术中主要应用于内窥镜引导，同时面临动作对齐、程序意识、信息交换和技能获取等挑战。", "conclusion": "综述总结了当前趋势，指出了关键限制，并提出了未来研究方向以提升手术中人机协作的可靠性和安全性。"}}
{"id": "2507.11464", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11464", "abs": "https://arxiv.org/abs/2507.11464", "authors": ["Ajay Shankar", "Keisuke Okumura", "Amanda Prorok"], "title": "LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control", "comment": "9 pages; under review for IEEE Robotics & Automation - Letters (RA-L)", "summary": "We propose a multi-robot control paradigm to solve point-to-point navigation\ntasks for a team of holonomic robots with access to the full environment\ninformation. The framework invokes two processes asynchronously at high\nfrequency: (i) a centralized, discrete, and full-horizon planner for computing\ncollision- and deadlock-free paths rapidly, leveraging recent advances in\nmulti-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal\ntrajectory controllers that ensure all robots independently follow their\nassigned paths reliably. This hierarchical shift in planning representation\nfrom (i) discrete and coupled to (ii) continuous and decoupled domains enables\nthe framework to maintain long-term scalable motion synthesis. As an\ninstantiation of this idea, we present LF, which combines a fast\nstate-of-the-art MAPF solver (LaCAM), and a robust feedback control stack\n(Freyja) for executing agile robot maneuvers. LF provides a robust and\nversatile mechanism for lifelong multi-robot navigation even under asynchronous\nand partial goal updates, and adapts to dynamic workspaces simply by quick\nreplanning. We present various multirotor and ground robot demonstrations,\nincluding the deployment of 15 real multirotors with random, consecutive target\nupdates while a person walks through the operational workspace.", "AI": {"tldr": "提出了一种多机器人控制框架，结合集中式离散规划和分布式连续控制，实现高效、可扩展的点对点导航。", "motivation": "解决多机器人在动态环境中的导航问题，确保无碰撞、无死锁，同时适应异步目标更新和动态工作空间。", "method": "采用分层框架：(1) 集中式离散规划器（LaCAM）快速生成无碰撞路径；(2) 分布式连续控制器（Freyja）独立执行轨迹跟踪。", "result": "展示了15个真实多旋翼机器人在动态环境中的导航能力，适应异步目标更新和人员干扰。", "conclusion": "该框架通过分层规划与控制实现了高效、鲁棒的多机器人导航，适用于动态和异步场景。"}}
{"id": "2507.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11498", "abs": "https://arxiv.org/abs/2507.11498", "authors": ["Asad Ali Shahid", "Francesco Braghin", "Loris Roveda"], "title": "Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming", "comment": null, "summary": "Humanoid robots have seen remarkable advances in dexterity, balance, and\nlocomotion, yet their role in expressive domains, such as music performance,\nremains largely unexplored. Musical tasks, like drumming, present unique\nchallenges, including split-second timing, rapid contacts, and multi-limb\ncoordination over pieces lasting minutes. In this paper, we introduce Robot\nDrummer, a humanoid system capable of expressive, high-precision drumming\nacross a diverse repertoire of songs. We formulate humanoid drumming as\nsequential fulfillment of timed-contacts and transform drum scores in to a\nRhythmic Contact Chain. To handle the long-horizon nature of musical\nperformance, we decompose each piece into fixed-length segments and train a\nsingle policy across all segments in parallel using reinforcement learning.\nThrough extensive experiments on over thirty popular rock, metal, and jazz\ntracks, our results demonstrate that Robot Drummer consistently achieves high\nF1 scores. The learned behaviors exhibit emergent human-like drumming\nstrategies, such as cross-arm strikes, and adaptive sticks assignments,\ndemonstrating the potential of reinforcement learning to bring humanoid robots\ninto the domain of creative musical performance. Project page:\n\\href{https://robot-drummer.github.io}{robot-drummer.github.io}", "AI": {"tldr": "本文介绍了Robot Drummer，一种能够演奏多样化歌曲的人形机器人系统，通过强化学习实现了高精度和表现力的鼓乐演奏。", "motivation": "探索人形机器人在音乐表演等表达性领域的潜力，解决鼓乐演奏中的分秒计时、快速接触和多肢协调等挑战。", "method": "将鼓乐演奏建模为定时接触的序列任务，将鼓谱转化为节奏接触链，并通过强化学习训练分段策略。", "result": "在超过三十首流行摇滚、金属和爵士曲目中，Robot Drummer表现出高F1分数，并涌现出类似人类的鼓乐策略。", "conclusion": "研究表明强化学习能够将人形机器人引入创造性音乐表演领域，展示了其潜力。"}}
{"id": "2507.11525", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11525", "abs": "https://arxiv.org/abs/2507.11525", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "LLM-based ambiguity detection in natural language instructions for collaborative surgical robots", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Ambiguity in natural language instructions poses significant risks in\nsafety-critical human-robot interaction, particularly in domains such as\nsurgery. To address this, we propose a framework that uses Large Language\nModels (LLMs) for ambiguity detection specifically designed for collaborative\nsurgical scenarios. Our method employs an ensemble of LLM evaluators, each\nconfigured with distinct prompting techniques to identify linguistic,\ncontextual, procedural, and critical ambiguities. A chain-of-thought evaluator\nis included to systematically analyze instruction structure for potential\nissues. Individual evaluator assessments are synthesized through conformal\nprediction, which yields non-conformity scores based on comparison to a labeled\ncalibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed\nclassification accuracy exceeding 60% in differentiating ambiguous from\nunambiguous surgical instructions. Our approach improves the safety and\nreliability of human-robot collaboration in surgery by offering a mechanism to\nidentify potentially ambiguous instructions before robot action.", "AI": {"tldr": "提出了一种基于大语言模型（LLMs）的框架，用于检测手术场景中的指令歧义，通过集成多个LLM评估器和共形预测提高分类准确性。", "motivation": "自然语言指令的歧义在安全关键的人机交互（如手术）中存在风险，需要一种机制提前识别歧义指令以提高安全性。", "method": "采用集成LLM评估器，结合不同提示技术识别多种歧义类型，并通过共形预测合成评估结果。", "result": "在区分手术指令歧义时，Llama 3.2 11B和Gemma 3 12B的分类准确率超过60%。", "conclusion": "该框架通过提前识别歧义指令，提升了手术中人机协作的安全性和可靠性。"}}
