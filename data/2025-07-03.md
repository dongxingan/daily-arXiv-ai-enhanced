<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios](https://arxiv.org/abs/2507.01111)
*Haosen Xing,Haoran Ma,Sijin Zhang,Hartmut Geyer*

Main category: cs.RO

TL;DR: 提出了一种融合环境感知和用户意图的新型控制策略，用于下肢假肢的障碍物导航，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有假肢控制策略缺乏对环境及用户意图的感知，尤其在复杂地形中表现不足。

Method: 利用机载深度摄像头检测障碍物，动态调整摆动轨迹，结合用户生物力学信号实现自然步态。

Result: 实验显示，在150多次跨越和30多次踏上障碍物的测试中，成功率达100%。

Conclusion: 该系统有效解决了障碍物导航问题，展示了在复杂地形中的适应性和应用潜力。

Abstract: Current control strategies for powered lower limb prostheses often lack
awareness of the environment and the user's intended interactions with it. This
limitation becomes particularly apparent in complex terrains. Obstacle
negotiation, a critical scenario exemplifying such challenges, requires both
real-time perception of obstacle geometry and responsiveness to user intention
about when and where to step over or onto, to dynamically adjust swing
trajectories. We propose a novel control strategy that fuses environmental
awareness and human cooperativeness: an on-board depth camera detects obstacles
ahead of swing phase, prompting an elevated early-swing trajectory to ensure
clearance, while late-swing control defers to natural biomechanical cues from
the user. This approach enables intuitive stepping strategies without requiring
unnatural movement patterns. Experiments with three non-amputee participants
demonstrated 100 percent success across more than 150 step-overs and 30
step-ons with randomly placed obstacles of varying heights (4-16 cm) and
distances (15-70 cm). By effectively addressing obstacle navigation -- a
gateway challenge for complex terrain mobility -- our system demonstrates
adaptability to both environmental constraints and user intentions, with
promising applications across diverse locomotion scenarios.

</details>


### [2] [VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting](https://arxiv.org/abs/2507.01125)
*Keiko Nagami,Timothy Chen,Javier Yu,Ola Shorinwa,Maximilian Adang,Carlyn Dougherty,Eric Cristofalo,Mac Schwager*

Main category: cs.RO

TL;DR: VISTA是一种主动探索方法，帮助机器人规划信息丰富的轨迹，提升任务相关区域的3D地图质量。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在开放词汇搜索任务中高效探索环境并构建语义3D地图的需求。

Method: 通过规划优先考虑语义相似性和未探索区域的轨迹，结合新颖的视点-语义覆盖度量评估。

Result: 在静态数据集和硬件实验中表现优于现有方法，计算速度和重建质量更优。

Conclusion: VISTA具有平台无关性，适用于多种机器人平台，显著提升任务成功率。

Abstract: We present VISTA (Viewpoint-based Image selection with Semantic Task
Awareness), an active exploration method for robots to plan informative
trajectories that improve 3D map quality in areas most relevant for task
completion. Given an open-vocabulary search instruction (e.g., "find a
person"), VISTA enables a robot to explore its environment to search for the
object of interest, while simultaneously building a real-time semantic 3D
Gaussian Splatting reconstruction of the scene. The robot navigates its
environment by planning receding-horizon trajectories that prioritize semantic
similarity to the query and exploration of unseen regions of the environment.
To evaluate trajectories, VISTA introduces a novel, efficient
viewpoint-semantic coverage metric that quantifies both the geometric view
diversity and task relevance in the 3D scene. On static datasets, our coverage
metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in
computation speed and reconstruction quality. In quadrotor hardware
experiments, VISTA achieves 6x higher success rates in challenging maps,
compared to baseline methods, while matching baseline performance in less
challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying
it on a quadrotor drone and a Spot quadruped robot. Open-source code will be
released upon acceptance of the paper.

</details>


### [3] [A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods](https://arxiv.org/abs/2507.01143)
*Reza Jalayer,Masoud Jalayer,Amirali Baniasadi*

Main category: cs.RO

TL;DR: 本文综述了机器人领域中基于深度学习的声源定位技术，填补了现有综述的不足，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述多关注通用音频应用，未充分考虑机器人领域的限制和深度学习的最新进展，本文旨在填补这一空白。

Method: 回顾了经典方法（如TDOA、波束成形等）和现代深度学习方法（如CNN、CRNN等），并探讨了数据和训练策略。

Result: 总结了机器人声源定位的当前挑战，包括环境鲁棒性、多声源问题等，并提出了未来研究方向。

Conclusion: 提出了实现下一代机器人稳健、高效、可解释的深度学习声源定位的行动路线。

Abstract: Sound source localization (SSL) adds a spatial dimension to auditory
perception, allowing a system to pinpoint the origin of speech, machinery
noise, warning tones, or other acoustic events, capabilities that facilitate
robot navigation, human-machine dialogue, and condition monitoring. While
existing surveys provide valuable historical context, they typically address
general audio applications and do not fully account for robotic constraints or
the latest advancements in deep learning. This review addresses these gaps by
offering a robotics-focused synthesis, emphasizing recent progress in deep
learning methodologies. We start by reviewing classical methods such as Time
Difference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and
subspace analysis. Subsequently, we delve into modern machine learning (ML) and
deep learning (DL) approaches, discussing traditional ML and neural networks
(NNs), convolutional neural networks (CNNs), convolutional recurrent neural
networks (CRNNs), and emerging attention-based architectures. The data and
training strategy that are the two cornerstones of DL-based SSL are explored.
Studies are further categorized by robot types and application domains to
facilitate researchers in identifying relevant work for their specific
contexts. Finally, we highlight the current challenges in SSL works in general,
regarding environmental robustness, sound source multiplicity, and specific
implementation constraints in robotics, as well as data and learning strategies
in DL-based SSL. Also, we sketch promising directions to offer an actionable
roadmap toward robust, adaptable, efficient, and explainable DL-based SSL for
next-generation robots.

</details>


### [4] [SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound](https://arxiv.org/abs/2507.01152)
*Yunke Ao,Masoud Moghani,Mayank Mittal,Manish Prajapat,Luohong Wu,Frederic Giraud,Fabio Carrillo,Andreas Krause,Philipp Fürnstahl*

Main category: cs.RO

TL;DR: SonoGym是一个用于复杂机器人超声任务的模拟平台，支持并行模拟，结合深度强化学习和模仿学习，用于骨科手术中的自主导航和任务训练。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习和模仿学习在复杂手术任务中的应用受限，主要因为缺乏针对这些任务的真实高效模拟环境。

Method: 提出SonoGym平台，支持基于物理和生成模型的实时超声数据模拟，集成机器人平台和骨科工具，用于训练深度强化学习和模仿学习策略。

Result: 实验展示了在多种场景下策略学习的成功，同时揭示了当前方法在临床相关环境中的局限性。

Conclusion: SonoGym有助于推动机器人学习在复杂手术应用中的研究，相关数据和代码已公开。

Abstract: Ultrasound (US) is a widely used medical imaging modality due to its
real-time capabilities, non-invasive nature, and cost-effectiveness. Robotic
ultrasound can further enhance its utility by reducing operator dependence and
improving access to complex anatomical regions. For this, while deep
reinforcement learning (DRL) and imitation learning (IL) have shown potential
for autonomous navigation, their use in complex surgical tasks such as anatomy
reconstruction and surgical guidance remains limited -- largely due to the lack
of realistic and efficient simulation environments tailored to these tasks. We
introduce SonoGym, a scalable simulation platform for complex robotic
ultrasound tasks that enables parallel simulation across tens to hundreds of
environments. Our framework supports realistic and real-time simulation of US
data from CT-derived 3D models of the anatomy through both a physics-based and
a generative modeling approach. Sonogym enables the training of DRL and recent
IL agents (vision transformers and diffusion policies) for relevant tasks in
robotic orthopedic surgery by integrating common robotic platforms and
orthopedic end effectors. We further incorporate submodular DRL -- a recent
method that handles history-dependent rewards -- for anatomy reconstruction and
safe reinforcement learning for surgery. Our results demonstrate successful
policy learning across a range of scenarios, while also highlighting the
limitations of current methods in clinically relevant environments. We believe
our simulation can facilitate research in robot learning approaches for such
challenging robotic surgery applications. Dataset, codes, and videos are
publicly available at https://sonogym.github.io/.

</details>


### [5] [Approximation-free Control of Unknown Euler-Lagrangian Systems under Input Constraints](https://arxiv.org/abs/2507.01426)
*Ratnangshu Das,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 提出了一种基于漏斗的跟踪控制算法，用于未知动态和输入约束的机器人系统，通过仿真和实验验证了其鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人系统在未知动态和输入约束下的性能与执行器安全之间的权衡问题。

Method: 采用欧拉-拉格朗日建模，提出两种无近似控制策略，确保跟踪误差在预设漏斗边界内演化。

Result: 通过仿真和实验验证了算法的鲁棒性能和安全性。

Conclusion: 该研究显著提升了漏斗控制在具有输入约束的实际机器人系统中的适用性。

Abstract: In this paper, we present a novel funnel-based tracking control algorithm for
robotic systems with unknown dynamics and prescribed input constraints. The
Euler-Lagrange formulation, a common modeling approach for robotic systems, has
been adopted in this study to address the trade-off between performance and
actuator safety. We establish feasibility conditions that ensure tracking
errors evolve within predefined funnel bounds while maintaining bounded control
efforts, a crucial consideration for robots with limited actuation
capabilities. We propose two approximation-free control strategies for
scenarios where these conditions are violated: one actively corrects the error,
and the other stops further deviation. Finally, we demonstrate the robust
performance and safety of the approach through simulations and experimental
validations. This work represents a significant advancement in funnel-based
control, enhancing its applicability to real-world robotics systems with input
constraints.

</details>


### [6] [A Differentiable Distance Metric for Robotics Through Generalized Alternating Projection](https://arxiv.org/abs/2507.01181)
*Vinicius M. Gonçalves,Shiqing Wei,Eduardo Malacarne S. de Souza,Krishnamurthy Prashanth,Anthony Tzes,Farshad Khorrami*

Main category: cs.RO

TL;DR: 本文提出了一种更简单且实用的平滑投影方法，用于计算凸多面体的可微距离度量，解决了现有方法的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得距离不可微，而现有可微距离度量方法存在重要缺陷，本文旨在解决这些问题。

Method: 提出了一种新的平滑投影方法，适用于一般凸多面体，并确保距离在物体重叠时消失。

Result: 实验结果表明，该方法有效且优于现有方法。

Conclusion: 本文提出的可微距离度量方法更简单实用，已通过Python仿真包UAIBot公开。

Abstract: In many robotics applications, it is necessary to compute not only the
distance between the robot and the environment, but also its derivative - for
example, when using control barrier functions. However, since the traditional
Euclidean distance is not differentiable, there is a need for alternative
distance metrics that possess this property. Recently, a metric with guaranteed
differentiability was proposed [1]. This approach has some important drawbacks,
which we address in this paper. We provide much simpler and practical
expressions for the smooth projection for general convex polytopes.
Additionally, as opposed to [1], we ensure that the distance vanishes as the
objects overlap. We show the efficacy of the approach in experimental results.
Our proposed distance metric is publicly available through the Python-based
simulation package UAIBot.

</details>


### [7] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Main category: cs.RO

TL;DR: 提出了一种结合采样和搜索的运动规划算法，利用自由配置空间的burs作为自适应运动基元，显著提高路径规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统固定尺寸运动基元在复杂场景中效率低，需要更高效的自适应方法。

Method: 在SMPL库中实现burs作为自适应运动基元，结合图搜索算法。

Result: 在复杂场景中，尤其高自由度机械臂，性能优于固定基元方法。

Conclusion: burs自适应基元显著提升规划效率，适用于复杂和高自由度场景。

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [8] [2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration](https://arxiv.org/abs/2507.01206)
*Kathy Zhuang,Zixun Huang,Yukun Song,Rui Li,Yinuo Zhou,Allen Y. Yang*

Main category: cs.RO

TL;DR: URSA是一个基于LLM的AR系统，用于NASA的SUITS挑战，结合AR设备、语音控制和机器人跟踪技术，支持实时机器人控制和任务可视化。


<details>
  <summary>Details</summary>
Motivation: 解决移动AR中的人机交互问题，特别是在复杂动态环境中的3D物体姿态估计，为未来的太空任务（如Artemis）提供支持。

Method: 整合头戴式AR设备、LLM驱动的语音控制、机器人跟踪算法和数字孪生定位技术，使用DTTD-Mobile数据集和ZED2相机。

Result: 开发了非侵入式AR界面、专用数据集、任务可视化控制台、优化的6DoF姿态估计器和端到端集成系统。

Conclusion: URSA推动了数字孪生在机器人领域的应用，为航空航天和工业领域提供了可扩展的解决方案。

Abstract: As modern computing advances, new interaction paradigms have emerged,
particularly in Augmented Reality (AR), which overlays virtual interfaces onto
physical objects. This evolution poses challenges in machine perception,
especially for tasks like 3D object pose estimation in complex, dynamic
environments. Our project addresses critical issues in human-robot interaction
within mobile AR, focusing on non-intrusive, spatially aware interfaces. We
present URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024
SUITS challenge, targeting future spaceflight needs such as the Artemis
missions. URSA integrates three core technologies: a head-mounted AR device
(e.g., HoloLens) for intuitive visual feedback, voice control powered by large
language models for hands-free interaction, and robot tracking algorithms that
enable accurate 3D localization in dynamic settings. To enhance precision, we
leverage digital twin localization technologies, using datasets like
DTTD-Mobile and specialized hardware such as the ZED2 camera for real-world
tracking under noise and occlusion. Our system enables real-time robot control
and monitoring via an AR interface, even in the absence of ground-truth
sensors--vital for hazardous or remote operations. Key contributions include:
(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based
dataset tailored for non-rigid robotic bodies; (3) a Local Mission Control
Console (LMCC) for mission visualization; (4) a transformer-based 6DoF pose
estimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)
end-to-end integration for astronaut mission support. This work advances
digital twin applications in robotics, offering scalable solutions for both
aerospace and industrial domains.

</details>


### [9] [Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion](https://arxiv.org/abs/2507.01243)
*Ziang Zheng,Guojian Zhan,Shiqi Liu,Yao Lyu,Tao Zhang,Shengbo Eben Li*

Main category: cs.RO

TL;DR: JumpER是一种通过自演化先验分阶段训练强化学习策略的框架，成功解决了四足机器人在极端欠驱动和极端地形下的单足跳跃任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法在同时应对极端欠驱动和极端地形时，由于早期交互不稳定和奖励反馈不可靠，难以直接训练有效的策略。

Method: JumpER通过多阶段策略学习，动态生成自演化先验，逐步优化策略，无需外部专家先验或手工奖励设计。

Result: JumpER使四足机器人首次在不可预测地形上实现稳健的单足跳跃，并成功应对传统方法难以处理的挑战性场景。

Conclusion: JumpER为极端欠驱动和极端地形下的运动任务提供了一种原则性和可扩展的解决方案。

Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.

</details>


### [10] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Main category: cs.RO

TL;DR: 提出了一种基于大语言模型（LLM）的框架，用于在CARLA模拟器中自动生成多样化和安全关键的驾驶场景，并结合视频生成技术提升场景的真实性。


<details>
  <summary>Details</summary>
Motivation: 评估自动驾驶系统需要多样化和安全关键的驾驶场景，但手动设计这些场景耗时且难以覆盖所有边缘情况。

Method: 利用LLM进行少样本代码生成，自动合成驾驶场景脚本，并通过视频生成技术（Cosmos-Transfer1与ControlNet）提升场景的真实性。

Result: 实验表明，该方法能高效生成多样、真实且安全关键的驾驶场景，包括罕见的边缘情况。

Conclusion: 该方法为自动驾驶系统的仿真测试提供了高效且可控的场景生成工具。

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [11] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: VLAD是一个结合视觉语言模型（VLM）和端到端自动驾驶系统（VAD）的模型，通过定制问答数据集微调VLM，提升空间推理能力，生成导航指令并解释驾驶决策，显著降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型（如LLaVA、Qwen-VL等）的通用知识，提升自动驾驶的感知、预测和规划能力。

Method: 提出VLAD模型，通过定制问答数据集微调VLM，增强其空间推理能力，生成导航指令并与VAD系统集成，同时提供自然语言解释。

Result: 在nuScenes数据集上测试，VLAD将平均碰撞率降低31.82%，优于基线方法。

Conclusion: VLAD为视觉语言模型增强的自动驾驶系统设定了新基准，提高了透明度和可信度。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [12] [LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction](https://arxiv.org/abs/2507.01308)
*Muhammad Atta ur Rahman,Dooseop Choi,KyoungWook Min*

Main category: cs.RO

TL;DR: 论文提出了一种基于多向量地图元素的运动预测模型，通过融合车道边界和道路边缘等信息，提升自动驾驶中轨迹预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于车道中心线的运动预测模型无法充分捕捉道路环境和交通规则，限制了预测能力。

Method: 开发了一种特征融合策略和修剪机制，结合多向量地图元素，并过滤无关连接以提高计算效率。

Result: 在Argoverse 2数据集上验证了方法的竞争力，性能有所提升。

Conclusion: 该方法克服了车道中心线模型的局限性，提供了更高效和全面的驾驶环境表示，推动了自动驾驶运动预测的技术进步。

Abstract: Accurate motion forecasting is critical for safe and efficient autonomous
driving, enabling vehicles to predict future trajectories and make informed
decisions in complex traffic scenarios. Most of the current designs of motion
prediction models are based on the major representation of lane centerlines,
which limits their capability to capture critical road environments and traffic
rules and constraints. In this work, we propose an enhanced motion forecasting
model informed by multiple vector map elements, including lane boundaries and
road edges, that facilitates a richer and more complete representation of
driving environments. An effective feature fusion strategy is developed to
merge information in different vector map components, where the model learns
holistic information on road structures and their interactions with agents.
Since encoding more information about the road environment increases memory
usage and is computationally expensive, we developed an effective pruning
mechanism that filters the most relevant map connections to the target agent,
ensuring computational efficiency while maintaining essential spatial and
semantic relationships for accurate trajectory prediction. Overcoming the
limitations of lane centerline-based models, our method provides a more
informative and efficient representation of the driving environment and
advances the state of the art for autonomous vehicle motion forecasting. We
verify our approach with extensive experiments on the Argoverse 2 motion
forecasting dataset, where our method maintains competitiveness on AV2 while
achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements,
road topology, connection pruning, Argoverse 2.

</details>


### [13] [TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control](https://arxiv.org/abs/2507.01424)
*Zhenyang Liu,Yongchong Gu,Sixiao Zheng,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: TriVLA是一种统一的三系统架构视觉-语言-动作模型，用于通用机器人控制，通过动态感知模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归VLA方法常忽略动态信息，而动态信息对机器人任务至关重要。

Method: TriVLA采用三系统架构：视觉-语言模块、动态感知模块和策略学习模块，结合预训练模型和机器人数据。

Result: TriVLA在36Hz下运行，性能优于现有模仿学习方法。

Conclusion: TriVLA通过动态感知模块显著提升了机器人控制的性能。

Abstract: Recent advancements in vision-language models (VLMs) for common-sense
reasoning have led to the development of vision-language-action (VLA) models,
enabling robots to perform generalized manipulation. Although existing
autoregressive VLA methods design a specific architecture like dual-system to
leverage large-scale pretrained knowledge, they tend to capture static
information, often neglecting the dynamic aspects vital for embodied tasks. To
this end, we propose TriVLA, a unified Vision-Language-Action model with a
triple-system architecture for general robot control. The vision-language
module (System 2) interprets the environment through vision and language
instructions. The dynamics perception module (System 3) inherently produces
visual representations that encompass both current static information and
predicted future dynamics, thereby providing valuable guidance for policy
learning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained
video foundation model on robot datasets along with internet human manipulation
data. The subsequent policy learning module (System 1) generates fluid motor
actions in real time. Experimental evaluation demonstrates that TriVLA operates
at approximately 36 Hz and surpasses state-of-the-art imitation learning
baselines on standard simulation benchmarks as well as challenging real-world
manipulation tasks.

</details>


### [14] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: 研究探讨混合量子-经典算法在工业环境中优化机器人检测轨迹的应用，与传统方法相比，量子方法在计算时间上显著减少。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在工业自动化中的潜力，特别是在优化复杂任务如机器人检测轨迹时。

Method: 将任务建模为3D旅行商问题，使用D-Wave量子求解器与传统方法（GUROBI、Google OR-Tools）对比。

Result: 在五个实际案例中，量子方法在解质量和计算时间上表现竞争力。

Conclusion: 量子方法在工业4.0自动化中具有潜力，尤其在需要快速优化的场景。

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [15] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Main category: cs.RO

TL;DR: BioMARS是一个结合大型语言模型（LLMs）、视觉语言模型（VLMs）和模块化机器人的智能平台，用于自主设计、规划和执行生物实验，性能优于人工操作。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs和VLMs在生物研究中的应用受限于协议设计的刚性、动态实验室条件的适应性不足、错误处理能力有限以及操作复杂性高。

Method: BioMARS采用分层架构：Biologist Agent通过检索增强生成合成协议；Technician Agent将其转化为可执行的机器人伪代码；Inspector Agent通过多模态感知和异常检测确保程序完整性。

Result: 系统在细胞传代和培养任务中表现优于或等同于人工操作，并在视网膜色素上皮细胞分化中优于传统策略。

Conclusion: BioMARS展示了通用AI驱动的实验室自动化的可行性，以及语言推理在生物研究中的变革作用。

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


### [16] [Dynamic System Model Generation for Online Fault Detection and Diagnosis of Robotic Systems](https://arxiv.org/abs/2507.01550)
*Johannes Kohl,Georg Muck,Georg Jäger,Sebastian Zug*

Main category: cs.RO

TL;DR: 提出了一种动态生成系统模型的方法，用于实时定位机器人系统中的故障根因，适用于多种机器人系统，且减少对专家干预的依赖。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统的复杂性增加，传统故障检测与诊断方法因依赖预定义模型和历史数据而难以应对动态变化。

Method: 动态生成系统模型并在运行时利用该模型定位故障根因。

Result: 方法适用于多种机器人系统，且具有低开销和减少专家依赖的特点。

Conclusion: 该方法为复杂机器人系统的故障诊断提供了一种灵活且高效的解决方案。

Abstract: With the rapid development of more complex robots, Fault Detection and
Diagnosis (FDD) becomes increasingly harder. Especially the need for
predetermined models and historic data is problematic because they do not
encompass the dynamic and fast-changing nature of such systems. To this end, we
propose a concept that actively generates a dynamic system model at runtime and
utilizes it to locate root causes. The goal is to be applicable to all kinds of
robotic systems that share a similar software design. Additionally, it should
exhibit minimal overhead and enhance independence from expert attention.

</details>


### [17] [Self-Closing Suction Grippers for Industrial Grasping via Form-Flexible Design](https://arxiv.org/abs/2507.01561)
*Huijiang Wang,Holger Kunz,Timon Adler,Fumiya Iida*

Main category: cs.RO

TL;DR: 提出了一种基于混合堵塞和吸力机制的形变柔性夹爪，用于自适应抓取，能够处理尺寸与夹爪开口差异较大的物体。


<details>
  <summary>Details</summary>
Motivation: 传统夹爪在抓取尺寸差异大的物体时表现不佳，需要一种能自适应形变的解决方案。

Method: 采用混合堵塞和吸力机制，通过被动形变界面协调压力和流速，实现自适应抓取。

Result: 夹爪能够安全抓取尺寸仅为开口54.5%的鸡蛋，最大负载质量比达到94.3。

Conclusion: 混合形变夹爪在自适应抓取方面表现出色，适用于工业场景。

Abstract: Shape-morphing robots have shown benefits in industrial grasping. We propose
form-flexible grippers for adaptive grasping. The design is based on the hybrid
jamming and suction mechanism, which deforms to handle objects that vary
significantly in size from the aperture, including both larger and smaller
parts. Compared with traditional grippers, the gripper achieves self-closing to
form an airtight seal. Under a vacuum, a wide range of grasping is realized
through the passive morphing mechanism at the interface that harmonizes
pressure and flow rate. This hybrid gripper showcases the capability to
securely grasp an egg, as small as 54.5% of its aperture, while achieving a
maximum load-to-mass ratio of 94.3.

</details>


### [18] [An RRT* algorithm based on Riemannian metric model for optimal path planning](https://arxiv.org/abs/2507.01697)
*Yu Zhang,Qi Zhou,Xiao-Song Yang*

Main category: cs.RO

TL;DR: 提出了一种基于黎曼度量的模型，用于解决高维空间中二维光滑子流形上的最优路径规划问题。通过构造新的黎曼度量，将高维问题转化为二维平面上的几何问题，并提出了增量算法RRT*-R。实验表明该算法在多维不均匀场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中二维光滑子流形上的最优路径规划问题，同时考虑环境因素（如高度变化、地面阻力等）对机器人路径的影响。

Method: 构造新的黎曼度量，将高维问题转化为二维平面上的几何问题，并提出增量算法RRT*-R。

Result: RRT*-R算法在多维不均匀场景中表现优异，路径平滑且优化性能优于原始RRT*算法，路径长度接近理论最小测地距离。

Conclusion: RRT*-R算法在高维路径规划中具有优越性，能有效应对复杂环境因素，路径更平滑且接近最优。

Abstract: This paper presents a Riemannian metric-based model to solve the optimal path
planning problem on two-dimensional smooth submanifolds in high-dimensional
space. Our model is based on constructing a new Riemannian metric on a
two-dimensional projection plane, which is induced by the high-dimensional
Euclidean metric on two-dimensional smooth submanifold and reflects the
environmental information of the robot. The optimal path planning problem in
high-dimensional space is therefore transformed into a geometric problem on the
two-dimensional plane with new Riemannian metric. Based on the new Riemannian
metric, we proposed an incremental algorithm RRT*-R on the projection plane.
The experimental results show that the proposed algorithm is suitable for
scenarios with uneven fields in multiple dimensions. The proposed algorithm can
help the robot to effectively avoid areas with drastic changes in height,
ground resistance and other environmental factors. More importantly, the RRT*-R
algorithm shows better smoothness and optimization properties compared with the
original RRT* algorithm using Euclidean distance in high-dimensional workspace.
The length of the entire path by RRT*-R is a good approximation of the
theoretical minimum geodesic distance on projection plane.

</details>


### [19] [Efficient Collision Detection for Long and Slender Robotic Links in Euclidean Distance Fields: Application to a Forestry Crane](https://arxiv.org/abs/2507.01705)
*Marc-Philip Ecker,Bernhard Bischof,Minh Nhat Vu,Christoph Fröhlich,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: 提出了一种针对细长机械臂的新型碰撞检测算法，显著提高了运动规划的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将机器人近似为球体，但对于细长机械臂（如林业起重机）效率低且不准确。

Method: 开发了一种专门利用机械臂细长结构的碰撞检测算法，无需调整近似精度参数。

Result: 在真实LiDAR数据和模拟环境中验证了算法的有效性。

Conclusion: 新算法显著提升了计算效率，适用于细长机械臂的运动规划。

Abstract: Collision-free motion planning in complex outdoor environments relies heavily
on perceiving the surroundings through exteroceptive sensors. A widely used
approach represents the environment as a voxelized Euclidean distance field,
where robots are typically approximated by spheres. However, for large-scale
manipulators such as forestry cranes, which feature long and slender links,
this conventional spherical approximation becomes inefficient and inaccurate.
This work presents a novel collision detection algorithm specifically designed
to exploit the elongated structure of such manipulators, significantly
enhancing the computational efficiency of motion planning algorithms. Unlike
traditional sphere decomposition methods, our approach not only improves
computational efficiency but also naturally eliminates the need to fine-tune
the approximation accuracy as an additional parameter. We validate the
algorithm's effectiveness using real-world LiDAR data from a forestry crane
application, as well as simulated environment data.

</details>


### [20] [SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space](https://arxiv.org/abs/2507.01723)
*Xupeng Zhu,Fan Wang,Robin Walters,Jane Shi*

Main category: cs.RO

TL;DR: 提出了一种SE(3)等变扩散策略（SDP），通过在球面傅里叶空间中嵌入状态、动作和去噪过程，实现对3D场景变换的鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在3D空间中物体新排列时泛化能力差，影响实际性能。

Method: 采用球面傅里叶空间嵌入和球面FiLM层，提出球面去噪时间U-net，实现时空等变性和计算效率。

Result: 在20个仿真任务和5个物理机器人任务中表现优于基线。

Conclusion: SDP通过SE(3)等变性实现了对3D场景变换的鲁棒泛化。

Abstract: Diffusion Policies are effective at learning closed-loop manipulation
policies from human demonstrations but generalize poorly to novel arrangements
of objects in 3D space, hurting real-world performance. To address this issue,
we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion
policy that adapts trajectories according to 3D transformations of the scene.
Such equivariance is achieved by embedding the states, actions, and the
denoising process in spherical Fourier space. Additionally, we employ novel
spherical FiLM layers to condition the action denoising process equivariantly
on the scene embeddings. Lastly, we propose a spherical denoising temporal
U-net that achieves spatiotemporal equivariance with computational efficiency.
In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization
across transformed 3D scenes. SDP demonstrates a large performance improvement
over strong baselines in 20 simulation tasks and 5 physical robot tasks
including single-arm and bi-manual embodiments. Code is available at
https://github.com/amazon-science/Spherical_Diffusion_Policy.

</details>


### [21] [Augmented Bridge Spinal Fixation: A New Concept for Addressing Pedicle Screw Pullout via a Steerable Drilling Robot and Flexible Pedicle Screws](https://arxiv.org/abs/2507.01753)
*Yash Kulkarni,Susheela Sharma,Omid Rezayof,Siddhartha Kapuria,Jordan P. Amadio,Mohsen Khadem,Maryam Tilton,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 提出了一种名为AB-SF的新型脊柱固定技术，通过柔性螺钉和机器人辅助钻孔解决传统刚性螺钉的松动和拔出问题。


<details>
  <summary>Details</summary>
Motivation: 传统刚性椎弓根螺钉在脊柱固定中存在松动和拔出的局限性，需要更可靠的解决方案。

Method: 使用CT-SDR机器人钻孔J形隧道，植入柔性螺钉并注入骨水泥形成增强桥接结构。

Result: 实验成功模拟了骨水泥增强过程，验证了AB-SF技术的可行性。

Conclusion: AB-SF技术有望提升脊柱固定的强度和可靠性。

Abstract: To address the screw loosening and pullout limitations of rigid pedicle
screws in spinal fixation procedures, and to leverage our recently developed
Concentric Tube Steerable Drilling Robot (CT-SDR) and Flexible Pedicle Screw
(FPS), in this paper, we introduce the concept of Augmented Bridge Spinal
Fixation (AB-SF). In this concept, two connecting J-shape tunnels are first
drilled through pedicles of vertebra using the CT-SDR. Next, two FPSs are
passed through this tunnel and bone cement is then injected through the
cannulated region of the FPS to form an augmented bridge between two pedicles
and reinforce strength of the fixated spine. To experimentally analyze and
study the feasibility of AB-SF technique, we first used our robotic system
(i.e., a CT-SDR integrated with a robotic arm) to create two different fixation
scenarios in which two J-shape tunnels, forming a bridge, were drilled at
different depth of a vertebral phantom. Next, we implanted two FPSs within the
drilled tunnels and then successfully simulated the bone cement augmentation
process.

</details>


### [22] [S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures](https://arxiv.org/abs/2507.01779)
*Daniyal Maroufi,Xinyuan Huang,Yash Kulkarni,Omid Rezayof,Susheela Sharma,Vaibhav Goggela,Jordan P. Amadio,Mohsen Khadem,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 本文介绍了S3D框架，一种用于机器人脊柱固定手术的空间可操控钻孔系统，旨在解决脊柱固定手术中的解剖限制问题。


<details>
  <summary>Details</summary>
Motivation: 脊柱固定手术中，钻孔需要适应复杂的解剖结构，现有技术难以满足需求。S3D旨在提供一种可操控的钻孔解决方案。

Method: 通过改进同心管可操控钻孔机器人（CT-SDR），并结合四阶段校准、配准和导航流程，实现脊柱各节段的钻孔操作。

Result: 在脊柱模型上进行了平面和非平面钻孔实验，验证了框架的功能性。

Conclusion: S3D框架能够有效支持脊柱固定手术中的可操控钻孔需求。

Abstract: In this paper, we introduce S3D: A Spatial Steerable Surgical Drilling
Framework for Robotic Spinal Fixation Procedures. S3D is designed to enable
realistic steerable drilling while accounting for the anatomical constraints
associated with vertebral access in spinal fixation (SF) procedures. To achieve
this, we first enhanced our previously designed concentric tube Steerable
Drilling Robot (CT-SDR) to facilitate steerable drilling across all vertebral
levels of the spinal column. Additionally, we propose a four-Phase calibration,
registration, and navigation procedure to perform realistic SF procedures on a
spine holder phantom by integrating the CT-SDR with a seven-degree-of-freedom
robotic manipulator. The functionality of this framework is validated through
planar and out-of-plane steerable drilling experiments in vertebral phantoms.

</details>


### [23] [Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures](https://arxiv.org/abs/2507.01811)
*Yash Kulkarni,Susheela Sharma,Sarah Go,Jordan P. Amadio,Mohsen Khadem,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 提出了一种新型4自由度骨盆同心管可操纵钻孔机器人（pelvic CT-SDR），用于解决传统刚性钻孔工具在骨盆复杂解剖结构中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统刚性钻孔工具限制了螺钉的放置路径，导致螺钉错位、手术时间延长和辐射暴露增加等问题。

Method: 设计并开发了一种4自由度的骨盆同心管可操纵钻孔机器人，能够实现S形钻孔轨迹。

Result: 在模拟骨模型上进行了多次S形钻孔实验，验证了其性能。

Conclusion: 该机器人能够适应骨盆的自然曲率，有望改善手术效果并减少并发症。

Abstract: Current pelvic fixation techniques rely on rigid drilling tools, which
inherently constrain the placement of rigid medical screws in the complex
anatomy of pelvis. These constraints prevent medical screws from following
anatomically optimal pathways and force clinicians to fixate screws in linear
trajectories. This suboptimal approach, combined with the unnatural placement
of the excessively long screws, lead to complications such as screw
misplacement, extended surgery times, and increased radiation exposure due to
repeated X-ray images taken ensure to safety of procedure. To address these
challenges, in this paper, we present the design and development of a unique 4
degree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic
CT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling
trajectories that follow the natural curvatures of the pelvic anatomy. The
performance of the pelvic CT-SDR was thoroughly evaluated through several
S-shape drilling experiments in simulated bone phantoms.

</details>


### [24] [MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics](https://arxiv.org/abs/2507.01843)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.RO

TL;DR: MoIRA是一个模块化的MoE框架，通过外部文本路由器协调专家模型，提供零-shot路由选项，并在机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统MoE架构中路由机制无法定制和需要额外训练的问题，提出更灵活、低开销的模块化方案。

Method: MoIRA采用嵌入相似性和提示驱动的语言模型推理作为零-shot路由选项，结合低秩适配器进行低开销推理。

Result: 在GR1 Humanoid和LIBERO基准测试中，MoIRA优于通用模型，并与其他MoE方案竞争。

Conclusion: MoIRA展示了模块化部署的可行性，为未来多专家机器人系统提供了可扩展的基础。

Abstract: Mixture-of-Experts (MoE) approaches have recently gained traction in robotics
applications due to their ability to dynamically allocate computational
resources and specialize sub-networks for distinct tasks or environmental
contexts, enabling more efficient decision-making. Such systems often comprise
sparsely activated experts combined under a single monolithic architecture and
require a well-configured internal routing mechanism, which does not allow for
selective low-level expert and router customization and requires additional
training. We propose MoIRA, an architecture-agnostic modular MoE framework
designed to coordinate existing experts with an external text-based router.
MoIRA incorporates two zero-shot routing options: embedding-based similarity
and prompt-driven language model inference. In our experiments, we choose large
Vision-Language-Action models, gr00t-N1 and $\pi_0$, as the underlying experts,
and train low-rank adapters for low-overhead inference. We evaluate MoIRA on
various GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it
consistently outperforms generalist models and competes with other MoE
pipelines. Additionally, we analyse the robustness of the proposed approach to
the variations of the instructions. While relying solely on textual
descriptions of tasks and experts, MoIRA demonstrates the practical viability
of modular deployment with precise, low-effort routing and provides an
alternative, scalable foundation for future multi-expert robotic systems.

</details>


### [25] [TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types](https://arxiv.org/abs/2507.01857)
*Yuhao Lin,Yi-Lin Wei,Haoran Liao,Mu Lin,Chengyi Xing,Hao Li,Dandan Zhang,Mark Cutkosky,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: TypeTele是一种类型引导的灵巧遥操作系统，通过引入灵巧操作类型库和MLLM辅助检索模块，使灵巧手能够执行不受人类动作模式限制的任务。


<details>
  <summary>Details</summary>
Motivation: 传统灵巧遥操作依赖手部重定向模仿人类手部姿态，未能充分利用灵巧手的结构优势。

Method: 提出TypeTele系统，构建灵巧操作类型库，并利用MLLM辅助检索模块选择适合的操作类型。

Result: 实验表明，TypeTele显著提升了灵巧机器人执行复杂任务的多样性和成功率。

Conclusion: TypeTele通过类型引导的遥操作，充分发挥了灵巧手的潜力，适用于多样化任务。

Abstract: Dexterous teleoperation plays a crucial role in robotic manipulation for
real-world data collection and remote robot control. Previous dexterous
teleoperation mostly relies on hand retargeting to closely mimic human hand
postures. However, these approaches may fail to fully leverage the inherent
dexterity of dexterous hands, which can execute unique actions through their
structural advantages compared to human hands. To address this limitation, we
propose TypeTele, a type-guided dexterous teleoperation system, which enables
dexterous hands to perform actions that are not constrained by human motion
patterns. This is achieved by introducing dexterous manipulation types into the
teleoperation system, allowing operators to employ appropriate types to
complete specific tasks. To support this system, we build an extensible
dexterous manipulation type library to cover comprehensive dexterous postures
used in manipulation tasks. During teleoperation, we employ a MLLM
(Multi-modality Large Language Model)-assisted type retrieval module to
identify the most suitable manipulation type based on the specific task and
operator commands. Extensive experiments of real-world teleoperation and
imitation learning demonstrate that the incorporation of manipulation types
significantly takes full advantage of the dexterous robot's ability to perform
diverse and complex tasks with higher success rates.

</details>


### [26] [A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://arxiv.org/abs/2507.01925)
*Yifan Zhong,Fengshuo Bai,Shaofei Cai,Xuchuan Huang,Zhang Chen,Xiaowei Zhang,Yuanfei Wang,Shaoyang Guo,Tianrui Guan,Ka Nam Lui,Zhiquan Qi,Yitao Liang,Yuanpei Chen,Yaodong Yang*

Main category: cs.RO

TL;DR: 该论文提出了一种统一框架，将当前视觉-语言-动作（VLA）模型归类为基于动作令牌的生成过程，并分析了不同动作令牌类型的优缺点，为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型缺乏对动作令牌的全面理解，阻碍了有效开发和未来方向探索。

Method: 通过系统综述和分析，将VLA模型归类为基于动作令牌的框架，并比较不同令牌类型的优劣。

Result: 总结了动作令牌的八种类型，并指出了每种类型的局限性和改进空间。

Conclusion: 论文为VLA模型的未来发展提供了系统化的展望，并提出了有潜力的研究方向。

Abstract: The remarkable advancements of vision and language foundation models in
multimodal understanding, reasoning, and generation has sparked growing efforts
to extend such intelligence to the physical world, fueling the flourishing of
vision-language-action (VLA) models. Despite seemingly diverse approaches, we
observe that current VLA models can be unified under a single framework: vision
and language inputs are processed by a series of VLA modules, producing a chain
of \textit{action tokens} that progressively encode more grounded and
actionable information, ultimately generating executable actions. We further
determine that the primary design choice distinguishing VLA models lies in how
action tokens are formulated, which can be categorized into language
description, code, affordance, trajectory, goal state, latent representation,
raw action, and reasoning. However, there remains a lack of comprehensive
understanding regarding action tokens, significantly impeding effective VLA
development and obscuring future directions. Therefore, this survey aims to
categorize and interpret existing VLA research through the lens of action
tokenization, distill the strengths and limitations of each token type, and
identify areas for improvement. Through this systematic review and analysis, we
offer a synthesized outlook on the broader evolution of VLA models, highlight
underexplored yet promising directions, and contribute guidance for future
research, hoping to bring the field closer to general-purpose intelligence.

</details>


### [27] [Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations](https://arxiv.org/abs/2507.01930)
*Wenhao Wang,Yanyan Li,Long Jiao,Jiawei Yuan*

Main category: cs.RO

TL;DR: 提出了一种基于LLM的闭环控制框架，通过两个LLM模块（代码生成器和评估器）实现可靠的无人机操作，并通过自然语言轨迹描述和模拟优化提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM在无人机操作中存在逻辑推理和复杂决策的挑战，影响可靠性，因此需要一种更可靠的LLM驱动框架。

Method: 采用闭环控制框架，包含代码生成器和评估器，将数值状态转换为自然语言描述，并通过模拟优化避免物理风险。

Result: 实验表明，该框架在任务复杂度增加时，成功率和完整性显著优于基线方法。

Conclusion: 该框架通过反馈和优化，显著提升了LLM驱动的无人机操作的可靠性。

Abstract: Large Language Models (LLMs) have revolutionized robotic autonomy, including
Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential
of LLMs for translating human instructions into executable control code for UAV
operations. However, LLMs still face challenges from logical reasoning and
complex decision-making, leading to concerns about the reliability of
LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop
control framework that enables reliable UAV operations powered by effective
feedback and refinement using two LLM modules, i.e., a Code Generator and an
Evaluator. Our framework transforms numerical state observations from UAV
operations into natural language trajectory descriptions to enhance the
evaluator LLM's understanding of UAV dynamics for precise feedback generation.
Our framework also enables a simulation-based refinement process, and hence
eliminates the risks to physical UAVs caused by incorrect code execution during
the refinement. Extensive experiments on UAV control tasks with different
complexities are conducted. The experimental results show that our framework
can achieve reliable UAV operations using LLMs, which significantly outperforms
baseline approaches in terms of success rate and completeness with the increase
of task complexity.

</details>


### [28] [AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation](https://arxiv.org/abs/2507.01961)
*Sixiang Chen,Jiaming Liu,Siyuan Qian,Han Jiang,Lily Li,Renrui Zhang,Zhuoyang Liu,Chenyang Gu,Chengkai Hou,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了一种自适应协调扩散变换器（AC-DiT），通过显式建模移动底座对机械臂的影响和动态调整多模态感知权重，提升移动操作任务中的协调能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在移动底座与机械臂协调方面存在不足，未能显式建模移动底座对机械臂的影响，且忽视了不同阶段的多模态感知需求。

Method: 引入移动底座到机械臂的条件机制和感知感知的多模态条件策略，动态调整2D和3D视觉输入的融合权重。

Result: 在仿真和真实世界的移动操作任务中验证了AC-DiT的有效性。

Conclusion: AC-DiT通过显式协调和动态感知策略，显著提升了移动底座与机械臂的协调能力。

Abstract: Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.

</details>
