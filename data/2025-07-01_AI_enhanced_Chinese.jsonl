{"id": "2506.23346", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23346", "abs": "https://arxiv.org/abs/2506.23346", "authors": ["Hao Wang", "Armand Jordana", "Ludovic Righetti", "Somil Bansal"], "title": "Safe and Performant Deployment of Autonomous Systems via Model Predictive Control and Hamilton-Jacobi Reachability Analysis", "comment": "RSS 2025 Workshop on Reliable Robotics", "summary": "While we have made significant algorithmic developments to enable autonomous\nsystems to perform sophisticated tasks, it remains difficult for them to\nperform tasks effective and safely. Most existing approaches either fail to\nprovide any safety assurances or substantially compromise task performance for\nsafety. In this work, we develop a framework, based on model predictive control\n(MPC) and Hamilton-Jacobi (HJ) reachability, to optimize task performance for\nautonomous systems while respecting the safety constraints. Our framework\nguarantees recursive feasibility for the MPC controller, and it is scalable to\nhigh-dimensional systems. We demonstrate the effectiveness of our framework\nwith two simulation studies using a 4D Dubins Car and a 6 Dof Kuka iiwa\nmanipulator, and the experiments show that our framework significantly improves\nthe safety constraints satisfaction of the systems over the baselines.", "AI": {"tldr": "提出了一种基于MPC和HJ可达性的框架，优化自主系统的任务性能同时满足安全约束。", "motivation": "现有方法难以在保证安全的同时高效完成任务，缺乏安全性保证或牺牲任务性能。", "method": "结合模型预测控制（MPC）和Hamilton-Jacobi（HJ）可达性理论，确保递归可行性并适用于高维系统。", "result": "在4D Dubins Car和6自由度Kuka iiwa机械臂的仿真实验中，框架显著提高了系统的安全性。", "conclusion": "该框架在保证安全性的同时优化了任务性能，适用于高维自主系统。"}}
{"id": "2506.23400", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23400", "abs": "https://arxiv.org/abs/2506.23400", "authors": ["Yifei Li", "Joshua A. Robbins", "Guha Manogharan", "Herschel C. Pangborn", "Ilya Kovalenko"], "title": "A Model Predictive Control Framework to Enhance Safety and Quality in Mobile Additive Manufacturing Systems", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "summary": "In recent years, the demand for customized, on-demand production has grown in\nthe manufacturing sector. Additive Manufacturing (AM) has emerged as a\npromising technology to enhance customization capabilities, enabling greater\nflexibility, reduced lead times, and more efficient material usage. However,\ntraditional AM systems remain constrained by static setups and human worker\ndependencies, resulting in long lead times and limited scalability. Mobile\nrobots can improve the flexibility of production systems by transporting\nproducts to designated locations in a dynamic environment. By integrating AM\nsystems with mobile robots, manufacturers can optimize travel time for\npreparatory tasks and distributed printing operations. Mobile AM robots have\nbeen deployed for on-site production of large-scale structures, but often\nneglect critical print quality metrics like surface roughness. Additionally,\nthese systems do not have the precision necessary for producing small,\nintricate components. We propose a model predictive control framework for a\nmobile AM platform that ensures safe navigation on the plant floor while\nmaintaining high print quality in a dynamic environment. Three case studies are\nused to test the feasibility and reliability of the proposed systems.", "AI": {"tldr": "提出了一种结合移动机器人与增材制造（AM）的模型预测控制框架，以提高生产灵活性和打印质量。", "motivation": "传统AM系统因静态设置和依赖人工导致生产周期长、扩展性差，移动机器人可提升灵活性但忽视打印质量。", "method": "集成AM系统与移动机器人，采用模型预测控制框架，确保安全导航和高打印质量。", "result": "通过三个案例研究验证了系统的可行性和可靠性。", "conclusion": "该框架为动态环境中的高质量AM生产提供了有效解决方案。"}}
{"id": "2506.23624", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23624", "abs": "https://arxiv.org/abs/2506.23624", "authors": ["Max Grobbel", "Tristan Schneider", "Sören Hohmann"], "title": "Towards Universal Shared Control in Teleoperation Without Haptic Feedback", "comment": "5 pages, submitted to IEEE Telepresence 2025 conference", "summary": "Teleoperation with non-haptic VR controllers deprives human operators of\ncritical motion feedback. We address this by embedding a multi-objective\noptimization problem that converts user input into collision-free UR5e joint\ntrajectories while actively suppressing liquid slosh in a glass. The controller\nmaintains 13 ms average planning latency, confirming real-time performance and\nmotivating the augmentation of this teleoperation approach to further\nobjectives.", "AI": {"tldr": "论文提出了一种通过多目标优化将用户输入转换为无碰撞UR5e关节轨迹的方法，以解决非触觉VR控制器缺乏运动反馈的问题，同时抑制玻璃中液体的晃动。", "motivation": "非触觉VR控制器缺乏关键的运动反馈，限制了操作效果。", "method": "嵌入多目标优化问题，将用户输入转换为无碰撞的UR5e关节轨迹，并主动抑制液体晃动。", "result": "控制器平均规划延迟为13毫秒，证实了实时性能。", "conclusion": "该方法适用于进一步扩展其他目标的远程操作。"}}
{"id": "2506.23781", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23781", "abs": "https://arxiv.org/abs/2506.23781", "authors": ["Savvas Papaioannou", "Panayiotis Kolios", "Christos G. Panayiotou", "Marios M. Polycarpou"], "title": "Data-Driven Predictive Planning and Control for Aerial 3D Inspection with Back-face Elimination", "comment": "2025 European Control Conference (ECC), Thessaloniki, Greece, 24-27\n  June 2025", "summary": "Automated inspection with Unmanned Aerial Systems (UASs) is a transformative\ncapability set to revolutionize various application domains. However, this task\nis inherently complex, as it demands the seamless integration of perception,\nplanning, and control which existing approaches often treat separately.\nMoreover, it requires accurate long-horizon planning to predict action\nsequences, in contrast to many current techniques, which tend to be myopic. To\novercome these limitations, we propose a 3D inspection approach that unifies\nperception, planning, and control within a single data-driven predictive\ncontrol framework. Unlike traditional methods that rely on known UAS dynamic\nmodels, our approach requires only input-output data, making it easily\napplicable to off-the-shelf black-box UASs. Our method incorporates back-face\nelimination, a visibility determination technique from 3D computer graphics,\ndirectly into the control loop, thereby enabling the online generation of\naccurate, long-horizon 3D inspection trajectories.", "AI": {"tldr": "提出了一种基于数据驱动预测控制的3D检测方法，统一了感知、规划与控制，适用于现成的黑盒无人机系统。", "motivation": "现有方法通常将感知、规划和控分离，且缺乏长时程规划能力，难以满足复杂检测任务的需求。", "method": "结合3D计算机图形学中的背面消除技术，在控制循环中在线生成长时程的3D检测轨迹。", "result": "方法无需已知无人机动态模型，仅需输入输出数据，适用于现成无人机。", "conclusion": "该方法为复杂检测任务提供了一种高效、统一的解决方案。"}}
{"id": "2506.22466", "categories": ["cs.RO", "cs.CY", "I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22466", "abs": "https://arxiv.org/abs/2506.22466", "authors": ["Marcel Heisler", "Christian Becker-Asano"], "title": "Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum", "comment": "To be published in IEEE RO-MAN 2025 conference proceedings; for\n  videos check https://ai.hdm-stuttgart.de/humanoid-lab", "summary": "The android robot Andrea was set up at a public museum in Germany for six\nconsecutive days to have conversations with visitors, fully autonomously. No\nspecific context was given, so visitors could state their opinions regarding\npossible use-cases in structured interviews, without any bias. Additionally the\n44 interviewees were asked for their general opinions of the robot, their\nreasons (not) to interact with it and necessary improvements for future use.\nThe android's voice and wig were changed between different days of operation to\ngive varying cues regarding its gender. This did not have a significant impact\non the positive overall perception of the robot. Most visitors want the robot\nto provide information about exhibits in the future, while opinions on other\nroles, like a receptionist, were both wanted and explicitly not wanted by\ndifferent visitors. Speaking more languages (than only English) and faster\nresponse times were the improvements most desired. These findings from the\ninterviews are in line with an analysis of the system logs, which revealed,\nthat after chitchat and personal questions, most of the 4436 collected requests\nasked for information related to the museum and to converse in a different\nlanguage. The valuable insights gained from these real-world interactions are\nnow used to improve the system to become a useful real-world application.", "AI": {"tldr": "Andrea机器人博物馆互动实验显示，访客对其总体评价积极，主要希望其提供展品信息，同时改进语言支持和响应速度。", "motivation": "研究公众对自主对话机器人的接受度及潜在应用场景。", "method": "在博物馆部署Andrea机器人，进行结构化访谈和系统日志分析。", "result": "访客对机器人总体评价积极，主要需求为展品信息和多语言支持。", "conclusion": "实验结果将用于改进机器人系统，以适应实际应用需求。"}}
{"id": "2506.22473", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22473", "abs": "https://arxiv.org/abs/2506.22473", "authors": ["Fernando Diaz Ledezma", "Valentin Marcel", "Matej Hoffmann"], "title": "Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity", "comment": "8 pages with 6 figures", "summary": "The movements of both animals and robots give rise to streams of\nhigh-dimensional motor and sensory information. Imagine the brain of a newborn\nor the controller of a baby humanoid robot trying to make sense of unprocessed\nsensorimotor time series. Here, we present a framework for studying the dynamic\nfunctional connectivity between the multimodal sensory signals of a robotic\nagent to uncover an underlying structure. Using instantaneous mutual\ninformation, we capture the time-varying functional connectivity (FC) between\nproprioceptive, tactile, and visual signals, revealing the sensorimotor\nrelationships. Using an infinite relational model, we identified sensorimotor\nmodules and their evolving connectivity. To further interpret these dynamic\ninteractions, we employed non-negative matrix factorization, which decomposed\nthe connectivity patterns into additive factors and their corresponding\ntemporal coefficients. These factors can be considered the agent's motion\nprimitives or movement synergies that the agent can use to make sense of its\nsensorimotor space and later for behavior selection. In the future, the method\ncan be deployed in robot learning as well as in the analysis of human movement\ntrajectories or brain signals.", "AI": {"tldr": "提出了一种分析机器人多模态感知信号动态功能连接的框架，揭示其底层结构。", "motivation": "研究如何从高维传感器运动信息中提取有意义的结构，帮助机器人或新生儿理解其感知运动空间。", "method": "使用瞬时互信息捕捉动态功能连接，结合无限关系模型和非负矩阵分解识别传感器运动模块及其演化模式。", "result": "发现了感知运动模块及其动态连接模式，这些模式可视为运动基元，用于行为选择。", "conclusion": "该方法可应用于机器人学习和人类运动轨迹或脑信号分析。"}}
{"id": "2506.22494", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22494", "abs": "https://arxiv.org/abs/2506.22494", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "AI": {"tldr": "DriveBLIP2框架基于BLIP2-OPT架构，通过注意力图生成器提升自动驾驶场景中的解释质量。", "motivation": "现有视觉语言模型在复杂多目标环境中表现不佳，尤其在实时自动驾驶场景中难以快速识别关键对象。", "method": "提出注意力图生成器，突出关键帧中对驾驶决策重要的对象，以生成更清晰的解释。", "result": "在DRAMA数据集上评估，BLEU、ROUGE、CIDEr和SPICE分数显著优于基线模型。", "conclusion": "定向注意力机制可提升视觉语言模型在实时自动驾驶中的可解释性。"}}
{"id": "2506.22572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22572", "abs": "https://arxiv.org/abs/2506.22572", "authors": ["Mrunmayi Mungekar", "Sanjith Menon", "M. Ravi Shankar", "M. Khalid Jawed"], "title": "Directed Shape Morphing using Kirigami-enhanced Thermoplastics", "comment": "Software and Data: https://github.com/structuresComp/Shrinky-Dink", "summary": "We present a simple, accessible method for autonomously transforming flat\nplastic sheets into intricate three-dimensional structures using only uniform\nheating and common tools such as household ovens and scissors. Our approach\ncombines heat-shrinkable thermoplastics with Kirigami patterns tailored to the\ntarget 3D shape, creating bilayer composites that morph into a wide range of\ncomplex structures, e.g., bowls, pyramids, and even custom ergonomic surfaces\nlike mouse covers. Critically, the transformation is driven by a\nlow-information stimulus (uniform heat) yet produces highly intricate shapes\nthrough programmed geometric design. The morphing behavior, confirmed by finite\nelement simulations, arises from strain mismatch between the contracting\nthermoplastic layer and the constraining Kirigami layer. By decoupling material\ncomposition from mechanical response, this method avoids detailed process\ncontrol and enables a broad class of self-morphing structures, offering a\nversatile platform for adaptive design and scalable manufacturing.", "AI": {"tldr": "提出了一种简单的方法，通过均匀加热和常见工具（如家用烤箱和剪刀）将平面塑料片自主转化为复杂三维结构。", "motivation": "探索一种无需复杂控制即可实现复杂三维形状的自变形方法，适用于自适应设计和规模化制造。", "method": "结合热收缩热塑性塑料和定制Kirigami图案，形成双层复合材料，通过均匀加热驱动变形。", "result": "成功制造出多种复杂结构（如碗、金字塔和鼠标盖），并通过有限元模拟验证了变形行为。", "conclusion": "该方法通过几何设计实现了低信息刺激下的高复杂度形状，为自适应设计和制造提供了通用平台。"}}
{"id": "2506.22593", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22593", "abs": "https://arxiv.org/abs/2506.22593", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "AI": {"tldr": "Pix2G方法通过实时生成结构化场景图，连接2D BIM与3D地图，支持资源受限机器人自主探索未知环境。", "motivation": "解决人类操作员与机器人在高风险任务中高效合作的需求，弥合2D BIM与3D地图之间的差距。", "method": "提出Pix2G，一种轻量级方法，从图像像素和LiDAR地图实时生成场景图，仅使用CPU。", "result": "生成去噪2D地图和结构分割的3D点云，通过多层图连接，实验验证了实时性能。", "conclusion": "Pix2G在资源受限平台上实现了高效的环境探索与地图构建。"}}
{"id": "2506.22766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22766", "abs": "https://arxiv.org/abs/2506.22766", "authors": ["Yiting Chen", "Kenneth Kimble", "Howard H. Qian", "Podshara Chanrungmaneekul", "Robert Seney", "Kaiyu Hang"], "title": "Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025; 16 pages, 10\n  figures", "summary": "Robust and adaptive robotic peg-in-hole assembly under tight tolerances is\ncritical to various industrial applications. However, it remains an open\nchallenge due to perceptual and physical uncertainties from contact-rich\ninteractions that easily exceed the allowed clearance. In this paper, we study\nhow to leverage contact between the peg and its matching hole to eliminate\nuncertainties in the assembly process under unstructured settings. By examining\nthe role of compliance under contact constraints, we present a manipulation\nsystem that plans collision-inclusive interactions for the peg to 1)\niteratively identify its task environment to localize the target hole and 2)\nexploit environmental contact constraints to refine insertion motions into the\ntarget hole without relying on precise perception, enabling a robust solution\nto peg-in-hole assembly. By conceptualizing the above process as the\ncomposition of funneling in different state spaces, we present a formal\napproach to constructing manipulation funnels as an uncertainty-absorbing\nparadigm for peg-in-hole assembly. The proposed system effectively generalizes\nacross diverse peg-in-hole scenarios across varying scales, shapes, and\nmaterials in a learning-free manner. Extensive experiments on a NIST Assembly\nTask Board (ATB) and additional challenging scenarios validate its robustness\nin real-world applications.", "AI": {"tldr": "提出了一种基于接触约束的鲁棒自适应机器人插孔装配方法，通过利用接触消除不确定性，无需精确感知即可完成装配。", "motivation": "解决紧密公差下机器人插孔装配中的感知和物理不确定性挑战。", "method": "利用接触约束规划碰撞包容的交互动作，通过漏斗化状态空间构建不确定性吸收范式。", "result": "系统在不同尺度、形状和材料的插孔场景中表现鲁棒，实验验证了其有效性。", "conclusion": "该方法为插孔装配提供了一种学习无关的鲁棒解决方案。"}}
{"id": "2506.22769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22769", "abs": "https://arxiv.org/abs/2506.22769", "authors": ["Changshi Zhou", "Feng Luan", "Jiarui Hu", "Shaoqiang Meng", "Zhipeng Wang", "Yanchao Dong", "Yanmin Zhou", "Bin He"], "title": "Learning Efficient Robotic Garment Manipulation with Standardization", "comment": null, "summary": "Garment manipulation is a significant challenge for robots due to the complex\ndynamics and potential self-occlusion of garments. Most existing methods of\nefficient garment unfolding overlook the crucial role of standardization of\nflattened garments, which could significantly simplify downstream tasks like\nfolding, ironing, and packing. This paper presents APS-Net, a novel approach to\ngarment manipulation that combines unfolding and standardization in a unified\nframework. APS-Net employs a dual-arm, multi-primitive policy with dynamic\nfling to quickly unfold crumpled garments and pick-and-place (p and p) for\nprecise alignment. The purpose of garment standardization during unfolding\ninvolves not only maximizing surface coverage but also aligning the garment's\nshape and orientation to predefined requirements. To guide effective robot\nlearning, we introduce a novel factorized reward function for standardization,\nwhich incorporates garment coverage (Cov), keypoint distance (KD), and\nintersection-over-union (IoU) metrics. Additionally, we introduce a spatial\naction mask and an Action Optimized Module to improve unfolding efficiency by\nselecting actions and operation points effectively. In simulation, APS-Net\noutperforms state-of-the-art methods for long sleeves, achieving 3.9 percent\nbetter coverage, 5.2 percent higher IoU, and a 0.14 decrease in KD (7.09\npercent relative reduction). Real-world folding tasks further demonstrate that\nstandardization simplifies the folding process. Project page: see\nhttps://hellohaia.github.io/APS/", "AI": {"tldr": "APS-Net提出了一种结合展开和标准化的统一框架，用于机器人衣物操作，通过动态抛掷和精确对齐实现高效展开和标准化。", "motivation": "现有方法忽视了衣物展开后的标准化，而标准化能显著简化后续任务（如折叠、熨烫和包装）。", "method": "采用双臂多基元策略，结合动态抛掷和精确对齐，引入因子化奖励函数（覆盖率、关键点距离和IoU）和空间动作掩码。", "result": "在仿真中，APS-Net在长袖衣物上表现优于现有方法，覆盖率提高3.9%，IoU提高5.2%，关键点距离减少7.09%。", "conclusion": "标准化显著简化了折叠任务，APS-Net在衣物操作中表现出高效性和实用性。"}}
{"id": "2506.22788", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22788", "abs": "https://arxiv.org/abs/2506.22788", "authors": ["Xuao Hou", "Yongquan Jia", "Shijin Zhang", "Yuqiang Wu"], "title": "SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information", "comment": null, "summary": "The widespread application of industrial robots in fields such as cutting and\nwelding has imposed increasingly stringent requirements on the trajectory\naccuracy of end-effectors. However, current error compensation methods face\nseveral critical challenges, including overly simplified mechanism modeling, a\nlack of physical consistency in data-driven approaches, and substantial data\nrequirements. These issues make it difficult to achieve both high accuracy and\nstrong generalization simultaneously. To address these challenges, this paper\nproposes a Spatial-Physical Informed Attention Residual Network (SPI-BoTER).\nThis method integrates the kinematic equations of the robotic manipulator with\na Transformer architecture enhanced by sparse self-attention masks. A\nparameter-adaptive hybrid loss function incorporating spatial and physical\ninformation is employed to iteratively optimize the network during training,\nenabling high-precision error compensation under small-sample conditions.\nAdditionally, inverse joint angle compensation is performed using a gradient\ndescent-based optimization method. Experimental results on a small-sample\ndataset from a UR5 robotic arm (724 samples, with a train:test:validation split\nof 8:1:1) demonstrate the superior performance of the proposed method. It\nachieves a 3D absolute positioning error of 0.2515 mm with a standard deviation\nof 0.15 mm, representing a 35.16\\% reduction in error compared to conventional\ndeep neural network (DNN) methods. Furthermore, the inverse angle compensation\nalgorithm converges to an accuracy of 0.01 mm within an average of 147\niterations. This study presents a solution that combines physical\ninterpretability with data adaptability for high-precision control of\nindustrial robots, offering promising potential for the reliable execution of\nprecision tasks in intelligent manufacturing.", "AI": {"tldr": "论文提出了一种结合物理模型与Transformer架构的SPI-BoTER方法，用于工业机器人末端执行器的高精度误差补偿，显著提升了小样本条件下的性能。", "motivation": "工业机器人轨迹精度需求日益严格，现有误差补偿方法存在建模简化、物理一致性不足和数据需求大等问题，难以同时实现高精度与强泛化。", "method": "提出SPI-BoTER方法，融合机器人运动学方程与稀疏自注意力Transformer架构，采用参数自适应混合损失函数迭代优化网络，并通过梯度下降优化逆关节角补偿。", "result": "在UR5机械臂小样本数据集上，3D绝对定位误差降至0.2515 mm（标准差0.15 mm），比传统DNN方法误差减少35.16%；逆角度补偿算法平均147次迭代收敛至0.01 mm精度。", "conclusion": "SPI-BoTER方法结合物理可解释性与数据适应性，为工业机器人高精度控制提供了有效解决方案，有望推动智能制造中精密任务的可靠执行。"}}
{"id": "2506.22827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22827", "abs": "https://arxiv.org/abs/2506.22827", "authors": ["André Schakkal", "Ben Zandonati", "Zhutian Yang", "Navid Azizan"], "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation", "comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models", "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 72.5% success rate in completing the full manipulation sequence.\nThese experiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.", "AI": {"tldr": "提出了一种分层规划与控制框架，用于实现人形机器人可靠的多步操作任务。", "motivation": "为了在工业和家庭环境中有效部署人形机器人，需要确保其能够可靠执行复杂的多步操作任务。", "method": "系统分为三层：低层RL控制器跟踪全身运动目标；中层模仿学习技能策略生成任务各步的运动目标；高层视觉语言规划模块决定技能执行并实时监控。", "result": "在Unitree G1人形机器人上进行的实验中，系统在40次试验中实现了72.5%的成功率。", "conclusion": "分层系统可行，视觉语言模型在技能规划和监控中具有优势。"}}
{"id": "2506.22894", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22894", "abs": "https://arxiv.org/abs/2506.22894", "authors": ["Bei Zhou", "Baha Zarrouki", "Mattia Piccinini", "Cheng Hu", "Lei Xie", "Johannes Betz"], "title": "Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example", "comment": null, "summary": "Autonomous drifting is a complex and crucial maneuver for safety-critical\nscenarios like slippery roads and emergency collision avoidance, requiring\nprecise motion planning and control. Traditional motion planning methods often\nstruggle with the high instability and unpredictability of drifting,\nparticularly when operating at high speeds. Recent learning-based approaches\nhave attempted to tackle this issue but often rely on expert knowledge or have\nlimited exploration capabilities. Additionally, they do not effectively address\nsafety concerns during learning and deployment. To overcome these limitations,\nwe propose a novel Safe Reinforcement Learning (RL)-based motion planner for\nautonomous drifting. Our approach integrates an RL agent with model-based drift\ndynamics to determine desired drift motion states, while incorporating a\nPredictive Safety Filter (PSF) that adjusts the agent's actions online to\nprevent unsafe states. This ensures safe and efficient learning, and stable\ndrift operation. We validate the effectiveness of our method through\nsimulations on a Matlab-Carsim platform, demonstrating significant improvements\nin drift performance, reduced tracking errors, and computational efficiency\ncompared to traditional methods. This strategy promises to extend the\ncapabilities of autonomous vehicles in safety-critical maneuvers.", "AI": {"tldr": "提出了一种基于安全强化学习的运动规划方法，用于自主漂移，结合模型漂移动力学和预测安全过滤器，确保安全高效的学习和操作。", "motivation": "传统运动规划方法在高不稳定性漂移中表现不佳，现有学习方法的探索能力有限且安全性不足。", "method": "结合强化学习代理与模型漂移动力学，引入预测安全过滤器在线调整动作以防止不安全状态。", "result": "在Matlab-Carsim平台上验证，显著提升漂移性能、减少跟踪误差并提高计算效率。", "conclusion": "该方法有望提升自动驾驶车辆在安全关键场景中的能力。"}}
{"id": "2506.22942", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22942", "abs": "https://arxiv.org/abs/2506.22942", "authors": ["Kartik A. Pant", "Jaehyeok Kim", "James M. Goppert", "Inseok Hwang"], "title": "Energy-Constrained Resilient Multi-Robot Coverage Control", "comment": "6 pages, 4 figures", "summary": "The problem of multi-robot coverage control becomes significantly challenging\nwhen multiple robots leave the mission space simultaneously to charge their\nbatteries, disrupting the underlying network topology for communication and\nsensing. To address this, we propose a resilient network design and control\napproach that allows robots to achieve the desired coverage performance while\nsatisfying energy constraints and maintaining network connectivity throughout\nthe mission. We model the combined motion, energy, and network dynamics of the\nmultirobot systems (MRS) as a hybrid system with three modes, i.e., coverage,\nreturn-to-base, and recharge, respectively. We show that ensuring the energy\nconstraints can be transformed into designing appropriate guard conditions for\nmode transition between each of the three modes. Additionally, we present a\nsystematic procedure to design, maintain, and reconfigure the underlying\nnetwork topology using an energy-aware bearing rigid network design, enhancing\nthe structural resilience of the MRS even when a subset of robots departs to\ncharge their batteries. Finally, we validate our proposed method using\nnumerical simulations.", "AI": {"tldr": "提出了一种多机器人覆盖控制的弹性网络设计与控制方法，解决机器人在充电时网络拓扑中断的问题。", "motivation": "多机器人同时离开任务空间充电会破坏通信和感知的网络拓扑，影响覆盖性能。", "method": "将多机器人系统的运动、能量和网络动态建模为混合系统，设计能量感知的轴承刚性网络以增强结构弹性。", "result": "通过数值模拟验证了方法的有效性，确保机器人在满足能量约束的同时保持网络连接。", "conclusion": "提出的方法能够有效维持多机器人系统的覆盖性能和网络弹性。"}}
{"id": "2506.22956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22956", "abs": "https://arxiv.org/abs/2506.22956", "authors": ["David Rodríguez-Martínez", "Dave van der Meer", "Junlin Song", "Abishek Bera", "C. J. Pérez-del-Pulgar", "Miguel Angel Olivares-Mendez"], "title": "SPICE-HL3: Single-Photon, Inertial, and Stereo Camera dataset for Exploration of High-Latitude Lunar Landscapes", "comment": "10 pages, 8 figures, dataset", "summary": "Exploring high-latitude lunar regions presents an extremely challenging\nvisual environment for robots. The low sunlight elevation angle and minimal\nlight scattering result in a visual field dominated by a high dynamic range\nfeaturing long, dynamic shadows. Reproducing these conditions on Earth requires\nsophisticated simulators and specialized facilities. We introduce a unique\ndataset recorded at the LunaLab from the SnT - University of Luxembourg, an\nindoor test facility designed to replicate the optical characteristics of\nmultiple lunar latitudes. Our dataset includes images, inertial measurements,\nand wheel odometry data from robots navigating seven distinct trajectories\nunder multiple illumination scenarios, simulating high-latitude lunar\nconditions from dawn to night time with and without the aid of headlights,\nresulting in 88 distinct sequences containing a total of 1.3M images. Data was\ncaptured using a stereo RGB-inertial sensor, a monocular monochrome camera, and\nfor the first time, a novel single-photon avalanche diode (SPAD) camera. We\nrecorded both static and dynamic image sequences, with robots navigating at\nslow (5 cm/s) and fast (50 cm/s) speeds. All data is calibrated, synchronized,\nand timestamped, providing a valuable resource for validating perception tasks\nfrom vision-based autonomous navigation to scientific imaging for future lunar\nmissions targeting high-latitude regions or those intended for robots operating\nacross perceptually degraded environments. The dataset can be downloaded from\nhttps://zenodo.org/records/13970078?preview=1, and a visual overview is\navailable at https://youtu.be/d7sPeO50_2I. All supplementary material can be\nfound at https://github.com/spaceuma/spice-hl3.", "AI": {"tldr": "论文介绍了一个独特的数据集，模拟高纬度月球环境，用于验证感知任务。", "motivation": "高纬度月球区域的视觉环境对机器人极具挑战性，需在地球上模拟这些条件以支持未来任务。", "method": "在LunaLab设施中记录数据集，包括图像、惯性测量和轮式里程数据，使用多种传感器。", "result": "数据集包含88个序列、130万张图像，涵盖多种光照条件和机器人速度。", "conclusion": "数据集为未来月球任务中的感知任务验证提供了宝贵资源。"}}
{"id": "2506.23023", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23023", "abs": "https://arxiv.org/abs/2506.23023", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "comment": "6 pages, 10 figures, submitted to a conference", "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "AI": {"tldr": "SAD-RL框架通过分层策略和场景化环境提升自动驾驶决策算法的泛化性和学习效率。", "motivation": "自动驾驶系统需在复杂开放环境中安全运行，现有强化学习方法在复杂任务中泛化性和效率不足。", "method": "提出SAD-RL框架，结合分层策略（高层选择模板，低层执行）和场景化环境，控制训练体验并引入挑战性场景。", "result": "实验表明SAD-RL能高效实现安全行为，消融研究证实分层策略和场景多样性是关键。", "conclusion": "SAD-RL为复杂自动驾驶任务提供了有效的解决方案。"}}
{"id": "2506.23078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23078", "abs": "https://arxiv.org/abs/2506.23078", "authors": ["Zhaoxing Zhang", "Xiaoxiang Wang", "Chengliang Zhang", "Yangyang Guo", "Zikang Yuan", "Xin Yang"], "title": "Event-based Stereo Visual-Inertial Odometry with Voxel Map", "comment": null, "summary": "The event camera, renowned for its high dynamic range and exceptional\ntemporal resolution, is recognized as an important sensor for visual odometry.\nHowever, the inherent noise in event streams complicates the selection of\nhigh-quality map points, which critically determine the precision of state\nestimation. To address this challenge, we propose Voxel-ESVIO, an event-based\nstereo visual-inertial odometry system that utilizes voxel map management,\nwhich efficiently filter out high-quality 3D points. Specifically, our\nmethodology utilizes voxel-based point selection and voxel-aware point\nmanagement to collectively optimize the selection and updating of map points on\na per-voxel basis. These synergistic strategies enable the efficient retrieval\nof noise-resilient map points with the highest observation likelihood in\ncurrent frames, thereby ensureing the state estimation accuracy. Extensive\nevaluations on three public benchmarks demonstrate that our Voxel-ESVIO\noutperforms state-of-the-art methods in both accuracy and computational\nefficiency.", "AI": {"tldr": "Voxel-ESVIO是一种基于事件相机的立体视觉惯性里程计系统，通过体素地图管理高效筛选高质量3D点，提升状态估计精度。", "motivation": "事件相机的高动态范围和优异时间分辨率使其成为视觉里程计的重要传感器，但事件流中的噪声影响了高质量地图点的选择，进而影响状态估计精度。", "method": "采用基于体素的点选择和体素感知的点管理策略，以体素为单位优化地图点的选择和更新，高效提取抗噪声的地图点。", "result": "在三个公开基准测试中，Voxel-ESVIO在精度和计算效率上均优于现有方法。", "conclusion": "Voxel-ESVIO通过体素地图管理有效解决了事件流噪声问题，显著提升了视觉惯性里程计的性能。"}}
{"id": "2506.23114", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23114", "abs": "https://arxiv.org/abs/2506.23114", "authors": ["Zhanxiang Cao", "Buqing Nie", "Yang Zhang", "Yue Gao"], "title": "Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications", "comment": "8 pages,6 figures, IROS2025", "summary": "Recent advancements in quadruped robot research have significantly improved\ntheir ability to traverse complex and unstructured outdoor environments.\nHowever, the issue of noise generated during locomotion is generally\noverlooked, which is critically important in noise-sensitive indoor\nenvironments, such as service and healthcare settings, where maintaining low\nnoise levels is essential. This study aims to optimize the acoustic noise\ngenerated by quadruped robots during locomotion through the development of\nadvanced motion control algorithms. To achieve this, we propose a novel\napproach that minimizes noise emissions by integrating optimized gait design\nwith tailored control strategies. This method achieves an average noise\nreduction of approximately 8 dBA during movement, thereby enhancing the\nsuitability of quadruped robots for deployment in noise-sensitive indoor\nenvironments. Experimental results demonstrate the effectiveness of this\napproach across various indoor settings, highlighting the potential of\nquadruped robots for quiet operation in noise-sensitive environments.", "AI": {"tldr": "该研究通过优化步态设计和控制策略，降低四足机器人在运动中的噪音，平均减少约8 dBA，适用于噪声敏感的室内环境。", "motivation": "四足机器人在复杂户外环境中的运动能力已显著提升，但其运动噪音在噪声敏感的室内环境（如服务和医疗场景）中被忽视，需要解决。", "method": "提出一种结合优化步态设计和定制控制策略的新方法，以减少噪音排放。", "result": "实验结果显示，该方法平均减少约8 dBA的噪音，适用于多种室内环境。", "conclusion": "该方法有效提升了四足机器人在噪声敏感环境中的适用性，展示了其安静运行的潜力。"}}
{"id": "2506.23125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23125", "abs": "https://arxiv.org/abs/2506.23125", "authors": ["Zhanxiang Cao", "Yang Zhang", "Buqing Nie", "Huangxuan Lin", "Haoyang Li", "Yue Gao"], "title": "Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots", "comment": "8 pages, 8 figures", "summary": "Learning policies for complex humanoid tasks remains both challenging and\ncompelling. Inspired by how infants and athletes rely on external support--such\nas parental walkers or coach-applied guidance--to acquire skills like walking,\ndancing, and performing acrobatic flips, we propose A2CF: Adaptive Assistive\nCurriculum Force for humanoid motion learning. A2CF trains a dual-agent system,\nin which a dedicated assistive force agent applies state-dependent forces to\nguide the robot through difficult initial motions and gradually reduces\nassistance as the robot's proficiency improves. Across three\nbenchmarks--bipedal walking, choreographed dancing, and backflip--A2CF achieves\nconvergence 30% faster than baseline methods, lowers failure rates by over 40%,\nand ultimately produces robust, support-free policies. Real-world experiments\nfurther demonstrate that adaptively applied assistive forces significantly\naccelerate the acquisition of complex skills in high-dimensional robotic\ncontrol.", "AI": {"tldr": "A2CF方法通过自适应辅助力加速人形机器人复杂动作学习，比基线方法快30%，失败率降低40%。", "motivation": "受婴儿和运动员依赖外部支持学习复杂动作的启发，提出A2CF方法以解决人形机器人任务学习的挑战。", "method": "A2CF训练双代理系统，辅助力代理根据状态施加力，逐步减少辅助以提升机器人熟练度。", "result": "在行走、跳舞和后空翻任务中，A2CF比基线方法快30%，失败率降低40%，生成无需支持的稳健策略。", "conclusion": "自适应辅助力显著加速高维机器人控制中复杂技能的获取，实验验证了其有效性。"}}
{"id": "2506.23126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23126", "abs": "https://arxiv.org/abs/2506.23126", "authors": ["Suning Huang", "Qianzhong Chen", "Xiaohan Zhang", "Jiankai Sun", "Mac Schwager"], "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation", "comment": null, "summary": "3D world models (i.e., learning-based 3D dynamics models) offer a promising\napproach to generalizable robotic manipulation by capturing the underlying\nphysics of environment evolution conditioned on robot actions. However,\nexisting 3D world models are primarily limited to single-material dynamics\nusing a particle-based Graph Neural Network model, and often require\ntime-consuming 3D scene reconstruction to obtain 3D particle tracks for\ntraining. In this work, we present ParticleFormer, a Transformer-based point\ncloud world model trained with a hybrid point cloud reconstruction loss,\nsupervising both global and local dynamics features in multi-material,\nmulti-object robot interactions. ParticleFormer captures fine-grained\nmulti-object interactions between rigid, deformable, and flexible materials,\ntrained directly from real-world robot perception data without an elaborate\nscene reconstruction. We demonstrate the model's effectiveness both in 3D scene\nforecasting tasks, and in downstream manipulation tasks using a Model\nPredictive Control (MPC) policy. In addition, we extend existing dynamics\nlearning benchmarks to include diverse multi-material, multi-object interaction\nscenarios. We validate our method on six simulation and three real-world\nexperiments, where it consistently outperforms leading baselines by achieving\nsuperior dynamics prediction accuracy and less rollout error in downstream\nvisuomotor tasks. Experimental videos are available at\nhttps://particleformer.github.io/.", "AI": {"tldr": "ParticleFormer是一种基于Transformer的点云世界模型，用于多材料、多物体的机器人交互，无需复杂场景重建即可训练，并在动态预测和下游任务中表现优异。", "motivation": "现有3D世界模型局限于单材料动态且需要耗时3D重建，无法有效处理多材料、多物体的复杂交互。", "method": "提出ParticleFormer，结合Transformer和混合点云重建损失，直接利用真实机器人感知数据训练，捕捉多材料动态。", "result": "在仿真和真实实验中，模型在动态预测和下游任务中表现优于基线方法。", "conclusion": "ParticleFormer为多材料、多物体的机器人交互提供了一种高效且通用的解决方案。"}}
{"id": "2506.23129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23129", "abs": "https://arxiv.org/abs/2506.23129", "authors": ["Hossein B. Jond", "Logan Beaver", "Martin Jiroušek", "Naiemeh Ahmadlou", "Veli Bakırcıoğlu", "Martin Saska"], "title": "Flatness-based Finite-Horizon Multi-UAV Formation Trajectory Planning and Directionally Aware Collision Avoidance Tracking", "comment": null, "summary": "Collision-free optimal formation control of unmanned aerial vehicle (UAV)\nteams is challenging. The state-of-the-art optimal control approaches often\nrely on numerical methods sensitive to initial guesses. This paper presents an\ninnovative collision-free finite-time formation control scheme for multiple\nUAVs leveraging the differential flatness of the UAV dynamics, eliminating the\nneed for numerical methods. We formulate a finite-time optimal control problem\nto plan a formation trajectory for feasible initial states. This formation\ntrajectory planning optimal control problem involves a collective performance\nindex to meet the formation requirements of achieving relative positions and\nvelocity consensus. It is solved by applying Pontryagin's principle.\nSubsequently, a collision-constrained regulating problem is addressed to ensure\ncollision-free tracking of the planned formation trajectory. The tracking\nproblem incorporates a directionally aware collision avoidance strategy that\nprioritizes avoiding UAVs in the forward path and relative approach. It assigns\nlower priority to those on the sides with an oblique relative approach and\ndisregards UAVs behind and not in the relative approach. The simulation results\nfor a four-UAV team (re)formation problem confirm the efficacy of the proposed\ncontrol scheme.", "AI": {"tldr": "提出了一种基于微分平坦性的无人机编队控制方案，避免了数值方法的依赖，并通过碰撞约束调节确保无碰撞跟踪。", "motivation": "现有最优控制方法对初始猜测敏感，难以实现无碰撞的无人机编队控制。", "method": "利用无人机动力学的微分平坦性，提出有限时间最优控制问题，结合Pontryagin原理求解，并引入方向感知的碰撞避免策略。", "result": "仿真验证了四无人机编队问题的有效性。", "conclusion": "该方案为无人机编队控制提供了一种高效且无碰撞的解决方案。"}}
{"id": "2506.23152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23152", "abs": "https://arxiv.org/abs/2506.23152", "authors": ["Youzhuo Wang", "Jiayi Ye", "Chuyang Xiao", "Yiming Zhong", "Heng Tao", "Hang Yu", "Yumeng Liu", "Jingyi Yu", "Yuexin Ma"], "title": "DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover", "comment": null, "summary": "Handover between a human and a dexterous robotic hand is a fundamental yet\nchallenging task in human-robot collaboration. It requires handling dynamic\nenvironments and a wide variety of objects and demands robust and adaptive\ngrasping strategies. However, progress in developing effective dynamic\ndexterous grasping methods is limited by the absence of high-quality,\nreal-world human-to-robot handover datasets. Existing datasets primarily focus\non grasping static objects or rely on synthesized handover motions, which\ndiffer significantly from real-world robot motion patterns, creating a\nsubstantial gap in applicability. In this paper, we introduce DexH2R, a\ncomprehensive real-world dataset for human-to-robot handovers, built on a\ndexterous robotic hand. Our dataset captures a diverse range of interactive\nobjects, dynamic motion patterns, rich visual sensor data, and detailed\nannotations. Additionally, to ensure natural and human-like dexterous motions,\nwe utilize teleoperation for data collection, enabling the robot's movements to\nalign with human behaviors and habits, which is a crucial characteristic for\nintelligent humanoid robots. Furthermore, we propose an effective solution,\nDynamicGrasp, for human-to-robot handover and evaluate various state-of-the-art\napproaches, including auto-regressive models and diffusion policy methods,\nproviding a thorough comparison and analysis. We believe our benchmark will\ndrive advancements in human-to-robot handover research by offering a\nhigh-quality dataset, effective solutions, and comprehensive evaluation\nmetrics.", "AI": {"tldr": "论文介绍了DexH2R数据集，用于人机交互中的动态灵巧抓取任务，并提出了DynamicGrasp解决方案。", "motivation": "现有数据集主要关注静态物体或合成动作，缺乏真实世界的人机交互数据，限制了动态灵巧抓取方法的发展。", "method": "通过遥操作收集数据，构建DexH2R数据集，并提出DynamicGrasp解决方案，评估多种先进方法。", "result": "提供了高质量数据集和解决方案，并进行了全面比较分析。", "conclusion": "DexH2R数据集和DynamicGrasp解决方案将推动人机交互研究的发展。"}}
{"id": "2506.23164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23164", "abs": "https://arxiv.org/abs/2506.23164", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "comment": "12 pages, 8 figures, submitted to a journal", "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "AI": {"tldr": "提出了一种新的评估框架，用于检测多模态轨迹预测中的模式崩溃问题，重点关注安全关键交互。", "motivation": "现有模型可能忽视交互模式的多样性，且缺乏定量评估模式崩溃的指标。", "method": "引入模式崩溃、模式正确性和覆盖率的度量标准，强调预测的时序维度。", "result": "测试四种多智能体轨迹预测模型，发现模式崩溃确实存在，且某些情况下模型无法预测正确的交互模式。", "conclusion": "该框架有助于开发更一致和准确的预测模型，提升自动驾驶系统的安全性。"}}
{"id": "2506.23316", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23316", "abs": "https://arxiv.org/abs/2506.23316", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "title": "InfGen: Scenario Generation as Next Token Group Prediction", "comment": null, "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "AI": {"tldr": "InfGen是一个基于Transformer的交通场景生成框架，支持动态、长时程场景模拟，并能持续插入新车辆。", "motivation": "现有数据驱动的交通模拟方法依赖静态初始化或日志回放，难以模拟动态、长时程场景。", "method": "将整个场景表示为包含交通信号、车辆状态和运动向量的token序列，通过Transformer模型自回归生成交通状态。", "result": "InfGen生成的行为真实、多样且自适应，强化学习策略在其场景中表现出更强的鲁棒性和泛化能力。", "conclusion": "InfGen是一个高保真度的自动驾驶模拟环境，支持无限场景生成。"}}
{"id": "2506.23326", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23326", "abs": "https://arxiv.org/abs/2506.23326", "authors": ["Sang-Yoep Lee", "Leonardo Zamora Yanez", "Jacob Rogatinsky", "Vi T. Vo", "Tanvi Shingade", "Tommaso Ranzani"], "title": "Simplifying Data-Driven Modeling of the Volume-Flow-Pressure Relationship in Hydraulic Soft Robotic Actuators", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Soft robotic systems are known for their flexibility and adaptability, but\ntraditional physics-based models struggle to capture their complex, nonlinear\nbehaviors. This study explores a data-driven approach to modeling the\nvolume-flow-pressure relationship in hydraulic soft actuators, focusing on\nlow-complexity models with high accuracy. We perform regression analysis on a\nstacked balloon actuator system using exponential, polynomial, and neural\nnetwork models with or without autoregressive inputs. The results demonstrate\nthat simpler models, particularly multivariate polynomials, effectively predict\npressure dynamics with fewer parameters. This research offers a practical\nsolution for real-time soft robotics applications, balancing model complexity\nand computational efficiency. Moreover, the approach may benefit various\ntechniques that require explicit analytical models.", "AI": {"tldr": "研究探索了一种数据驱动的方法，用于建模液压软执行器的体积-流量-压力关系，重点关注低复杂度高精度模型。", "motivation": "传统物理模型难以捕捉软机器人系统的复杂非线性行为。", "method": "使用指数、多项式和神经网络模型（带或不带自回归输入）对堆叠气球执行器系统进行回归分析。", "result": "结果表明，较简单的模型（尤其是多元多项式）能以较少参数有效预测压力动态。", "conclusion": "该研究为实时软机器人应用提供了实用解决方案，平衡了模型复杂度和计算效率。"}}
{"id": "2506.23333", "categories": ["cs.RO", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.23333", "abs": "https://arxiv.org/abs/2506.23333", "authors": ["Javier Garcia", "Jonas Friemel", "Ramin Kosfeld", "Michael Yannuzzi", "Peter Kramer", "Christian Rieck", "Christian Scheffer", "Arne Schmidt", "Harm Kube", "Dan Biediger", "Sándor P. Fekete", "Aaron T. Becker"], "title": "Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks", "comment": "8 pages, 12 figures. To appear in the proceedings of the 2025 IEEE\n  21st International Conference on Automation Science and Engineering (CASE\n  2025)", "summary": "We implement and evaluate different methods for the reconfiguration of a\nconnected arrangement of tiles into a desired target shape, using a single\nactive robot that can move along the tile structure. This robot can pick up,\ncarry, or drop off one tile at a time, but it must maintain a single connected\nconfiguration at all times.\n  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms\nas canonical intermediate configurations, guaranteeing performance within a\nconstant factor of the optimal solution if the start and target configuration\nare well-separated. We implement and evaluate this algorithm, both in a\nsimulated and practical setting, using an inchworm type robot to compare it\nwith two existing heuristic algorithms.", "AI": {"tldr": "论文研究了使用单个机器人重新配置连接瓷砖结构的方法，比较了基于直方图的算法与两种启发式算法的性能。", "motivation": "探索如何高效地将瓷砖结构从初始形状重新配置为目标形状，同时保持连接性。", "method": "实现并评估了Becker等人提出的基于直方图的算法，并与两种启发式算法在模拟和实际环境中进行比较。", "result": "基于直方图的算法在初始和目标配置分离良好的情况下，性能接近最优解。", "conclusion": "该算法在特定条件下表现优越，为瓷砖结构的重新配置提供了有效解决方案。"}}
{"id": "2506.23351", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23351", "abs": "https://arxiv.org/abs/2506.23351", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "AI": {"tldr": "论文介绍了RoboTwin双臂协作挑战赛，旨在推动双臂机器人处理复杂任务的研究，吸引了全球团队参与，并提出了通用双臂策略学习的见解。", "motivation": "推动自主系统在复杂物理环境中的感知、推理和行动能力，特别是双臂协作在刚性、可变形和触觉敏感物体任务中的应用。", "method": "基于RoboTwin仿真平台和AgileX COBOT-Magic机器人平台，设计了三个阶段的比赛，涵盖17种双臂操作任务。", "result": "吸引了64个全球团队和400多名参与者，产生了如SEM和AnchorDP3等优秀解决方案，并提供了通用双臂策略学习的见解。", "conclusion": "挑战赛为未来研究提供了宝贵数据和方法，支持开发鲁棒且通用的双臂操作策略。"}}
{"id": "2506.23369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23369", "abs": "https://arxiv.org/abs/2506.23369", "authors": ["Xiao'ao Song", "Konstantinos Karydis"], "title": "GS-NBV: a Geometry-based, Semantics-aware Viewpoint Planning Algorithm for Avocado Harvesting under Occlusions", "comment": "Accepted for publication in CASE 2025, 6 pages, 8 figures", "summary": "Efficient identification of picking points is critical for automated fruit\nharvesting. Avocados present unique challenges owing to their irregular shape,\nweight, and less-structured growing environments, which require specific\nviewpoints for successful harvesting. We propose a geometry-based,\nsemantics-aware viewpoint-planning algorithm to address these challenges. The\nplanning process involves three key steps: viewpoint sampling, evaluation, and\nexecution. Starting from a partially occluded view, the system first detects\nthe fruit, then leverages geometric information to constrain the viewpoint\nsearch space to a 1D circle, and uniformly samples four points to balance the\nefficiency and exploration. A new picking score metric is introduced to\nevaluate the viewpoint suitability and guide the camera to the next-best view.\nWe validate our method through simulation against two state-of-the-art\nalgorithms. Results show a 100% success rate in two case studies with\nsignificant occlusions, demonstrating the efficiency and robustness of our\napproach. Our code is available at https://github.com/lineojcd/GSNBV", "AI": {"tldr": "提出了一种基于几何和语义感知的视点规划算法，用于高效识别不规则形状水果（如牛油果）的采摘点。", "motivation": "自动化水果采摘中，牛油果因其不规则形状、重量和非结构化生长环境，需要特定视点才能成功采摘。", "method": "算法分为视点采样、评估和执行三步，利用几何信息约束搜索空间为1D圆，并引入新的采摘评分指标。", "result": "在模拟实验中，与两种先进算法对比，实现了100%的成功率，证明了方法的效率和鲁棒性。", "conclusion": "该方法在遮挡严重的情况下仍能高效规划视点，适用于自动化水果采摘。"}}
{"id": "2506.23433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23433", "abs": "https://arxiv.org/abs/2506.23433", "authors": ["Tim Puphal", "Vipul Ramtekkar", "Kenji Nishimiya"], "title": "Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset", "comment": null, "summary": "Improving automated vehicle software requires driving data rich in valuable\nroad user interactions. In this paper, we propose a risk-based filtering\napproach that helps identify such valuable driving situations from large\ndatasets. Specifically, we use a probabilistic risk model to detect high-risk\nsituations. Our method stands out by considering a) first-order situations\n(where one vehicle directly influences another and induces risk) and b)\nsecond-order situations (where influence propagates through an intermediary\nvehicle). In experiments, we show that our approach effectively selects\nvaluable driving situations in the Waymo Open Motion Dataset. Compared to the\ntwo baseline interaction metrics of Kalman difficulty and Tracks-To-Predict\n(TTP), our filtering approach identifies complex and complementary situations,\nenriching the quality in automated vehicle testing. The risk data is made\nopen-source: https://github.com/HRI-EU/RiskBasedFiltering.", "AI": {"tldr": "提出一种基于风险的过滤方法，从大规模驾驶数据中识别有价值的交互场景，用于改进自动驾驶软件。", "motivation": "自动驾驶软件的改进需要丰富的道路用户交互数据，但如何从海量数据中筛选出有价值的情境是关键问题。", "method": "使用概率风险模型检测高风险情境，包括直接影响的一阶情境和通过中介车辆传播影响的二阶情境。", "result": "在Waymo Open Motion Dataset上验证，该方法能有效筛选出复杂且互补的驾驶情境，优于基线方法。", "conclusion": "基于风险的过滤方法能显著提升自动驾驶测试数据的质量，相关数据已开源。"}}
{"id": "2506.23514", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23514", "abs": "https://arxiv.org/abs/2506.23514", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "comment": "Accepted to IROS 2025", "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "AI": {"tldr": "MGPRL是一种基于Wi-Fi信号的多机器人相对定位框架，利用高斯过程和凸包对齐实现高效定位。", "motivation": "解决GPS缺失环境下多机器人系统依赖昂贵或短程传感器的问题。", "method": "使用多输出高斯过程预测RSSI场，结合凸包对齐进行相对位姿估计。", "result": "MGPRL在定位精度和计算效率上优于现有方法。", "conclusion": "MGPRL是一种无需预校准的高效分布式定位方案，已开源为ROS包。"}}
{"id": "2506.23573", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23573", "abs": "https://arxiv.org/abs/2506.23573", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "title": "Online Human Action Detection during Escorting", "comment": "Accepted in IEEE RO-MAN '25", "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "AI": {"tldr": "论文提出了一种新型神经网络架构，用于实时行人重识别和动作预测，以提升拥挤环境中机器人护送服务的效率。", "motivation": "现有护送机器人依赖导航策略，假设被护送者会顺利跟随，但在拥挤环境中常因人类动态行为（如分心、受阻）而失效。缺乏专门数据集和实时模型限制了机器人对护送对象动作的理解和调整。", "method": "提出一种新型神经网络架构，结合行人重识别和动作预测功能，使机器人能动态调整速度并应对中断。", "result": "对比实验表明，该系统在效率和效果上优于基线模型，显著提升了复杂场景中的护送服务。", "conclusion": "该研究为拥挤环境中的机器人护送服务提供了高效解决方案，具有实际应用潜力。"}}
{"id": "2506.23614", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2506.23614", "abs": "https://arxiv.org/abs/2506.23614", "authors": ["Jing Huang", "Hao Su", "Kwok Wai Samuel Au"], "title": "Passage-traversing optimal path planning with sampling-based algorithms", "comment": "30 pages, 22 figures, 6 tables, journal paper", "summary": "This paper introduces a new paradigm of optimal path planning, i.e.,\npassage-traversing optimal path planning (PTOPP), that optimizes paths'\ntraversed passages for specified optimization objectives. In particular, PTOPP\nis utilized to find the path with optimal accessible free space along its\nentire length, which represents a basic requirement for paths in robotics. As\npassages are places where free space shrinks and becomes constrained, the core\nidea is to leverage the path's passage traversal status to characterize its\naccessible free space comprehensively. To this end, a novel passage detection\nand free space decomposition method using proximity graphs is proposed,\nenabling fast detection of sparse but informative passages and environment\ndecompositions. Based on this preprocessing, optimal path planning with\naccessible free space objectives or constraints is formulated as PTOPP problems\ncompatible with sampling-based optimal planners. Then, sampling-based\nalgorithms for PTOPP, including their dependent primitive procedures, are\ndeveloped leveraging partitioned environments for fast passage traversal check.\nAll these methods are implemented and thoroughly tested for effectiveness and\nefficiency validation. Compared to existing approaches, such as clearance-based\nmethods, PTOPP demonstrates significant advantages in configurability, solution\noptimality, and efficiency, addressing prior limitations and incapabilities. It\nis believed to provide an efficient and versatile solution to accessible free\nspace optimization over conventional avenues and more generally, to a broad\nclass of path planning problems that can be formulated as PTOPP.", "AI": {"tldr": "提出了一种新的最优路径规划范式PTOPP，通过优化路径的通过通道来满足特定优化目标，特别适用于机器人路径规划中的自由空间优化。", "motivation": "传统路径规划方法在自由空间优化方面存在局限性，PTOPP旨在通过通道检测和自由空间分解，提供更优的路径规划解决方案。", "method": "提出了一种基于邻近图的通道检测和自由空间分解方法，并开发了采样算法以快速检查通道通过性。", "result": "PTOPP在配置性、解决方案最优性和效率方面显著优于现有方法，如基于间隙的方法。", "conclusion": "PTOPP为自由空间优化提供了一种高效且通用的解决方案，适用于广泛的路径规划问题。"}}
{"id": "2506.23723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23723", "abs": "https://arxiv.org/abs/2506.23723", "authors": ["Jozsef Palmieri", "Paolo Di Lillo", "Stefano Chiaverini", "Alessandro Marino"], "title": "A comprehensive control architecture for semi-autonomous dual-arm robots in agriculture settings", "comment": null, "summary": "The adoption of mobile robotic platforms in complex environments, such as\nagricultural settings, requires these systems to exhibit a flexible yet\neffective architecture that integrates perception and control. In such\nscenarios, several tasks need to be accomplished simultaneously, ranging from\nmanaging robot limits to performing operational tasks and handling human\ninputs. The purpose of this paper is to present a comprehensive control\narchitecture for achieving complex tasks such as robotized harvesting in\nvineyards within the framework of the European project CANOPIES. In detail, a\n16-DOF dual-arm mobile robot is employed, controlled via a Hierarchical\nQuadratic Programming (HQP) approach capable of handling both equality and\ninequality constraints at various priorities to harvest grape bunches selected\nby the perception system developed within the project. Furthermore, given the\ncomplexity of the scenario and the uncertainty in the perception system, which\ncould potentially lead to collisions with the environment, the handling of\ninteraction forces is necessary. Remarkably, this was achieved using the same\nHQP framework. This feature is further leveraged to enable semi-autonomous\noperations, allowing a human operator to assist the robotic counterpart in\ncompleting harvesting tasks. Finally, the obtained results are validated\nthrough extensive testing conducted first in a laboratory environment to prove\nindividual functionalities, then in a real vineyard, encompassing both\nautonomous and semi-autonomous grape harvesting operations.", "AI": {"tldr": "本文提出了一种用于复杂农业环境的移动机器人控制架构，采用分层二次规划（HQP）方法，实现了葡萄采摘的自主和半自主操作。", "motivation": "在复杂农业环境中，移动机器人需要灵活且高效的架构来整合感知与控制，同时完成多项任务，如葡萄采摘。", "method": "使用16自由度双臂移动机器人，通过HQP方法处理优先级不同的等式和不等式约束，并结合感知系统选择葡萄串。", "result": "在实验室和实际葡萄园中验证了自主和半自主采摘功能，成功处理了感知不确定性及环境交互力。", "conclusion": "提出的HQP框架有效支持复杂任务，如葡萄采摘，并实现了人机协作的半自主操作。"}}
{"id": "2506.23725", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23725", "abs": "https://arxiv.org/abs/2506.23725", "authors": ["Atharva Gundawar", "Som Sagar", "Ransalu Senanayake"], "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", "AI": {"tldr": "PAC Bench是一个用于评估视觉语言模型（VLMs）在物理属性、功能性和约束理解上的基准测试，揭示了当前模型的局限性。", "motivation": "尽管VLMs在机器人操作任务中广泛应用，但其对低层物理先决条件的理解能力尚未得到验证，这可能导致操作不可靠。", "method": "提出PAC Bench基准，包含多样化的数据集（30,000+标注、673张真实图像、100个真实场景和120个模拟场景），用于系统评估VLMs。", "result": "评估显示当前VLMs在基础物理概念理解上存在显著不足，不适合可靠的机器人操作。", "conclusion": "PAC Bench为评估VLMs的物理推理能力提供了标准化基准，并指导开发更鲁棒的模型。"}}
{"id": "2506.23739", "categories": ["cs.RO", "cs.CE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.23739", "abs": "https://arxiv.org/abs/2506.23739", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen Müller"], "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "AI": {"tldr": "论文提出了一种结合车辆在环测试和运动实验室的测试环境，用于评估自动驾驶系统与弱势道路使用者的交互，验证了人体姿态估计方法的可行性。", "motivation": "确保自动驾驶系统与弱势道路使用者在城市环境中的安全、真实交互需要先进的测试方法。", "method": "结合车辆在环测试和运动实验室，通过真实世界与虚拟场景的比较分析，验证人体姿态估计方法，并使用单目摄像头3D骨骼检测AI。", "result": "结果显示在稳定运动模式下，真实与虚拟场景的人体姿态估计高度一致，但在动态运动和遮挡情况下存在显著误差。", "conclusion": "研究为优化下一代基于AI的车辆感知测试方法提供了依据，并改进了自动驾驶车辆与弱势道路使用者在虚拟环境中的交互模型。"}}
{"id": "2506.23768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23768", "abs": "https://arxiv.org/abs/2506.23768", "authors": ["Vittorio La Barbera", "Steven Bohez", "Leonard Hasenclever", "Yuval Tassa", "John R. Hutchinson"], "title": "Motion Tracking with Muscles: Predictive Control of a Parametric Musculoskeletal Canine Model", "comment": null, "summary": "We introduce a novel musculoskeletal model of a dog, procedurally generated\nfrom accurate 3D muscle meshes. Accompanying this model is a motion\ncapture-based locomotion task compatible with a variety of control algorithms,\nas well as an improved muscle dynamics model designed to enhance convergence in\ndifferentiable control frameworks. We validate our approach by comparing\nsimulated muscle activation patterns with experimentally obtained\nelectromyography (EMG) data from previous canine locomotion studies. This work\naims to bridge gaps between biomechanics, robotics, and computational\nneuroscience, offering a robust platform for researchers investigating muscle\nactuation and neuromuscular control.We plan to release the full model along\nwith the retargeted motion capture clips to facilitate further research and\ndevelopment.", "AI": {"tldr": "提出了一种基于3D肌肉网格的狗骨骼肌肉模型，并开发了兼容多种控制算法的运动捕捉任务和改进的肌肉动力学模型。通过模拟肌肉激活模式与实验EMG数据对比验证了模型的有效性。", "motivation": "旨在填补生物力学、机器人和计算神经科学之间的研究空白，为肌肉驱动和神经肌肉控制研究提供平台。", "method": "利用3D肌肉网格生成骨骼肌肉模型，结合运动捕捉任务和改进的肌肉动力学模型。", "result": "模拟肌肉激活模式与实验EMG数据一致，验证了模型的有效性。", "conclusion": "该模型为相关研究提供了可靠工具，未来将公开模型和运动捕捉数据以促进研究发展。"}}
{"id": "2506.23771", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23771", "abs": "https://arxiv.org/abs/2506.23771", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "AI": {"tldr": "提出了一种多时间尺度的分层强化学习方法，用于自动驾驶，结合长短时间尺度的策略设计，提升驾驶效率、行为一致性和安全性。", "motivation": "现有RL方法在自动驾驶中忽视策略结构设计，导致驾驶行为波动或无法统一优化驾驶行为与控制。", "method": "采用分层策略结构，高低层RL策略联合训练，分别生成长时间尺度运动指导和短时间尺度控制命令，并设计分层安全机制。", "result": "在仿真和HighD数据集的高速多车道场景中，显著提升自动驾驶性能，提高效率、行为一致性和安全性。", "conclusion": "多时间尺度分层强化学习方法有效解决了自动驾驶中的策略设计问题，实现了驾驶行为与控制的统一优化。"}}
{"id": "2506.23919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23919", "abs": "https://arxiv.org/abs/2506.23919", "authors": ["Haonan Chen", "Bangjun Wang", "Jingxiang Guo", "Tianrui Zhang", "Yiwen Hou", "Xuchuan Huang", "Chenrui Tie", "Lin Shao"], "title": "World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation", "comment": null, "summary": "Improving data efficiency and generalization in robotic manipulation remains\na core challenge. We propose a novel framework that leverages a pre-trained\nmultimodal image-generation model as a world model to guide policy learning. By\nexploiting its rich visual-semantic representations and strong generalization\nacross diverse scenes, the model generates open-ended future state predictions\nthat inform downstream manipulation. Coupled with zero-shot low-level control\nmodules, our approach enables general-purpose robotic manipulation without\ntask-specific training. Experiments in both simulation and real-world\nenvironments demonstrate that our method achieves effective performance across\na wide range of manipulation tasks with no additional data collection or\nfine-tuning. Supplementary materials are available on our website:\nhttps://world4omni.github.io/.", "AI": {"tldr": "提出了一种利用预训练多模态图像生成模型作为世界模型来指导策略学习的新框架，实现了无需任务特定训练的高效机器人操作。", "motivation": "提高机器人操作的数据效率和泛化能力是核心挑战。", "method": "利用预训练多模态图像生成模型生成开放式的未来状态预测，结合零样本低级控制模块。", "result": "在仿真和真实环境中验证了方法的有效性，适用于多种操作任务，无需额外数据收集或微调。", "conclusion": "该方法为通用机器人操作提供了一种高效且泛化能力强的解决方案。"}}
{"id": "2506.23944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23944", "abs": "https://arxiv.org/abs/2506.23944", "authors": ["Fuhang Kuang", "Jiacheng You", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning", "comment": null, "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.", "AI": {"tldr": "论文提出了一种解决模仿学习中本体感觉偏移问题的方法，通过域适应框架和Wasserstein距离优化训练与部署分布的对齐。", "motivation": "模仿学习中直接使用所有本体感觉状态会导致性能下降，原因是训练与部署时的本体感觉分布存在显著差异。", "method": "提出基于Wasserstein距离的域适应框架，通过向专家和部署数据添加噪声来最小化分布差异。", "result": "实验证明该方法能有效利用本体感觉并缓解其负面影响，性能优于丢弃本体感觉的简单方案及其他基线方法。", "conclusion": "该方法成功解决了本体感觉偏移问题，提升了模仿学习在机器人任务中的性能。"}}
{"id": "2506.23999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23999", "abs": "https://arxiv.org/abs/2506.23999", "authors": ["Zeyu Han", "Mengchi Cai", "Chaoyi Chen", "Qingwen Meng", "Guangwei Wang", "Ying Liu", "Qing Xu", "Jianqiang Wang", "Keqiang Li"], "title": "Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles", "comment": null, "summary": "The safe trajectory planning of intelligent and connected vehicles is a key\ncomponent in autonomous driving technology. Modeling the environment risk\ninformation by field is a promising and effective approach for safe trajectory\nplanning. However, existing risk assessment theories only analyze the risk by\ncurrent information, ignoring future prediction. This paper proposes a\npredictive risk analysis and safe trajectory planning framework for intelligent\nand connected vehicles. This framework first predicts future trajectories of\nobjects by a local risk-aware algorithm, following with a\nspatiotemporal-discretised predictive risk analysis using the prediction\nresults. Then the safe trajectory is generated based on the predictive risk\nanalysis. Finally, simulation and vehicle experiments confirm the efficacy and\nreal-time practicability of our approach.", "AI": {"tldr": "提出了一种基于预测风险分析的智能网联车辆安全轨迹规划框架，通过预测未来轨迹和时空离散化风险分析生成安全轨迹。", "motivation": "现有风险评估理论仅基于当前信息，忽略了未来预测，无法满足智能网联车辆的安全需求。", "method": "结合局部风险感知算法预测未来轨迹，进行时空离散化预测风险分析，并生成安全轨迹。", "result": "仿真和车辆实验验证了该方法的有效性和实时实用性。", "conclusion": "该框架为智能网联车辆的安全轨迹规划提供了新的解决方案。"}}
{"id": "2506.24046", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.24046", "abs": "https://arxiv.org/abs/2506.24046", "authors": ["Olivia Richards", "Keith L. Obstein", "Nabil Simaan"], "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy", "comment": null, "summary": "New endoscopists require a large volume of expert-proctored colonoscopies to\nattain minimal competency. Developing multi-fingered, synchronized control of a\ncolonoscope requires significant time and exposure to the device. Current\ntraining methods inhibit this development by relying on tool hand-off for\nexpert demonstrations. There is a need for colonoscopy training tools that\nenable in-hand expert guidance in real-time. We present a new concept of a\ntandem training system that uses a telemanipulated preceptor colonoscope to\nguide novice users as they perform a colonoscopy. This system is capable of\ndual-control and can automatically toggle between expert and novice control of\na standard colonoscope's angulation control wheels. Preliminary results from a\nuser study with novice and expert users show the effectiveness of this device\nas a skill acquisition tool. We believe that this device has the potential to\naccelerate skill acquisition for colonoscopy and, in the future, enable\nindividualized instruction and responsive teaching through bidirectional\nactuation.", "AI": {"tldr": "提出了一种新型结肠镜训练系统，通过双控制机制实现专家实时指导，加速新手技能掌握。", "motivation": "当前结肠镜培训依赖工具交接，限制了新手对设备的多指同步控制技能的快速掌握，需要实时专家指导工具。", "method": "设计了一种双控制训练系统，通过远程操作专家结肠镜指导新手，支持专家和新手对标准结肠镜角度控制轮的双重控制切换。", "result": "初步用户研究表明，该系统作为技能获取工具有效。", "conclusion": "该系统有望加速结肠镜技能学习，未来可能通过双向驱动实现个性化教学。"}}
