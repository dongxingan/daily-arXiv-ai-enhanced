{"id": "2508.09003", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09003", "abs": "https://arxiv.org/abs/2508.09003", "authors": ["Filippo A. Spinelli", "Yifan Zhai", "Fang Nan", "Pascal Egli", "Julian Nubert", "Thilo Bleumer", "Lukas Miller", "Ferdinand Hofmann", "Marco Hutter"], "title": "Large Scale Robotic Material Handling: Learning, Planning, and Control", "comment": "Preliminary version, currently undergoing review process", "summary": "Bulk material handling involves the efficient and precise moving of large\nquantities of materials, a core operation in many industries, including cargo\nship unloading, waste sorting, construction, and demolition. These repetitive,\nlabor-intensive, and safety-critical operations are typically performed using\nlarge hydraulic material handlers equipped with underactuated grippers. In this\nwork, we present a comprehensive framework for the autonomous execution of\nlarge-scale material handling tasks. The system integrates specialized modules\nfor environment perception, pile attack point selection, path planning, and\nmotion control. The main contributions of this work are two reinforcement\nlearning-based modules: an attack point planner that selects optimal grasping\nlocations on the material pile to maximize removal efficiency and minimize the\nnumber of scoops, and a robust trajectory following controller that addresses\nthe precision and safety challenges associated with underactuated grippers in\nmovement, while utilizing their free-swinging nature to release material\nthrough dynamic throwing. We validate our framework through real-world\nexperiments on a 40 t material handler in a representative worksite, focusing\non two key tasks: high-throughput bulk pile management and high-precision truck\nloading. Comparative evaluations against human operators demonstrate the\nsystem's effectiveness in terms of precision, repeatability, and operational\nsafety. To the best of our knowledge, this is the first complete automation of\nmaterial handling tasks on a full scale.", "AI": {"tldr": "本文提出了一种用于大规模物料搬运任务的自主执行框架，结合了环境感知、抓取点选择、路径规划和运动控制模块，并通过强化学习方法优化抓取和轨迹控制。", "motivation": "物料搬运是许多行业中的核心操作，但通常依赖人工操作，存在效率低和安全风险的问题。本文旨在通过自动化技术提升效率和安全性。", "method": "系统整合了环境感知、抓取点选择、路径规划和运动控制模块，并采用强化学习方法优化抓取位置选择和轨迹跟踪。", "result": "在真实场景中验证了系统的有效性，与人工操作相比，在精度、重复性和安全性方面表现更优。", "conclusion": "本文首次实现了全尺寸物料搬运任务的完整自动化，为行业提供了高效、安全的解决方案。"}}
{"id": "2508.08258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08258", "abs": "https://arxiv.org/abs/2508.08258", "authors": ["Gerald Brantner"], "title": "Humanoid Robot Acrobatics Utilizing Complete Articulated Rigid Body Dynamics", "comment": null, "summary": "Endowing humanoid robots with the ability to perform highly dynamic motions\nakin to human-level acrobatics has been a long-standing challenge. Successfully\nperforming these maneuvers requires close consideration of the underlying\nphysics in both trajectory optimization for planning and control during\nexecution. This is particularly challenging due to humanoids' high\ndegree-of-freedom count and associated exponentially scaling complexities,\nwhich makes planning on the explicit equations of motion intractable. Typical\nworkarounds include linearization methods and model approximations. However,\nneither are sufficient because they produce degraded performance on the true\nrobotic system. This paper presents a control architecture comprising\ntrajectory optimization and whole-body control, intermediated by a matching\nmodel abstraction, that enables the execution of acrobatic maneuvers, including\nconstraint and posture behaviors, conditioned on the unabbreviated equations of\nmotion of the articulated rigid body model. A review of underlying modeling and\ncontrol methods is given, followed by implementation details including model\nabstraction, trajectory optimization and whole-body controller. The system's\neffectiveness is analyzed in simulation.", "AI": {"tldr": "提出了一种结合轨迹优化和全身控制的架构，通过模型抽象实现人形机器人高动态运动。", "motivation": "解决人形机器人高自由度带来的运动规划和控制难题，避免传统线性化和模型近似方法的性能下降问题。", "method": "采用轨迹优化和全身控制结合的架构，引入匹配的模型抽象，基于完整的刚体动力学方程。", "result": "在仿真中验证了系统执行高动态运动（如杂技动作）的有效性。", "conclusion": "该架构通过模型抽象和完整动力学方程，成功实现了人形机器人的高动态运动控制。"}}
{"id": "2508.08259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08259", "abs": "https://arxiv.org/abs/2508.08259", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting", "comment": null, "summary": "Online optimal control of quadruped robots would enable them to adapt to\nvarying inputs and changing conditions in real time. A common way of achieving\nthis is linear model predictive control (LMPC), where a quadratic programming\n(QP) problem is formulated over a finite horizon with a quadratic cost and\nlinear constraints obtained by linearizing the equations of motion and solved\non the fly. However, the model linearization may lead to model inaccuracies. In\nthis paper, we use the Koopman operator to create a linear model of the\nquadrupedal system in high dimensional space which preserves the nonlinearity\nof the equations of motion. Then using LMPC, we demonstrate high fidelity\ntracking and disturbance rejection on a quadrupedal robot. This is the first\nwork that uses the Koopman operator theory for LMPC of quadrupedal locomotion.", "AI": {"tldr": "使用Koopman算子为四足机器人构建高维线性模型，结合LMPC实现高精度跟踪和干扰抑制。", "motivation": "在线最优控制使四足机器人能实时适应变化输入和条件，但传统LMPC的线性化模型可能不准确。", "method": "利用Koopman算子在保留非线性运动方程的高维空间中构建线性模型，并应用LMPC。", "result": "实现了四足机器人的高精度跟踪和干扰抑制。", "conclusion": "首次将Koopman算子理论应用于四足机器人LMPC，提升了控制性能。"}}
{"id": "2508.08264", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08264", "abs": "https://arxiv.org/abs/2508.08264", "authors": ["Hadush Hailu", "Bruk Gebregziabher", "Prudhvi Raj"], "title": "Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance", "comment": "8 pages, 4 figures and 1 table", "summary": "The Iterative Forecast Planner (IFP) is a geometric planning approach that\noffers lightweight computations, scalable, and reactive solutions for\nmulti-robot path planning in decentralized, communication-free settings.\nHowever, it struggles in symmetric configurations, where mirrored interactions\noften lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and\nextended version of IFP that improves robustness and path consistency in dense,\ndynamic environments. The method refines threat prioritization using a\ntime-to-collision heuristic, stabilizes path generation through cost-based\nvia-point selection, and ensures dynamic feasibility by incorporating model\npredictive control (MPC) into the planning process. These enhancements are\ntightly integrated into the IFP to preserve its efficiency while improving its\nadaptability and stability. Extensive simulations across symmetric and\nhigh-density scenarios show that eIFP-MPC significantly reduces oscillations,\nensures collision-free motion, and improves trajectory efficiency. The results\ndemonstrate that geometric planners can be strengthened through optimization,\nenabling robust performance at scale in complex multi-agent environments.", "AI": {"tldr": "eIFP-MPC是IFP的优化版本，通过改进威胁优先级、路径稳定性和动态可行性，提升了在多机器人路径规划中的鲁棒性和效率。", "motivation": "IFP在对称配置中表现不佳，容易导致碰撞和死锁，因此需要一种更稳健的解决方案。", "method": "结合时间碰撞启发式优化威胁优先级，通过成本选择稳定路径，并集成模型预测控制（MPC）确保动态可行性。", "result": "在对称和高密度场景中，eIFP-MPC显著减少振荡，确保无碰撞运动，并提高轨迹效率。", "conclusion": "几何规划器通过优化可以提升性能，适用于复杂多智能体环境。"}}
{"id": "2508.08269", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08269", "abs": "https://arxiv.org/abs/2508.08269", "authors": ["Sagar Verma"], "title": "emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands", "comment": "Accepted in Robotics: Science and Systems (RSS 2025)", "summary": "Tendon-driven robotic hands offer unparalleled dexterity for manipulation\ntasks, but learning control policies for such systems presents unique\nchallenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a\ndirect one-to-one mapping between motion capture (mocap) data and tendon\ncontrols, making the learning process complex and expensive. Additionally,\nvisual tracking methods for real-world applications are prone to occlusions and\ninaccuracies, further complicating joint tracking. Wrist-wearable surface\nelectromyography (sEMG) sensors present an inexpensive, robust alternative to\ncapture hand motion. However, mapping sEMG signals to tendon control remains a\nsignificant challenge despite the availability of EMG-to-pose data sets and\nregression-based models in the existing literature.\n  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic\nhands, extending the emg2pose dataset, which includes recordings from 193\nsubjects, spanning 370 hours and 29 stages with diverse gestures. This dataset\nincorporates tendon control signals derived using the MyoSuite MyoHand model,\naddressing limitations such as invalid poses in prior methods. We provide three\nbaseline regression models to demonstrate emg2tendon utility and propose a\nnovel diffusion-based regression model for predicting tendon control from sEMG\nrecordings. This dataset and modeling framework marks a significant step\nforward for tendon-driven dexterous robotic manipulation, laying the groundwork\nfor scalable and accurate tendon control in robotic hands.\nhttps://emg2tendon.github.io/", "AI": {"tldr": "论文提出了首个大规模EMG到肌腱控制数据集，用于肌腱驱动机器人手的控制学习，并提出了基于扩散的回归模型。", "motivation": "肌腱驱动机器人手的控制学习复杂且昂贵，现有视觉跟踪方法易受遮挡和不准确性影响，而sEMG传感器提供了一种廉价且鲁棒的替代方案。然而，将sEMG信号映射到肌腱控制仍具挑战性。", "method": "扩展了emg2pose数据集，创建了包含193名受试者、370小时数据的EMG-to-Tendon Control数据集，并提供了三种基线回归模型及一种新型扩散回归模型。", "result": "数据集和模型框架为肌腱驱动的灵巧机器人操作提供了重要进展，解决了现有方法的局限性。", "conclusion": "该研究为机器人手的可扩展和精确肌腱控制奠定了基础。"}}
{"id": "2508.08303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08303", "abs": "https://arxiv.org/abs/2508.08303", "authors": ["Yasuyuki Fujii", "Dinh Tuan Tran", "Joo-Ho Lee"], "title": "Evaluation of an Autonomous Surface Robot Equipped with a Transformable Mobility Mechanism for Efficient Mobility Control", "comment": "5 pages, 6 figures. Presented at the ICRA 2025 Workshop on REaCT:\n  Robotics for Environmental and Climate Assessment", "summary": "Efficient mobility and power consumption are critical for autonomous water\nsurface robots in long-term water environmental monitoring. This study develops\nand evaluates a transformable mobility mechanism for a water surface robot with\ntwo control modes: station-keeping and traveling to improve energy efficiency\nand maneuverability. Field experiments show that, in a round-trip task between\ntwo points, the traveling mode reduces power consumption by 10\\% and decreases\nthe total time required for travel by 5\\% compared to the station-keeping mode.\nThese results confirm the effectiveness of the transformable mobility mechanism\nfor enhancing operational efficiency in patrolling on water surface.", "AI": {"tldr": "该研究开发了一种可变形的移动机制，用于水面机器人，以提高能源效率和机动性。实验表明，旅行模式比驻留模式节省10%的能耗，并减少5%的旅行时间。", "motivation": "提高水面机器人在长期环境监测中的移动效率和能源效率。", "method": "开发并评估了一种具有两种控制模式（驻留和旅行）的可变形移动机制。", "result": "旅行模式比驻留模式节省10%的能耗，并减少5%的旅行时间。", "conclusion": "可变形的移动机制显著提高了水面机器人的操作效率。"}}
{"id": "2508.08328", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08328", "abs": "https://arxiv.org/abs/2508.08328", "authors": ["Qiwei Liang", "Boyang Cai", "Rongyi He", "Hui Li", "Tao Teng", "Haihan Duan", "Changxin Huang", "Runhao Zeng"], "title": "Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators", "comment": null, "summary": "Quadrupedal robots with manipulators offer strong mobility and adaptability\nfor grasping in unstructured, dynamic environments through coordinated\nwhole-body control. However, existing research has predominantly focused on\nstatic-object grasping, neglecting the challenges posed by dynamic targets and\nthus limiting applicability in dynamic scenarios such as logistics sorting and\nhuman-robot collaboration. To address this, we introduce DQ-Bench, a new\nbenchmark that systematically evaluates dynamic grasping across varying object\nmotions, velocities, heights, object types, and terrain complexities, along\nwith comprehensive evaluation metrics. Building upon this benchmark, we propose\nDQ-Net, a compact teacher-student framework designed to infer grasp\nconfigurations from limited perceptual cues. During training, the teacher\nnetwork leverages privileged information to holistically model both the static\ngeometric properties and dynamic motion characteristics of the target, and\nintegrates a grasp fusion module to deliver robust guidance for motion\nplanning. Concurrently, we design a lightweight student network that performs\ndual-viewpoint temporal modeling using only the target mask, depth map, and\nproprioceptive state, enabling closed-loop action outputs without reliance on\nprivileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net\nachieves robust dynamic objects grasping across multiple task settings,\nsubstantially outperforming baseline methods in both success rate and\nresponsiveness.", "AI": {"tldr": "DQ-Bench是一个新基准，用于评估动态抓取任务，DQ-Net是一个紧凑的师生框架，通过有限感知线索推断抓取配置，显著优于基线方法。", "motivation": "现有研究主要关注静态物体抓取，忽视了动态目标的挑战，限制了在动态场景中的应用。", "method": "提出DQ-Bench基准，并设计DQ-Net框架，包括教师网络（利用特权信息建模静态和动态特性）和学生网络（仅使用目标掩码、深度图和本体状态进行闭环输出）。", "result": "DQ-Net在多个任务设置中实现了稳健的动态物体抓取，成功率和响应性显著优于基线方法。", "conclusion": "DQ-Net和DQ-Bench为动态抓取任务提供了有效的解决方案，扩展了四足机器人抓取的应用场景。"}}
{"id": "2508.08473", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.08473", "abs": "https://arxiv.org/abs/2508.08473", "authors": ["Hossein B. Jond"], "title": "A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems", "comment": null, "summary": "Collective behaviors such as swarming and flocking emerge from simple,\ndecentralized interactions in biological systems. Existing models, such as\nVicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber\nmodel imposes rigid formations, limiting their applicability in swarm robotics.\nTo address these limitations, this paper proposes a minimal yet expressive\nmodel that governs agent dynamics using relative positions, velocities, and\nlocal density, modulated by two tunable parameters: the spatial offset and\nkinetic offset. The model achieves spatially flexible, collision-free behaviors\nthat reflect naturalistic group dynamics. Furthermore, we extend the framework\nto cognitive autonomous systems, enabling energy-aware phase transitions\nbetween swarming and flocking through adaptive control parameter tuning. This\ncognitively inspired approach offers a robust foundation for real-world\napplications in multi-robot systems, particularly autonomous aerial swarms.", "AI": {"tldr": "提出了一种新的模型，通过相对位置、速度和局部密度调控代理动态，实现灵活、无碰撞的群体行为，并扩展至认知自主系统。", "motivation": "现有模型（如Vicsek、Cucker-Smale和Olfati-Saber）在碰撞避免或刚性结构上存在不足，限制了其在群体机器人中的应用。", "method": "使用相对位置、速度和局部密度调控代理动态，引入空间偏移和动力学偏移两个可调参数。", "result": "模型实现了空间灵活且无碰撞的群体行为，并扩展至认知自主系统，支持能量感知的相变。", "conclusion": "该模型为多机器人系统（如自主空中群体）提供了鲁棒的基础。"}}
{"id": "2508.08507", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08507", "abs": "https://arxiv.org/abs/2508.08507", "authors": ["Shaun Macdonald", "Salma ElSayed", "Mark McGill"], "title": "AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality", "comment": "Companion of the 2025 ACM/IEEE International Conference on\n  Human-Robot Interaction (RO-MAN 2025)", "summary": "Zoomorphic robots could serve as accessible and practical alternatives for\nusers unable or unwilling to keep pets. However, their affective interactions\nare often simplistic and short-lived, limiting their potential for domestic\nadoption. In order to facilitate more dynamic and nuanced affective\ninteractions and relationships between users and zoomorphic robots we present\nAZRA, a novel augmented reality (AR) framework that extends the affective\ncapabilities of these robots without physical modifications. To demonstrate\nAZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays\n(face, light, sound, thought bubbles) and interaction modalities (voice, touch,\nproximity, gaze). Additionally, AZRA features a computational model of emotion\nto calculate the robot's emotional responses, daily moods, evolving personality\nand needs. We highlight how AZRA can be used for rapid participatory\nprototyping and enhancing existing robots, then discuss implications on future\nzoomorphic robot development.", "AI": {"tldr": "AZRA是一个增强现实框架，通过扩展情感交互能力提升仿生机器人与用户的互动，无需物理改造。", "motivation": "仿生机器人的情感交互通常简单且短暂，限制了其在家用场景中的潜力。", "method": "提出AZRA框架，通过AR技术为仿生机器人增加情感显示和交互方式，并引入情感计算模型。", "result": "展示了AZRA如何增强Petit Qoobo机器人的情感交互能力，并支持快速原型设计。", "conclusion": "AZRA为未来仿生机器人开发提供了新的可能性，强调了情感交互的重要性。"}}
{"id": "2508.08574", "categories": ["cs.RO", "cs.MA", "I.2.9; I.2.11"], "pdf": "https://arxiv.org/pdf/2508.08574", "abs": "https://arxiv.org/abs/2508.08574", "authors": ["Ameya Agaskar", "Sriram Siva", "William Pickering", "Kyle O'Brien", "Charles Kekeh", "Ang Li", "Brianna Gallo Sarker", "Alicia Chua", "Mayur Nemade", "Charun Thattai", "Jiaming Di", "Isaac Iyengar", "Ramya Dharoor", "Dino Kirouani", "Jimmy Erskine", "Tamir Hegazy", "Scott Niekum", "Usman A. Khan", "Federico Pecora", "Joseph W. Durham"], "title": "DeepFleet: Multi-Agent Foundation Models for Mobile Robots", "comment": "25 pages, 10 figures, 2 tables", "summary": "We introduce DeepFleet, a suite of foundation models designed to support\ncoordination and planning for large-scale mobile robot fleets. These models are\ntrained on fleet movement data, including robot positions, goals, and\ninteractions, from hundreds of thousands of robots in Amazon warehouses\nworldwide. DeepFleet consists of four architectures that each embody a distinct\ninductive bias and collectively explore key points in the design space for\nmulti-agent foundation models: the robot-centric (RC) model is an\nautoregressive decision transformer operating on neighborhoods of individual\nrobots; the robot-floor (RF) model uses a transformer with cross-attention\nbetween robots and the warehouse floor; the image-floor (IF) model applies\nconvolutional encoding to a multi-channel image representation of the full\nfleet; and the graph-floor (GF) model combines temporal attention with graph\nneural networks for spatial relationships. In this paper, we describe these\nmodels and present our evaluation of the impact of these design choices on\nprediction task performance. We find that the robot-centric and graph-floor\nmodels, which both use asynchronous robot state updates and incorporate the\nlocalized structure of robot interactions, show the most promise. We also\npresent experiments that show that these two models can make effective use of\nlarger warehouses operation datasets as the models are scaled up.", "AI": {"tldr": "DeepFleet是一套用于大规模移动机器人车队协调与规划的基础模型，包含四种架构，其中机器人中心模型和图-地面模型表现最佳。", "motivation": "解决大规模机器人车队的协调与规划问题，利用仓库数据优化模型设计。", "method": "四种模型架构：机器人中心模型（RC）、机器人-地面模型（RF）、图像-地面模型（IF）和图-地面模型（GF），分别采用不同的归纳偏置和设计思路。", "result": "RC和GF模型表现最优，能有效利用更大规模的数据集。", "conclusion": "DeepFleet展示了多智能体基础模型的设计潜力，RC和GF模型因其异步更新和局部交互结构表现突出。"}}
{"id": "2508.08576", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08576", "abs": "https://arxiv.org/abs/2508.08576", "authors": ["Deniz Karanfil", "Daniel Lindmark", "Martin Servin", "David Torick", "Bahram Ravani"], "title": "Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles", "comment": null, "summary": "This paper presents the development of a calibrated digital twin of a wheel\nloader. A calibrated digital twin integrates a construction vehicle with a\nhigh-fidelity digital model allowing for automated diagnostics and optimization\nof operations as well as pre-planning simulations enhancing automation\ncapabilities. The high-fidelity digital model is a virtual twin of the physical\nwheel loader. It uses a physics-based multibody dynamic model of the wheel\nloader in the software AGX Dynamics. Interactions of the wheel loader's bucket\nwhile in use in construction can be simulated in the virtual model. Calibration\nmakes this simulation of high-fidelity which can enhance realistic planning for\nautomation of construction operations. In this work, a wheel loader was\ninstrumented with several sensors used to calibrate the digital model. The\ncalibrated digital twin was able to estimate the magnitude of the forces on the\nbucket base with high accuracy, providing a high-fidelity simulation.", "AI": {"tldr": "本文开发了一个校准的轮式装载机数字孪生体，通过高保真数字模型实现自动化诊断、操作优化和预规划模拟。", "motivation": "提升建筑操作的自动化能力，通过高保真模拟实现更现实的规划。", "method": "使用物理基础的多体动力学模型（AGX Dynamics软件）构建数字孪生体，并通过传感器校准模型。", "result": "校准后的数字孪生体能高精度估计铲斗受力，提供高保真模拟。", "conclusion": "校准的数字孪生体为建筑操作的自动化提供了有效的工具。"}}
{"id": "2508.08607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08607", "abs": "https://arxiv.org/abs/2508.08607", "authors": ["Justin London"], "title": "Autonomous Mobile Plant Watering Robot : A Kinematic Approach", "comment": null, "summary": "Plants need regular and the appropriate amount of watering to thrive and\nsurvive. While agricultural robots exist that can spray water on plants and\ncrops such as the , they are expensive and have limited mobility and/or\nfunctionality. We introduce a novel autonomous mobile plant watering robot that\nuses a 6 degree of freedom (DOF) manipulator, connected to a 4 wheel drive\nalloy chassis, to be able to hold a garden hose, recognize and detect plants,\nand to water them with the appropriate amount of water by being able to insert\na soil humidity/moisture sensor into the soil. The robot uses Jetson Nano and\nArduino microcontroller and real sense camera to perform computer vision to\ndetect plants using real-time YOLOv5 with the Pl@ntNet-300K dataset. The robot\nuses LIDAR for object and collision avoideance and does not need to move on a\npre-defined path and can keep track of which plants it has watered. We provide\nthe Denavit-Hartenberg (DH) Table, forward kinematics, differential driving\nkinematics, and inverse kinematics along with simulation and experiment results", "AI": {"tldr": "介绍了一种新型自主移动植物浇水机器人，具备6自由度机械臂和4轮驱动底盘，能识别植物并精准浇水。", "motivation": "现有农业机器人昂贵且功能有限，需更灵活高效的解决方案。", "method": "使用Jetson Nano和Arduino微控制器，结合YOLOv5和Pl@ntNet-300K数据集进行植物识别，LIDAR避障，无需预设路径。", "result": "提供了DH表、运动学分析及仿真实验结果，验证了机器人功能。", "conclusion": "该机器人能高效自主完成植物浇水任务，具有实用性和推广潜力。"}}
{"id": "2508.08624", "categories": ["cs.RO", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.08624", "abs": "https://arxiv.org/abs/2508.08624", "authors": ["Chenxuan Liu", "He Li", "Zongze Li", "Shuai Wang", "Wei Xu", "Kejiang Ye", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization", "comment": "14 pages, 18 figures, to appear in IEEE Transactions on Cognitive\n  Communications and Networking", "summary": "Realizing low-cost communication in robotic mixed reality (RoboMR) systems\npresents a challenge, due to the necessity of uploading high-resolution images\nthrough wireless channels. This paper proposes Gaussian splatting (GS) RoboMR\n(GSMR), which enables the simulator to opportunistically render a\nphoto-realistic view from the robot's pose by calling ``memory'' from a GS\nmodel, thus reducing the need for excessive image uploads. However, the GS\nmodel may involve discrepancies compared to the actual environments. To this\nend, a GS cross-layer optimization (GSCLO) framework is further proposed, which\njointly optimizes content switching (i.e., deciding whether to upload image or\nnot) and power allocation (i.e., adjusting to content profiles) across\ndifferent frames by minimizing a newly derived GSMR loss function. The GSCLO\nproblem is addressed by an accelerated penalty optimization (APO) algorithm\nthat reduces computational complexity by over $10$x compared to traditional\nbranch-and-bound and search algorithms. Moreover, variants of GSCLO are\npresented to achieve robust, low-power, and multi-robot GSMR. Extensive\nexperiments demonstrate that the proposed GSMR paradigm and GSCLO method\nachieve significant improvements over existing benchmarks on both wheeled and\nlegged robots in terms of diverse metrics in various scenarios. For the first\ntime, it is found that RoboMR can be achieved with ultra-low communication\ncosts, and mixture of data is useful for enhancing GS performance in dynamic\nscenarios.", "AI": {"tldr": "论文提出了一种基于高斯泼溅（GS）的机器人混合现实（RoboMR）系统GSMR，通过优化内容切换和功率分配，显著降低了通信成本。", "motivation": "解决机器人混合现实系统中因上传高分辨率图像导致的通信成本高的问题。", "method": "提出GSMR框架和GSCLO优化方法，采用APO算法降低计算复杂度。", "result": "实验表明，GSMR和GSCLO在多种场景下优于现有基准，首次实现超低通信成本的RoboMR。", "conclusion": "GSMR和GSCLO方法有效降低了通信成本，动态场景中混合数据有助于提升GS性能。"}}
{"id": "2508.08690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08690", "abs": "https://arxiv.org/abs/2508.08690", "authors": ["Zhenjiang Wang", "Yunhua Jiang", "Zikun Zhen", "Yifan Jiang", "Yubin Tan", "Wubin Wang"], "title": "ZS-Puffin: Design, Modeling and Implementation of an Unmanned Aerial-Aquatic Vehicle with Amphibious Wings", "comment": "Accepted to IROS 2025", "summary": "Unmanned aerial-aquatic vehicles (UAAVs) can operate both in the air and\nunderwater, giving them broad application prospects. Inspired by the\ndual-function wings of puffins, we propose a UAAV with amphibious wings to\naddress the challenge posed by medium differences on the vehicle's propulsion\nsystem. The amphibious wing, redesigned based on a fixed-wing structure,\nfeatures a single degree of freedom in pitch and requires no additional\ncomponents. It can generate lift in the air and function as a flapping wing for\npropulsion underwater, reducing disturbance to marine life and making it\nenvironmentally friendly. Additionally, an artificial central pattern generator\n(CPG) is introduced to enhance the smoothness of the flapping motion. This\npaper presents the prototype, design details, and practical implementation of\nthis concept.", "AI": {"tldr": "提出一种基于海雀翅膀启发的两栖翼无人空中-水下飞行器（UAAV），通过单自由度俯仰设计实现空中和水下的高效推进，并引入人工中央模式生成器（CPG）优化运动平滑性。", "motivation": "解决传统UAAV在不同介质（空气和水）中推进系统面临的挑战，同时减少对海洋生物的干扰，实现环保设计。", "method": "重新设计固定翼结构为两栖翼，具有单自由度俯仰功能，无需额外组件；引入CPG优化水下扑翼运动的平滑性。", "result": "原型机展示了两栖翼在空中产生升力和水下扑翼推进的双重功能，验证了设计的可行性和环保性。", "conclusion": "两栖翼UAAV设计有效解决了介质差异带来的问题，兼具高效和环保特性，为未来应用提供了新思路。"}}
{"id": "2508.08706", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08706", "abs": "https://arxiv.org/abs/2508.08706", "authors": ["Zhengxue Cheng", "Yiqian Zhang", "Wenkang Zhang", "Haoyu Li", "Keyu Wang", "Li Song", "Hengdi Zhang"], "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing", "comment": "15 pages, 7 figures, 8 tables", "summary": "Recent vision-language-action (VLA) models build upon vision-language\nfoundations, and have achieved promising results and exhibit the possibility of\ntask generalization in robot manipulation. However, due to the heterogeneity of\ntactile sensors and the difficulty of acquiring tactile data, current VLA\nmodels significantly overlook the importance of tactile perception and fail in\ncontact-rich tasks. To address this issue, this paper proposes OmniVTLA, a\nnovel architecture involving tactile sensing. Specifically, our contributions\nare threefold. First, our OmniVTLA features a dual-path tactile encoder\nframework. This framework enhances tactile perception across diverse\nvision-based and force-based tactile sensors by using a pretrained vision\ntransformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we\nintroduce ObjTac, a comprehensive force-based tactile dataset capturing\ntextual, visual, and tactile information for 56 objects across 10 categories.\nWith 135K tri-modal samples, ObjTac supplements existing visuo-tactile\ndatasets. Third, leveraging this dataset, we train a semantically-aligned\ntactile encoder to learn a unified tactile representation, serving as a better\ninitialization for OmniVTLA. Real-world experiments demonstrate substantial\nimprovements over state-of-the-art VLA baselines, achieving 96.9% success rates\nwith grippers, (21.9% higher over baseline) and 100% success rates with\ndexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,\nOmniVTLA significantly reduces task completion time and generates smoother\ntrajectories through tactile sensing compared to existing VLA.", "AI": {"tldr": "OmniVTLA是一种新型视觉-语言-动作（VLA）模型，通过引入触觉感知解决了现有模型在接触密集型任务中的不足。", "motivation": "现有VLA模型因触觉传感器异质性和数据获取困难而忽视触觉感知，导致在接触密集型任务中表现不佳。", "method": "提出双路径触觉编码器框架，结合预训练视觉变换器（ViT）和语义对齐触觉ViT（SA-ViT），并引入多模态触觉数据集ObjTac。", "result": "在真实实验中，OmniVTLA在抓取任务中成功率显著提升（夹爪96.9%，灵巧手100%），并减少任务完成时间和生成更平滑轨迹。", "conclusion": "OmniVTLA通过触觉感知显著提升了VLA模型的性能，尤其在接触密集型任务中表现优异。"}}
{"id": "2508.08707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08707", "abs": "https://arxiv.org/abs/2508.08707", "authors": ["Haoran Ding", "Anqing Duan", "Zezhou Sun", "Leonel Rozo", "Noémie Jaquier", "Dezhen Song", "Yoshihiko Nakamura"], "title": "Towards Safe Imitation Learning via Potential Field-Guided Flow Matching", "comment": "8 pages, 6 figures, Accepted to IROS 2025", "summary": "Deep generative models, particularly diffusion and flow matching models, have\nrecently shown remarkable potential in learning complex policies through\nimitation learning. However, the safety of generated motions remains\noverlooked, particularly in complex environments with inherent obstacles. In\nthis work, we address this critical gap by proposing Potential Field-Guided\nFlow Matching Policy (PF2MP), a novel approach that simultaneously learns task\npolicies and extracts obstacle-related information, represented as a potential\nfield, from the same set of successful demonstrations. During inference, PF2MP\nmodulates the flow matching vector field via the learned potential field,\nenabling safe motion generation. By leveraging these complementary fields, our\napproach achieves improved safety without compromising task success across\ndiverse environments, such as navigation tasks and robotic manipulation\nscenarios. We evaluate PF2MP in both simulation and real-world settings,\ndemonstrating its effectiveness in task space and joint space control.\nExperimental results demonstrate that PF2MP enhances safety, achieving a\nsignificant reduction of collisions compared to baseline policies. This work\npaves the way for safer motion generation in unstructured and obstaclerich\nenvironments.", "AI": {"tldr": "PF2MP是一种结合势场引导和流匹配的深度生成模型，用于安全运动生成，显著减少碰撞。", "motivation": "现有深度生成模型在模仿学习中生成复杂策略时，安全性常被忽视，尤其是在复杂障碍环境中。", "method": "提出PF2MP方法，通过成功演示同时学习任务策略和提取障碍信息（势场），在推理时通过势场调节流匹配向量场。", "result": "在仿真和实际环境中验证，PF2MP显著减少碰撞，提升安全性，同时保持任务成功率。", "conclusion": "PF2MP为复杂障碍环境中的安全运动生成提供了有效解决方案。"}}
{"id": "2508.08709", "categories": ["cs.RO", "cs.AR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.08709", "abs": "https://arxiv.org/abs/2508.08709", "authors": ["Lukas Krupp", "Maximilian Schöffel", "Elias Biehl", "Norbert Wehn"], "title": "CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems", "comment": "Accepted for presentation at the 22nd International SoC Conference\n  (ISOCC 2025). Proceedings to be included in IEEE Xplore", "summary": "This paper presents CRADLE, a conversational framework for design space\nexploration of RTL designs using LLM-based multi-agent systems. Unlike existing\nrigid approaches, CRADLE enables user-guided flows with internal\nself-verification, correction, and optimization. We demonstrate the framework\nwith a generator-critic agent system targeting FPGA resource minimization using\nstate-of-the-art LLMs. Experimental results on the RTLLM benchmark show that\nCRADLE achieves significant reductions in resource usage with averages of 48%\nand 40% in LUTs and FFs across all benchmark designs.", "AI": {"tldr": "CRADLE是一个基于LLM多智能体系统的对话框架，用于RTL设计空间探索，支持用户引导的流程，并具备自我验证、修正和优化功能。实验显示其在FPGA资源最小化方面表现优异。", "motivation": "现有方法过于僵化，CRADLE旨在提供更灵活、用户友好的设计探索框架。", "method": "采用生成器-批评家智能体系统，结合先进LLM技术，实现FPGA资源最小化。", "result": "在RTLLM基准测试中，CRADLE平均减少LUTs和FFs资源使用48%和40%。", "conclusion": "CRADLE通过多智能体协作和LLM技术，显著提升了RTL设计空间探索的效率和效果。"}}
{"id": "2508.08743", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08743", "abs": "https://arxiv.org/abs/2508.08743", "authors": ["Haoyu Zhang", "Long Cheng"], "title": "Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos", "comment": null, "summary": "Learning from demonstrations (LfD) typically relies on large amounts of\naction-labeled expert trajectories, which fundamentally constrains the scale of\navailable training data. A promising alternative is to learn directly from\nunlabeled video demonstrations. However, we find that existing methods tend to\nencode latent actions that share little mutual information with the true robot\nactions, leading to suboptimal control performance. To address this limitation,\nwe introduce a novel framework that explicitly maximizes the mutual information\nbetween latent actions and true actions, even in the absence of action labels.\nOur method leverage the variational information-bottleneck to extract\naction-relevant representations while discarding task-irrelevant information.\nWe provide a theoretical analysis showing that our objective indeed maximizes\nthe mutual information between latent and true actions. Finally, we validate\nour approach through extensive experiments: first in simulated robotic\nenvironments and then on real-world robotic platforms, the experimental results\ndemonstrate that our method significantly enhances mutual information and\nconsistently improves policy performance.", "AI": {"tldr": "论文提出了一种新框架，通过最大化潜在动作与真实动作的互信息，直接从无标签视频演示中学习，提升控制性能。", "motivation": "传统基于演示的学习（LfD）依赖大量带标签的专家轨迹，限制了训练数据的规模。直接从无标签视频中学习是一种有前景的替代方案，但现有方法提取的潜在动作与真实动作互信息不足，导致控制性能不佳。", "method": "引入了一种新框架，利用变分信息瓶颈技术，显式最大化潜在动作与真实动作的互信息，即使在没有动作标签的情况下。", "result": "理论分析表明该方法确实能最大化潜在动作与真实动作的互信息。实验验证表明，该方法显著提升了互信息和策略性能。", "conclusion": "该方法通过最大化互信息，有效提升了从无标签视频中学习的控制性能，适用于模拟和真实机器人环境。"}}
{"id": "2508.08748", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08748", "abs": "https://arxiv.org/abs/2508.08748", "authors": ["Muhammad A. Muttaqien", "Tomohiro Motoda", "Ryo Hanai", "Yukiyasu Domae"], "title": "Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT", "comment": null, "summary": "Robotic pick-and-place tasks in convenience stores pose challenges due to\ndense object arrangements, occlusions, and variations in object properties such\nas color, shape, size, and texture. These factors complicate trajectory\nplanning and grasping. This paper introduces a perception-action pipeline\nleveraging annotation-guided visual prompting, where bounding box annotations\nidentify both pickable objects and placement locations, providing structured\nspatial guidance. Instead of traditional step-by-step planning, we employ\nAction Chunking with Transformers (ACT) as an imitation learning algorithm,\nenabling the robotic arm to predict chunked action sequences from human\ndemonstrations. This facilitates smooth, adaptive, and data-driven\npick-and-place operations. We evaluate our system based on success rate and\nvisual analysis of grasping behavior, demonstrating improved grasp accuracy and\nadaptability in retail environments.", "AI": {"tldr": "论文提出了一种基于标注引导视觉提示的感知-动作管道，用于解决便利店中机器人抓取任务因物体密集、遮挡和多样性带来的挑战。", "motivation": "便利店中的机器人抓取任务因物体密集、遮挡和多样性（如颜色、形状、大小和纹理）而复杂化，传统方法难以应对。", "method": "采用标注引导的视觉提示（通过边界框标注识别可抓取物体和放置位置），并结合基于模仿学习的Action Chunking with Transformers (ACT)算法，预测分块动作序列。", "result": "实验表明，系统在抓取成功率和适应性方面表现优异，适用于零售环境。", "conclusion": "该方法通过结构化空间指导和数据驱动的动作预测，提升了机器人抓取任务的准确性和适应性。"}}
{"id": "2508.08767", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08767", "abs": "https://arxiv.org/abs/2508.08767", "authors": ["Kazuki Komura", "Kumi Ozaki", "Seiji Yamada"], "title": "Robot can reduce superior's dominance in group discussions with human social hierarchy", "comment": "8 pages, 7 figures. International Conference on Human-Agent\n  Interaction (HAI '24), November 24-27, 2024, Swansea, United Kingdom", "summary": "This study investigated whether robotic agents that deal with social\nhierarchical relationships can reduce the dominance of superiors and equalize\nparticipation among participants in discussions with hierarchical structures.\nThirty doctors and students having hierarchical relationship were gathered as\nparticipants, and an intervention experiment was conducted using a robot that\ncan encourage participants to speak depending on social hierarchy. These were\ncompared with strategies that intervened equally for all participants without\nconsidering hierarchy and with a no-action. The robots performed follow\nactions, showing backchanneling to speech, and encourage actions, prompting\nspeech from members with less speaking time, on the basis of the hierarchical\nrelationships among group members to equalize participation. The experimental\nresults revealed that the robot's actions could potentially influence the\nspeaking time among members, but it could not be conclusively stated that there\nwere significant differences between the robot's action conditions. However,\nthe results suggested that it might be possible to influence speaking time\nwithout decreasing the satisfaction of superiors. This indicates that in\ndiscussion scenarios where experienced superiors are likely to dominate,\ncontrolling the robot's backchanneling behavior could potentially suppress\ndominance and equalize participation among group members.", "AI": {"tldr": "研究探讨机器人能否通过社交层级干预减少上级主导，促进讨论平等参与。实验结果显示机器人行为可能影响发言时间，但未显著差异，且不影响上级满意度。", "motivation": "解决层级讨论中上级主导问题，促进平等参与。", "method": "30名医生和学生参与实验，机器人根据层级关系鼓励发言，对比无干预和均等干预策略。", "result": "机器人行为可能影响发言时间，但未显著差异，且不影响上级满意度。", "conclusion": "机器人可通过控制反馈行为抑制上级主导，促进讨论平等。"}}
{"id": "2508.08896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08896", "abs": "https://arxiv.org/abs/2508.08896", "authors": ["Haoyu Zhao", "Linghao Zhuang", "Xingyue Zhao", "Cheng Zeng", "Haoran Xu", "Yuming Jiang", "Jun Cen", "Kexiang Wang", "Jiayan Guo", "Siteng Huang", "Xin Li", "Deli Zhao", "Hua Zou"], "title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors", "comment": "13 pages, 8 figures", "summary": "A dexterous hand capable of generalizable grasping objects is fundamental for\nthe development of general-purpose embodied AI. However, previous methods focus\nnarrowly on low-level grasp stability metrics, neglecting affordance-aware\npositioning and human-like poses which are crucial for downstream manipulation.\nTo address these limitations, we propose AffordDex, a novel framework with\ntwo-stage training that learns a universal grasping policy with an inherent\nunderstanding of both motion priors and object affordances. In the first stage,\na trajectory imitator is pre-trained on a large corpus of human hand motions to\ninstill a strong prior for natural movement. In the second stage, a residual\nmodule is trained to adapt these general human-like motions to specific object\ninstances. This refinement is critically guided by two components: our Negative\nAffordance-aware Segmentation (NAA) module, which identifies functionally\ninappropriate contact regions, and a privileged teacher-student distillation\nprocess that ensures the final vision-based policy is highly successful.\nExtensive experiments demonstrate that AffordDex not only achieves universal\ndexterous grasping but also remains remarkably human-like in posture and\nfunctionally appropriate in contact location. As a result, AffordDex\nsignificantly outperforms state-of-the-art baselines across seen objects,\nunseen instances, and even entirely novel categories.", "AI": {"tldr": "AffordDex是一个两阶段训练框架，通过学习运动先验和物体功能理解，实现通用灵巧抓取，显著优于现有方法。", "motivation": "现有方法过于关注低级抓取稳定性，忽视了功能感知定位和类人姿态，这对下游操作至关重要。", "method": "AffordDex采用两阶段训练：第一阶段预训练轨迹模仿器学习人类手部运动先验；第二阶段通过残差模块和NAA模块调整运动以适应具体物体。", "result": "AffordDex实现了通用灵巧抓取，姿态类人且接触位置功能合理，在已知物体、未知实例和新类别上均显著优于基线。", "conclusion": "AffordDex通过结合运动先验和功能理解，显著提升了灵巧抓取的通用性和功能性。"}}
{"id": "2508.08982", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08982", "abs": "https://arxiv.org/abs/2508.08982", "authors": ["Seungeun Rho", "Kartik Garg", "Morgan Byrd", "Sehoon Ha"], "title": "Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion", "comment": "Conference on Robot Learning 2025", "summary": "Exploration is crucial for enabling legged robots to learn agile locomotion\nbehaviors that can overcome diverse obstacles. However, such exploration is\ninherently challenging, and we often rely on extensive reward engineering,\nexpert demonstrations, or curriculum learning - all of which limit\ngeneralizability. In this work, we propose Skill Discovery as Exploration\n(SDAX), a novel learning framework that significantly reduces human engineering\neffort. SDAX leverages unsupervised skill discovery to autonomously acquire a\ndiverse repertoire of skills for overcoming obstacles. To dynamically regulate\nthe level of exploration during training, SDAX employs a bi-level optimization\nprocess that autonomously adjusts the degree of exploration. We demonstrate\nthat SDAX enables quadrupedal robots to acquire highly agile behaviors\nincluding crawling, climbing, leaping, and executing complex maneuvers such as\njumping off vertical walls. Finally, we deploy the learned policy on real\nhardware, validating its successful transfer to the real world.", "AI": {"tldr": "SDAX是一种新型学习框架，通过无监督技能发现减少人工工程需求，使四足机器人掌握多种敏捷行为。", "motivation": "探索对机器人学习敏捷运动行为至关重要，但传统方法依赖人工设计，限制了泛化能力。", "method": "SDAX利用无监督技能发现和双层优化动态调节探索程度。", "result": "SDAX使机器人学会爬行、攀爬、跳跃等复杂行为，并成功迁移到现实硬件。", "conclusion": "SDAX显著减少人工干预，提升机器人行为的多样性和适应性。"}}
{"id": "2508.08983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08983", "abs": "https://arxiv.org/abs/2508.08983", "authors": ["Ben Zandonati", "Tomás Lozano-Pérez", "Leslie Pack Kaelbling"], "title": "Rational Inverse Reasoning", "comment": null, "summary": "Humans can observe a single, imperfect demonstration and immediately\ngeneralize to very different problem settings. Robots, in contrast, often\nrequire hundreds of examples and still struggle to generalize beyond the\ntraining conditions. We argue that this limitation arises from the inability to\nrecover the latent explanations that underpin intelligent behavior, and that\nthese explanations can take the form of structured programs consisting of\nhigh-level goals, sub-task decomposition, and execution constraints. In this\nwork, we introduce Rational Inverse Reasoning (RIR), a framework for inferring\nthese latent programs through a hierarchical generative model of behavior. RIR\nframes few-shot imitation as Bayesian program induction: a vision-language\nmodel iteratively proposes structured symbolic task hypotheses, while a\nplanner-in-the-loop inference scheme scores each by the likelihood of the\nobserved demonstration under that hypothesis. This loop yields a posterior over\nconcise, executable programs. We evaluate RIR on a suite of continuous\nmanipulation tasks designed to test one-shot and few-shot generalization across\nvariations in object pose, count, geometry, and layout. With as little as one\ndemonstration, RIR infers the intended task structure and generalizes to novel\nsettings, outperforming state-of-the-art vision-language model baselines.", "AI": {"tldr": "论文提出Rational Inverse Reasoning (RIR)框架，通过分层生成模型推断潜在程序，实现机器人从少量演示中泛化到新任务。", "motivation": "人类能从单一不完美演示中泛化到不同问题，而机器人需要大量数据且泛化能力差。作者认为这是因为机器人无法恢复智能行为的潜在解释。", "method": "RIR通过分层生成模型推断潜在程序，结合视觉语言模型和规划器进行贝叶斯程序归纳，生成可执行程序。", "result": "RIR在连续操作任务中，仅需一次演示即可推断任务结构并泛化到新场景，优于现有视觉语言模型基线。", "conclusion": "RIR框架通过推断潜在程序，显著提升了机器人从少量演示中泛化的能力。"}}
{"id": "2508.08999", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08999", "abs": "https://arxiv.org/abs/2508.08999", "authors": ["Chao Wang", "Michael Gienger", "Fan Zhang"], "title": "Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality", "comment": "4", "summary": "Expressive behaviors in robots are critical for effectively conveying their\nemotional states during interactions with humans. In this work, we present a\nframework that autonomously generates realistic and diverse robotic emotional\nexpressions based on expert human demonstrations captured in Mixed Reality\n(MR). Our system enables experts to teleoperate a virtual robot from a\nfirst-person perspective, capturing their facial expressions, head movements,\nand upper-body gestures, and mapping these behaviors onto corresponding robotic\ncomponents including eyes, ears, neck, and arms. Leveraging a\nflow-matching-based generative process, our model learns to produce coherent\nand varied behaviors in real-time in response to moving objects, conditioned\nexplicitly on given emotional states. A preliminary test validated the\neffectiveness of our approach for generating autonomous expressions.", "AI": {"tldr": "提出了一种基于混合现实（MR）的框架，通过专家示范自主生成机器人情感表达。", "motivation": "机器人情感表达对与人类互动至关重要，需多样化且真实。", "method": "利用MR捕捉专家示范，通过流匹配生成模型实时生成多样化行为。", "result": "初步测试验证了方法的有效性。", "conclusion": "该框架能有效生成自主情感表达。"}}
{"id": "2508.09071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09071", "abs": "https://arxiv.org/abs/2508.09071", "authors": ["Lin Sun", "Bin Xie", "Yingfei Liu", "Hao Shi", "Tiancai Wang", "Jiale Cao"], "title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models", "comment": "The project is visible at https://linsun449.github.io/GeoVLA/", "summary": "Vision-Language-Action (VLA) models have emerged as a promising approach for\nenabling robots to follow language instructions and predict corresponding\nactions.However, current VLA models mainly rely on 2D visual inputs, neglecting\nthe rich geometric information in the 3D physical world, which limits their\nspatial awareness and adaptability. In this paper, we present GeoVLA, a novel\nVLA framework that effectively integrates 3D information to advance robotic\nmanipulation. It uses a vision-language model (VLM) to process images and\nlanguage instructions,extracting fused vision-language embeddings. In parallel,\nit converts depth maps into point clouds and employs a customized point\nencoder, called Point Embedding Network, to generate 3D geometric embeddings\nindependently. These produced embeddings are then concatenated and processed by\nour proposed spatial-aware action expert, called 3D-enhanced Action Expert,\nwhich combines information from different sensor modalities to produce precise\naction sequences. Through extensive experiments in both simulation and\nreal-world environments, GeoVLA demonstrates superior performance and\nrobustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2\nsimulation benchmarks and shows remarkable robustness in real-world tasks\nrequiring height adaptability, scale awareness and viewpoint invariance.", "AI": {"tldr": "GeoVLA是一个新型的视觉-语言-动作（VLA）框架，通过整合3D几何信息提升机器人操作的性能。", "motivation": "当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界的几何信息，限制了空间感知和适应性。", "method": "结合视觉语言模型处理图像和语言指令，同时使用点嵌入网络处理深度图生成3D几何嵌入，最后由3D增强动作专家生成动作序列。", "result": "在LIBERO和ManiSkill2仿真基准测试中表现优异，并在需要高度适应、尺度感知和视角不变性的真实任务中展现出鲁棒性。", "conclusion": "GeoVLA通过整合3D信息显著提升了机器人操作的性能和适应性。"}}
