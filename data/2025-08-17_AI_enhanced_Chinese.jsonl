{"id": "2508.10203", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.10203", "abs": "https://arxiv.org/abs/2508.10203", "authors": ["Matthew D. Osburn", "Cameron K. Peterson", "John L. Salmon"], "title": "Systematic Constraint Formulation and Collision-Free Trajectory Planning Using Space-Time Graphs of Convex Sets", "comment": "21 pages with references, 20 figures", "summary": "In this paper, we create optimal, collision-free, time-dependent trajectories\nthrough cluttered dynamic environments. The many spatial and temporal\nconstraints make finding an initial guess for a numerical solver difficult.\nGraphs of Convex Sets (GCS) and the recently developed Space-Time Graphs of\nConvex Sets formulation (ST-GCS) enable us to generate optimal minimum distance\ncollision-free trajectories without providing an initial guess to the solver.\nWe also explore the derivation of general GCS-compatible constraints and\ndocument an intuitive strategy for adapting general constraints to the\nframework. We show that ST-GCS produces equivalent trajectories to the standard\nGCS formulation when the environment is static. We then show ST-GCS operating\nin dynamic environments to find minimum distance collision-free trajectories.", "AI": {"tldr": "提出了一种基于空间-时间凸集图（ST-GCS）的方法，用于在动态环境中生成最优无碰撞轨迹，无需初始猜测。", "motivation": "在复杂动态环境中，传统方法难以找到满足时空约束的初始轨迹猜测。", "method": "利用空间-时间凸集图（ST-GCS）生成最优无碰撞轨迹，无需初始猜测，并探索了通用GCS兼容约束的推导。", "result": "ST-GCS在静态环境中与标准GCS等效，在动态环境中能生成最小距离无碰撞轨迹。", "conclusion": "ST-GCS是一种高效的方法，适用于动态环境中的轨迹规划。"}}
{"id": "2508.10423", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10423", "abs": "https://arxiv.org/abs/2508.10423", "authors": ["Qi Liu", "Xiaopeng Zhang", "Mingshan Tan", "Shuaikang Ma", "Jinliang Ding", "Yanjie Li"], "title": "MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion", "comment": null, "summary": "This paper proposes a novel method to enhance locomotion for a single\nhumanoid robot through cooperative-heterogeneous multi-agent deep reinforcement\nlearning (MARL). While most existing methods typically employ single-agent\nreinforcement learning algorithms for a single humanoid robot or MARL\nalgorithms for multi-robot system tasks, we propose a distinct paradigm:\napplying cooperative-heterogeneous MARL to optimize locomotion for a single\nhumanoid robot. The proposed method, multi-agent reinforcement learning for\nsingle humanoid locomotion (MASH), treats each limb (legs and arms) as an\nindependent agent that explores the robot's action space while sharing a global\ncritic for cooperative learning. Experiments demonstrate that MASH accelerates\ntraining convergence and improves whole-body cooperation ability, outperforming\nconventional single-agent reinforcement learning methods. This work advances\nthe integration of MARL into single-humanoid-robot control, offering new\ninsights into efficient locomotion strategies.", "AI": {"tldr": "提出了一种通过协作异构多智能体深度强化学习（MARL）优化单个人形机器人运动的新方法MASH。", "motivation": "现有方法通常采用单智能体强化学习或多智能体强化学习用于多机器人任务，而本研究提出将协作异构MARL用于单个人形机器人运动优化。", "method": "将机器人的每个肢体（腿和手臂）视为独立智能体，共享全局批评器进行协作学习。", "result": "实验表明MASH加速了训练收敛并提高了全身协作能力，优于传统单智能体强化学习方法。", "conclusion": "该工作推动了MARL在单个人形机器人控制中的应用，为高效运动策略提供了新见解。"}}
{"id": "2508.10634", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10634", "abs": "https://arxiv.org/abs/2508.10634", "authors": ["Mehdi Heydari Shahna", "Jouni Mattila"], "title": "Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots", "comment": null, "summary": "Deep neural networks (DNNs) can enable precise control while maintaining low\ncomputational costs by circumventing the need for dynamic modeling. However,\nthe deployment of such black-box approaches remains challenging for heavy-duty\nwheeled mobile robots (WMRs), which are subject to strict international\nstandards and prone to faults and disturbances. We designed a hierarchical\ncontrol policy for heavy-duty WMRs, monitored by two safety layers with\ndiffering levels of authority. To this end, a DNN policy was trained and\ndeployed as the primary control strategy, providing high-precision performance\nunder nominal operating conditions. When external disturbances arise and reach\na level of intensity such that the system performance falls below a predefined\nthreshold, a low-level safety layer intervenes by deactivating the primary\ncontrol policy and activating a model-free robust adaptive control (RAC)\npolicy. This transition enables the system to continue operating while ensuring\nstability by effectively managing the inherent trade-off between system\nrobustness and responsiveness. Regardless of the control policy in use, a\nhigh-level safety layer continuously monitors system performance during\noperation. It initiates a shutdown only when disturbances become sufficiently\nsevere such that compensation is no longer viable and continued operation would\njeopardize the system or its environment. The proposed synthesis of DNN and RAC\npolicy guarantees uniform exponential stability of the entire WMR system while\nadhering to safety standards to some extent. The effectiveness of the proposed\napproach was further validated through real-time experiments using a 6,000 kg\nWMR.", "AI": {"tldr": "论文提出了一种结合深度神经网络（DNN）和鲁棒自适应控制（RAC）的分层控制策略，用于重型轮式移动机器人（WMRs），以在保证安全性的同时实现高精度控制。", "motivation": "重型WMRs在严格国际标准下运行且易受干扰，传统黑盒DNN方法难以满足其安全需求。", "method": "设计了分层控制策略，包括DNN主控策略和两层安全监控层（低层RAC策略和高层停机保护）。", "result": "实验验证了该方法在6000 kg WMR上的有效性，实现了系统稳定性和安全性。", "conclusion": "DNN与RAC的结合为重型WMRs提供了一种兼顾精度与安全的控制方案。"}}
{"id": "2508.10780", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10780", "abs": "https://arxiv.org/abs/2508.10780", "authors": ["Alessandro Adami", "Aris Synodinos", "Matteo Iovino", "Ruggero Carli", "Pietro Falco"], "title": "Learning Task Execution Hierarchies for Redundant Robots", "comment": null, "summary": "Modern robotic systems, such as mobile manipulators, humanoids, and aerial\nrobots with arms, often possess high redundancy, enabling them to perform\nmultiple tasks simultaneously. Managing this redundancy is key to achieving\nreliable and flexible behavior. A widely used approach is the Stack of Tasks\n(SoT), which organizes control objectives by priority within a unified\nframework. However, traditional SoTs are manually designed by experts, limiting\ntheir adaptability and accessibility. This paper introduces a novel framework\nthat automatically learns both the hierarchy and parameters of a SoT from\nuser-defined objectives. By combining Reinforcement Learning and Genetic\nProgramming, the system discovers task priorities and control strategies\nwithout manual intervention. A cost function based on intuitive metrics such as\nprecision, safety, and execution time guides the learning process. We validate\nour method through simulations and experiments on the mobile-YuMi platform, a\ndual-arm mobile manipulator with high redundancy. Results show that the learned\nSoTs enable the robot to dynamically adapt to changing environments and inputs,\nbalancing competing objectives while maintaining robust task execution. This\napproach provides a general and user-friendly solution for redundancy\nmanagement in complex robots, advancing human-centered robot programming and\nreducing the need for expert design.", "AI": {"tldr": "论文提出了一种自动学习任务堆叠（SoT）层次结构和参数的新框架，结合强化学习和遗传编程，无需人工干预即可发现任务优先级和控制策略。", "motivation": "现代机器人系统的高冗余性需要灵活可靠的管理方法，传统SoT依赖专家手动设计，限制了适应性和可访问性。", "method": "通过结合强化学习和遗传编程，自动学习SoT的层次结构和参数，并使用基于精度、安全性和执行时间的成本函数指导学习。", "result": "在移动-YuMi平台上验证，结果显示学习到的SoT使机器人能动态适应环境和输入，平衡竞争目标并保持任务执行的鲁棒性。", "conclusion": "该方法为复杂机器人冗余管理提供了通用且用户友好的解决方案，减少了专家设计的依赖。"}}
{"id": "2508.10144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10144", "abs": "https://arxiv.org/abs/2508.10144", "authors": ["Xu Ma", "Jiajie Zhang", "Fujing Xie", "Sören Schwertfeger"], "title": "WiFi-based Global Localization in Large-Scale Environments Leveraging Structural Priors from osmAG", "comment": null, "summary": "Global localization is essential for autonomous robotics, especially in\nindoor environments where the GPS signal is denied. We propose a novel\nWiFi-based localization framework that leverages ubiquitous wireless\ninfrastructure and the OpenStreetMap Area Graph (osmAG) for large-scale indoor\nenvironments. Our approach integrates signal propagation modeling with osmAG's\ngeometric and topological priors. In the offline phase, an iterative\noptimization algorithm localizes WiFi Access Points (APs) by modeling wall\nattenuation, achieving a mean localization error of 3.79 m (35.3\\% improvement\nover trilateration). In the online phase, real-time robot localization uses the\naugmented osmAG map, yielding a mean error of 3.12 m in fingerprinted areas\n(8.77\\% improvement over KNN fingerprinting) and 3.83 m in non-fingerprinted\nareas (81.05\\% improvement). Comparison with a fingerprint-based method shows\nthat our approach is much more space efficient and achieves superior\nlocalization accuracy, especially for positions where no fingerprint data are\navailable. Validated across a complex 11,025 &m^2& multi-floor environment,\nthis framework offers a scalable, cost-effective solution for indoor robotic\nlocalization, solving the kidnapped robot problem. The code and dataset are\navailable at https://github.com/XuMa369/osmag-wifi-localization.", "AI": {"tldr": "提出了一种基于WiFi和OpenStreetMap Area Graph (osmAG)的室内定位框架，结合信号传播建模和几何拓扑先验，显著提高了定位精度和空间效率。", "motivation": "解决GPS信号缺失的室内环境中自主机器人的全局定位问题，利用现有WiFi基础设施和osmAG提供低成本、可扩展的解决方案。", "method": "离线阶段通过迭代优化算法定位WiFi接入点，建模墙壁衰减；在线阶段利用增强osmAG地图进行实时机器人定位。", "result": "离线阶段平均定位误差3.79米（比三边测量提升35.3%）；在线阶段指纹区平均误差3.12米（比KNN指纹提升8.77%），非指纹区3.83米（提升81.05%）。", "conclusion": "该框架在复杂多楼层环境中验证有效，解决了绑架机器人问题，提供了高精度、空间效率的室内定位方案。"}}
{"id": "2508.10867", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10867", "abs": "https://arxiv.org/abs/2508.10867", "authors": ["Yizhi Zhou", "Ziwei Kang", "Jiawei Xia", "Xuan Wang"], "title": "CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups", "comment": null, "summary": "Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial\nodometry (VIO) systems. Consistency is crucial for ensuring the estimation\naccuracy of a UWBaided VIO system. An inconsistent estimator can degrade\nlocalization performance, where the inconsistency primarily arises from two\nmain factors: (1) the estimator fails to preserve the correct system\nobservability, and (2) UWB anchor positions are assumed to be known, leading to\nimproper neglect of calibration uncertainty. In this paper, we propose a\nconsistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system\nbased on the Lie group. Our method incorporates the UWB anchor state into the\nsystem state, explicitly accounting for UWB calibration uncertainty and\nenabling the joint and consistent estimation of both robot and anchor states.\nFurthermore, observability consistency is ensured by leveraging the invariant\nerror properties of the Lie group. We analytically prove that the CVIRO\nalgorithm naturally maintains the system's correct unobservable subspace,\nthereby preserving estimation consistency. Extensive simulations and\nexperiments demonstrate that CVIRO achieves superior localization accuracy and\nconsistency compared to existing methods.", "AI": {"tldr": "提出了一种基于李群的视觉-惯性-测距里程计系统（CVIRO），通过联合估计机器人状态和UWB锚点状态，解决了UWB辅助VIO系统中的不一致性问题。", "motivation": "UWB辅助VIO系统中，不一致性主要由系统可观测性未正确保持和UWB锚点位置假设已知导致校准不确定性被忽略引起，影响定位性能。", "method": "提出CVIRO系统，将UWB锚点状态纳入系统状态，显式考虑UWB校准不确定性，并利用李群的不变误差特性确保可观测一致性。", "result": "通过分析和实验证明，CVIRO在定位精度和一致性上优于现有方法。", "conclusion": "CVIRO通过联合估计和可观测性保持，显著提升了UWB辅助VIO系统的性能。"}}
{"id": "2508.10269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10269", "abs": "https://arxiv.org/abs/2508.10269", "authors": ["Kejun Li", "Jeeseop Kim", "Maxime Brunet", "Marine Pétriaux", "Yisong Yue", "Aaron D. Ames"], "title": "Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis", "comment": "8 pages; 8 figures", "summary": "Robust bipedal locomotion in exoskeletons requires the ability to dynamically\nreact to changes in the environment in real time. This paper introduces the\nhybrid data-driven predictive control (HDDPC) framework, an extension of the\ndata-enabled predictive control, that addresses these challenges by\nsimultaneously planning foot contact schedules and continuous domain\ntrajectories. The proposed framework utilizes a Hankel matrix-based\nrepresentation to model system dynamics, incorporating step-to-step (S2S)\ntransitions to enhance adaptability in dynamic environments. By integrating\ncontact scheduling with trajectory planning, the framework offers an efficient,\nunified solution for locomotion motion synthesis that enables robust and\nreactive walking through online replanning. We validate the approach on the\nAtalante exoskeleton, demonstrating improved robustness and adaptability.", "AI": {"tldr": "论文提出了一种混合数据驱动预测控制（HDDPC）框架，用于增强外骨骼机器人在动态环境中的稳健行走能力。", "motivation": "解决外骨骼机器人在实时动态环境中行走时需快速适应环境变化的问题。", "method": "采用Hankel矩阵建模系统动力学，结合步间（S2S）过渡，同时规划脚部接触时间表和连续域轨迹。", "result": "在Atalante外骨骼上验证了该方法，显示出更强的稳健性和适应性。", "conclusion": "HDDPC框架为外骨骼机器人提供了一种高效、统一的运动合成解决方案，支持在线重新规划以实现稳健行走。"}}
{"id": "2508.10333", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10333", "abs": "https://arxiv.org/abs/2508.10333", "authors": ["Wenxuan Song", "Ziyang Zhou", "Han Zhao", "Jiayi Chen", "Pengxiang Ding", "Haodong Yan", "Yuxin Huang", "Feilong Tang", "Donglin Wang", "Haoang Li"], "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic\nagents to integrate multimodal understanding with action execution. However,\nour empirical analysis reveals that current VLAs struggle to allocate visual\nattention to target regions. Instead, visual attention is always dispersed. To\nguide the visual attention grounding on the correct target, we propose\nReconVLA, a reconstructive VLA model with an implicit grounding paradigm.\nConditioned on the model's visual outputs, a diffusion transformer aims to\nreconstruct the gaze region of the image, which corresponds to the target\nmanipulated objects. This process prompts the VLA model to learn fine-grained\nrepresentations and accurately allocate visual attention, thus effectively\nleveraging task-specific visual information and conducting precise\nmanipulation. Moreover, we curate a large-scale pretraining dataset comprising\nover 100k trajectories and 2 million data samples from open-source robotic\ndatasets, further boosting the model's generalization in visual reconstruction.\nExtensive experiments in simulation and the real world demonstrate the\nsuperiority of our implicit grounding method, showcasing its capabilities of\nprecise manipulation and generalization. Our project page is\nhttps://zionchow.github.io/ReconVLA/.", "AI": {"tldr": "ReconVLA是一种通过重构视觉注意力区域来优化视觉-语言-动作模型的方法，解决了当前模型注意力分散的问题。", "motivation": "当前视觉-语言-动作模型在视觉注意力分配上表现不佳，注意力分散，无法聚焦目标区域。", "method": "提出ReconVLA，利用扩散变换器重构图像中的目标区域（注视区域），以引导模型学习细粒度表示和准确分配视觉注意力。", "result": "实验表明，ReconVLA在模拟和现实环境中均表现出色，能够实现精确操作和泛化能力。", "conclusion": "ReconVLA通过隐式注意力引导机制，显著提升了视觉-语言-动作模型的性能。"}}
{"id": "2508.10363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10363", "abs": "https://arxiv.org/abs/2508.10363", "authors": ["Donipolo Ghimire", "Aamodh Suresh", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "BEASST: Behavioral Entropic Gradient based Adaptive Source Seeking for Mobile Robots", "comment": null, "summary": "This paper presents BEASST (Behavioral Entropic Gradient-based Adaptive\nSource Seeking for Mobile Robots), a novel framework for robotic source seeking\nin complex, unknown environments. Our approach enables mobile robots to\nefficiently balance exploration and exploitation by modeling normalized signal\nstrength as a surrogate probability of source location. Building on Behavioral\nEntropy(BE) with Prelec's probability weighting function, we define an\nobjective function that adapts robot behavior from risk-averse to risk-seeking\nbased on signal reliability and mission urgency. The framework provides\ntheoretical convergence guarantees under unimodal signal assumptions and\npractical stability under bounded disturbances. Experimental validation across\nDARPA SubT and multi-room scenarios demonstrates that BEASST consistently\noutperforms state-of-the-art methods, achieving 15% reduction in path length\nand 20% faster source localization through intelligent uncertainty-driven\nnavigation that dynamically transitions between aggressive pursuit and cautious\nexploration.", "AI": {"tldr": "BEASST是一种新型机器人源搜索框架，通过行为熵和概率加权函数动态调整机器人行为，在复杂环境中高效平衡探索与利用。", "motivation": "解决机器人在未知复杂环境中高效定位源的问题，通过动态行为调整提升性能。", "method": "利用行为熵和Prelec概率加权函数定义目标函数，根据信号可靠性和任务紧迫性调整机器人行为。", "result": "实验显示BEASST在路径长度减少15%和源定位速度提升20%上优于现有方法。", "conclusion": "BEASST框架在理论和实践中均表现出色，适用于复杂环境中的源搜索任务。"}}
{"id": "2508.10371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10371", "abs": "https://arxiv.org/abs/2508.10371", "authors": ["Wenqi Zheng", "Yutaka Arakawa"], "title": "Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning", "comment": null, "summary": "Reinforcement learning in large reasoning models enables learning from\nfeedback on their outputs, making it particularly valuable in scenarios where\nfine-tuning data is limited. However, its application in multi-modal human\nactivity recognition (HAR) domains remains largely underexplored. Our work\nextends reinforcement learning to the human activity recognition domain with\nmultimodal large language models. By incorporating visual reinforcement\nlearning in the training process, the model's generalization ability on\nfew-shot recognition can be greatly improved. Additionally, visual\nreinforcement learning can enhance the model's reasoning ability and enable\nexplainable analysis in the inference stage. We name our few-shot human\nactivity recognition method with visual reinforcement learning FAVOR.\nSpecifically, our approach first utilizes a multimodal large language model\n(MLLM) to generate multiple candidate responses for the human activity image,\neach containing reasoning traces and final answers. These responses are then\nevaluated using reward functions, and the MLLM model is subsequently optimized\nusing the Group Relative Policy Optimization (GRPO) algorithm. In this way, the\nMLLM model can be adapted to human activity recognition with only a few\nsamples. Extensive experiments on four human activity recognition datasets and\nfive different settings demonstrate the superiority of the proposed method.", "AI": {"tldr": "论文提出了一种名为FAVOR的方法，通过视觉强化学习提升多模态大语言模型在少样本人类活动识别中的泛化能力和推理能力。", "motivation": "在少样本数据场景下，强化学习在多模态人类活动识别（HAR）中的应用尚未充分探索，本研究旨在填补这一空白。", "method": "使用多模态大语言模型（MLLM）生成候选响应，结合视觉强化学习和GRPO算法优化模型。", "result": "在四个HAR数据集和五种不同设置下的实验证明了该方法的优越性。", "conclusion": "FAVOR方法显著提升了少样本人类活动识别的性能，并增强了模型的可解释性。"}}
{"id": "2508.10378", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10378", "abs": "https://arxiv.org/abs/2508.10378", "authors": ["Yu Chen", "Shu Miao", "Chunyu Wu", "Jingsong Mu", "Bo OuYang", "Xiang Li"], "title": "A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons", "comment": null, "summary": "Upper-limb exoskeletons are primarily designed to provide assistive support\nby accurately interpreting and responding to human intentions. In home-care\nscenarios, exoskeletons are expected to adapt their assistive configurations\nbased on the semantic information of the task, adjusting appropriately in\naccordance with the nature of the object being manipulated. However, existing\nsolutions often lack the ability to understand task semantics or\ncollaboratively plan actions with the user, limiting their generalizability. To\naddress this challenge, this paper introduces a semantic-aware framework that\nintegrates large language models into the task planning framework, enabling the\ndelivery of safe and intent-integrative assistance. The proposed approach\nbegins with the exoskeleton operating in transparent mode to capture the\nwearer's intent during object grasping. Once semantic information is extracted\nfrom the task description, the system automatically configures appropriate\nassistive parameters. In addition, a diffusion-based anomaly detector is used\nto continuously monitor the state of human-robot interaction and trigger\nreal-time replanning in response to detected anomalies. During task execution,\nonline trajectory refinement and impedance control are used to ensure safety\nand regulate human-robot interaction. Experimental results demonstrate that the\nproposed method effectively aligns with the wearer's cognition, adapts to\nsemantically varying tasks, and responds reliably to anomalies.", "AI": {"tldr": "论文提出了一种语义感知框架，将大语言模型集成到任务规划中，以提升上肢外骨骼的辅助能力，使其能理解任务语义并实时调整。", "motivation": "现有上肢外骨骼在家庭护理场景中缺乏对任务语义的理解和协作规划能力，限制了其通用性。", "method": "通过透明模式捕捉用户意图，结合大语言模型提取语义信息，自动配置辅助参数，并使用扩散异常检测器实时监控和调整。", "result": "实验表明，该方法能有效适应用户认知、语义变化任务，并对异常做出可靠响应。", "conclusion": "提出的框架显著提升了外骨骼的语义理解和协作能力，为家庭护理提供了更安全的辅助支持。"}}
{"id": "2508.10398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10398", "abs": "https://arxiv.org/abs/2508.10398", "authors": ["Wei Gao", "Jie Zhang", "Mingle Zhao", "Zhiyuan Zhang", "Shu Kong", "Maani Ghaffari", "Dezhen Song", "Cheng-Zhong Xu", "Hui Kong"], "title": "Super LiDAR Reflectance for Robotic Perception", "comment": null, "summary": "Conventionally, human intuition often defines vision as a modality of passive\noptical sensing, while active optical sensing is typically regarded as\nmeasuring rather than the default modality of vision. However, the situation\nnow changes: sensor technologies and data-driven paradigms empower active\noptical sensing to redefine the boundaries of vision, ushering in a new era of\nactive vision. Light Detection and Ranging (LiDAR) sensors capture reflectance\nfrom object surfaces, which remains invariant under varying illumination\nconditions, showcasing significant potential in robotic perception tasks such\nas detection, recognition, segmentation, and Simultaneous Localization and\nMapping (SLAM). These applications often rely on dense sensing capabilities,\ntypically achieved by high-resolution, expensive LiDAR sensors. A key challenge\nwith low-cost LiDARs lies in the sparsity of scan data, which limits their\nbroader application. To address this limitation, this work introduces an\ninnovative framework for generating dense LiDAR reflectance images from sparse\ndata, leveraging the unique attributes of non-repeating scanning LiDAR\n(NRS-LiDAR). We tackle critical challenges, including reflectance calibration\nand the transition from static to dynamic scene domains, facilitating the\nreconstruction of dense reflectance images in real-world settings. The key\ncontributions of this work include a comprehensive dataset for LiDAR\nreflectance image densification, a densification network tailored for\nNRS-LiDAR, and diverse applications such as loop closure and traffic lane\ndetection using the generated dense reflectance images.", "AI": {"tldr": "论文提出了一种从稀疏LiDAR数据生成密集反射图像的创新框架，解决了低成本LiDAR数据稀疏性的问题，并展示了其在机器人感知任务中的应用。", "motivation": "传统上，主动光学传感被视为测量而非视觉的默认模态，但现代技术和数据驱动方法使其能够重新定义视觉边界。低成本LiDAR的稀疏数据限制了其广泛应用，因此需要一种方法生成密集反射图像。", "method": "论文提出了一种创新框架，利用非重复扫描LiDAR（NRS-LiDAR）的特性，解决反射校准和从静态到动态场景转换的挑战，实现密集反射图像的重建。", "result": "研究贡献包括一个LiDAR反射图像密集化的数据集、专为NRS-LiDAR设计的密集化网络，以及生成密集反射图像在环路闭合和交通车道检测等应用中的表现。", "conclusion": "该工作通过生成密集LiDAR反射图像，扩展了低成本LiDAR的应用范围，为机器人感知任务提供了新的可能性。"}}
{"id": "2508.10399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10399", "abs": "https://arxiv.org/abs/2508.10399", "authors": ["Wenlong Liang", "Rui Zhou", "Yang Ma", "Bing Zhang", "Songlin Li", "Yijia Liao", "Ping Kuang"], "title": "Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning", "comment": null, "summary": "Embodied AI aims to develop intelligent systems with physical forms capable\nof perceiving, decision-making, acting, and learning in real-world\nenvironments, providing a promising way to Artificial General Intelligence\n(AGI). Despite decades of explorations, it remains challenging for embodied\nagents to achieve human-level intelligence for general-purpose tasks in open\ndynamic environments. Recent breakthroughs in large models have revolutionized\nembodied AI by enhancing perception, interaction, planning and learning. In\nthis article, we provide a comprehensive survey on large model empowered\nembodied AI, focusing on autonomous decision-making and embodied learning. We\ninvestigate both hierarchical and end-to-end decision-making paradigms,\ndetailing how large models enhance high-level planning, low-level execution,\nand feedback for hierarchical decision-making, and how large models enhance\nVision-Language-Action (VLA) models for end-to-end decision making. For\nembodied learning, we introduce mainstream learning methodologies, elaborating\non how large models enhance imitation learning and reinforcement learning\nin-depth. For the first time, we integrate world models into the survey of\nembodied AI, presenting their design methods and critical roles in enhancing\ndecision-making and learning. Though solid advances have been achieved,\nchallenges still exist, which are discussed at the end of this survey,\npotentially as the further research directions.", "AI": {"tldr": "本文综述了大模型赋能的具身AI，重点探讨了自主决策与具身学习，并首次将世界模型纳入具身AI的研究框架。", "motivation": "具身AI是实现通用人工智能（AGI）的重要途径，但在开放动态环境中实现人类水平智能仍具挑战性。大模型的突破为具身AI带来了革命性进展。", "method": "分析了分层与端到端决策范式，以及大模型如何增强高层规划、低层执行和反馈；探讨了大模型如何提升模仿学习与强化学习。", "result": "大模型显著提升了具身AI的感知、交互、规划和学习能力，世界模型在决策与学习中发挥关键作用。", "conclusion": "尽管取得进展，仍存在挑战，这些挑战可能成为未来研究方向。"}}
{"id": "2508.10416", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10416", "abs": "https://arxiv.org/abs/2508.10416", "authors": ["Zhuoyuan Yu", "Yuxing Long", "Zihan Yang", "Chengyan Zeng", "Hongwei Fan", "Jiyao Zhang", "Hao Dong"], "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model", "comment": null, "summary": "Existing vision-and-language navigation models often deviate from the correct\ntrajectory when executing instructions. However, these models lack effective\nerror correction capability, hindering their recovery from errors. To address\nthis challenge, we propose Self-correction Flywheel, a novel post-training\nparadigm. Instead of considering the model's error trajectories on the training\nset as a drawback, our paradigm emphasizes their significance as a valuable\ndata source. We have developed a method to identify deviations in these error\ntrajectories and devised innovative techniques to automatically generate\nself-correction data for perception and action. These self-correction data\nserve as fuel to power the model's continued training. The brilliance of our\nparadigm is revealed when we re-evaluate the model on the training set,\nuncovering new error trajectories. At this time, the self-correction flywheel\nbegins to spin. Through multiple flywheel iterations, we progressively enhance\nour monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE\nand RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success\nrates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%\nand 16.4%. Real robot tests in various indoor and outdoor environments\ndemonstrate \\method's superior capability of error correction, dynamic obstacle\navoidance, and long instruction following.", "AI": {"tldr": "提出了一种名为Self-correction Flywheel的后训练范式，通过利用模型的错误轨迹生成自校正数据，逐步提升视觉语言导航模型的性能。", "motivation": "现有视觉语言导航模型在执行指令时容易偏离正确轨迹，且缺乏有效的错误校正能力。", "method": "通过识别错误轨迹中的偏差，自动生成感知和动作的自校正数据，并利用这些数据持续训练模型。", "result": "在R2R-CE和RxR-CE基准测试中，CorrectNav模型分别达到65.1%和69.3%的成功率，显著优于之前的最佳模型。", "conclusion": "Self-correction Flywheel范式显著提升了模型的错误校正能力和导航性能，适用于复杂环境。"}}
{"id": "2508.10497", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10497", "abs": "https://arxiv.org/abs/2508.10497", "authors": ["Abdullah Farrukh", "Achim Wagner", "Martin Ruskowski"], "title": "Enabling Generic Robot Skill Implementation Using Object Oriented Programming", "comment": "34th International Conference on Robotics in Alpe-Adria-Danube Region\n  (RAAD 2025)", "summary": "Developing robotic algorithms and integrating a robotic subsystem into a\nlarger system can be a difficult task. Particularly in small and medium-sized\nenterprises (SMEs) where robotics expertise is lacking, implementing,\nmaintaining and developing robotic systems can be a challenge. As a result,\nmany companies rely on external expertise through system integrators, which, in\nsome cases, can lead to vendor lock-in and external dependency. In the academic\nresearch on intelligent manufacturing systems, robots play a critical role in\nthe design of robust autonomous systems. Similar challenges are faced by\nresearchers who want to use robotic systems as a component in a larger smart\nsystem, without having to deal with the complexity and vastness of the robot\ninterfaces in detail. In this paper, we propose a software framework that\nreduces the effort required to deploy a working robotic system. The focus is\nsolely on providing a concept for simplifying the different interfaces of a\nmodern robot system and using an abstraction layer for different manufacturers\nand models. The Python programming language is used to implement a prototype of\nthe concept. The target system is a bin-picking cell containing a Yaskawa\nMotoman GP4.", "AI": {"tldr": "提出了一种简化机器人系统接口的软件框架，旨在降低部署机器人系统的复杂性，特别针对中小企业和研究人员。", "motivation": "中小企业和研究人员在缺乏机器人专业知识的情况下，难以实现和维护机器人系统，且依赖外部专家可能导致供应商锁定。", "method": "使用Python实现一个原型框架，通过抽象层统一不同制造商和型号的机器人接口。", "result": "开发了一个针对Yaskawa Motoman GP4的拣选单元的原型框架。", "conclusion": "该框架简化了机器人系统的部署，减少了对外部专家的依赖。"}}
{"id": "2508.10511", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10511", "abs": "https://arxiv.org/abs/2508.10511", "authors": ["Andrea Rosasco", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection", "comment": "9th Conference on Robot Learning (CoRL 2025), Seoul, Korea", "summary": "Learning robot policies that capture multimodality in the training data has\nbeen a long-standing open challenge for behavior cloning. Recent approaches\ntackle the problem by modeling the conditional action distribution with\ngenerative models. One of these approaches is Diffusion Policy, which relies on\na diffusion model to denoise random points into robot action trajectories.\nWhile achieving state-of-the-art performance, it has two main drawbacks that\nmay lead the robot out of the data distribution during policy execution. First,\nthe stochasticity of the denoising process can highly impact on the quality of\ngenerated trajectory of actions. Second, being a supervised learning approach,\nit can learn data outliers from the dataset used for training. Recent work\nfocuses on mitigating these limitations by combining Diffusion Policy either\nwith large-scale training or with classical behavior cloning algorithms.\nInstead, we propose KDPE, a Kernel Density Estimation-based strategy that\nfilters out potentially harmful trajectories output of Diffusion Policy while\nkeeping a low test-time computational overhead. For Kernel Density Estimation,\nwe propose a manifold-aware kernel to model a probability density function for\nactions composed of end-effector Cartesian position, orientation, and gripper\nstate. KDPE overall achieves better performance than Diffusion Policy on\nsimulated single-arm tasks and real robot experiments.\n  Additional material and code are available on our project page\nhttps://hsp-iit.github.io/KDPE/.", "AI": {"tldr": "KDPE是一种基于核密度估计的策略，用于过滤Diffusion Policy生成的有害轨迹，同时保持低计算开销，在模拟和真实机器人实验中表现优于Diffusion Policy。", "motivation": "解决Diffusion Policy在机器人行为克隆中的两个主要缺点：去噪过程的随机性影响动作轨迹质量，以及监督学习可能学习到数据异常值。", "method": "提出KDPE，利用流形感知核的核密度估计方法，对动作（包括末端执行器位置、方向和夹持器状态）建模概率密度函数，过滤有害轨迹。", "result": "KDPE在模拟单臂任务和真实机器人实验中表现优于Diffusion Policy。", "conclusion": "KDPE通过核密度估计有效提升了机器人策略的鲁棒性和性能，同时保持低计算开销。"}}
{"id": "2508.10538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10538", "abs": "https://arxiv.org/abs/2508.10538", "authors": ["Xin Liu", "Bida Ma", "Chenkun Qi", "Yan Ding", "Zhaxizhuoma", "Guorong Zhang", "Pengan Chen", "Kehui Liu", "Zhongjie Jia", "Chuyue Guan", "Yule Mo", "Jiaqi Liu", "Feng Gao", "Jiangwei Zhong", "Bin Zhao", "Xuelong Li"], "title": "MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm", "comment": null, "summary": "Whole-body loco-manipulation for quadruped robots with arm remains a\nchallenging problem, particularly in achieving multi-task control. To address\nthis, we propose MLM, a reinforcement learning framework driven by both\nreal-world and simulation data. It enables a six-DoF robotic arm--equipped\nquadruped robot to perform whole-body loco-manipulation for multiple tasks\nautonomously or under human teleoperation. To address the problem of balancing\nmultiple tasks during the learning of loco-manipulation, we introduce a\ntrajectory library with an adaptive, curriculum-based sampling mechanism. This\napproach allows the policy to efficiently leverage real-world collected\ntrajectories for learning multi-task loco-manipulation. To address deployment\nscenarios with only historical observations and to enhance the performance of\npolicy execution across tasks with different spatial ranges, we propose a\nTrajectory-Velocity Prediction policy network. It predicts unobservable future\ntrajectories and velocities. By leveraging extensive simulation data and\ncurriculum-based rewards, our controller achieves whole-body behaviors in\nsimulation and zero-shot transfer to real-world deployment. Ablation studies in\nsimulation verify the necessity and effectiveness of our approach, while\nreal-world experiments on the Go2 robot with an Airbot robotic arm demonstrate\nthe policy's good performance in multi-task execution.", "AI": {"tldr": "提出了一种名为MLM的强化学习框架，结合真实世界和仿真数据，使六自由度机械臂四足机器人能够自主或通过远程操作完成多任务全身运动控制。", "motivation": "解决四足机器人配备机械臂时多任务全身运动控制的挑战。", "method": "引入轨迹库和自适应课程采样机制，提出轨迹-速度预测策略网络，结合仿真数据和课程奖励。", "result": "仿真和真实世界实验验证了方法的有效性和零样本迁移能力。", "conclusion": "MLM框架在多任务执行中表现出色，为机器人全身运动控制提供了有效解决方案。"}}
{"id": "2508.10603", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10603", "abs": "https://arxiv.org/abs/2508.10603", "authors": ["Agnes Axelsson", "Merle Reimann", "Ronald Cumbal", "Hannah Pelikan", "Divesh Lala"], "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality", "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages", "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.", "AI": {"tldr": "论文提出使用民族志小故事（vignettes）来揭示人机交互（HRI）中的失败案例，弥补现有研究的不足。", "motivation": "尽管LLMs提升了人机交互质量，但与人人交互相比仍存在不足，且失败案例因情境而异，难以泛化。", "method": "提出使用民族志小故事方法，基于个人经验撰写案例，突出多学科视角下的失败。", "result": "小故事能有效揭示罕见失败案例，促进透明度和意外行为的记录。", "conclusion": "建议将小故事作为现有交互评估方法的补充工具。"}}
{"id": "2508.10686", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.10686", "abs": "https://arxiv.org/abs/2508.10686", "authors": ["Carla Wehner", "Finn Schubert", "Heiko Hellkamp", "Julius Hahnewald", "Kilian Scheafer", "Muhammad Bilal Khan", "Oliver Gutfleisch"], "title": "An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)", "comment": null, "summary": "Soft robots, particularly magnetic soft robots, require specialized\nsimulation tools to accurately model their deformation under external magnetic\nfields. However, existing platforms often lack dedicated support for magnetic\nmaterials, making them difficult to use for researchers at different expertise\nlevels. This work introduces an open-source, user-friendly simulation interface\nusing the Simulation Open Framework Architecture (SOFA), specifically designed\nto model magnetic soft robots. The tool enables users to define material\nproperties, apply magnetic fields, and observe resulting deformations in real\ntime. By integrating intuitive controls and stress analysis capabilities, it\naims to bridge the gap between theoretical modeling and practical design. Four\nbenchmark models - a beam, three- and four-finger grippers, and a butterfly -\ndemonstrate its functionality. The software's ease of use makes it accessible\nto both beginners and advanced researchers. Future improvements will refine\naccuracy through experimental validation and comparison with industry-standard\nfinite element solvers, ensuring realistic and predictive simulations of\nmagnetic soft robots.", "AI": {"tldr": "本文介绍了一个基于SOFA的开源、用户友好的仿真工具，专门用于模拟磁性软体机器人，填补了现有平台对磁性材料支持的不足。", "motivation": "现有仿真平台缺乏对磁性材料的专门支持，使得不同专业水平的研究人员难以使用。", "method": "开发了一个基于SOFA的开源仿真界面，支持定义材料属性、施加磁场并实时观察变形，集成了直观控制和应力分析功能。", "result": "通过四个基准模型（梁、三指和四指夹持器、蝴蝶）验证了工具的功能，其易用性适合初学者和高级研究人员。", "conclusion": "未来将通过实验验证和与行业标准有限元求解器的对比，进一步提高仿真精度，确保预测的准确性。"}}
{"id": "2508.10689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10689", "abs": "https://arxiv.org/abs/2508.10689", "authors": ["Matteo Luperto", "Valerii Stakanov", "Giacomo Boracchi", "Nicola Basilico", "Francesco Amigoni"], "title": "Biasing Frontier-Based Exploration with Saliency Areas", "comment": "Accepted at the European Confrence on Mobile Robots (ECMR) 2025", "summary": "Autonomous exploration is a widely studied problem where a robot\nincrementally builds a map of a previously unknown environment. The robot\nselects the next locations to reach using an exploration strategy. To do so,\nthe robot has to balance between competing objectives, like exploring the\nentirety of the environment, while being as fast as possible. Most exploration\nstrategies try to maximise the explored area to speed up exploration; however,\nthey do not consider that parts of the environment are more important than\nothers, as they lead to the discovery of large unknown areas. We propose a\nmethod that identifies \\emph{saliency areas} as those areas that are of high\ninterest for exploration, by using saliency maps obtained from a neural network\nthat, given the current map, implements a termination criterion to estimate\nwhether the environment can be considered fully-explored or not. We use\nsaliency areas to bias some widely used exploration strategies, showing, with\nan extensive experimental campaign, that this knowledge can significantly\ninfluence the behavior of the robot during exploration.", "AI": {"tldr": "论文提出了一种基于显著性区域的方法，用于优化机器人在未知环境中的自主探索策略。", "motivation": "现有探索策略通常只关注最大化探索面积，而忽略了环境中的某些区域可能对探索更为重要。", "method": "使用神经网络生成的显著性地图识别关键区域（显著性区域），并将其用于优化现有探索策略。", "result": "实验表明，该方法能显著影响机器人的探索行为，提高探索效率。", "conclusion": "通过引入显著性区域，可以更高效地指导机器人在未知环境中的探索。"}}
{"id": "2508.10798", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10798", "abs": "https://arxiv.org/abs/2508.10798", "authors": ["Troi Williams"], "title": "The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems", "comment": "4 pages, 4 figures, accepted to the Workshop on Public Trust in\n  Autonomous Systems at the 2025 IEEE International Conference on Robotics &\n  Automation", "summary": "Future autonomous systems promise significant societal benefits, yet their\ndeployment raises concerns about safety and trustworthiness. A key concern is\nassuring the reliability of robot perception, as perception seeds safe\ndecision-making. Failures in perception are often due to complex yet common\nenvironmental factors and can lead to accidents that erode public trust. To\naddress this concern, we introduce the SET (Self, Environment, and Target)\nPerceptual Factors Framework. We designed the framework to systematically\nanalyze how factors such as weather, occlusion, or sensor limitations\nnegatively impact perception. To achieve this, the framework employs SET State\nTrees to categorize where such factors originate and SET Factor Trees to model\nhow these sources and factors impact perceptual tasks like object detection or\npose estimation. Next, we develop Perceptual Factor Models using both trees to\nquantify the uncertainty for a given task. Our framework aims to promote\nrigorous safety assurances and cultivate greater public understanding and trust\nin autonomous systems by offering a transparent and standardized method for\nidentifying, modeling, and communicating perceptual risks.", "AI": {"tldr": "论文提出SET感知因素框架，通过系统分析环境、目标和自身因素对感知的影响，提升自动驾驶系统的安全性和可信度。", "motivation": "自动驾驶系统的部署引发了对安全性和可信度的担忧，尤其是感知可靠性问题。感知失败常由复杂环境因素引起，可能导致事故并削弱公众信任。", "method": "引入SET（自身、环境、目标）感知因素框架，利用SET状态树和因素树分类建模感知任务中的不确定性，并开发感知因素模型量化风险。", "result": "框架提供了一种透明、标准化的方法，用于识别、建模和传达感知风险，从而增强安全保证和公众信任。", "conclusion": "SET框架为自动驾驶系统的感知可靠性提供了系统化的分析工具，有助于提升安全性和公众信任。"}}
{"id": "2508.10828", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10828", "abs": "https://arxiv.org/abs/2508.10828", "authors": ["Henry Powell", "Guy Laban", "Emily S. Cross"], "title": "A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots", "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Subjective self-disclosure is an important feature of human social\ninteraction. While much has been done in the social and behavioural literature\nto characterise the features and consequences of subjective self-disclosure,\nlittle work has been done thus far to develop computational systems that are\nable to accurately model it. Even less work has been done that attempts to\nmodel specifically how human interactants self-disclose with robotic partners.\nIt is becoming more pressing as we require social robots to work in conjunction\nwith and establish relationships with humans in various social settings. In\nthis paper, our aim is to develop a custom multimodal attention network based\non models from the emotion recognition literature, training this model on a\nlarge self-collected self-disclosure video corpus, and constructing a new loss\nfunction, the scale preserving cross entropy loss, that improves upon both\nclassification and regression versions of this problem. Our results show that\nthe best performing model, trained with our novel loss function, achieves an F1\nscore of 0.83, an improvement of 0.48 from the best baseline model. This result\nmakes significant headway in the aim of allowing social robots to pick up on an\ninteraction partner's self-disclosures, an ability that will be essential in\nsocial robots with social cognition.", "AI": {"tldr": "论文提出了一种基于多模态注意力网络的模型，用于模拟人类与机器人互动中的主观自我披露行为，并通过新的损失函数显著提升了性能。", "motivation": "随着社交机器人在多种社交场景中的应用需求增加，准确模拟人类与机器人互动中的自我披露行为变得尤为重要。", "method": "开发了一种基于情感识别文献的多模态注意力网络，使用自收集的视频语料库进行训练，并提出了尺度保持交叉熵损失函数。", "result": "最佳模型在F1分数上达到0.83，比基线模型提升了0.48。", "conclusion": "该研究为社交机器人感知互动伙伴的自我披露行为提供了重要进展，对社交机器人的社会认知能力至关重要。"}}
{"id": "2508.10872", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10872", "abs": "https://arxiv.org/abs/2508.10872", "authors": ["Anantha Narayanan", "Battu Bhanu Teja", "Pruthwik Mishra"], "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning", "comment": "8 pages, 6 figures, 5 tables", "summary": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning.", "AI": {"tldr": "论文提出了一种基于强化学习的框架（A2C算法），用于优化卫星轨道参数以实现精确的地面覆盖，并在计算效率和收敛速度上优于PPO算法。", "motivation": "随着近地轨道（LEO）的日益拥挤，卫星部署和运行面临更高的碰撞风险，需要更高效的轨道规划方法。", "method": "使用A2C算法在自定义的OpenAI Gymnasium环境中模拟轨道动力学，通过调整五个轨道参数优化地面覆盖。", "result": "A2C算法在累积奖励和收敛速度上显著优于PPO算法（奖励提高5.8倍，收敛速度快31.5倍），并适用于实时任务规划。", "conclusion": "强化学习为LEO任务规划提供了一种计算高效且可扩展的解决方案，验证了A2C算法在连续轨道控制中的优越性。"}}
