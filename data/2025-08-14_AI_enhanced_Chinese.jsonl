{"id": "2508.09304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09304", "abs": "https://arxiv.org/abs/2508.09304", "authors": ["Kelen C. Teixeira Vivaldini", "Robert Pěnička", "Martin Saska"], "title": "Decision-Making-Based Path Planning for Autonomous UAVs: A Survey", "comment": null, "summary": "One of the most critical features for the successful operation of autonomous\nUAVs is the ability to make decisions based on the information acquired from\ntheir surroundings. Each UAV must be able to make decisions during the flight\nin order to deal with uncertainties in its system and the environment, and to\nfurther act upon the information being received. Such decisions influence the\nfuture behavior of the UAV, which is expressed as the path plan. Thus,\ndecision-making in path planning is an enabling technique for deploying\nautonomous UAVs in real-world applications. This survey provides an overview of\nexisting studies that use aspects of decision-making in path planning,\npresenting the research strands for Exploration Path Planning and Informative\nPath Planning, and focusing on characteristics of how data have been modeled\nand understood. Finally, we highlight the existing challenges for relevant\ntopics in this field.", "AI": {"tldr": "综述探讨了自主无人机路径规划中的决策方法，重点分析了探索路径规划和信息路径规划的研究方向及数据建模特点，并指出了该领域的挑战。", "motivation": "自主无人机需基于环境信息做出决策以应对系统和环境的不确定性，决策能力是其成功运行的关键。", "method": "综述分析了现有研究中决策在路径规划中的应用，包括探索路径规划和信息路径规划，并关注数据建模和理解的特点。", "result": "总结了决策在无人机路径规划中的研究现状，提出了相关研究方向。", "conclusion": "强调了该领域面临的挑战，为未来研究提供了方向。"}}
{"id": "2508.09346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09346", "abs": "https://arxiv.org/abs/2508.09346", "authors": ["Zhenjiang Mao", "Mrinall Eashaan Umasudhan", "Ivan Ruchkin"], "title": "How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy", "comment": null, "summary": "Autonomous robots that rely on deep neural network controllers pose critical\nchallenges for safety prediction, especially under partial observability and\ndistribution shift. Traditional model-based verification techniques are limited\nin scalability and require access to low-dimensional state models, while\nmodel-free methods often lack reliability guarantees. This paper addresses\nthese limitations by introducing a framework for calibrated safety prediction\nin end-to-end vision-controlled systems, where neither the state-transition\nmodel nor the observation model is accessible. Building on the foundation of\nworld models, we leverage variational autoencoders and recurrent predictors to\nforecast future latent trajectories from raw image sequences and estimate the\nprobability of satisfying safety properties. We distinguish between monolithic\nand composite prediction pipelines and introduce a calibration mechanism to\nquantify prediction confidence. In long-horizon predictions from\nhigh-dimensional observations, the forecasted inputs to the safety evaluator\ncan deviate significantly from the training distribution due to compounding\nprediction errors and changing environmental conditions, leading to\nmiscalibrated risk estimates. To address this, we incorporate unsupervised\ndomain adaptation to ensure robustness of safety evaluation under distribution\nshift in predictions without requiring manual labels. Our formulation provides\ntheoretical calibration guarantees and supports practical evaluation across\nlong prediction horizons. Experimental results on three benchmarks show that\nour UDA-equipped evaluators maintain high accuracy and substantially lower\nfalse positive rates under distribution shift. Similarly, world model-based\ncomposite predictors outperform their monolithic counterparts on long-horizon\ntasks, and our conformal calibration provides reliable statistical bounds.", "AI": {"tldr": "该论文提出了一种用于端到端视觉控制系统的校准安全预测框架，解决了传统方法在可扩展性和可靠性上的不足。通过结合变分自编码器和循环预测器，预测未来潜在轨迹并评估安全概率，同时引入校准机制和领域适应技术以提高分布偏移下的鲁棒性。", "motivation": "自主机器人依赖深度神经网络控制器时，在部分可观测性和分布偏移下的安全预测面临挑战。传统方法在可扩展性和可靠性上存在局限，需要新的解决方案。", "method": "利用变分自编码器和循环预测器预测未来潜在轨迹，评估安全概率，并引入校准机制和领域适应技术以应对分布偏移。", "result": "实验结果表明，该方法在三个基准测试中保持高准确性，显著降低分布偏移下的假阳性率，且复合预测器在长时任务中表现优于单一预测器。", "conclusion": "该框架为端到端视觉控制系统提供了理论校准保证和实际评估支持，显著提升了安全预测的可靠性和鲁棒性。"}}
{"id": "2508.09354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09354", "abs": "https://arxiv.org/abs/2508.09354", "authors": ["Kejun Li", "Zachary Olkin", "Yisong Yue", "Aaron D. Ames"], "title": "CLF-RL: Control Lyapunov Function Guided Reinforcement Learning", "comment": "8 pages; 8 figures", "summary": "Reinforcement learning (RL) has shown promise in generating robust locomotion\npolicies for bipedal robots, but often suffers from tedious reward design and\nsensitivity to poorly shaped objectives. In this work, we propose a structured\nreward shaping framework that leverages model-based trajectory generation and\ncontrol Lyapunov functions (CLFs) to guide policy learning. We explore two\nmodel-based planners for generating reference trajectories: a reduced-order\nlinear inverted pendulum (LIP) model for velocity-conditioned motion planning,\nand a precomputed gait library based on hybrid zero dynamics (HZD) using\nfull-order dynamics. These planners define desired end-effector and joint\ntrajectories, which are used to construct CLF-based rewards that penalize\ntracking error and encourage rapid convergence. This formulation provides\nmeaningful intermediate rewards, and is straightforward to implement once a\nreference is available. Both the reference trajectories and CLF shaping are\nused only during training, resulting in a lightweight policy at deployment. We\nvalidate our method both in simulation and through extensive real-world\nexperiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved\nrobustness relative to the baseline RL policy and better performance than a\nclassic tracking reward RL formulation.", "AI": {"tldr": "提出了一种基于模型轨迹生成和控制李雅普诺夫函数（CLF）的结构化奖励塑造框架，用于强化学习（RL）策略训练，显著提高了双足机器人的鲁棒性和性能。", "motivation": "传统RL在双足机器人运动控制中存在奖励设计繁琐和目标敏感性高的问题，需要一种更有效的奖励塑造方法。", "method": "结合模型轨迹生成（LIP模型和HZD步态库）和CLF构建奖励函数，引导策略学习，训练后仅保留轻量级策略。", "result": "在仿真和Unitree G1机器人实验中，CLF-RL表现出比基线RL策略更高的鲁棒性和性能。", "conclusion": "结构化奖励塑造框架有效解决了RL在双足机器人运动控制中的挑战，具有实际应用价值。"}}
{"id": "2508.09444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09444", "abs": "https://arxiv.org/abs/2508.09444", "authors": ["Haoxiang Shi", "Xiang Deng", "Zaijing Li", "Gongwei Chen", "Yaowei Wang", "Liqiang Nie"], "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation", "comment": null, "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to follow natural language instructions through free-form 3D spaces.\nExisting VLN-CE approaches typically use a two-stage waypoint planning\nframework, where a high-level waypoint predictor generates the navigable\nwaypoints, and then a navigation planner suggests the intermediate goals in the\nhigh-level action space. However, this two-stage decomposition framework\nsuffers from: (1) global sub-optimization due to the proxy objective in each\nstage, and (2) a performance bottleneck caused by the strong reliance on the\nquality of the first-stage predicted waypoints. To address these limitations,\nwe propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE\npolicy that unifies the traditional two stages, i.e. waypoint generation and\nplanning, into a single diffusion policy. Notably, DifNav employs a conditional\ndiffusion policy to directly model multi-modal action distributions over future\nactions in continuous navigation space, eliminating the need for a waypoint\npredictor while enabling the agent to capture multiple possible\ninstruction-following behaviors. To address the issues of compounding error in\nimitation learning and enhance spatial reasoning in long-horizon navigation\ntasks, we employ DAgger for online policy training and expert trajectory\naugmentation, and use the aggregated data to further fine-tune the policy. This\napproach significantly improves the policy's robustness and its ability to\nrecover from error states. Extensive experiments on benchmark datasets\ndemonstrate that, even without a waypoint predictor, the proposed method\nsubstantially outperforms previous state-of-the-art two-stage waypoint-based\nmodels in terms of navigation performance. Our code is available at:\nhttps://github.com/Tokishx/DifNav.", "AI": {"tldr": "论文提出了一种名为DifNav的端到端优化方法，通过扩散策略统一了传统两阶段的路径点生成与规划，解决了现有方法的全局次优化和性能瓶颈问题。", "motivation": "现有两阶段路径点规划框架存在全局次优化和性能瓶颈问题，需要一种更高效的方法。", "method": "提出DAgger Diffusion Navigation (DifNav)，使用条件扩散策略直接建模连续导航空间中的多模态动作分布，并结合DAgger进行在线策略训练和专家轨迹增强。", "result": "实验表明，DifNav在导航性能上显著优于现有的两阶段路径点模型。", "conclusion": "DifNav通过端到端优化和扩散策略，显著提升了连续环境中的视觉语言导航性能。"}}
{"id": "2508.09595", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09595", "abs": "https://arxiv.org/abs/2508.09595", "authors": ["Michael Fennel", "Markus Walker", "Dominik Pikos", "Uwe D. Hanebeck"], "title": "HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control", "comment": "Final Version - Accepted on IEEE Transactions on Haptics", "summary": "Research in virtual reality and haptic technologies has consistently aimed to\nenhance immersion. While advanced head-mounted displays are now commercially\navailable, kinesthetic haptic interfaces still face challenges such as limited\nworkspaces, insufficient degrees of freedom, and kinematics not matching the\nhuman arm. In this paper, we present HapticGiant, a novel large-scale\nkinesthetic haptic interface designed to match the properties of the human arm\nas closely as possible and to facilitate natural user locomotion while\nproviding full haptic feedback. The interface incorporates a novel\nadmittance-type force control scheme, leveraging hierarchical optimization to\nrender both arbitrary serial kinematic chains and Cartesian admittances.\nNotably, the proposed control scheme natively accounts for system limitations,\nincluding joint and Cartesian constraints, as well as singularities.\nExperimental results demonstrate the effectiveness of HapticGiant and its\ncontrol scheme, paving the way for highly immersive virtual reality\napplications.", "AI": {"tldr": "论文提出了一种名为HapticGiant的新型大型动觉触觉接口，旨在匹配人类手臂特性并提供自然用户运动与完整触觉反馈。", "motivation": "当前动觉触觉接口存在工作空间有限、自由度不足及与人类手臂运动学不匹配等问题，限制了虚拟现实的沉浸感。", "method": "采用新型导纳型力控制方案，利用分层优化技术实现任意串行运动链和笛卡尔导纳的渲染，并考虑系统限制如关节约束和奇异性。", "result": "实验证明HapticGiant及其控制方案有效，为高沉浸感虚拟现实应用铺平道路。", "conclusion": "HapticGiant通过匹配人类手臂特性和优化控制方案，显著提升了虚拟现实的触觉沉浸感。"}}
{"id": "2508.09502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09502", "abs": "https://arxiv.org/abs/2508.09502", "authors": ["Junheon Yoon", "Woo-Jeong Baek", "Jaeheung Park"], "title": "Reactive Model Predictive Contouring Control for Robot Manipulators", "comment": "8 pages, 7 figures, 3 tables, conference paper, Accepted for\n  publication at IEEE/RSJ International Conference on Intelligent Robots and\n  Systems(IROS) 2025", "summary": "This contribution presents a robot path-following framework via Reactive\nModel Predictive Contouring Control (RMPCC) that successfully avoids obstacles,\nsingularities and self-collisions in dynamic environments at 100 Hz. Many\npath-following methods rely on the time parametrization, but struggle to handle\ncollision and singularity avoidance while adhering kinematic limits or other\nconstraints. Specifically, the error between the desired path and the actual\nposition can become large when executing evasive maneuvers. Thus, this paper\nderives a method that parametrizes the reference path by a path parameter and\nperforms the optimization via RMPCC. In particular, Control Barrier Functions\n(CBFs) are introduced to avoid collisions and singularities in dynamic\nenvironments. A Jacobian-based linearization and Gauss-Newton Hessian\napproximation enable solving the nonlinear RMPCC problem at 100 Hz,\noutperforming state-of-the-art methods by a factor of 10. Experiments confirm\nthat the framework handles dynamic obstacles in real-world settings with low\ncontouring error and low robot acceleration.", "AI": {"tldr": "提出了一种基于RMPCC的机器人路径跟踪框架，能在动态环境中以100 Hz频率避开障碍物、奇异点和自碰撞。", "motivation": "现有路径跟踪方法依赖时间参数化，难以同时处理碰撞和奇异点避免，且运动学限制下误差较大。", "method": "通过路径参数化参考路径，利用RMPCC优化，引入CBF避免碰撞和奇异点，采用Jacobian线性化和Gauss-Newton Hessian近似实现高速求解。", "result": "实验表明，该方法在动态环境中表现优异，轮廓误差和机器人加速度低，性能优于现有方法10倍。", "conclusion": "RMPCC框架在动态环境中高效、稳定，适用于实时路径跟踪任务。"}}
{"id": "2508.09606", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09606", "abs": "https://arxiv.org/abs/2508.09606", "authors": ["Alejandro Posadas-Nava", "Alejandro Carrasco", "Richard Linares"], "title": "BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots", "comment": "Accepted for presentation on ICCR Kyoto 2025", "summary": "\\textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality\n(VR) teleoperation system for robots, designed to unify real-time control, data\nrecording, and policy learning across heterogeneous robotic platforms. BEAVR\nenables real-time, dexterous teleoperation using commodity VR hardware,\nsupports modular integration with robots ranging from 7-DoF manipulators to\nfull-body humanoids, and records synchronized multi-modal demonstrations\ndirectly in the LeRobot dataset schema. Our system features a zero-copy\nstreaming architecture achieving $\\leq$35\\,ms latency, an asynchronous\n``think--act'' control loop for scalable inference, and a flexible network API\noptimized for real-time, multi-robot operation. We benchmark BEAVR across\ndiverse manipulation tasks and demonstrate its compatibility with leading\nvisuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is\npublicly available, and datasets are released on Hugging Face\\footnote{Code,\ndatasets, and VR app available at https://github.com/ARCLab-MIT/BEAVR-Bot.", "AI": {"tldr": "BEAVR是一个开源的、双手机器人虚拟现实（VR）远程操作系统，支持多机器人平台，整合了实时控制、数据记录和策略学习功能。", "motivation": "设计BEAVR的目的是为了统一异构机器人平台的实时控制、数据记录和策略学习，提升远程操作的灵活性和效率。", "method": "BEAVR采用零拷贝流架构实现低延迟（≤35ms），支持异步“思考-行动”控制循环和模块化机器人集成，兼容多种视觉运动策略。", "result": "系统在多样化操作任务中表现优异，并兼容ACT、DiffusionPolicy和SmolVLA等主流视觉运动策略。", "conclusion": "BEAVR是一个高效、灵活的VR远程操作系统，适用于多机器人平台，代码和数据已开源。"}}
{"id": "2508.09508", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09508", "abs": "https://arxiv.org/abs/2508.09508", "authors": ["Reema Raval", "Shalabh Gupta"], "title": "SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents", "comment": null, "summary": "Typical marine environments are highly complex with spatio-temporally varying\ncurrents and dynamic obstacles, presenting significant challenges to Unmanned\nSurface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need\nto continuously adapt their paths with real-time information to avoid\ncollisions and follow the path of least resistance to the goal via exploiting\nocean currents. In this regard, we introduce a novel algorithm, called\nSelf-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents\n(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic\nenvironments. SMART-OC integrates the obstacle risks along a path with the time\ncost to reach the goal to find the time-risk optimal path. The effectiveness of\nSMART-OC is validated by simulation experiments, which demonstrate that the USV\nperforms fast replannings to avoid dynamic obstacles and exploit ocean currents\nto successfully reach the goal.", "AI": {"tldr": "SMART-OC算法为无人水面艇（USV）在动态海洋环境中提供实时时间-风险最优路径重规划。", "motivation": "海洋环境复杂多变，USV需实时适应以避免碰撞并利用洋流高效导航。", "method": "SMART-OC结合路径上的障碍风险和到达目标的时间成本，实现时间-风险最优路径规划。", "result": "仿真实验表明，USV能快速重规划路径，避开动态障碍并利用洋流成功到达目标。", "conclusion": "SMART-OC有效解决了USV在动态环境中的实时导航问题。"}}
{"id": "2508.09876", "categories": ["cs.RO", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.09876", "abs": "https://arxiv.org/abs/2508.09876", "authors": ["Xiaowei Tan", "Weizhong Jiang", "Bi Zhang", "Wanxin Chen", "Yiwen Zhao", "Ning Li", "Lianqing Liu", "Xingang Zhao"], "title": "A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion", "comment": "49 pages, 20 figures, 4 tables", "summary": "Exoskeletons have been shown to effectively assist humans during steady\nlocomotion. However, their effects on non-steady locomotion, characterized by\nnonlinear phase progression within a gait cycle, remain insufficiently\nexplored, particularly across diverse activities. This work presents a shank\nangle-based control system that enables the exoskeleton to maintain real-time\ncoordination with human gait, even under phase perturbations, while dynamically\nshaping assistance profiles to match the biological ankle moment patterns\nacross walking, running, stair negotiation tasks. The control system consists\nof an assistance profile online generation method and a model-based feedforward\ncontrol method. The assistance profile is formulated as a dual-Gaussian model\nwith the shank angle as the independent variable. Leveraging only IMU\nmeasurements, the model parameters are updated online each stride to adapt to\ninter- and intra-individual biomechanical variability. The profile tracking\ncontrol employs a human-exoskeleton kinematics and stiffness model as a\nfeedforward component, reducing reliance on historical control data due to the\nlack of clear and consistent periodicity in non-steady locomotion. Three\nexperiments were conducted using a lightweight soft exoskeleton with multiple\nsubjects. The results validated the effectiveness of each individual method,\ndemonstrated the robustness of the control system against gait perturbations\nacross various activities, and revealed positive biomechanical and\nphysiological responses of human users to the exoskeleton's mechanical\nassistance.", "AI": {"tldr": "该论文提出了一种基于小腿角度的外骨骼控制系统，能够在非稳态步态中实时协调人类步态，并动态调整辅助模式以适应不同活动中的生物踝关节力矩模式。", "motivation": "探索外骨骼在非线性步态（如行走、跑步、上下楼梯）中的效果，填补非稳态步态辅助的研究空白。", "method": "采用双高斯模型生成辅助模式，结合基于模型的反馈控制方法，利用IMU实时更新参数以适应个体差异。", "result": "实验验证了控制系统的有效性、鲁棒性，并展示了外骨骼对人类用户的生物力学和生理学积极影响。", "conclusion": "该系统为外骨骼在复杂步态活动中的应用提供了可行方案，具有实际推广潜力。"}}
{"id": "2508.09558", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09558", "abs": "https://arxiv.org/abs/2508.09558", "authors": ["Jiahui Zuo", "Boyang Zhang", "Fumin Zhang"], "title": "CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail", "comment": null, "summary": "The manipulation of deformable linear flexures has a wide range of\napplications in industry, such as cable routing in automotive manufacturing and\ntextile production. Cable routing, as a complex multi-stage robot manipulation\nscenario, is a challenging task for robot automation. Common parallel\ntwo-finger grippers have the risk of over-squeezing and over-tension when\ngrasping and guiding cables. In this paper, a novel eagle-inspired fingernail\nis designed and mounted on the gripper fingers, which helps with cable grasping\non planar surfaces and in-hand cable guiding operations. Then we present a\nsingle-grasp end-to-end 3D cable routing framework utilizing the proposed\nfingernails, instead of the common pick-and-place strategy. Continuous control\nis achieved to efficiently manipulate cables through vision-based state\nestimation of task configurations and offline trajectory planning based on\nmotion primitives. We evaluate the effectiveness of the proposed framework with\na variety of cables and channel slots, significantly outperforming the\npick-and-place manipulation process under equivalent perceptual conditions. Our\nreconfigurable task setting and the proposed framework provide a reference for\nfuture cable routing manipulations in 3D space.", "AI": {"tldr": "论文提出了一种受鹰爪启发的指甲设计，用于机器人抓取和引导电缆，并开发了一种端到端的3D电缆布线框架，显著优于传统的拾取放置策略。", "motivation": "电缆布线是工业中的复杂任务，传统夹具有挤压和张力过大的风险，需要更高效的方法。", "method": "设计了鹰爪启发的指甲，结合视觉状态估计和离线轨迹规划，实现连续控制。", "result": "在多种电缆和槽道测试中，该方法显著优于传统拾取放置策略。", "conclusion": "提出的框架为未来3D空间中的电缆布线提供了参考。"}}
{"id": "2508.09581", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09581", "abs": "https://arxiv.org/abs/2508.09581", "authors": ["Junkai Jiang", "Yihe Chen", "Yibin Yang", "Ruochen Li", "Shaobing Xu", "Jianqiang Wang"], "title": "ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots", "comment": null, "summary": "Multi-vehicle trajectory planning (MVTP) is one of the key challenges in\nmulti-robot systems (MRSs) and has broad applications across various fields.\nThis paper presents ESCoT, an enhanced step-based coordinate trajectory\nplanning method for multiple car-like robots. ESCoT incorporates two key\nstrategies: collaborative planning for local robot groups and replanning for\nduplicate configurations. These strategies effectively enhance the performance\nof step-based MVTP methods. Through extensive experiments, we show that ESCoT\n1) in sparse scenarios, significantly improves solution quality compared to\nbaseline step-based method, achieving up to 70% improvement in typical conflict\nscenarios and 34% in randomly generated scenarios, while maintaining high\nsolving efficiency; and 2) in dense scenarios, outperforms all baseline\nmethods, maintains a success rate of over 50% even in the most challenging\nconfigurations. The results demonstrate that ESCoT effectively solves MVTP,\nfurther extending the capabilities of step-based methods. Finally, practical\nrobot tests validate the algorithm's applicability in real-world scenarios.", "AI": {"tldr": "ESCoT是一种改进的多车轨迹规划方法，通过协作规划和重复配置重规划策略，显著提升了稀疏和密集场景下的性能。", "motivation": "多车轨迹规划（MVTP）是多机器人系统（MRS）中的关键挑战，具有广泛应用。现有步进方法在性能上存在不足，需要改进。", "method": "ESCoT采用协作规划和重复配置重规划策略，优化步进式MVTP方法。", "result": "在稀疏场景中，ESCoT显著提升解决方案质量（冲突场景提升70%，随机场景提升34%）；在密集场景中，成功率超过50%。", "conclusion": "ESCoT有效解决了MVTP问题，扩展了步进式方法的能力，并通过实际机器人测试验证了其适用性。"}}
{"id": "2508.09621", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09621", "abs": "https://arxiv.org/abs/2508.09621", "authors": ["Ingrid Maéva Chekam", "Ines Pastor-Martinez", "Ali Tourani", "Jose Andres Millan-Romera", "Laura Ribeiro", "Pedro Miguel Bastos Soares", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Interpretable Robot Control via Structured Behavior Trees and Large Language Models", "comment": "15 pages, 5 figures, 3 tables", "summary": "As intelligent robots become more integrated into human environments, there\nis a growing need for intuitive and reliable Human-Robot Interaction (HRI)\ninterfaces that are adaptable and more natural to interact with. Traditional\nrobot control methods often require users to adapt to interfaces or memorize\npredefined commands, limiting usability in dynamic, unstructured environments.\nThis paper presents a novel framework that bridges natural language\nunderstanding and robotic execution by combining Large Language Models (LLMs)\nwith Behavior Trees. This integration enables robots to interpret natural\nlanguage instructions given by users and translate them into executable actions\nby activating domain-specific plugins. The system supports scalable and modular\nintegration, with a primary focus on perception-based functionalities, such as\nperson tracking and hand gesture recognition. To evaluate the system, a series\nof real-world experiments was conducted across diverse environments.\nExperimental results demonstrate that the proposed approach is practical in\nreal-world scenarios, with an average cognition-to-execution accuracy of\napproximately 94%, making a significant contribution to HRI systems and robots.\nThe complete source code of the framework is publicly available at\nhttps://github.com/snt-arg/robot_suite.", "AI": {"tldr": "论文提出了一种结合大型语言模型（LLMs）和行为树的新框架，用于自然语言驱动的机器人控制，提升了人机交互的直观性和适应性。", "motivation": "传统机器人控制方法需要用户适应接口或记忆命令，限制了在动态环境中的可用性，因此需要更自然的交互方式。", "method": "通过整合LLMs和行为树，将自然语言指令转化为可执行动作，支持模块化集成和感知功能（如人员跟踪和手势识别）。", "result": "实验结果显示，系统在真实场景中的认知到执行准确率约为94%，验证了其实用性。", "conclusion": "该框架为人机交互系统提供了显著贡献，源代码已公开。"}}
{"id": "2508.09700", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09700", "abs": "https://arxiv.org/abs/2508.09700", "authors": ["Mahdi Hejrati", "Jouni Mattila"], "title": "Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions", "comment": "This work has been accepted for presentation at the 2025 IEEE\n  Conference on Telepresence, to be held in Leiden, Netherlands", "summary": "Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents\nunique challenges that differ fundamentally from conventional human-scale\nsystems. As these platforms gain relevance in industrial domains such as\nconstruction, mining, and disaster response, immersive interfaces must be\nrethought to support scalable, safe, and effective human-robot collaboration.\nThis paper investigates the control, cognitive, and interface-level challenges\nof immersive teleoperation in BHSRMs, with a focus on ensuring operator safety,\nminimizing sensorimotor mismatch, and enhancing the sense of embodiment. We\nanalyze design trade-offs in haptic and visual feedback systems, supported by\nearly experimental comparisons of exoskeleton- and joystick-based control\nsetups. Finally, we outline key research directions for developing new\nevaluation tools, scaling strategies, and human-centered safety models tailored\nto large-scale robotic telepresence.", "AI": {"tldr": "本文探讨了超大型机械臂（BHSRMs）的远程操作挑战，重点关注控制、认知和界面设计，提出了改进安全性和操作体验的方法。", "motivation": "随着BHSRMs在工业领域的应用增加，需要重新设计沉浸式界面以支持安全高效的人机协作。", "method": "分析了触觉和视觉反馈系统的设计权衡，并比较了外骨骼和摇杆控制设置的实验。", "result": "提出了改进操作安全性、减少感知运动不匹配和增强操作者体验的方法。", "conclusion": "未来研究应关注开发新的评估工具、扩展策略和以人为中心的安全模型。"}}
{"id": "2508.09797", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09797", "abs": "https://arxiv.org/abs/2508.09797", "authors": ["Dongcheng Cao", "Jin Zhou", "Xian Wang", "Shuo Li"], "title": "FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning", "comment": null, "summary": "Agile flight for the quadrotor cable-suspended payload system is a formidable\nchallenge due to its underactuated, highly nonlinear, and hybrid dynamics.\nTraditional optimization-based methods often struggle with high computational\ncosts and the complexities of cable mode transitions, limiting their real-time\napplicability and maneuverability exploitation. In this letter, we present\nFLARE, a reinforcement learning (RL) framework that directly learns agile\nnavigation policy from high-fidelity simulation. Our method is validated across\nthree designed challenging scenarios, notably outperforming a state-of-the-art\noptimization-based approach by a 3x speedup during gate traversal maneuvers.\nFurthermore, the learned policies achieve successful zero-shot sim-to-real\ntransfer, demonstrating remarkable agility and safety in real-world\nexperiments, running in real time on an onboard computer.", "AI": {"tldr": "FLARE是一个基于强化学习的框架，用于解决四旋翼悬挂负载系统的敏捷飞行问题，显著优于传统优化方法。", "motivation": "四旋翼悬挂负载系统的动态特性复杂，传统优化方法计算成本高且难以实时应用。", "method": "采用强化学习框架FLARE，直接从高保真仿真中学习敏捷导航策略。", "result": "在三个挑战性场景中，FLARE比现有优化方法快3倍，并实现零样本仿真到现实的迁移。", "conclusion": "FLARE在实时性和敏捷性上表现优异，适用于实际应用。"}}
{"id": "2508.09836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09836", "abs": "https://arxiv.org/abs/2508.09836", "authors": ["Anirvan Dutta", "Alexis WM Devillard", "Zhihuan Zhang", "Xiaoxiao Cheng", "Etienne Burdet"], "title": "Embodied Tactile Perception of Soft Objects Properties", "comment": null, "summary": "To enable robots to develop human-like fine manipulation, it is essential to\nunderstand how mechanical compliance, multi-modal sensing, and purposeful\ninteraction jointly shape tactile perception. In this study, we use a dedicated\nmodular e-Skin with tunable mechanical compliance and multi-modal sensing\n(normal, shear forces and vibrations) to systematically investigate how sensing\nembodiment and interaction strategies influence robotic perception of objects.\nLeveraging a curated set of soft wave objects with controlled viscoelastic and\nsurface properties, we explore a rich set of palpation primitives-pressing,\nprecession, sliding that vary indentation depth, frequency, and directionality.\nIn addition, we propose the latent filter, an unsupervised, action-conditioned\ndeep state-space model of the sophisticated interaction dynamics and infer\ncausal mechanical properties into a structured latent space. This provides\ngeneralizable and in-depth interpretable representation of how embodiment and\ninteraction determine and influence perception. Our investigation demonstrates\nthat multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced\ninteraction between the environment and mechanical properties of e-Skin, which\nshould be examined alongside the interaction by incorporating temporal\ndynamics.", "AI": {"tldr": "研究探讨了机械顺应性、多模态传感和交互策略如何影响机器人触觉感知，提出了一种无监督的深度状态空间模型，证明多模态传感优于单模态传感。", "motivation": "为了实现类似人类的精细操作，需要理解机械顺应性、多模态传感和交互如何共同塑造触觉感知。", "method": "使用模块化电子皮肤（e-Skin）和一组软波物体，探索多种触诊动作，并提出无监督的潜在滤波器模型。", "result": "多模态传感优于单模态传感，揭示了电子皮肤机械特性与环境的复杂交互。", "conclusion": "机械特性和交互策略需结合时间动态来深入理解触觉感知。"}}
{"id": "2508.09846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09846", "abs": "https://arxiv.org/abs/2508.09846", "authors": ["Donghoon Baek", "Amartya Purushottam", "Jason J. Choi", "Joao Ramos"], "title": "Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation", "comment": null, "summary": "This paper presents an object-aware whole-body bilateral teleoperation\nframework for wheeled humanoid loco-manipulation. This framework combines\nwhole-body bilateral teleoperation with an online multi-stage object inertial\nparameter estimation module, which is the core technical contribution of this\nwork. The multi-stage process sequentially integrates a vision-based object\nsize estimator, an initial parameter guess generated by a large vision-language\nmodel (VLM), and a decoupled hierarchical sampling strategy. The visual size\nestimate and VLM prior offer a strong initial guess of the object's inertial\nparameters, significantly reducing the search space for sampling-based\nrefinement and improving the overall estimation speed. A hierarchical strategy\nfirst estimates mass and center of mass, then infers inertia from object size\nto ensure physically feasible parameters, while a decoupled multi-hypothesis\nscheme enhances robustness to VLM prior errors. Our estimator operates in\nparallel with high-fidelity simulation and hardware, enabling real-time online\nupdates. The estimated parameters are then used to update the wheeled\nhumanoid's equilibrium point, allowing the operator to focus more on locomotion\nand manipulation. This integration improves the haptic force feedback for\ndynamic synchronization, enabling more dynamic whole-body teleoperation. By\ncompensating for object dynamics using the estimated parameters, the framework\nalso improves manipulation tracking while preserving compliant behavior. We\nvalidate the system on a customized wheeled humanoid with a robotic gripper and\nhuman-machine interface, demonstrating real-time execution of lifting,\ndelivering, and releasing tasks with a payload weighing approximately one-third\nof the robot's body weight.", "AI": {"tldr": "提出了一种结合全身双边遥操作与在线多阶段物体惯性参数估计的轮式人形机器人框架，显著提升了操作动态性和参数估计速度。", "motivation": "解决轮式人形机器人在搬运任务中动态同步和参数估计的挑战，提升操作效率和实时性。", "method": "结合视觉尺寸估计、大视觉语言模型（VLM）初始参数猜测和分层采样策略，分阶段估计物体惯性参数。", "result": "实现了实时在线参数更新，提升了操作动态性和跟踪性能，成功完成搬运任务。", "conclusion": "该框架显著提升了轮式人形机器人的动态遥操作能力，适用于复杂搬运场景。"}}
{"id": "2508.09855", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.09855", "abs": "https://arxiv.org/abs/2508.09855", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes", "comment": "3 pages, 3 figures", "summary": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human\nand robot interactions, especially for close-proximity collaboration tasks such\nas human-robot handovers. Learning robot manipulation policies from raw,\nreal-world image data requires a large number of robot-action trials in the\nphysical environment. Although simulation training offers a cost-effective\nalternative, the visual domain gap between simulation and robot workspace\nremains a major limitation. We introduce a method for training HRT policies,\nfocusing on human-to-robot handovers, solely from RGB images without the need\nfor real-robot training or real-robot data collection. The goal is to enable\nthe robot to reliably receive objects from a human with stable grasping while\navoiding collisions with the human hand. The proposed policy learner leverages\nsparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes\nto generate robot demonstrations containing image-action pairs captured with a\ncamera mounted on the robot gripper. As a result, the simulated camera pose\nchanges in the reconstructed scene can be directly translated into gripper pose\nchanges. Experiments in both Gaussian Splatting reconstructed scene and\nreal-world human-to-robot handover experiments demonstrate that our method\nserves as a new and effective representation for the human-to-robot handover\ntask, contributing to more seamless and robust HRT.", "AI": {"tldr": "提出一种仅从RGB图像训练人机协作策略的方法，无需真实机器人数据，通过高斯泼溅重建场景生成演示。", "motivation": "解决人机协作中真实机器人数据收集成本高及仿真与真实视觉域差距的问题。", "method": "利用稀疏视图高斯泼溅重建人机交接场景，生成图像-动作对演示，直接映射到机器人夹爪姿态变化。", "result": "在高斯泼溅重建场景和真实人机交接实验中验证了方法的有效性和鲁棒性。", "conclusion": "该方法为人机交接任务提供了一种新的有效表示，促进了更无缝和鲁棒的人机协作。"}}
{"id": "2508.09950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09950", "abs": "https://arxiv.org/abs/2508.09950", "authors": ["Bida Ma", "Nuo Xu", "Chenkun Qi", "Xin Liu", "Yule Mo", "Jinkai Wang", "Chunpeng Lu"], "title": "PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces", "comment": null, "summary": "The legged locomotion in spatially constrained structures (called crawl\nspaces) is challenging. In crawl spaces, current exteroceptive locomotion\nlearning methods are limited by large noises and errors of the sensors in\npossible low visibility conditions, and current proprioceptive locomotion\nlearning methods are difficult in traversing crawl spaces because only ground\nfeatures are inferred. In this study, a point cloud supervised proprioceptive\nlocomotion reinforcement learning method for legged robots in crawl spaces is\nproposed. A state estimation network is designed to estimate the robot's\nsurrounding ground and spatial features as well as the robot's collision states\nusing historical proprioceptive sensor data. The point cloud is represented in\npolar coordinate frame and a point cloud processing method is proposed to\nefficiently extract the ground and spatial features that are used to supervise\nthe state estimation network learning. Comprehensive reward functions that\nguide the robot to traverse through crawl spaces after collisions are designed.\nExperiments demonstrate that, compared to existing methods, our method exhibits\nmore agile locomotion in crawl spaces. This study enhances the ability of\nlegged robots to traverse spatially constrained environments without requiring\nexteroceptive sensors.", "AI": {"tldr": "提出了一种基于点云监督的强化学习方法，用于腿式机器人在狭窄空间中的运动，无需依赖外部感知传感器。", "motivation": "现有方法在狭窄空间中因传感器噪声和低可见性条件受限，或仅能推断地面特征，难以实现高效运动。", "method": "设计状态估计网络，利用历史本体感知数据估计周围环境和碰撞状态；提出点云处理方法提取特征；设计综合奖励函数。", "result": "实验表明，该方法在狭窄空间中表现出更敏捷的运动能力。", "conclusion": "该方法提升了腿式机器人在无外部传感器条件下的狭窄环境穿越能力。"}}
{"id": "2508.09960", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09960", "abs": "https://arxiv.org/abs/2508.09960", "authors": ["Yifei Yao", "Chengyuan Luo", "Jiaheng Du", "Wentao He", "Jun-Guo Lu"], "title": "GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation", "comment": null, "summary": "The creation of human-like humanoid robots is hindered by a fundamental\nfragmentation: data processing and learning algorithms are rarely universal\nacross different robot morphologies. This paper introduces the Generalized\nBehavior Cloning (GBC) framework, a comprehensive and unified solution designed\nto solve this end-to-end challenge. GBC establishes a complete pathway from\nhuman motion to robot action through three synergistic innovations. First, an\nadaptive data pipeline leverages a differentiable IK network to automatically\nretarget any human MoCap data to any humanoid. Building on this foundation, our\nnovel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,\nhigh-fidelity imitation policies. To complete the ecosystem, the entire\nframework is delivered as an efficient, open-source platform based on Isaac\nLab, empowering the community to deploy the full workflow via simple\nconfiguration scripts. We validate the power and generality of GBC by training\npolicies on multiple heterogeneous humanoids, demonstrating excellent\nperformance and transfer to novel motions. This work establishes the first\npractical and unified pathway for creating truly generalized humanoid\ncontrollers.", "AI": {"tldr": "论文提出通用行为克隆（GBC）框架，解决人形机器人数据与算法不通用的问题，通过自适应数据管道、DAgger-MMPPO算法和开源平台实现高效、统一的机器人控制。", "motivation": "人形机器人开发面临数据与算法不通用的问题，限制了其通用性。", "method": "GBC框架包含自适应数据管道（利用可微分IK网络）、DAgger-MMPPO算法（结合MMTransformer架构）和开源平台（基于Isaac Lab）。", "result": "在多种异构人形机器人上验证了GBC的高性能和泛化能力。", "conclusion": "GBC为创建通用人形控制器提供了首个实用且统一的解决方案。"}}
{"id": "2508.09971", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09971", "abs": "https://arxiv.org/abs/2508.09971", "authors": ["Zihan Wang", "Nina Mahmoudian"], "title": "Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model", "comment": "Submitted to Robotics and Autonomous Systems (RAS) journal", "summary": "Vision-driven autonomous river following by Unmanned Aerial Vehicles is\ncritical for applications such as rescue, surveillance, and environmental\nmonitoring, particularly in dense riverine environments where GPS signals are\nunreliable. We formalize river following as a coverage control problem in which\nthe reward function is submodular, yielding diminishing returns as more unique\nriver segments are visited, thereby framing the task as a Submodular Markov\nDecision Process. First, we introduce Marginal Gain Advantage Estimation, which\nrefines the reward advantage function by using a sliding window baseline\ncomputed from historical episodic returns, thus aligning the advantage\nestimation with the agent's evolving recognition of action value in\nnon-Markovian settings. Second, we develop a Semantic Dynamics Model based on\npatchified water semantic masks that provides more interpretable and\ndata-efficient short-term prediction of future observations compared to latent\nvision dynamics models. Third, we present the Constrained Actor Dynamics\nEstimator architecture, which integrates the actor, the cost estimator, and SDM\nfor cost advantage estimation to form a model-based SafeRL framework capable of\nsolving partially observable Constrained Submodular Markov Decision Processes.\nSimulation results demonstrate that MGAE achieves faster convergence and\nsuperior performance over traditional critic-based methods like Generalized\nAdvantage Estimation. SDM provides more accurate short-term state predictions\nthat enable the cost estimator to better predict potential violations. Overall,\nCADE effectively integrates safety regulation into model-based RL, with the\nLagrangian approach achieving the soft balance of reward and safety during\ntraining, while the safety layer enhances performance during inference by hard\naction overlay.", "AI": {"tldr": "论文提出了一种基于视觉的无人机自主沿河飞行方法，通过子模马尔可夫决策过程（Submodular MDP）建模，结合边际增益优势估计（MGAE）、语义动态模型（SDM）和约束演员动态估计器（CADE），实现了高效、安全的河流覆盖任务。", "motivation": "在密集河流环境中，GPS信号不可靠，视觉驱动的无人机自主沿河飞行对救援、监视和环境监测等应用至关重要。", "method": "1. 提出边际增益优势估计（MGAE），通过滑动窗口基线优化奖励优势函数；2. 开发基于语义掩码的语义动态模型（SDM），提供高效短期预测；3. 设计约束演员动态估计器（CADE），整合模型和安全强化学习框架。", "result": "仿真结果表明，MGAE收敛更快且性能优于传统方法；SDM提供更准确的短期预测；CADE成功整合安全约束。", "conclusion": "CADE框架在训练和推理中有效平衡奖励与安全，为复杂环境下的无人机任务提供了可靠解决方案。"}}
{"id": "2508.09976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09976", "abs": "https://arxiv.org/abs/2508.09976", "authors": ["Marion Lepert", "Jiaying Fang", "Jeannette Bohg"], "title": "Masquerade: Learning from In-the-wild Human Videos using Data-Editing", "comment": "Project website at https://masquerade-robot.github.io/", "summary": "Robot manipulation research still suffers from significant data scarcity:\neven the largest robot datasets are orders of magnitude smaller and less\ndiverse than those that fueled recent breakthroughs in language and vision. We\nintroduce Masquerade, a method that edits in-the-wild egocentric human videos\nto bridge the visual embodiment gap between humans and robots and then learns a\nrobot policy with these edited videos. Our pipeline turns each human video into\nrobotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the\nhuman arms, and (iii) overlaying a rendered bimanual robot that tracks the\nrecovered end-effector trajectories. Pre-training a visual encoder to predict\nfuture 2-D robot keypoints on 675K frames of these edited clips, and continuing\nthat auxiliary loss while fine-tuning a diffusion policy head on only 50 robot\ndemonstrations per task, yields policies that generalize significantly better\nthan prior work. On three long-horizon, bimanual kitchen tasks evaluated in\nthree unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations\nshow that both the robot overlay and co-training are indispensable, and\nperformance scales logarithmically with the amount of edited human video. These\nresults demonstrate that explicitly closing the visual embodiment gap unlocks a\nvast, readily available source of data from human videos that can be used to\nimprove robot policies.", "AI": {"tldr": "Masquerade方法通过编辑人类视频生成机器人演示，显著提升了机器人策略的泛化能力。", "motivation": "机器人操作研究面临数据稀缺问题，现有数据集远小于语言和视觉领域的规模。", "method": "通过估计3D手部姿势、修复人类手臂并叠加机器人轨迹，将人类视频转化为机器人演示，再通过预训练和微调学习策略。", "result": "在三个未见过的厨房任务中，Masquerade性能优于基线5-6倍。", "conclusion": "通过视觉体现差距的显式缩小，人类视频成为提升机器人策略的宝贵数据源。"}}
