{"id": "2508.04834", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04834", "abs": "https://arxiv.org/abs/2508.04834", "authors": ["Morten Roed Frederiksen", "Kasper Støy"], "title": "On the causality between affective impact and coordinated human-robot reactions", "comment": "7 pages, 5 figures, 29th IEEE International Workshop on Robot and\n  Human Communication (ROMAN)", "summary": "In an effort to improve how robots function in social contexts, this paper\ninvestigates if a robot that actively shares a reaction to an event with a\nhuman alters how the human perceives the robot's affective impact. To verify\nthis, we created two different test setups. One to highlight and isolate the\nreaction element of affective robot expressions, and one to investigate the\neffects of applying specific timing delays to a robot reacting to a physical\nencounter with a human. The first test was conducted with two different groups\n(n=84) of human observers, a test group and a control group both interacting\nwith the robot. The second test was performed with 110 participants using\nincreasingly longer reaction delays for the robot with every ten participants.\nThe results show a statistically significant change (p$<$.05) in perceived\naffective impact for the robots when they react to an event shared with a human\nobserver rather than reacting at random. The result also shows for shared\nphysical interaction, the near-human reaction times from the robot are most\nappropriate for the scenario. The paper concludes that a delay time around\n200ms may render the biggest impact on human observers for small-sized\nnon-humanoid robots. It further concludes that a slightly shorter reaction time\naround 100ms is most effective when the goal is to make the human observers\nfeel they made the biggest impact on the robot.", "AI": {"tldr": "研究机器人是否通过主动与人类共享事件反应来改变人类对其情感影响的感知，发现共享反应和特定延迟时间显著影响感知。", "motivation": "改善机器人在社交环境中的表现，探究其情感表达对人类感知的影响。", "method": "设计两个实验：1）隔离机器人情感表达的反应元素；2）测试不同反应延迟时间对物理互动的影响。", "result": "共享事件的机器人反应显著改变人类感知（p<0.05）；物理互动中，接近人类的反应时间（200ms）效果最佳。", "conclusion": "200ms延迟对小型非人形机器人最有效；100ms延迟则让人类感觉对机器人影响最大。"}}
{"id": "2508.04931", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04931", "abs": "https://arxiv.org/abs/2508.04931", "authors": ["Jin Wang", "Weijie Wang", "Boyuan Deng", "Heng Zhang", "Rui Dai", "Nikos Tsagarakis"], "title": "INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM", "comment": "Project Web: https://robo-intention.github.io", "summary": "Traditional control and planning for robotic manipulation heavily rely on\nprecise physical models and predefined action sequences. While effective in\nstructured environments, such approaches often fail in real-world scenarios due\nto modeling inaccuracies and struggle to generalize to novel tasks. In\ncontrast, humans intuitively interact with their surroundings, demonstrating\nremarkable adaptability, making efficient decisions through implicit physical\nunderstanding. In this work, we propose INTENTION, a novel framework enabling\nrobots with learned interactive intuition and autonomous manipulation in\ndiverse scenarios, by integrating Vision-Language Models (VLMs) based scene\nreasoning with interaction-driven memory. We introduce Memory Graph to record\nscenes from previous task interactions which embodies human-like understanding\nand decision-making about different tasks in real world. Meanwhile, we design\nan Intuitive Perceptor that extracts physical relations and affordances from\nvisual scenes. Together, these components empower robots to infer appropriate\ninteraction behaviors in new scenes without relying on repetitive instructions.\nVideos: https://robo-intention.github.io", "AI": {"tldr": "论文提出INTENTION框架，结合视觉语言模型和交互驱动记忆，赋予机器人直觉式交互能力，实现自主操作。", "motivation": "传统机器人控制依赖精确模型和预定义动作，难以适应复杂现实场景，而人类通过直觉和物理理解表现出强大适应性。", "method": "提出Memory Graph记录任务交互场景，设计Intuitive Perceptor提取物理关系和功能属性，结合视觉语言模型进行推理。", "result": "机器人能在新场景中推断合适交互行为，无需重复指令。", "conclusion": "INTENTION框架提升了机器人在多样化场景中的自主操作能力。"}}
{"id": "2508.04981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04981", "abs": "https://arxiv.org/abs/2508.04981", "authors": ["Tianyuan Zheng", "Jingang Yi", "Kaiyan Yu"], "title": "Optimal Planning for Multi-Robot Simultaneous Area and Line Coverage Using Hierarchical Cyclic Merging Regulation", "comment": null, "summary": "The double coverage problem focuses on determining efficient, collision-free\nroutes for multiple robots to simultaneously cover linear features (e.g.,\nsurface cracks or road routes) and survey areas (e.g., parking lots or local\nregions) in known environments. In these problems, each robot carries two\nfunctional roles: service (linear feature footprint coverage) and exploration\n(complete area coverage). Service has a smaller operational footprint but\nincurs higher costs (e.g., time) compared to exploration. We present optimal\nplanning algorithms for the double coverage problems using hierarchical cyclic\nmerging regulation (HCMR). To reduce the complexity for optimal planning\nsolutions, we analyze the manifold attachment process during graph traversal\nfrom a Morse theory perspective. We show that solutions satisfying minimum path\nlength and collision-free constraints must belong to a Morse-bounded\ncollection. To identify this collection, we introduce the HCMR algorithm. In\nHCMR, cyclic merging search regulates traversal behavior, while edge sequence\nback propagation converts these regulations into graph edge traversal\nsequences. Incorporating balanced partitioning, the optimal sequence is\nselected to generate routes for each robot. We prove the optimality of the HCMR\nalgorithm under a fixed sweep direction. The multi-robot simulation results\ndemonstrate that the HCMR algorithm significantly improves planned path length\nby at least 10.0%, reduces task time by at least 16.9% in average, and ensures\nconflict-free operation compared to other state-of-the-art planning methods.", "AI": {"tldr": "该论文提出了一种基于分层循环合并调节（HCMR）的算法，用于解决多机器人同时覆盖线性特征和区域的优化路径规划问题，显著提升了效率和避免冲突。", "motivation": "研究多机器人在已知环境中高效、无冲突地完成线性特征覆盖和区域探索的双重任务，以降低操作成本并提高效率。", "method": "采用分层循环合并调节（HCMR）算法，结合Morse理论分析图遍历中的流形附着过程，并通过边序列反向传播和平衡分区生成最优路径。", "result": "HCMR算法在模拟中显著优于现有方法，路径长度减少至少10.0%，任务时间平均降低16.9%，且确保无冲突操作。", "conclusion": "HCMR算法在固定扫描方向下被证明是最优的，为多机器人双重覆盖问题提供了高效且无冲突的解决方案。"}}
{"id": "2508.04994", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04994", "abs": "https://arxiv.org/abs/2508.04994", "authors": ["Wenjie Hu", "Ye Zhou", "Hann Woei Ho"], "title": "Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots", "comment": null, "summary": "Maze navigation is a fundamental challenge in robotics, requiring agents to\ntraverse complex environments efficiently. While the Deep Deterministic Policy\nGradient (DDPG) algorithm excels in control tasks, its performance in maze\nnavigation suffers from sparse rewards, inefficient exploration, and\nlong-horizon planning difficulties, often leading to low success rates and\naverage rewards, sometimes even failing to achieve effective navigation. To\naddress these limitations, this paper proposes an efficient Hierarchical DDPG\n(HDDPG) algorithm, which includes high-level and low-level policies. The\nhigh-level policy employs an advanced DDPG framework to generate intermediate\nsubgoals from a long-term perspective and on a higher temporal scale. The\nlow-level policy, also powered by the improved DDPG algorithm, generates\nprimitive actions by observing current states and following the subgoal\nassigned by the high-level policy. The proposed method enhances stability with\noff-policy correction, refining subgoal assignments by relabeling historical\nexperiences. Additionally, adaptive parameter space noise is utilized to\nimprove exploration, and a reshaped intrinsic-extrinsic reward function is\nemployed to boost learning efficiency. Further optimizations, including\ngradient clipping and Xavier initialization, are employed to improve\nrobustness. The proposed algorithm is rigorously evaluated through numerical\nsimulation experiments executed using the Robot Operating System (ROS) and\nGazebo. Regarding the three distinct final targets in autonomous maze\nnavigation tasks, HDDPG significantly overcomes the limitations of standard\nDDPG and its variants, improving the success rate by at least 56.59% and\nboosting the average reward by a minimum of 519.03 compared to baseline\nalgorithms.", "AI": {"tldr": "论文提出了一种高效的层次化DDPG（HDDPG）算法，通过高、低层策略结合，解决了标准DDPG在迷宫导航中的稀疏奖励和探索效率低等问题，显著提升了成功率和平均奖励。", "motivation": "标准DDPG算法在迷宫导航任务中表现不佳，主要由于稀疏奖励、探索效率低和长时规划困难，导致成功率低和奖励不足。", "method": "提出HDDPG算法，包含高层策略（生成子目标）和低层策略（生成动作），结合离策略校正、自适应参数噪声和重塑奖励函数，优化探索和学习效率。", "result": "在ROS和Gazebo仿真实验中，HDDPG相比基线算法，成功率提升至少56.59%，平均奖励增加至少519.03。", "conclusion": "HDDPG有效解决了标准DDPG的局限性，显著提升了迷宫导航任务的性能。"}}
{"id": "2412.14762", "categories": ["cs.RO", "cs.SY", "eess.SY", "Robotics (cs.RO)"], "pdf": "https://arxiv.org/pdf/2412.14762", "abs": "https://arxiv.org/abs/2412.14762", "authors": ["Maddalena Feder", "Giorgio Grioli", "Manuel G. Catalano", "Antonio Bicchi"], "title": "A General Control Method for Human-Robot Integration", "comment": "Submitted to the International Journal of Robotics Research (IJRR),\n  under review since October 2024, 16 pages, 30 figures", "summary": "This paper introduces a new generalized control method designed for\nmulti-degrees-of-freedom devices to help people with limited motion\ncapabilities in their daily activities. The challenge lies in finding the most\nadapted strategy for the control interface to effectively map user's motions in\na low-dimensional space to complex robotic assistive devices, such as\nprostheses, supernumerary limbs, up to remote robotic avatars. The goal is a\nsystem which integrates the human and the robotic parts into a unique system,\nmoving so as to reach the targets decided by the human while autonomously\nreducing the user's effort and discomfort. We present a framework to control\ngeneral multi DoFs assistive systems, which translates user-performed\ncompensatory motions into the necessary robot commands for reaching targets\nwhile canceling or reducing compensation. The framework extends to prostheses\nof any number of DoF up to full robotic avatars, regarded here as a sort of\nwhole-body prosthesis of the person who sees the robot as an artificial\nextension of their own body without a physical link but with a sensory-motor\nintegration. We have validated and applied this control strategy through tests\nencompassing simulated scenarios and real-world trials involving a virtual twin\nof the robotic parts (prosthesis and robot) and a physical humanoid avatar.", "AI": {"tldr": "本文提出了一种新的广义控制方法，用于多自由度设备，帮助运动能力有限的人群完成日常活动。该方法通过将用户的低维运动映射到复杂机器人辅助设备（如假肢、超级肢体或远程机器人替身）中，实现人与机器人的无缝集成，减少用户的不适和努力。", "motivation": "为运动能力受限的人群提供一种高效的控制方法，使其能够通过简单的动作控制复杂的机器人辅助设备，从而提升生活质量。", "method": "提出了一种框架，将用户执行的补偿性动作转化为机器人命令，以实现目标并减少补偿。该方法适用于从假肢到全身机器人替身的各种多自由度设备。", "result": "通过模拟场景和真实世界试验（包括虚拟双胞胎和物理人形替身）验证了该控制策略的有效性。", "conclusion": "该框架为多自由度辅助系统的控制提供了一种通用解决方案，能够显著减少用户的努力和不适，具有广泛的应用潜力。"}}
{"id": "2508.05021", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05021", "abs": "https://arxiv.org/abs/2508.05021", "authors": ["Weifan Zhang", "Tingguang Li", "Yuzhen Liu"], "title": "MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding", "comment": null, "summary": "Visual navigation in unknown environments based solely on natural language\ndescriptions is a key capability for intelligent robots. In this work, we\npropose a navigation framework built upon off-the-shelf Visual Language Models\n(VLMs), enhanced with two human-inspired mechanisms: perspective-based active\ngrounding, which dynamically adjusts the robot's viewpoint for improved visual\ninspection, and historical memory backtracking, which enables the system to\nretain and re-evaluate uncertain observations over time. Unlike existing\napproaches that passively rely on incidental visual inputs, our method actively\noptimizes perception and leverages memory to resolve ambiguity, significantly\nimproving vision-language grounding in complex, unseen environments. Our\nframework operates in a zero-shot manner, achieving strong generalization to\ndiverse and open-ended language descriptions without requiring labeled data or\nmodel fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show\nthat our method outperforms state-of-the-art approaches in language-driven\nobject navigation. We further demonstrate its practicality through real-world\ndeployment on a quadruped robot, achieving robust and effective navigation\nperformance.", "AI": {"tldr": "提出了一种基于视觉语言模型（VLM）的导航框架，通过主动视角调整和历史记忆回溯机制，显著提升了复杂未知环境中的视觉语言定位能力，并在零样本条件下实现了强泛化性能。", "motivation": "解决智能机器人在未知环境中仅依赖自然语言描述进行视觉导航的挑战。", "method": "结合视角主动定位和历史记忆回溯机制，动态优化视觉感知并利用记忆解决模糊问题，无需标注数据或模型微调。", "result": "在HM3D数据集上优于现有方法，并在四足机器人上验证了实际导航性能。", "conclusion": "该框架通过主动感知和记忆机制，显著提升了语言驱动导航的性能和实用性。"}}
{"id": "2508.05368", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05368", "abs": "https://arxiv.org/abs/2508.05368", "authors": ["Tong Hua", "Jiale Han", "Wei Ouyang"], "title": "A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry", "comment": null, "summary": "Invariant Extended Kalman Filter (IEKF) has been a significant technique in\nvision-aided sensor fusion. However, it usually suffers from high computational\nburden when jointly optimizing camera poses and the landmarks. To improve its\nefficiency and applicability for multi-sensor fusion, we present a multi-view\npose-only estimation approach with its application to GNSS-Visual-Inertial\nOdometry (GVIO) in this paper. Our main contribution is deriving a visual\nmeasurement model which directly associates landmark representation with\nmultiple camera poses and observations. Such a pose-only measurement is proven\nto be tightly-coupled between landmarks and poses, and maintain a perfect null\nspace that is independent of estimated poses. Finally, we apply the proposed\napproach to a filter based GVIO with a novel feature management strategy. Both\nsimulation tests and real-world experiments are conducted to demonstrate the\nsuperiority of the proposed method in terms of efficiency and accuracy.", "AI": {"tldr": "提出了一种多视角仅姿态估计方法，用于GNSS-视觉-惯性里程计（GVIO），通过直接关联地标表示与多相机姿态和观测，提高了计算效率。", "motivation": "传统的IEKF在联合优化相机姿态和地标时计算负担高，限制了其在多传感器融合中的效率和适用性。", "method": "推导了一种视觉测量模型，直接关联地标表示与多相机姿态和观测，形成紧密耦合的姿态-地标关系，并保持完美的零空间。", "result": "仿真和真实实验表明，该方法在效率和精度上优于传统方法。", "conclusion": "提出的方法显著提高了GVIO的计算效率和精度，适用于多传感器融合场景。"}}
{"id": "2508.05027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05027", "abs": "https://arxiv.org/abs/2508.05027", "authors": ["Philip Huang", "Yorai Shaoul", "Jiaoyang Li"], "title": "Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning", "comment": "9 pages, 6 figures, accepted for publication at 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Generating high-quality motion plans for multiple robot arms is challenging\ndue to the high dimensionality of the system and the potential for inter-arm\ncollisions. Traditional motion planning methods often produce motions that are\nsuboptimal in terms of smoothness and execution time for multi-arm systems.\nPost-processing via shortcutting is a common approach to improve motion quality\nfor efficient and smooth execution. However, in multi-arm scenarios, optimizing\none arm's motion must not introduce collisions with other arms. Although\nexisting multi-arm planning works often use some form of shortcutting\ntechniques, their exact methodology and impact on performance are often vaguely\ndescribed. In this work, we present a comprehensive study quantitatively\ncomparing existing shortcutting methods for multi-arm trajectories across\ndiverse simulated scenarios. We carefully analyze the pros and cons of each\nshortcutting method and propose two simple strategies for combining these\nmethods to achieve the best performance-runtime tradeoff. Video, code, and\ndataset are available at https://philip-huang.github.io/mr-shortcut/.", "AI": {"tldr": "本文研究了多机械臂运动规划中的捷径优化方法，比较了现有方法并提出两种组合策略以实现最佳性能与运行时间的平衡。", "motivation": "多机械臂系统的高维度和潜在的碰撞问题使得高质量运动规划具有挑战性，传统方法常产生不理想的运动轨迹。", "method": "通过定量比较现有捷径优化方法，分析其优缺点，并提出两种组合策略。", "result": "研究提供了不同模拟场景下的性能比较，并展示了组合策略的优势。", "conclusion": "提出的组合策略在多机械臂运动规划中实现了性能与运行时间的最佳平衡。"}}
{"id": "2508.05634", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05634", "abs": "https://arxiv.org/abs/2508.05634", "authors": ["Jianpeng Yao", "Xiaopan Zhang", "Yu Xia", "Zejin Wang", "Amit K. Roy-Chowdhury", "Jiachen Li"], "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling", "comment": "9th Conference on Robot Learning (CoRL 2025); Project website:\n  https://gen-safe-nav.github.io/. arXiv admin note: text overlap with\n  arXiv:2407.17460", "summary": "Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/.", "AI": {"tldr": "论文提出一种基于自适应共形推理和约束强化学习的方法，提升移动机器人在人群导航中的鲁棒性，尤其在分布偏移场景下表现优异。", "motivation": "解决强化学习训练的移动机器人在面对分布外场景时性能下降的问题。", "method": "通过自适应共形推理生成行人预测不确定性估计，并利用约束强化学习指导机器人行为。", "result": "在分布内场景中，成功率提高8.80%，碰撞减少3.72倍，侵入轨迹减少2.43倍；在分布外场景中表现出更强的鲁棒性。", "conclusion": "该方法在真实机器人上验证有效，能够安全地与稀疏和密集人群交互。"}}
{"id": "2508.05040", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05040", "abs": "https://arxiv.org/abs/2508.05040", "authors": ["Boyang Zhang", "Jiahui Zuo", "Zeyu Duan", "Fumin Zhang"], "title": "A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System", "comment": null, "summary": "External collisions to robot actuators typically pose risks to grasping\ncircular objects. This work presents a vision-based sensing module capable of\ndetecting collisions to maintain stable grasping with a soft gripper system.\nThe system employs an eye-in-palm camera with a broad field of view to\nsimultaneously monitor the motion of fingers and the grasped object.\nFurthermore, we have developed a collision-rich grasping strategy to ensure the\nstability and security of the entire dynamic grasping process. A physical soft\ngripper was manufactured and affixed to a collaborative robotic arm to evaluate\nthe performance of the collision detection mechanism. An experiment regarding\ntesting the response time of the mechanism confirmed the system has the\ncapability to react to the collision instantaneously. A dodging test was\nconducted to demonstrate the gripper can detect the direction and scale of\nexternal collisions precisely.", "AI": {"tldr": "提出了一种基于视觉的传感模块，用于检测碰撞以保持软夹持系统的稳定抓取。", "motivation": "外部碰撞对机器人执行器抓取圆形物体构成风险，需要一种方法确保抓取的稳定性与安全性。", "method": "使用眼在手相机监测手指和抓取物体的运动，并开发了碰撞丰富的抓取策略。", "result": "实验验证了系统能即时响应碰撞，并能精确检测碰撞方向和大小。", "conclusion": "该系统能有效提升软夹持器在动态抓取中的稳定性和安全性。"}}
{"id": "2508.05104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05104", "abs": "https://arxiv.org/abs/2508.05104", "authors": ["Andrej Lúčny", "Matilde Antonj", "Carlo Mazzola", "Hana Hornáčková", "Ana Farić", "Kristína Malinovská", "Michal Vavrecka", "Igor Farkaš"], "title": "Examining the legibility of humanoid robot arm movements in a pointing task", "comment": "Published at ICSR 2025", "summary": "Human--robot interaction requires robots whose actions are legible, allowing\nhumans to interpret, predict, and feel safe around them. This study\ninvestigates the legibility of humanoid robot arm movements in a pointing task,\naiming to understand how humans predict robot intentions from truncated\nmovements and bodily cues. We designed an experiment using the NICO humanoid\nrobot, where participants observed its arm movements towards targets on a\ntouchscreen. Robot cues varied across conditions: gaze, pointing, and pointing\nwith congruent or incongruent gaze. Arm trajectories were stopped at 60\\% or\n80\\% of their full length, and participants predicted the final target. We\ntested the multimodal superiority and ocular primacy hypotheses, both of which\nwere supported by the experiment.", "AI": {"tldr": "研究探讨人形机器人手臂动作的可读性，通过实验验证了多模态优越性和视觉主导假设。", "motivation": "提升人机交互中机器人动作的可读性，使人类能更准确地预测机器人意图。", "method": "使用NICO人形机器人进行实验，观察参与者对不同线索（如目光、指向动作）的反应，手臂轨迹被截断至60%或80%。", "result": "实验支持多模态优越性和视觉主导假设。", "conclusion": "机器人的目光和指向动作结合能显著提高人类对其意图的预测准确性。"}}
{"id": "2508.05143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05143", "abs": "https://arxiv.org/abs/2508.05143", "authors": ["Siméon Capy", "Thomas M. Kwok", "Kevin Joseph", "Yuichiro Kawasumi", "Koichi Nagashima", "Tomoya Sasaki", "Yue Hu", "Eiichi Yoshida"], "title": "From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation", "comment": "Author preprint - Accepted for Humanoids 2025", "summary": "Robot teleoperation (RTo) has emerged as a viable alternative to local\ncontrol, particularly when human intervention is still necessary. This research\naims to study the distance effect on user perception in RTo, exploring the\npotential of teleoperated robots for older adult care. We propose an evaluation\nof non-expert users' perception of long-distance RTo, examining how their\nperception changes before and after interaction, as well as comparing it to\nthat of locally operated robots. We have designed a specific protocol\nconsisting of multiple questionnaires, along with a dedicated software\narchitecture using the Robotics Operating System (ROS) and Unity. The results\nrevealed no statistically significant differences between the local and remote\nrobot conditions, suggesting that robots may be a viable alternative to\ntraditional local control.", "AI": {"tldr": "研究探讨了远程操作机器人（RTo）在老年护理中的潜力，分析了距离对用户感知的影响，发现远程与本地操作无显著差异。", "motivation": "探索远程操作机器人作为本地控制替代方案的可行性，特别是在老年护理领域。", "method": "设计了包含问卷调查和专用软件架构（ROS和Unity）的协议，比较远程与本地操作的用户感知。", "result": "远程与本地操作的用户感知无显著差异，表明远程操作机器人是可行的替代方案。", "conclusion": "远程操作机器人可能成为老年护理中本地控制的有效替代方案。"}}
{"id": "2508.05148", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05148", "abs": "https://arxiv.org/abs/2508.05148", "authors": ["Francisco Munguia-Galeano", "Zhengxue Zhou", "Satheeshkumar Veeramani", "Hatem Fakhruldeen", "Louis Longley", "Rob Clowes", "Andrew I. Cooper"], "title": "Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories", "comment": null, "summary": "The integration of robotics and automation into self-driving laboratories\n(SDLs) can introduce additional safety complexities, in addition to those that\nalready apply to conventional research laboratories. Personal protective\nequipment (PPE) is an essential requirement for ensuring the safety and\nwell-being of workers in laboratories, self-driving or otherwise. Fires are\nanother important risk factor in chemical laboratories. In SDLs, fires that\noccur close to mobile robots, which use flammable lithium batteries, could have\nincreased severity. Here, we present Chemist Eye, a distributed safety\nmonitoring system designed to enhance situational awareness in SDLs. The system\nintegrates multiple stations equipped with RGB, depth, and infrared cameras,\ndesigned to monitor incidents in SDLs. Chemist Eye is also designed to spot\nworkers who have suffered a potential accident or medical emergency, PPE\ncompliance and fire hazards. To do this, Chemist Eye uses decision-making\ndriven by a vision-language model (VLM). Chemist Eye is designed for seamless\nintegration, enabling real-time communication with robots. Based on the VLM\nrecommendations, the system attempts to drive mobile robots away from potential\nfire locations, exits, or individuals not wearing PPE, and issues audible\nwarnings where necessary. It also integrates with third-party messaging\nplatforms to provide instant notifications to lab personnel. We tested Chemist\nEye with real-world data from an SDL equipped with three mobile robots and\nfound that the spotting of possible safety hazards and decision-making\nperformances reached 97 % and 95 %, respectively.", "AI": {"tldr": "Chemist Eye是一个分布式安全监控系统，用于提升自动驾驶实验室（SDL）的安全性，通过视觉-语言模型（VLM）实时监控危险并指导机器人行动。", "motivation": "自动驾驶实验室中，机器人使用易燃锂电池增加了火灾风险，同时PPE合规性和人员安全也是关键问题。需要一种系统来增强安全意识和应急响应。", "method": "系统整合RGB、深度和红外摄像头，利用VLM进行决策，实时监控危险并指导机器人远离危险区域，同时通过第三方平台通知人员。", "result": "在真实SDL环境中测试，Chemist Eye对安全危险的识别和决策性能分别达到97%和95%。", "conclusion": "Chemist Eye有效提升了SDL的安全性，为实验室自动化提供了可靠的安全监控解决方案。"}}
{"id": "2508.05153", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.6; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.05153", "abs": "https://arxiv.org/abs/2508.05153", "authors": ["Mohammed Daba", "Jing Qiu"], "title": "FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction", "comment": "7 pages, 3 figures, 1 table. Submitted to IEEE Robotics and\n  Automation Letters", "summary": "Category-level generalization for robotic garment manipulation, such as\nbimanual smoothing, remains a significant hurdle due to high dimensionality,\ncomplex dynamics, and intra-category variations. Current approaches often\nstruggle, either overfitting with concurrently learned visual features for a\nspecific instance or, despite category-level perceptual generalization, failing\nto predict the value of synergistic bimanual actions. We propose the\nFeature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point\nclouds to specifically enhance category-level policy generalization for garment\nsmoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,\nfrozen dense geometric features, ensuring robustness to intra-category garment\nvariations. Trainable downstream components then learn a task-specific policy\nusing these static features. In simulated GarmentLab experiments with the\nCLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.\nIt exhibited only an 11.5% efficiency drop (Steps80) on unseen garments\ncompared to 96.2% for a 2D image-based baseline, and achieved 89% final\ncoverage, outperforming an 83% coverage from a 3D correspondence-based baseline\nthat uses identical per-point geometric features but a fixed primitive. These\nresults highlight that the decoupling of geometric understanding from bimanual\naction value learning enables better category-level generalization.", "AI": {"tldr": "FCBV-Net利用预训练的几何特征提升机器人衣物操作的类别级泛化能力，显著优于现有方法。", "motivation": "解决机器人衣物操作中因高维性、复杂动态和类别内差异导致的泛化难题。", "method": "提出FCBV-Net，基于3D点云，利用冻结的几何特征预测双手动作价值，下游可训练组件学习任务策略。", "result": "在模拟实验中，FCBV-Net对未见衣物的效率下降仅11.5%，优于2D图像基线（96.2%），最终覆盖率达89%，优于3D基线（83%）。", "conclusion": "几何理解与双手动作价值学习的解耦能实现更好的类别级泛化。"}}
{"id": "2508.05186", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05186", "abs": "https://arxiv.org/abs/2508.05186", "authors": ["Yongjie Bai", "Zhouxia Wang", "Yang Liu", "Weixing Chen", "Ziliang Chen", "Mingtong Dai", "Yongsen Zheng", "Lingbo Liu", "Guanbin Li", "Liang Lin"], "title": "Learning to See and Act: Task-Aware View Planning for Robotic Manipulation", "comment": "7 pages, 9 figures, project page: https://hcplab-sysu.github.io/TAVP", "summary": "Recent vision-language-action (VLA) models for multi-task robotic\nmanipulation commonly rely on static viewpoints and shared visual encoders,\nwhich limit 3D perception and cause task interference, hindering robustness and\ngeneralization. In this work, we propose Task-Aware View Planning (TAVP), a\nframework designed to overcome these challenges by integrating active view\nplanning with task-specific representation learning. TAVP employs an efficient\nexploration policy, accelerated by a novel pseudo-environment, to actively\nacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)\nvisual encoder to disentangle features across different tasks, boosting both\nrepresentation fidelity and task generalization. By learning to see the world\nin a task-aware way, TAVP generates more complete and discriminative visual\nrepresentations, demonstrating significantly enhanced action prediction across\na wide array of manipulation challenges. Extensive experiments on RLBench tasks\nshow that our proposed TAVP model achieves superior performance over\nstate-of-the-art fixed-view approaches. Visual results and code are provided\nat: https://hcplab-sysu.github.io/TAVP.", "AI": {"tldr": "TAVP框架通过任务感知的视角规划和专家混合视觉编码器，提升了机器人操作的3D感知和任务泛化能力。", "motivation": "静态视角和共享视觉编码器限制了3D感知并导致任务干扰，影响了机器人操作的鲁棒性和泛化能力。", "method": "提出任务感知视角规划（TAVP），结合主动视角规划和任务特定表示学习，使用伪环境加速探索策略，并引入专家混合视觉编码器。", "result": "在RLBench任务上，TAVP显著优于固定视角方法，展示了更强的动作预测能力。", "conclusion": "TAVP通过任务感知的视觉表示学习，显著提升了机器人操作的性能和泛化能力。"}}
{"id": "2508.05208", "categories": ["cs.RO", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.05208", "abs": "https://arxiv.org/abs/2508.05208", "authors": ["Victor Ngo", "Rachel", "Ramchurn", "Roma Patel", "Alan Chamberlain", "Ayse Kucukyilmaz"], "title": "Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting", "comment": "published by Springer", "summary": "This paper presents an evaluation of 18 children's in-the-wild experiences\nwith the autonomous robot arm performer NED (Never-Ending Dancer) within the\nThingamabobas installation, showcased across the UK. We detail NED's design,\nincluding costume, behaviour, and human interactions, all integral to the\ninstallation. Our observational analysis revealed three key challenges in\nchild-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of\nrobot expressivity and reciprocity, and 3) Unmet expectations. Our findings\nshow that children are naturally curious, and adept at interacting with a\nrobotic art performer. However, our observations emphasise the critical need to\noptimise human-robot interaction (HRI) systems through careful consideration of\naudience's capabilities, perceptions, and expectations, within the performative\narts context, to enable engaging and meaningful experiences, especially for\nyoung audiences.", "AI": {"tldr": "评估儿童与自主机器人NED在艺术装置中的互动，发现三大挑战，强调优化人机交互以提升儿童体验。", "motivation": "研究儿童与机器人NED在艺术装置中的互动，探索如何优化人机交互以提升儿童参与度。", "method": "通过观察分析18名儿童与NED的互动，总结设计特点及互动挑战。", "result": "发现三大挑战：互动启动与维持、机器人表现力不足、期望未满足；儿童对机器人表现出好奇与适应能力。", "conclusion": "需优化人机交互系统，考虑观众能力与期望，以创造更有意义的艺术体验。"}}
{"id": "2508.05294", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05294", "abs": "https://arxiv.org/abs/2508.05294", "authors": ["Sahar Salimpour", "Lei Fu", "Farhad Keramat", "Leonardo Militano", "Giovanni Toffetti", "Harry Edelman", "Jorge Peña Queralta"], "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction", "comment": null, "summary": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (BLMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those words advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature.", "AI": {"tldr": "本文综述了基础模型（如LLMs、VLMs和VLAs/BLMs）在机器人自主性和人机交互中的应用，探讨了代理架构的发展及其在任务规划、感知和操作中的作用。", "motivation": "研究基础模型如何推动机器人系统的智能化和多功能化，特别是在代理架构中的应用。", "method": "通过文献综述和社区项目分析，提出模型集成方法的分类法，并对不同解决方案中代理的角色进行比较分析。", "result": "展示了基础模型在机器人领域的多样化应用，包括任务规划、API调用和诊断辅助等。", "conclusion": "基础模型和代理架构正在快速推动机器人技术的发展，未来需要进一步标准化和优化模型集成方法。"}}
{"id": "2508.05298", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05298", "abs": "https://arxiv.org/abs/2508.05298", "authors": ["Jian Gong", "Youwei Huang", "Bo Yuan", "Ming Zhu", "Juncheng Zhan", "Jinke Wang", "Hang Shu", "Mingyue Xiong", "Yanjun Ye", "Yufan Zu", "Yang Zhou", "Yihan Ding", "Xuannian Chen", "Xingyu Lu", "Runjie Ban", "Bingchao Huang", "Fusen Liu"], "title": "GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming", "comment": "17 pages, 5 figures, conference", "summary": "We present GhostShell, a novel approach that leverages Large Language Models\n(LLMs) to enable streaming and concurrent behavioral programming for embodied\nsystems. In contrast to conventional methods that rely on pre-scheduled action\nsequences or behavior trees, GhostShell drives embodied systems to act\non-the-fly by issuing function calls incrementally as tokens are streamed from\nthe LLM. GhostShell features a streaming XML function token parser, a dynamic\nfunction interface mapper, and a multi-channel scheduler that orchestrates\nintra-channel synchronous and inter-channel asynchronous function calls,\nthereby coordinating serial-parallel embodied actions across multiple robotic\ncomponents as directed by the LLM. We evaluate GhostShell on our robot\nprototype COCO through comprehensive grounded experiments across 34 real-world\ninteraction tasks and multiple LLMs. The results demonstrate that our approach\nachieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4\nSonnet and up to 66X faster response times compared to LLM native function\ncalling APIs. GhostShell also proves effective in long-horizon multimodal\ntasks, demonstrating strong robustness and generalization.", "AI": {"tldr": "GhostShell利用大型语言模型（LLMs）实现流式并发行为编程，通过动态函数调用和调度器协调机器人动作，显著提升响应速度和任务正确率。", "motivation": "传统方法依赖预定义动作序列或行为树，限制了机器人系统的灵活性和实时性。GhostShell旨在通过LLM的流式输出实现动态行为编程。", "method": "GhostShell包含流式XML函数标记解析器、动态函数接口映射器和多通道调度器，支持同步和异步函数调用，协调多机器人组件动作。", "result": "在34个真实世界交互任务中，GhostShell的行为正确率指标达到0.85，响应速度比原生LLM API快66倍，并在长时多模态任务中表现稳健。", "conclusion": "GhostShell为机器人系统提供了一种高效、灵活的行为编程方法，显著提升了实时性和任务完成能力。"}}
{"id": "2508.05342", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05342", "abs": "https://arxiv.org/abs/2508.05342", "authors": ["Shunlei Li", "Longsen Gao", "Jin Wang", "Chang Che", "Xi Xiao", "Jiuwen Cao", "Yingbai Hu", "Hamid Reza Karimi"], "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control", "comment": "Journal under review", "summary": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.", "AI": {"tldr": "GF-VLA框架通过信息论和场景图技术，使双臂机器人直接从人类演示中学习任务级推理和执行，表现出高准确性和泛化能力。", "motivation": "解决传统低级别轨迹模仿在跨对象类型、空间布局和操纵器配置时泛化能力不足的问题。", "method": "提取基于香农信息的任务相关手和物体线索，编码为时序场景图，结合语言条件变换器生成行为树和运动指令，并引入跨手选择策略。", "result": "在双臂块组装任务中，图准确率超95%，子任务分割达93%，执行成功率分别为抓取94%、放置89%、整体任务90%。", "conclusion": "GF-VLA在多样空间和语义变化中表现出强泛化能力和鲁棒性，为机器人学习提供了可靠框架。"}}
{"id": "2508.05359", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05359", "abs": "https://arxiv.org/abs/2508.05359", "authors": ["Morten Roed Frederiksen", "Kasper Støy"], "title": "Affecta-Context: The Context-Guided Behavior Adaptation Framework", "comment": "6 pages, Intelligent Autonomous Systems 18. IAS 2023", "summary": "This paper presents Affecta-context, a general framework to facilitate\nbehavior adaptation for social robots. The framework uses information about the\nphysical context to guide its behaviors in human-robot interactions. It\nconsists of two parts: one that represents encountered contexts and one that\nlearns to prioritize between behaviors through human-robot interactions. As\nphysical contexts are encountered the framework clusters them by their measured\nphysical properties. In each context, the framework learns to prioritize\nbetween behaviors to optimize the physical attributes of the robot's behavior\nin line with its current environment and the preferences of the users it\ninteracts with. This paper illlustrates the abilities of the Affecta-context\nframework by enabling a robot to autonomously learn the prioritization of\ndiscrete behaviors. This was achieved by training across 72 interactions in two\ndifferent physical contexts with 6 different human test participants. The paper\ndemonstrates the trained Affecta-context framework by verifying the robot's\nability to generalize over the input and to match its behaviors to a previously\nunvisited physical context.", "AI": {"tldr": "Affecta-context框架通过物理上下文信息指导社交机器人行为适应，包括上下文表示和行为优先级学习，通过实验验证其泛化能力。", "motivation": "为社交机器人提供一种基于物理上下文的行为适应框架，以优化其在不同环境和用户偏好下的行为表现。", "method": "框架分为两部分：上下文表示和行为优先级学习，通过聚类物理属性和交互学习实现行为优化。", "result": "在72次交互实验中，机器人成功学习行为优先级，并能泛化到未访问的物理上下文。", "conclusion": "Affecta-context框架有效支持社交机器人在不同环境中的行为适应和优化。"}}
{"id": "2508.05373", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05373", "abs": "https://arxiv.org/abs/2508.05373", "authors": ["Morten Roed Frederiksen", "Kasper Støy"], "title": "Robots can defuse high-intensity conflict situations", "comment": "7 pages, 6 figures, 2020 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) October 25-29, 2020, Las Vegas, NV, USA", "summary": "This paper investigates the specific scenario of high-intensity\nconfrontations between humans and robots, to understand how robots can defuse\nthe conflict. It focuses on the effectiveness of using five different affective\nexpression modalities as main drivers for defusing the conflict. The aim is to\ndiscover any strengths or weaknesses in using each modality to mitigate the\nhostility that people feel towards a poorly performing robot. The defusing of\nthe situation is accomplished by making the robot better at acknowledging the\nconflict and by letting it express remorse. To facilitate the tests, we used a\ncustom affective robot in a simulated conflict situation with 105 test\nparticipants. The results show that all tested expression modalities can\nsuccessfully be used to defuse the situation and convey an acknowledgment of\nthe confrontation. The ratings were remarkably similar, but the movement\nmodality was different (ANON p$<$.05) than the other modalities. The test\nparticipants also had similar affective interpretations on how impacted the\nrobot was of the confrontation across all expression modalities. This indicates\nthat defusing a high-intensity interaction may not demand special attention to\nthe expression abilities of the robot, but rather require attention to the\nabilities of being socially aware of the situation and reacting in accordance\nwith it.", "AI": {"tldr": "研究探讨了在人与机器人高强度对抗场景中，通过五种情感表达方式缓解冲突的效果，发现所有方式均有效，但运动方式略有不同。", "motivation": "理解机器人如何通过情感表达缓解人类对低性能机器人的敌意。", "method": "使用定制情感机器人在模拟冲突场景中测试105名参与者，评估五种表达方式的效果。", "result": "所有表达方式均能成功缓解冲突，运动方式与其他方式略有差异，参与者对各方式的感知相似。", "conclusion": "缓解高强度互动可能更需关注机器人的社会情境感知能力，而非特定表达方式。"}}
{"id": "2508.05396", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05396", "abs": "https://arxiv.org/abs/2508.05396", "authors": ["Yufei Duan", "Hang Yin", "Danica Kragic"], "title": "Real-Time Iteration Scheme for Diffusion Policy", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Diffusion Policies have demonstrated impressive performance in robotic\nmanipulation tasks. However, their long inference time, resulting from an\nextensive iterative denoising process, and the need to execute an action chunk\nbefore the next prediction to maintain consistent actions limit their\napplicability to latency-critical tasks or simple tasks with a short cycle\ntime. While recent methods explored distillation or alternative policy\nstructures to accelerate inference, these often demand additional training,\nwhich can be resource-intensive for large robotic models. In this paper, we\nintroduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a\nmethod from optimal control that accelerates optimization by leveraging\nsolutions from previous time steps as initial guesses for subsequent\niterations. We explore the application of this scheme in diffusion inference\nand propose a scaling-based method to effectively handle discrete actions, such\nas grasping, in robotic manipulation. The proposed scheme significantly reduces\nruntime computational costs without the need for distillation or policy\nredesign. This enables a seamless integration into many pre-trained\ndiffusion-based models, in particular, to resource-demanding large models. We\nalso provide theoretical conditions for the contractivity which could be useful\nfor estimating the initial denoising step. Quantitative results from extensive\nsimulation experiments show a substantial reduction in inference time, with\ncomparable overall performance compared with Diffusion Policy using full-step\ndenoising. Our project page with additional resources is available at:\nhttps://rti-dp.github.io/.", "AI": {"tldr": "本文提出了一种基于实时迭代（RTI）方案的新方法，显著降低了扩散策略的推理时间，无需额外训练或策略重新设计。", "motivation": "扩散策略在机器人操作任务中表现优异，但其推理时间过长且需要执行动作块以保持一致性，限制了其在延迟敏感任务中的应用。", "method": "采用实时迭代（RTI）方案，利用前一时间步的解作为初始猜测，并提出了处理离散动作的缩放方法。", "result": "实验表明，该方法显著降低了推理时间，同时保持了与全步去噪扩散策略相当的性能。", "conclusion": "该方法为资源密集型大型模型的扩散策略提供了高效推理方案，且无需额外训练。"}}
{"id": "2508.05402", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05402", "abs": "https://arxiv.org/abs/2508.05402", "authors": ["Rui Yu", "Xianghang Zhang", "Runkai Zhao", "Huaicheng Yan", "Meng Wang"], "title": "DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model", "comment": null, "summary": "End-to-end autonomous driving has been recently seen rapid development,\nexerting a profound influence on both industry and academia. However, the\nexisting work places excessive focus on ego-vehicle status as their sole\nlearning objectives and lacks of planning-oriented understanding, which limits\nthe robustness of the overall decision-making prcocess. In this work, we\nintroduce DistillDrive, an end-to-end knowledge distillation-based autonomous\ndriving model that leverages diversified instance imitation to enhance\nmulti-mode motion feature learning. Specifically, we employ a planning model\nbased on structured scene representations as the teacher model, leveraging its\ndiversified planning instances as multi-objective learning targets for the\nend-to-end model. Moreover, we incorporate reinforcement learning to enhance\nthe optimization of state-to-decision mappings, while utilizing generative\nmodeling to construct planning-oriented instances, fostering intricate\ninteractions within the latent space. We validate our model on the nuScenes and\nNAVSIM datasets, achieving a 50\\% reduction in collision rate and a 3-point\nimprovement in closed-loop performance compared to the baseline model. Code and\nmodel are publicly available at https://github.com/YuruiAI/DistillDrive", "AI": {"tldr": "DistillDrive是一种基于知识蒸馏的端到端自动驾驶模型，通过多样化实例模仿增强多模态运动特征学习，显著提升了决策过程的鲁棒性。", "motivation": "现有研究过于关注自车状态作为学习目标，缺乏规划导向的理解，限制了决策过程的鲁棒性。", "method": "采用基于结构化场景表示的规划模型作为教师模型，利用其多样化规划实例作为多目标学习目标；结合强化学习和生成建模优化状态到决策的映射。", "result": "在nuScenes和NAVSIM数据集上验证，碰撞率降低50%，闭环性能提升3分。", "conclusion": "DistillDrive通过知识蒸馏和多样化实例模仿，显著提升了自动驾驶决策的鲁棒性和性能。"}}
{"id": "2508.05410", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05410", "abs": "https://arxiv.org/abs/2508.05410", "authors": ["Manas Bhargava", "Takefumi Hiraki", "Malina Strugaru", "Michal Piovarci", "Chiara Daraio", "Daisuke Iwai", "Bernd Bickel"], "title": "Computational Design and Fabrication of Modular Robots with Untethered Control", "comment": null, "summary": "Natural organisms use distributed actuation via their musculoskeletal systems\nto adapt their gait for traversing diverse terrains or to morph their bodies to\nperform varied tasks. A longstanding challenge in the field of robotics is to\nmimic this extensive adaptability and range of motion. This has led humans to\ndevelop various soft robotic systems that emulate natural organisms. However,\nsuch systems are generally optimized for a single functionality, lack the\nability to change form or function on demand, or are often tethered to bulky\ncontrol systems. To address these challenges, we present our framework for\ndesigning and controlling robots that mimic nature's blueprint by utilizing\ndistributed actuation. We propose a novel building block that combines\n3D-printed bones with liquid crystal elastomer (LCE) muscles as lightweight\nactuators and enables the modular assembly of musculoskeletal robots. We\ndeveloped LCE rods that contract in response to infrared radiation, thereby\nachieving local and untethered control over the distributed network of bones,\nwhich in turn results in global deformation of the robot. Furthermore, to\ncapitalize on the extensive design space, we develop two computational tools:\none to optimize the robot's skeletal graph, enabling multiple target\ndeformations, and another to co-optimize the skeletal designs and control gaits\nto achieve target locomotion. We validate our system by building several robots\nthat show complex shape morphing, varying control schemes, and adaptability to\ntheir environment. Our system integrates advances in modular material building,\nuntethered and distributed control, and computational design to introduce a new\ngeneration of robots that brings us closer to the capabilities of living\norganisms.", "AI": {"tldr": "提出了一种模仿生物分布式驱动的机器人框架，结合3D打印骨骼和液晶弹性体肌肉，实现模块化组装和无线控制。", "motivation": "模仿生物体的适应性和运动范围，解决现有软机器人系统功能单一、无法动态改变形态或依赖笨重控制的问题。", "method": "利用3D打印骨骼和液晶弹性体（LCE）肌肉作为轻量化驱动器，开发红外响应LCE杆实现无线分布式控制，并设计计算工具优化骨骼结构和控制步态。", "result": "构建了多个机器人，展示复杂形态变化、多样化控制方案和环境适应性。", "conclusion": "通过模块化材料、无线分布式控制和计算设计的结合，推动了机器人技术向生物体能力的靠近。"}}
{"id": "2508.05415", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05415", "abs": "https://arxiv.org/abs/2508.05415", "authors": ["Alexander Fabisch", "Wadhah Zai El Amri", "Chandandeep Singh", "Nicolás Navarro-Guerrero"], "title": "Do Robots Really Need Anthropomorphic Hands?", "comment": null, "summary": "Human manipulation skills represent a pinnacle of their voluntary motor\nfunctions, requiring the coordination of many degrees of freedom and processing\nof high-dimensional sensor input to achieve such a high level of dexterity.\nThus, we set out to answer whether the human hand, with its associated\nbiomechanical properties, sensors, and control mechanisms, is an ideal that we\nshould strive for in robotics-do we really need anthropomorphic robotic hands?\n  This survey can help practitioners to make the trade-off between hand\ncomplexity and potential manipulation skills. We provide an overview of the\nhuman hand, a comparison of commercially available robotic and prosthetic\nhands, and a systematic review of hand mechanisms and skills that they are\ncapable of. This leads to follow-up questions. What is the minimum requirement\nfor mechanisms and sensors to implement most skills that a robot needs? What is\nmissing to reach human-level dexterity? Can we improve upon human dexterity?\n  Although complex five-fingered hands are often used as the ultimate goal for\nrobotic manipulators, they are not necessary for all tasks. We found that wrist\nflexibility and finger abduction/adduction are important for manipulation\ncapabilities. On the contrary, increasing the number of fingers, actuators, or\ndegrees of freedom is often not necessary. Three fingers are a good compromise\nbetween simplicity and dexterity. Non-anthropomorphic hand designs with two\nopposing pairs of fingers or human hands with six fingers can further increase\ndexterity, suggesting that the human hand may not be the optimum.", "AI": {"tldr": "该论文探讨了人类手部是否是机器人手的理想模型，并分析了机器人手的复杂性与操作技能之间的权衡。", "motivation": "研究人类手部的生物力学特性、传感器和控制机制是否应作为机器人手的理想目标，以及是否需要拟人化的机器人手。", "method": "通过综述人类手部特性、比较商用机器人手和假肢手，并系统回顾其机制和技能，分析机器人手的设计需求。", "result": "研究发现，手腕灵活性和手指外展/内收对操作能力更重要，而增加手指数量或自由度并非必需。非拟人化设计可能优于人类手部。", "conclusion": "人类手部并非机器人手的唯一理想模型，简化设计（如三指）或非拟人化设计（如两对对立手指）可能更高效。"}}
{"id": "2508.05535", "categories": ["cs.RO", "cs.CL", "cs.HC", "cs.LG", "cs.MA", "I.2.9; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.05535", "abs": "https://arxiv.org/abs/2508.05535", "authors": ["Albert Yu", "Chengshu Li", "Luca Macesanu", "Arnav Balaji", "Ruchira Ray", "Raymond Mooney", "Roberto Martín-Martín"], "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation", "comment": "Project website at https://robin-lab.cs.utexas.edu/MicoBot/", "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/.", "AI": {"tldr": "MICoBot是一个用于人机协作的混合主动对话系统，通过三层决策机制优化任务分配和协作策略，显著提升任务成功率和用户体验。", "motivation": "解决人机协作中因人类伙伴行为多样性和动态变化导致的沟通与任务分配挑战。", "method": "采用三层决策机制：元规划器制定协作策略，规划器基于机器人能力和人类可用性分配任务，执行器决定具体行动或对话。", "result": "在仿真和真实环境中验证，MICoBot显著优于纯LLM基线和其他任务分配模型。", "conclusion": "MICoBot通过动态调整协作策略和任务分配，有效提升了人机协作的效率和用户体验。"}}
{"id": "2508.05543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05543", "abs": "https://arxiv.org/abs/2508.05543", "authors": ["Wenbo Li", "Guanting Chen", "Tao Zhao", "Jiyao Wang", "Tianxin Hu", "Yuwen Liao", "Weixiang Guo", "Shenghai Yuan"], "title": "CleanUpBench: Embodied Sweeping and Grasping Benchmark", "comment": null, "summary": "Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,\nbut most target complex humanoid agents or large-scale simulations that are far\nfrom real-world deployment. In contrast, mobile cleaning robots with dual mode\ncapabilities, such as sweeping and grasping, are rapidly emerging as realistic\nand commercially viable platforms. However, no benchmark currently exists that\nsystematically evaluates these agents in structured, multi-target cleaning\ntasks, revealing a critical gap between academic research and real-world\napplications. We introduce CleanUpBench, a reproducible and extensible\nbenchmark for evaluating embodied agents in realistic indoor cleaning\nscenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service\nrobot equipped with a sweeping mechanism and a six-degree-of-freedom robotic\narm, enabling interaction with heterogeneous objects. The benchmark includes\nmanually designed environments and one procedurally generated layout to assess\ngeneralization, along with a comprehensive evaluation suite covering task\ncompletion, spatial efficiency, motion quality, and control performance. To\nsupport comparative studies, we provide baseline agents based on heuristic\nstrategies and map-based planning. CleanUpBench bridges the gap between\nlow-level skill evaluation and full-scene testing, offering a scalable testbed\nfor grounded, embodied intelligence in everyday settings.", "AI": {"tldr": "CleanUpBench是一个用于评估移动清洁机器人在真实室内清洁场景中的可扩展基准，填补了学术研究与实际应用之间的空白。", "motivation": "现有基准多针对复杂人形代理或大规模模拟，而缺乏对商业化可行的移动清洁机器人的系统评估。", "method": "基于NVIDIA Isaac Sim构建，模拟配备清扫机制和六自由度机械臂的移动服务机器人，评估任务完成、空间效率、运动质量和控制性能。", "result": "提供了基线代理和评估套件，支持比较研究，并测试了泛化能力。", "conclusion": "CleanUpBench为日常场景中的具体智能体提供了可扩展的测试平台。"}}
{"id": "2508.05584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05584", "abs": "https://arxiv.org/abs/2508.05584", "authors": ["Van Cuong Pham", "Minh Hai Tran", "Phuc Anh Nguyen", "Ngoc Son Vu", "Nga Nguyen Thi"], "title": "Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator", "comment": null, "summary": "This research proposes a robust adaptive fuzzy sliding mode control (AFSMC)\napproach to enhance the trajectory tracking performance of cylindrical robotic\nmanipulators, extensively utilized in applications such as CNC and 3D printing.\nThe proposed approach integrates fuzzy logic with sliding mode control (SMC) to\nbolster adaptability and robustness, with fuzzy logic approximating the\nuncertain dynamics of the system, while SMC ensures strong performance.\nSimulation results in MATLAB/Simulink demonstrate that AFSMC significantly\nimproves trajectory tracking accuracy, stability, and disturbance rejection\ncompared to traditional methods. This research underscores the effectiveness of\nAFSMC in controlling robotic manipulators, contributing to enhanced precision\nin industrial robotic applications.", "AI": {"tldr": "提出了一种鲁棒自适应模糊滑模控制（AFSMC）方法，用于提升圆柱形机器人机械臂的轨迹跟踪性能，结合模糊逻辑与滑模控制以提高适应性和鲁棒性。", "motivation": "圆柱形机器人机械臂在CNC和3D打印等应用中广泛使用，但传统方法在轨迹跟踪精度和抗干扰能力上存在不足。", "method": "将模糊逻辑与滑模控制结合，模糊逻辑用于近似系统的不确定性动态，滑模控制确保强性能。", "result": "MATLAB/Simulink仿真显示，AFSMC在轨迹跟踪精度、稳定性和抗干扰能力上显著优于传统方法。", "conclusion": "AFSMC在机器人机械臂控制中表现出高效性，有助于提升工业机器人应用的精度。"}}
{"id": "2508.05635", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05635", "abs": "https://arxiv.org/abs/2508.05635", "authors": ["Yue Liao", "Pengfei Zhou", "Siyuan Huang", "Donglin Yang", "Shengcong Chen", "Yuxin Jiang", "Yue Hu", "Jingbin Cai", "Si Liu", "Jianlan Luo", "Liliang Chen", "Shuicheng Yan", "Maoqing Yao", "Guanghui Ren"], "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation", "comment": "https://genie-envisioner.github.io/", "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly.", "AI": {"tldr": "Genie Envisioner (GE) 是一个统一的机器人操作平台，集成了策略学习、评估和仿真，通过视频生成框架实现。", "motivation": "为机器人操作提供一个统一的、可扩展的基础平台，支持指令驱动的通用智能体开发。", "method": "GE-Base 是一个大规模的视频扩散模型，GE-Act 通过轻量级解码器将潜在表示映射为可执行动作，GE-Sim 作为神经模拟器生成高保真仿真数据。", "result": "平台支持跨多样化的机器人实现精确且通用的策略推断，并通过 EWMBench 标准化评估套件衡量性能。", "conclusion": "Genie Envisioner 是一个可扩展且实用的基础平台，适用于指令驱动的通用智能体开发。"}}
