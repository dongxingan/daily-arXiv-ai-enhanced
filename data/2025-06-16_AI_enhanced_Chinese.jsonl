{"id": "2506.11234", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11234", "abs": "https://arxiv.org/abs/2506.11234", "authors": ["Luke Rowe", "Rodrigue de Schaetzen", "Roger Girgis", "Christopher Pal", "Liam Paull"], "title": "Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving", "comment": null, "summary": "We present Poutine, a 3B-parameter vision-language model (VLM) tailored for\nend-to-end autonomous driving in long-tail driving scenarios. Poutine is\ntrained in two stages. To obtain strong base driving capabilities, we train\nPoutine-Base in a self-supervised vision-language-trajectory (VLT) next-token\nprediction fashion on 83 hours of CoVLA nominal driving and 11 hours of Waymo\nlong-tail driving. Accompanying language annotations are auto-generated with a\n72B-parameter VLM. Poutine is obtained by fine-tuning Poutine-Base with Group\nRelative Policy Optimization (GRPO) using less than 500 preference-labeled\nframes from the Waymo validation set. We show that both VLT pretraining and RL\nfine-tuning are critical to attain strong driving performance in the long-tail.\nPoutine-Base achieves a rater-feedback score (RFS) of 8.12 on the validation\nset, nearly matching Waymo's expert ground-truth RFS. The final Poutine model\nachieves an RFS of 7.99 on the official Waymo test set, placing 1st in the 2025\nWaymo Vision-Based End-to-End Driving Challenge by a significant margin. These\nresults highlight the promise of scalable VLT pre-training and lightweight RL\nfine-tuning to enable robust and generalizable autonomy.", "AI": {"tldr": "Poutine是一个3B参数的视觉语言模型，专为长尾驾驶场景的端到端自动驾驶设计，通过两阶段训练实现高性能。", "motivation": "解决长尾驾驶场景中的自动驾驶问题，提升模型在复杂环境中的鲁棒性和泛化能力。", "method": "1. 自监督视觉-语言-轨迹（VLT）预训练；2. 使用GRPO进行轻量级强化学习微调。", "result": "Poutine-Base验证集RFS为8.12，接近专家水平；最终模型在Waymo测试集RFS为7.99，排名第一。", "conclusion": "VLT预训练和轻量级RL微调是实现鲁棒自动驾驶的有效方法。"}}
{"id": "2506.11261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11261", "abs": "https://arxiv.org/abs/2506.11261", "authors": ["Shizhe Chen", "Ricardo Garcia", "Paul Pacaud", "Cordelia Schmid"], "title": "Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation", "comment": null, "summary": "Robotic manipulation faces a significant challenge in generalizing across\nunseen objects, environments and tasks specified by diverse language\ninstructions. To improve generalization capabilities, recent research has\nincorporated large language models (LLMs) for planning and action execution.\nWhile promising, these methods often fall short in generating grounded plans in\nvisual environments. Although efforts have been made to perform visual\ninstructional tuning on LLMs for robotic manipulation, existing methods are\ntypically constrained by single-view image input and struggle with precise\nobject grounding. In this work, we introduce Gondola, a novel grounded\nvision-language planning model based on LLMs for generalizable robotic\nmanipulation. Gondola takes multi-view images and history plans to produce the\nnext action plan with interleaved texts and segmentation masks of target\nobjects and locations. To support the training of Gondola, we construct three\ntypes of datasets using the RLBench simulator, namely robot grounded planning,\nmulti-view referring expression and pseudo long-horizon task datasets. Gondola\noutperforms the state-of-the-art LLM-based method across all four\ngeneralization levels of the GemBench dataset, including novel placements,\nrigid objects, articulated objects and long-horizon tasks.", "AI": {"tldr": "Gondola是一种基于LLM的视觉语言规划模型，通过多视角图像和历史计划生成动作计划，提升机器人在未见对象、环境和任务中的泛化能力。", "motivation": "机器人操作在泛化到未见对象、环境和多样化语言指令任务时面临挑战，现有基于LLM的方法在视觉环境中生成接地计划的能力不足。", "method": "提出Gondola模型，利用多视角图像和历史计划生成包含文本和目标对象分割掩码的动作计划，并通过RLBench模拟器构建三类数据集进行训练。", "result": "Gondola在GemBench数据集的四个泛化级别（新放置、刚性对象、铰接对象和长时任务）上均优于现有LLM方法。", "conclusion": "Gondola通过多视角输入和接地规划显著提升了机器人操作的泛化能力。"}}
{"id": "2506.11262", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11262", "abs": "https://arxiv.org/abs/2506.11262", "authors": ["Shijie Fang", "Hang Yu", "Qidi Fang", "Reuben M. Aronson", "Elaine S. Short"], "title": "Demonstration Sidetracks: Categorizing Systematic Non-Optimality in Human Demonstrations", "comment": null, "summary": "Learning from Demonstration (LfD) is a popular approach for robots to acquire\nnew skills, but most LfD methods suffer from imperfections in human\ndemonstrations. Prior work typically treats these suboptimalities as random\nnoise. In this paper we study non-optimal behaviors in non-expert\ndemonstrations and show that they are systematic, forming what we call\ndemonstration sidetracks. Using a public space study with 40 participants\nperforming a long-horizon robot task, we recreated the setup in simulation and\nannotated all demonstrations. We identify four types of sidetracks\n(Exploration, Mistake, Alignment, Pause) and one control pattern (one-dimension\ncontrol). Sidetracks appear frequently across participants, and their temporal\nand spatial distribution is tied to task context. We also find that users'\ncontrol patterns depend on the control interface. These insights point to the\nneed for better models of suboptimal demonstrations to improve LfD algorithms\nand bridge the gap between lab training and real-world deployment. All\ndemonstrations, infrastructure, and annotations are available at\nhttps://github.com/AABL-Lab/Human-Demonstration-Sidetracks.", "AI": {"tldr": "论文研究了非专家演示中的非最优行为，发现这些行为是系统性的，称为“演示偏离”。通过实验识别了四种偏离类型，并指出其对LfD算法改进的重要性。", "motivation": "人类演示中的非最优行为通常被视为随机噪声，但本文发现这些行为是系统性的，需要更好的模型来改进LfD算法。", "method": "通过40名参与者的公共空间研究，在仿真中重现任务并标注演示，识别了四种偏离类型和一种控制模式。", "result": "发现偏离行为频繁出现且与任务上下文相关，控制模式依赖于控制接口。", "conclusion": "研究强调了改进非最优演示模型的必要性，以缩小实验室训练与实际部署之间的差距。"}}
{"id": "2506.11263", "categories": ["cs.RO", "cs.IT", "cs.NA", "cs.SY", "eess.SY", "math.IT", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.11263", "abs": "https://arxiv.org/abs/2506.11263", "authors": ["Christian Brommer", "Alessandro Fornasier", "Jan Steinbrener", "Stephan Weiss"], "title": "Sensor Model Identification via Simultaneous Model Selection and State Variable Determination", "comment": null, "summary": "We present a method for the unattended gray-box identification of sensor\nmodels commonly used by localization algorithms in the field of robotics. The\nobjective is to determine the most likely sensor model for a time series of\nunknown measurement data, given an extendable catalog of predefined sensor\nmodels. Sensor model definitions may require states for rigid-body calibrations\nand dedicated reference frames to replicate a measurement based on the robot's\nlocalization state. A health metric is introduced, which verifies the outcome\nof the selection process in order to detect false positives and facilitate\nreliable decision-making. In a second stage, an initial guess for identified\ncalibration states is generated, and the necessity of sensor world reference\nframes is evaluated. The identified sensor model with its parameter information\nis then used to parameterize and initialize a state estimation application,\nthus ensuring a more accurate and robust integration of new sensor elements.\nThis method is helpful for inexperienced users who want to identify the source\nand type of a measurement, sensor calibrations, or sensor reference frames. It\nwill also be important in the field of modular multi-agent scenarios and\nmodularized robotic platforms that are augmented by sensor modalities during\nruntime. Overall, this work aims to provide a simplified integration of sensor\nmodalities to downstream applications and circumvent common pitfalls in the\nusage and development of localization approaches.", "AI": {"tldr": "提出了一种用于机器人定位中传感器模型的无人值守灰盒识别方法，旨在从未知测量数据中确定最可能的传感器模型，并支持传感器校准和参考帧的评估。", "motivation": "简化传感器模态的集成，避免定位方法开发和使用的常见问题，特别适用于模块化多智能体场景和运行时扩展的机器人平台。", "method": "通过预定义传感器模型目录识别未知测量数据的传感器模型，引入健康指标验证选择结果，生成校准状态的初始猜测，并评估传感器世界参考帧的必要性。", "result": "识别出的传感器模型及其参数信息可用于状态估计应用，提高新传感器集成的准确性和鲁棒性。", "conclusion": "该方法为不熟悉传感器类型和校准的用户提供了便利，同时支持模块化机器人平台的传感器扩展，简化了下游应用的传感器集成。"}}
{"id": "2506.11264", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.11264", "abs": "https://arxiv.org/abs/2506.11264", "authors": ["Jiachen Li", "Chu Jian", "Feiyang Zhao", "Shihao Li", "Wei Li", "Dongmei Chen"], "title": "Robust Optimal Task Planning to Maximize Battery Life", "comment": null, "summary": "This paper proposes a control-oriented optimization platform for autonomous\nmobile robots (AMRs), focusing on extending battery life while ensuring task\ncompletion. The requirement of fast AMR task planning while maintaining minimum\nbattery state of charge, thus maximizing the battery life, renders a bilinear\noptimization problem. McCormick envelop technique is proposed to linearize the\nbilinear term. A novel planning algorithm with relaxed constraints is also\ndeveloped to handle parameter uncertainties robustly with high efficiency\nensured. Simulation results are provided to demonstrate the utility of the\nproposed methods in reducing battery degradation while satisfying task\ncompletion requirements.", "AI": {"tldr": "提出了一种面向控制的优化平台，旨在延长自主移动机器人（AMR）的电池寿命，同时确保任务完成。通过McCormick包络技术线性化双线性项，并开发了带松弛约束的新型规划算法以高效处理参数不确定性。", "motivation": "解决AMR在快速任务规划中如何保持最低电量状态以延长电池寿命的双线性优化问题。", "method": "采用McCormick包络技术线性化双线性项，并开发带松弛约束的规划算法以应对参数不确定性。", "result": "仿真结果表明，所提方法在减少电池退化的同时满足任务完成需求。", "conclusion": "该平台有效延长了AMR电池寿命，同时确保了任务的高效完成。"}}
{"id": "2506.11335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11335", "abs": "https://arxiv.org/abs/2506.11335", "authors": ["Levi Cai", "Youenn Jézéquel", "T. Aran Mooney", "Yogesh Girdhar"], "title": "Measuring and Minimizing Disturbance of Marine Animals to Underwater Vehicles", "comment": "Accepted to ISER 2025", "summary": "Do fish respond to the presence of underwater vehicles, potentially biasing\nour estimates about them? If so, are there strategies to measure and mitigate\nthis response? This work provides a theoretical and practical framework towards\nbias-free estimation of animal behavior from underwater vehicle observations.\nWe also provide preliminary results from the field in coral reef environments\nto address these questions.", "AI": {"tldr": "研究探讨鱼类是否对水下车辆存在反应，并提出测量和减少这种偏差的理论与实践框架。", "motivation": "了解鱼类对水下车辆的反应是否会影响行为估计，并寻求减少偏差的方法。", "method": "提出理论框架，并通过珊瑚礁环境的实地研究进行验证。", "result": "提供了初步的实地研究结果，支持鱼类对水下车辆存在反应的可能性。", "conclusion": "研究为减少水下车辆观测中的行为偏差提供了理论和实践基础。"}}
{"id": "2506.11387", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "93C85 (Primary), 93B52 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.11387", "abs": "https://arxiv.org/abs/2506.11387", "authors": ["Rongfei Li"], "title": "Control Architecture and Design for a Multi-robotic Visual Servoing System in Automated Manufacturing Environment", "comment": "272 pages, 171 figures, PhD dissertation, University of California,\n  Davis, 2025. To be published in ProQuest ETD", "summary": "The use of robotic technology has drastically increased in manufacturing in\nthe 21st century. But by utilizing their sensory cues, humans still outperform\nmachines, especially in micro scale manufacturing, which requires\nhigh-precision robot manipulators. These sensory cues naturally compensate for\nhigh levels of uncertainties that exist in the manufacturing environment.\nUncertainties in performing manufacturing tasks may come from measurement\nnoise, model inaccuracy, joint compliance (e.g., elasticity), etc. Although\nadvanced metrology sensors and high precision microprocessors, which are\nutilized in modern robots, have compensated for many structural and dynamic\nerrors in robot positioning, a well-designed control algorithm still works as a\ncomparable and cheaper alternative to reduce uncertainties in automated\nmanufacturing. Our work illustrates that a multi-robot control system that\nsimulates the positioning process for fastening and unfastening applications\ncan reduce various uncertainties, which may occur in this process, to a great\nextent. In addition, most research papers in visual servoing mainly focus on\ndeveloping control and observation architectures in various scenarios, but few\nhave discussed the importance of the camera's location in the configuration. In\na manufacturing environment, the quality of camera estimations may vary\nsignificantly from one observation location to another, as the combined effects\nof environmental conditions result in different noise levels of a single image\nshot at different locations. Therefore, in this paper, we also propose a novel\nalgorithm for the camera's moving policy so that it explores the camera\nworkspace and searches for the optimal location where the image noise level is\nminimized.", "AI": {"tldr": "论文探讨了在微尺度制造中，通过多机器人控制系统模拟定位过程以减少不确定性，并提出了一种优化相机位置的算法以最小化图像噪声。", "motivation": "人类在微尺度制造中仍优于机器人，主要依赖感官线索补偿环境不确定性。现有技术虽能部分解决机器人定位误差，但控制算法仍是一种经济有效的替代方案。此外，相机位置对视觉伺服质量影响显著，但相关研究较少。", "method": "提出多机器人控制系统模拟定位过程以减少不确定性，并设计算法优化相机位置以最小化图像噪声。", "result": "多机器人控制系统能显著减少制造过程中的不确定性，相机位置优化算法能有效降低图像噪声。", "conclusion": "多机器人控制系统和相机位置优化算法为自动化制造提供了高效且经济的解决方案，提升了制造精度和稳定性。"}}
{"id": "2506.11384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11384", "abs": "https://arxiv.org/abs/2506.11384", "authors": ["Hikaru Sasaki", "Naoto Komeno", "Takumi Hachimine", "Kei Takahashi", "Yu-ya Ohnishi", "Tetsunori Sugawara", "Araki Wakiuchi", "Miho Hatanaka", "Tomoyuki Miyao", "Hiroharu Ajiro", "Mikiya Fujii", "Takamitsu Matsubara"], "title": "Robotic System for Chemical Experiment Automation with Dual Demonstration of End-effector and Jig Operations", "comment": null, "summary": "While robotic automation has demonstrated remarkable performance, such as\nexecuting hundreds of experiments continuously over several days, it is\nchallenging to design a program that synchronizes the robot's movements with\nthe experimental jigs to conduct an experiment. We propose a concept that\nenables the automation of experiments by utilizing dual demonstrations of robot\nmotions and jig operations by chemists in an experimental environment\nconstructed to be controlled by a robot. To verify this concept, we developed a\nchemical-experiment-automation system consisting of jigs to assist the robot in\nexperiments, a motion-demonstration interface, a jig-control interface, and a\nmobile manipulator. We validate the concept through polymer-synthesis\nexperiments, focusing on critical liquid-handling tasks such as pipetting and\ndilution. The experimental results indicate high reproducibility of the\ndemonstrated motions and robust task-success rates. This comprehensive concept\nnot only simplifies the robot programming process for chemists but also\nprovides a flexible and efficient solution to accommodate a wide range of\nexperimental conditions, contributing significantly to the field of chemical\nexperiment automation.", "AI": {"tldr": "提出了一种通过化学家演示机器人动作和夹具操作来实现实验自动化的概念，验证了其高效性和灵活性。", "motivation": "解决机器人编程与实验夹具同步的挑战，简化化学实验自动化过程。", "method": "开发了包含辅助夹具、动作演示界面、夹具控制界面和移动机械臂的化学实验自动化系统。", "result": "在聚合物合成实验中验证了高重复性和任务成功率。", "conclusion": "该概念简化了机器人编程，为化学实验自动化提供了灵活高效的解决方案。"}}
{"id": "2506.11470", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11470", "abs": "https://arxiv.org/abs/2506.11470", "authors": ["Shunpeng Yang", "Zhen Fu", "Zhefeng Cao", "Guo Junde", "Patrick Wensing", "Wei Zhang", "Hua Chen"], "title": "Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion", "comment": "19 pages", "summary": "Generalizing locomotion policies across diverse legged robots with varying\nmorphologies is a key challenge due to differences in observation/action\ndimensions and system dynamics. In this work, we propose Multi-Loco, a novel\nunified framework combining a morphology-agnostic generative diffusion model\nwith a lightweight residual policy optimized via reinforcement learning (RL).\nThe diffusion model captures morphology-invariant locomotion patterns from\ndiverse cross-embodiment datasets, improving generalization and robustness. The\nresidual policy is shared across all embodiments and refines the actions\ngenerated by the diffusion model, enhancing task-aware performance and\nrobustness for real-world deployment. We evaluated our method with a rich\nlibrary of four legged robots in both simulation and real-world experiments.\nCompared to a standard RL framework with PPO, our approach -- replacing the\nGaussian policy with a diffusion model and residual term -- achieves a 10.35%\naverage return improvement, with gains up to 13.57% in wheeled-biped locomotion\ntasks. These results highlight the benefits of cross-embodiment data and\ncomposite generative architectures in learning robust, generalized locomotion\nskills.", "AI": {"tldr": "Multi-Loco框架通过结合扩散模型和残差策略，提升了多足机器人的运动泛化能力，平均性能提升10.35%。", "motivation": "解决不同形态机器人运动策略泛化问题，因观测/动作维度和系统动力学差异而具有挑战性。", "method": "提出Multi-Loco框架，结合形态无关的扩散模型和轻量级残差策略，扩散模型捕捉跨形态运动模式，残差策略优化动作。", "result": "在仿真和实验中，相比标准PPO框架，性能平均提升10.35%，轮式双足任务提升13.57%。", "conclusion": "跨形态数据和生成架构有助于学习鲁棒、泛化的运动技能。"}}
{"id": "2506.11526", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11526", "abs": "https://arxiv.org/abs/2506.11526", "authors": ["Yuan Gao", "Mattia Piccinini", "Yuchen Zhang", "Dingrui Wang", "Korbinian Moller", "Roberto Brusnicki", "Baha Zarrouki", "Alessio Gambi", "Jan Frederik Totz", "Kai Storms", "Steven Peters", "Andrea Stocco", "Bassam Alrifaee", "Marco Pavone", "Johannes Betz"], "title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis", "comment": null, "summary": "For autonomous vehicles, safe navigation in complex environments depends on\nhandling a broad range of diverse and rare driving scenarios. Simulation- and\nscenario-based testing have emerged as key approaches to development and\nvalidation of autonomous driving systems. Traditional scenario generation\nrelies on rule-based systems, knowledge-driven models, and data-driven\nsynthesis, often producing limited diversity and unrealistic safety-critical\ncases. With the emergence of foundation models, which represent a new\ngeneration of pre-trained, general-purpose AI models, developers can process\nheterogeneous inputs (e.g., natural language, sensor data, HD maps, and control\nactions), enabling the synthesis and interpretation of complex driving\nscenarios. In this paper, we conduct a survey about the application of\nfoundation models for scenario generation and scenario analysis in autonomous\ndriving (as of May 2025). Our survey presents a unified taxonomy that includes\nlarge language models, vision-language models, multimodal large language\nmodels, diffusion models, and world models for the generation and analysis of\nautonomous driving scenarios. In addition, we review the methodologies,\nopen-source datasets, simulation platforms, and benchmark challenges, and we\nexamine the evaluation metrics tailored explicitly to scenario generation and\nanalysis. Finally, the survey concludes by highlighting the open challenges and\nresearch questions, and outlining promising future research directions. All\nreviewed papers are listed in a continuously maintained repository, which\ncontains supplementary materials and is available at\nhttps://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.", "AI": {"tldr": "本文综述了基础模型在自动驾驶场景生成与分析中的应用，包括分类、方法、数据集、平台及挑战，并指出了未来研究方向。", "motivation": "自动驾驶在复杂环境中的安全导航需要处理多样且罕见的驾驶场景，传统方法生成场景的多样性和真实性有限，基础模型提供了新的解决方案。", "method": "通过统一分类法（如大语言模型、视觉语言模型等）综述基础模型在场景生成与分析中的应用，并回顾相关方法、数据集和评估指标。", "result": "提出了一个全面的分类框架，总结了现有方法、工具和挑战，并提供了开源资源库。", "conclusion": "基础模型在自动驾驶场景生成与分析中具有潜力，但仍面临开放挑战，未来研究需进一步探索。"}}
{"id": "2506.11570", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11570", "abs": "https://arxiv.org/abs/2506.11570", "authors": ["Jihao Li", "Keqi Zhu", "Guodong Lu", "I-Ming Chen", "Huixu Dong"], "title": "Construction of a Multiple-DOF Under-actuated Gripper with Force-Sensing via Deep Learning", "comment": null, "summary": "We present a novel under-actuated gripper with two 3-joint fingers, which\nrealizes force feedback control by the deep learning technique- Long Short-Term\nMemory (LSTM) model, without any force sensor. First, a five-linkage mechanism\nstacked by double four-linkages is designed as a finger to automatically\nachieve the transformation between parallel and enveloping grasping modes. This\nenables the creation of a low-cost under-actuated gripper comprising a single\nactuator and two 3-phalange fingers. Second, we devise theoretical models of\nkinematics and power transmission based on the proposed gripper, accurately\nobtaining fingertip positions and contact forces. Through coupling and\ndecoupling of five-linkage mechanisms, the proposed gripper offers the expected\ncapabilities of grasping payload/force/stability and objects with large\ndimension ranges. Third, to realize the force control, an LSTM model is\nproposed to determine the grasping mode for synthesizing force-feedback control\npolicies that exploit contact sensing after outlining the uncertainty of\ncurrents using a statistical method. Finally, a series of experiments are\nimplemented to measure quantitative indicators, such as the payload, grasping\nforce, force sensing, grasping stability and the dimension ranges of objects to\nbe grasped. Additionally, the grasping performance of the proposed gripper is\nverified experimentally to guarantee the high versatility and robustness of the\nproposed gripper.", "AI": {"tldr": "提出了一种新型欠驱动夹持器，采用双3关节手指和LSTM模型实现无传感器力反馈控制，具有低成本、高适应性和稳定性。", "motivation": "设计一种低成本、高适应性的欠驱动夹持器，无需力传感器即可实现力反馈控制，提升抓取性能。", "method": "1. 设计五连杆机构实现平行和包络抓取模式切换；2. 建立运动学和动力传输理论模型；3. 使用LSTM模型实现力控制。", "result": "实验验证了夹持器的负载能力、抓取力、力感知、稳定性及对大尺寸范围物体的适应性。", "conclusion": "该夹持器具有高通用性和鲁棒性，适用于多种抓取任务。"}}
{"id": "2506.11650", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11650", "abs": "https://arxiv.org/abs/2506.11650", "authors": ["Lambert Lee", "Joshua Lau"], "title": "Robot Context Protocol (RCP): A Runtime-Agnostic Interface for Agent-Aware Robot Control", "comment": null, "summary": "The Robot Context Protocol (RCP) is a lightweight, middleware-agnostic\ncommunication protocol designed to simplify the complexity of robotic systems\nand enable seamless interaction between robots, users, and autonomous agents.\nRCP provides a unified and semantically meaningful interface that decouples\nclient-facing operations from backend implementations, supporting a wide range\nof deployment environments including physical robots, cloud-based\norchestrators, and simulated platforms. Built on HTTP and WebSocket transport\nlayers, the protocol defines a schema-driven message format with structured\noperations such as read, write, execute, and subscribe. It integrates features\nsuch as runtime introspection, asynchronous feedback, multi-tenant namespace\nisolation, and strict type validation to ensure robustness, scalability, and\nsecurity. The architecture, message structure, interface model, and\nadapter-based backend integration strategy of RCP are described, along with\ndeployment practices and applicability across industries including\nmanufacturing, logistics, and healthcare. RCP enables intelligent, resilient,\nand safe robotic operations in complex, multi-agent ecosystems.", "AI": {"tldr": "RCP是一种轻量级、中间件无关的通信协议，旨在简化机器人系统的复杂性，支持机器人、用户和自主代理之间的无缝交互。", "motivation": "解决机器人系统中通信复杂性和异构性问题，提供统一接口。", "method": "基于HTTP和WebSocket，定义结构化消息格式（如读、写、执行、订阅），支持运行时自省、异步反馈等功能。", "result": "实现了鲁棒性、可扩展性和安全性，适用于多行业（如制造、物流、医疗）。", "conclusion": "RCP为复杂多智能体生态系统提供了智能、弹性和安全的机器人操作解决方案。"}}
{"id": "2506.11723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11723", "abs": "https://arxiv.org/abs/2506.11723", "authors": ["Ziren Xiao", "Ruxin Xiao", "Chang Liu", "Xinheng Wang"], "title": "Dynamic Collaborative Material Distribution System for Intelligent Robots In Smart Manufacturing", "comment": null, "summary": "The collaboration and interaction of multiple robots have become integral\naspects of smart manufacturing. Effective planning and management play a\ncrucial role in achieving energy savings and minimising overall costs. This\npaper addresses the real-time Dynamic Multiple Sources to Single Destination\n(DMS-SD) navigation problem, particularly with a material distribution case for\nmultiple intelligent robots in smart manufacturing. Enumerated solutions, such\nas in \\cite{xiao2022efficient}, tackle the problem by generating as many\noptimal or near-optimal solutions as possible but do not learn patterns from\nthe previous experience, whereas the method in \\cite{xiao2023collaborative}\nonly uses limited information from the earlier trajectories. Consequently,\nthese methods may take a considerable amount of time to compute results on\nlarge maps, rendering real-time operations impractical. To overcome this\nchallenge, we propose a lightweight Deep Reinforcement Learning (DRL) method to\naddress the DMS-SD problem. The proposed DRL method can be efficiently trained\nand rapidly converges to the optimal solution using the designed target-guided\nreward function. A well-trained DRL model significantly reduces the computation\ntime for the next movement to a millisecond level, which improves the time up\nto 100 times in our experiments compared to the enumerated solutions. Moreover,\nthe trained DRL model can be easily deployed on lightweight devices in smart\nmanufacturing, such as Internet of Things devices and mobile phones, which only\nrequire limited computational resources.", "AI": {"tldr": "本文提出了一种轻量级深度强化学习（DRL）方法，用于解决智能制造中多机器人动态导航问题，显著提升了计算效率。", "motivation": "现有方法在解决动态多源单目标（DMS-SD）导航问题时，要么无法从历史经验中学习，要么仅利用有限信息，导致计算时间长，难以实时操作。", "method": "采用轻量级DRL方法，设计了目标导向的奖励函数，快速收敛至最优解。", "result": "实验表明，该方法将计算时间缩短至毫秒级，比枚举方法快100倍，且适用于轻量级设备。", "conclusion": "提出的DRL方法高效解决了DMS-SD问题，适用于智能制造的实时操作和资源受限设备。"}}
{"id": "2506.11748", "categories": ["cs.RO", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.11748", "abs": "https://arxiv.org/abs/2506.11748", "authors": ["Federico Zocco", "Monica Malvezzi"], "title": "CIRO7.2: A Material Network with Circularity of -7.2 and Reinforcement-Learning-Controlled Robotic Disassembler", "comment": "To be submitted", "summary": "The competition over natural reserves of minerals is expected to increase in\npart because of the linear-economy paradigm based on take-make-dispose.\nSimultaneously, the linear economy considers end-of-use products as waste\nrather than as a resource, which results in large volumes of waste whose\nmanagement remains an unsolved problem. Since a transition to a circular\neconomy can mitigate these open issues, in this paper we begin by enhancing the\nnotion of circularity based on compartmental dynamical thermodynamics, namely,\n$\\lambda$, and then, we model a thermodynamical material network processing a\nbatch of 2 solid materials of criticality coefficients of 0.1 and 0.95, with a\nrobotic disassembler compartment controlled via reinforcement learning (RL),\nand processing 2-7 kg of materials. Subsequently, we focused on the design of\nthe robotic disassembler compartment using state-of-the-art RL algorithms and\nassessing the algorithm performance with respect to $\\lambda$ (Fig. 1). The\nhighest circularity is -2.1 achieved in the case of disassembling 2 parts of 1\nkg each, whereas it reduces to -7.2 in the case of disassembling 4 parts of 1\nkg each contained inside a chassis of 3 kg. Finally, a sensitivity analysis\nhighlighted that the impact on $\\lambda$ of the performance of an RL controller\nhas a positive correlation with the quantity and the criticality of the\nmaterials to be disassembled. This work also gives the principles of the\nemerging research fields indicated as circular intelligence and robotics\n(CIRO). Source code is publicly available.", "AI": {"tldr": "论文提出基于热力学动态分区的循环经济模型，结合强化学习控制的机器人拆卸器，分析了材料关键性和数量对循环性的影响。", "motivation": "线性经济模式导致资源浪费和未解决的废弃物管理问题，转向循环经济可缓解这些问题。", "method": "基于热力学动态分区（λ）建模材料网络，使用强化学习算法设计机器人拆卸器，并评估其性能。", "result": "最高循环性为-2.1（拆卸2个1kg部件），最低为-7.2（拆卸4个1kg部件加3kg外壳）。RL控制器性能与材料关键性和数量正相关。", "conclusion": "研究为循环智能与机器人（CIRO）领域奠定基础，开源代码可供进一步研究。"}}
{"id": "2506.11775", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11775", "abs": "https://arxiv.org/abs/2506.11775", "authors": ["Zilin Si", "Jose Enrique Chen", "M. Emre Karagozler", "Antonia Bronars", "Jonathan Hutchinson", "Thomas Lampe", "Nimrod Gileadi", "Taylor Howell", "Stefano Saliceti", "Lukasz Barczyk", "Ilan Olivarez Correa", "Tom Erez", "Mohit Shridhar", "Murilo Fernandes Martins", "Konstantinos Bousmalis", "Nicolas Heess", "Francesco Nori", "Maria Bauza Villalonga"], "title": "ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations", "comment": null, "summary": "Recent advancements in teleoperation systems have enabled high-quality data\ncollection for robotic manipulators, showing impressive results in learning\nmanipulation at scale. This progress suggests that extending these capabilities\nto robotic hands could unlock an even broader range of manipulation skills,\nespecially if we could achieve the same level of dexterity that human hands\nexhibit. However, teleoperating robotic hands is far from a solved problem, as\nit presents a significant challenge due to the high degrees of freedom of\nrobotic hands and the complex dynamics occurring during contact-rich settings.\nIn this work, we present ExoStart, a general and scalable learning framework\nthat leverages human dexterity to improve robotic hand control. In particular,\nwe obtain high-quality data by collecting direct demonstrations without a robot\nin the loop using a sensorized low-cost wearable exoskeleton, capturing the\nrich behaviors that humans can demonstrate with their own hands. We also\npropose a simulation-based dynamics filter that generates dynamically feasible\ntrajectories from the collected demonstrations and use the generated\ntrajectories to bootstrap an auto-curriculum reinforcement learning method that\nrelies only on simple sparse rewards. The ExoStart pipeline is generalizable\nand yields robust policies that transfer zero-shot to the real robot. Our\nresults demonstrate that ExoStart can generate dexterous real-world hand\nskills, achieving a success rate above 50% on a wide range of complex tasks\nsuch as opening an AirPods case or inserting and turning a key in a lock. More\ndetails and videos can be found in https://sites.google.com/view/exostart.", "AI": {"tldr": "ExoStart是一个利用人类灵巧性改进机器人手控制的通用学习框架，通过低成本可穿戴外骨骼收集高质量数据，并结合仿真动态过滤器与强化学习，实现零样本迁移到真实机器人。", "motivation": "扩展机器人手的操作能力，尤其是实现类似人类手的灵巧性，但现有远程操作系统难以应对高自由度和复杂动态的挑战。", "method": "使用传感器化低成本可穿戴外骨骼收集直接演示数据，提出仿真动态过滤器生成动态可行轨迹，并通过稀疏奖励的自适应强化学习方法训练策略。", "result": "ExoStart能生成灵巧的机器人手技能，在复杂任务（如打开AirPods盒或插入钥匙）中成功率超过50%。", "conclusion": "ExoStart框架具有通用性和可扩展性，能有效提升机器人手的操作能力，并实现零样本迁移。"}}
{"id": "2506.11827", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.11827", "abs": "https://arxiv.org/abs/2506.11827", "authors": ["Saitarun Nadipineni", "Chapa Sirithunge", "Yue Xie", "Fumiya Iida", "Thilina Dulantha Lalitharatne"], "title": "Auditory-Tactile Congruence for Synthesis of Adaptive Pain Expressions in RoboPatients", "comment": "17 pages, 9 figures, journal", "summary": "Misdiagnosis can lead to delayed treatments and harm. Robotic patients offer\na controlled way to train and evaluate clinicians in rare, subtle, or complex\ncases, reducing diagnostic errors. We present RoboPatient, a medical robotic\nsimulator aimed at multimodal pain synthesis based on haptic and auditory\nfeedback during palpation-based training scenarios. The robopatient functions\nas an adaptive intermediary, capable of synthesizing plausible pain expressions\nvocal and facial in response to tactile stimuli generated during palpation.\nUsing an abdominal phantom, robopatient captures and processes haptic input via\nan internal palpation-to-pain mapping model. To evaluate perceptual congruence\nbetween palpation and the corresponding auditory output, we conducted a study\ninvolving 7680 trials across 20 participants, where they evaluated pain\nintensity through sound. Results show that amplitude and pitch significantly\ninfluence agreement with the robot's pain expressions, irrespective of pain\nsounds. Stronger palpation forces elicited stronger agreement, aligning with\npsychophysical patterns. The study revealed two key dimensions: pitch and\namplitude are central to how people perceive pain sounds, with pitch being the\nmost influential cue. These acoustic features shape how well the sound matches\nthe applied force during palpation, impacting perceived realism. This approach\nlays the groundwork for high-fidelity robotic patients in clinical education\nand diagnostic simulation.", "AI": {"tldr": "RoboPatient是一种医疗机器人模拟器，通过触觉和听觉反馈模拟疼痛表达，用于临床培训和诊断模拟。", "motivation": "减少因误诊导致的治疗延误和伤害，通过机器人患者提供可控的训练和评估方式。", "method": "利用腹部模型和触觉-疼痛映射模型，通过触觉输入生成多模态疼痛表达，并通过声音评估感知一致性。", "result": "振幅和音高显著影响对疼痛表达的认同，触觉力量与声音感知一致，音高是最关键的影响因素。", "conclusion": "该方法为临床教育和诊断模拟中高保真机器人患者奠定了基础。"}}
{"id": "2506.11829", "categories": ["cs.RO", "cs.HC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2506.11829", "abs": "https://arxiv.org/abs/2506.11829", "authors": ["Ana Müller", "Anja Richert"], "title": "The Space Between Us: A Methodological Framework for Researching Bonding and Proxemics in Situated Group-Agent Interactions", "comment": "Accepted for presentation at the Workshop on Advancing Group\n  Understanding and Robots' Adaptive Behavior (GROUND), held at the Intelligent\n  Autonomous Systems (IAS) Conference 2025, Genoa, Italy", "summary": "This paper introduces a multimethod framework for studying spatial and social\ndynamics in real-world group-agent interactions with socially interactive\nagents. Drawing on proxemics and bonding theories, the method combines\nsubjective self-reports and objective spatial tracking. Applied in two field\nstudies in a museum (N = 187) with a robot and a virtual agent, the paper\naddresses the challenges in aligning human perception and behavior. We focus on\npresenting an open source, scalable, and field-tested toolkit for future\nstudies.", "AI": {"tldr": "本文提出了一种多方法框架，用于研究现实世界中人与社交互动代理的空间和社交动态，结合了主观自我报告和客观空间追踪。", "motivation": "解决人类感知与行为一致性的挑战，并为未来研究提供工具。", "method": "结合近体学和联结理论，使用主观自我报告和客观空间追踪方法，在博物馆进行了两项实地研究。", "result": "开发了一个开源、可扩展且经过实地测试的工具包。", "conclusion": "该框架为未来研究提供了实用工具，并展示了其在现实场景中的应用潜力。"}}
{"id": "2506.11842", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11842", "abs": "https://arxiv.org/abs/2506.11842", "authors": ["Zhipeng Bao", "Qianwen Li"], "title": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated Driving Systems", "comment": "10 figures,29 pages, one colummn", "summary": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving.", "AI": {"tldr": "PACE-ADS是一个基于心理学和认知的自动驾驶框架，通过三个智能体（驾驶员、心理学家和协调者）实现与乘员的双向交互，提升舒适度和信任感。", "motivation": "当前自动驾驶车辆缺乏与乘员的有效双向沟通，限制了个人化和故障恢复能力，影响舒适度和信任感，阻碍广泛采用。", "method": "PACE-ADS包含三个基于基础模型的智能体：驾驶员智能体分析驾驶环境，心理学家智能体解读乘员心理信号和认知指令，协调者智能体整合输入并生成行为决策。", "result": "仿真结果显示，PACE-ADS能根据乘员状态调整驾驶风格，提升舒适度，并通过自主推理或人工指导安全恢复故障。", "conclusion": "PACE-ADS展示了基于LLM的框架在弥合机器自主性和以人为本驾驶之间差距的潜力。"}}
{"id": "2506.11906", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11906", "abs": "https://arxiv.org/abs/2506.11906", "authors": ["Chapa Sirithunge", "Yue Xie", "Saitarun Nadipineni", "Fumiya Iida", "Thilina Dulantha Lalitharatne"], "title": "Palpation Alters Auditory Pain Expressions with Gender-Specific Variations in Robopatients", "comment": "11 pages, 9 figures, journal", "summary": "Diagnostic errors remain a major cause of preventable deaths, particularly in\nresource-limited regions. Medical training simulators, including robopatients,\nplay a vital role in reducing these errors by mimicking real patients for\nprocedural training such as palpation. However, generating multimodal feedback,\nespecially auditory pain expressions, remains challenging due to the complex\nrelationship between palpation behavior and sound. The high-dimensional nature\nof pain sounds makes exploration challenging with conventional methods. This\nstudy introduces a novel experimental paradigm for pain expressivity in\nrobopatients where they dynamically generate auditory pain expressions in\nresponse to palpation force, by co-optimizing human feedback using machine\nlearning. Using Proximal Policy Optimization (PPO), a reinforcement learning\n(RL) technique optimized for continuous adaptation, our robot iteratively\nrefines pain sounds based on real-time human feedback. This robot initializes\nrandomized pain responses to palpation forces, and the RL agent learns to\nadjust these sounds to align with human preferences. The results demonstrated\nthat the system adapts to an individual's palpation forces and sound\npreferences and captures a broad spectrum of pain intensity, from mild\ndiscomfort to acute distress, through RL-guided exploration of the auditory\npain space. The study further showed that pain sound perception exhibits\nsaturation at lower forces with gender specific thresholds. These findings\nhighlight the system's potential to enhance abdominal palpation training by\noffering a controllable and immersive simulation platform.", "AI": {"tldr": "论文提出了一种基于强化学习的机器人患者系统，通过动态生成听觉疼痛反馈来模拟真实患者的触诊训练，优化了疼痛声音与人类偏好的匹配。", "motivation": "诊断错误是导致可预防死亡的主要原因之一，尤其是在资源有限的地区。现有的医疗培训模拟器在生成多模态反馈（如听觉疼痛表达）方面存在挑战。", "method": "研究采用近端策略优化（PPO）强化学习技术，通过实时人类反馈迭代优化机器人患者的疼痛声音生成。", "result": "系统能够根据个体触诊力和声音偏好动态调整疼痛反馈，并捕捉从轻度不适到急性痛苦的广泛疼痛强度。", "conclusion": "该系统为腹部触诊训练提供了一个可控且沉浸式的模拟平台，具有显著的应用潜力。"}}
{"id": "2506.11916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11916", "abs": "https://arxiv.org/abs/2506.11916", "authors": ["Elvis Nava", "Victoriano Montesinos", "Erik Bauer", "Benedek Forrai", "Jonas Pai", "Stefan Weirich", "Stephan-Daniel Gravert", "Philipp Wand", "Stephan Polinski", "Benjamin F. Grewe", "Robert K. Katzschmann"], "title": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity", "comment": null, "summary": "We present a diffusion-based model recipe for real-world control of a highly\ndexterous humanoid robotic hand, designed for sample-efficient learning and\nsmooth fine-motor action inference. Our system features a newly designed 16-DoF\ntendon-driven hand, equipped with wide angle wrist cameras and mounted on a\nFranka Emika Panda arm. We develop a versatile teleoperation pipeline and data\ncollection protocol using both glove-based and VR interfaces, enabling\nhigh-quality data collection across diverse tasks such as pick and place, item\nsorting and assembly insertion. Leveraging high-frequency generative control,\nwe train end-to-end policies from raw sensory inputs, enabling smooth,\nself-correcting motions in complex manipulation scenarios. Real-world\nevaluations demonstrate up to 93.3% out of distribution success rates, with up\nto a +33.3% performance boost due to emergent self-correcting behaviors, while\nalso revealing scaling trends in policy performance. Our results advance the\nstate-of-the-art in dexterous robotic manipulation through a fully integrated,\npractical approach to hardware, learning, and real-world deployment.", "AI": {"tldr": "提出了一种基于扩散模型的高效学习方法，用于控制16自由度的人形机器人手，实现了93.3%的成功率和33.3%的性能提升。", "motivation": "解决高自由度机器人手的实时控制和精细动作推断问题，提高样本效率和任务成功率。", "method": "设计了16自由度的肌腱驱动手，结合手腕摄像头和Franka Emika Panda机械臂，开发了多功能的远程操作和数据收集流程，利用高频生成控制训练端到端策略。", "result": "在复杂操作场景中实现了93.3%的成功率，性能提升33.3%，并展示了策略性能的扩展趋势。", "conclusion": "通过硬件、学习和实际部署的集成方法，推动了灵巧机器人操作的前沿技术。"}}
{"id": "2506.11948", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11948", "abs": "https://arxiv.org/abs/2506.11948", "authors": ["Nadun Ranawaka Arachchige", "Zhenyang Chen", "Wonsuhk Jung", "Woo Chul Shin", "Rohan Bansal", "Pierre Barroso", "Yu Hang He", "Yingyang Celine Lin", "Benjamin Joffe", "Shreyas Kousik", "Danfei Xu"], "title": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies", "comment": "The first two authors contributed equally", "summary": "Offline Imitation Learning (IL) methods such as Behavior Cloning are\neffective at acquiring complex robotic manipulation skills. However, existing\nIL-trained policies are confined to executing the task at the same speed as\nshown in demonstration data. This limits the task throughput of a robotic\nsystem, a critical requirement for applications such as industrial automation.\nIn this paper, we introduce and formalize the novel problem of enabling\nfaster-than-demonstration execution of visuomotor policies and identify\nfundamental challenges in robot dynamics and state-action distribution shifts.\nWe instantiate the key insights as SAIL (Speed Adaptation for Imitation\nLearning), a full-stack system integrating four tightly-connected components:\n(1) a consistency-preserving action inference algorithm for smooth motion at\nhigh speed, (2) high-fidelity tracking of controller-invariant motion targets,\n(3) adaptive speed modulation that dynamically adjusts execution speed based on\nmotion complexity, and (4) action scheduling to handle real-world system\nlatencies. Experiments on 12 tasks across simulation and two real, distinct\nrobot platforms show that SAIL achieves up to a 4x speedup over demonstration\nspeed in simulation and up to 3.2x speedup in the real world. Additional detail\nis available at https://nadunranawaka1.github.io/sail-policy", "AI": {"tldr": "SAIL系统通过一致性动作推断、高保真跟踪、自适应速度调节和动作调度，实现了离线模仿学习策略的快速执行，实验显示仿真中速度提升4倍，实际机器人中提升3.2倍。", "motivation": "现有离线模仿学习策略的执行速度受限于演示数据，限制了机器人系统的任务吞吐量，尤其在工业自动化等应用中。", "method": "SAIL系统包含四个组件：一致性动作推断算法、高保真跟踪、自适应速度调节和动作调度。", "result": "在仿真和实际机器人平台上，SAIL实现了最高4倍和3.2倍的速度提升。", "conclusion": "SAIL为离线模仿学习策略的快速执行提供了有效解决方案，提升了任务吞吐量。"}}
