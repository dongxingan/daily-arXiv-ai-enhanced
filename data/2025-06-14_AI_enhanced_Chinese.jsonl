{"id": "2506.10106", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10106", "abs": "https://arxiv.org/abs/2506.10106", "authors": ["Marcos Abel Zuzuárregui", "Mustafa Melih Toslak", "Stefano Carpin"], "title": "One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture", "comment": "Accepted to International Federation of Automatic Control (IFAC)\n  Sensing, Control and Automation Technologies for Agriculture - 8th\n  AGRICONTROL 2025", "summary": "Artificial intelligence is transforming precision agriculture, offering\nfarmers new tools to streamline their daily operations. While these\ntechnological advances promise increased efficiency, they often introduce\nadditional complexity and steep learning curves that are particularly\nchallenging for non-technical users who must balance tech adoption with\nexisting workloads. In this paper, we present a natural language (NL) robotic\nmission planner that enables non-specialists to control heterogeneous robots\nthrough a common interface. By leveraging large language models (LLMs) and\npredefined primitives, our architecture seamlessly translates human language\ninto intermediate descriptions that can be executed by different robotic\nplatforms. With this system, users can formulate complex agricultural missions\nwithout writing any code. In the work presented in this paper, we extend our\nprevious system tailored for wheeled robot mission planning through a new class\nof experiments involving robotic manipulation and computer vision tasks. Our\nresults demonstrate that the architecture is both general enough to support a\ndiverse set of robots and powerful enough to execute complex mission requests.\nThis work represents a significant step toward making robotic automation in\nprecision agriculture more accessible to non-technical users.", "AI": {"tldr": "论文提出了一种基于自然语言的机器人任务规划器，帮助非技术用户通过简单界面控制异构机器人，利用大语言模型和预定义原语实现复杂农业任务的无代码操作。", "motivation": "人工智能在精准农业中的应用虽提高了效率，但也增加了复杂性，非技术用户难以适应。本文旨在通过自然语言界面降低技术门槛。", "method": "采用大语言模型（LLMs）和预定义原语，将人类语言转化为中间描述，供不同机器人平台执行。", "result": "系统支持异构机器人执行复杂任务，实验验证了其在机器人操作和计算机视觉任务中的通用性和强大功能。", "conclusion": "该研究为非技术用户提供了更易用的机器人自动化工具，推动了精准农业的普及。"}}
{"id": "2506.10172", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10172", "abs": "https://arxiv.org/abs/2506.10172", "authors": ["Yicheng Duan", "Kaiyu tang"], "title": "A Navigation Framework Utilizing Vision-Language Models", "comment": null, "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied\nAI, requiring agents to interpret natural language instructions and navigate\nthrough visually rich, unfamiliar environments. Recent advances in large\nvision-language models (LVLMs), such as CLIP and Flamingo, have significantly\nimproved multimodal understanding but introduced new challenges related to\ncomputational cost and real-time deployment. In this project, we propose a\nmodular, plug-and-play navigation framework that decouples vision-language\nunderstanding from action planning. By integrating a frozen vision-language\nmodel, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to\nachieve flexible, fast, and adaptable navigation without extensive model\nfine-tuning. Our framework leverages prompt engineering, structured history\nmanagement, and a two-frame visual input strategy to enhance decision-making\ncontinuity across navigation steps. We evaluate our system on the Room-to-Room\nbenchmark within the VLN-CE setting using the Matterport3D dataset and\nHabitat-Lab simulation environment. Although our initial results reveal\nchallenges in generalizing to unseen environments under strict evaluation\nsettings, our modular approach lays a foundation for scalable and efficient\nnavigation systems, highlighting promising directions for future improvement\nthrough enhanced environmental priors and expanded multimodal input\nintegration.", "AI": {"tldr": "提出了一种模块化的导航框架，将视觉语言理解与动作规划解耦，结合轻量级规划逻辑，实现快速灵活的导航。", "motivation": "解决视觉语言导航（VLN）中计算成本高和实时部署的挑战，同时提升多模态理解能力。", "method": "使用冻结的视觉语言模型Qwen2.5-VL-7B-Instruct，结合提示工程、结构化历史管理和双帧视觉输入策略。", "result": "在Room-to-Room基准测试中初步结果显示出泛化能力不足，但模块化设计为未来改进奠定了基础。", "conclusion": "模块化方法为高效导航系统提供了可扩展的框架，未来可通过增强环境先验和多模态输入集成进一步优化。"}}
{"id": "2506.10240", "categories": ["cs.RO", "cs.SY", "eess.SY", "93B52 (Primary), 93C85 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.10240", "abs": "https://arxiv.org/abs/2506.10240", "authors": ["Rongfei Li", "Francis Assadian"], "title": "Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators", "comment": "22 pages, 13 figures. To appear in: Innovative Adaptive Image-Based\n  Visual Servoing Control of 6 DoFs Industrial Robot Manipulators, IntechOpen,\n  2024. For published version, see this http URL:\n  https://doi.org/10.5772/intechopen.1004857", "summary": "Image-based visual servoing (IBVS) methods have been well developed and used\nin many applications, especially in pose (position and orientation) alignment.\nHowever, most research papers focused on developing control solutions when 3D\npoint features can be detected inside the field of view. This work proposes an\ninnovative feedforward-feedback adaptive control algorithm structure with the\nYoula Parameterization method. A designed feature estimation loop ensures\nstable and fast motion control when point features are outside the field of\nview. As 3D point features move inside the field of view, the IBVS feedback\nloop preserves the precision of the pose at the end of the control period.\nAlso, an adaptive controller is developed in the feedback loop to stabilize the\nsystem in the entire range of operations. The nonlinear camera and robot\nmanipulator model is linearized and decoupled online by an adaptive algorithm.\nThe adaptive controller is then computed based on the linearized model\nevaluated at current linearized point. The proposed solution is robust and easy\nto implement in different industrial robotic systems. Various scenarios are\nused in simulations to validate the effectiveness and robust performance of the\nproposed controller.", "AI": {"tldr": "提出了一种基于Youla参数化的前馈-反馈自适应控制算法，用于解决3D点特征在视野外时的视觉伺服控制问题，并通过自适应控制器保证系统稳定性。", "motivation": "现有视觉伺服控制方法多依赖视野内的3D点特征，而视野外特征的控制问题尚未充分研究。", "method": "结合前馈-反馈结构和Youla参数化方法，设计特征估计环路和自适应控制器，在线线性化解耦相机与机器人模型。", "result": "仿真验证了控制器在视野内外均能实现稳定、快速且高精度的位姿控制。", "conclusion": "所提方法鲁棒性强，易于在不同工业机器人系统中实现，适用于复杂场景。"}}
{"id": "2506.10239", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10239", "abs": "https://arxiv.org/abs/2506.10239", "authors": ["Maximilian Mühlbauer", "Freek Stulp", "Sylvain Calinon", "Alin Albu-Schäffer", "João Silvério"], "title": "A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures", "comment": null, "summary": "Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the\nmost suitable haptic feedback for each phase of a task, based on learned or\nperceived uncertainty. While keeping the human in the loop remains essential,\nfor instance, to ensure high precision, partial automation of certain task\nphases is critical for productivity. We present a unified framework for\nprobabilistic VFs that seamlessly switches between manual fixtures,\nsemi-automated fixtures (with the human handling precise tasks), and full\nautonomy. We introduce a novel probabilistic Dynamical System-based VF for\ncoarse guidance, enabling the robot to autonomously complete certain task\nphases while keeping the human operator in the loop. For tasks requiring\nprecise guidance, we extend probabilistic position-based trajectory fixtures\nwith automation allowing for seamless human interaction as well as\ngeometry-awareness and optimal impedance gains. For manual tasks requiring very\nprecise guidance, we also extend visual servoing fixtures with the same\ngeometry-awareness and impedance behaviour. We validate our approach\nexperimentally on different robots, showcasing multiple operation modes and the\nease of programming fixtures.", "AI": {"tldr": "论文提出了一种统一的概率虚拟夹具（VFs）框架，能够根据任务阶段自适应选择最合适的触觉反馈，支持手动、半自动和全自动模式的切换。", "motivation": "在保持人类参与的同时，通过部分自动化提高任务效率，同时确保高精度。", "method": "结合概率动态系统实现粗引导，扩展概率位置轨迹夹具和视觉伺服夹具，支持几何感知和最优阻抗增益。", "result": "实验验证了框架在不同机器人上的有效性，展示了多种操作模式和编程的便捷性。", "conclusion": "该框架为任务阶段提供了灵活的自动化支持，同时保持了人类操作的精确性。"}}
{"id": "2506.10252", "categories": ["cs.RO", "cs.SY", "eess.SY", "93B52 (Primary), 93C85 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.10252", "abs": "https://arxiv.org/abs/2506.10252", "authors": ["Rongfei Li", "Francis Assadian"], "title": "A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control", "comment": "36 pages, 19 figures, Journal, Published in: Applied Sciences, 2025,\n  vol. 15, article 4991. For published version, see this http URL:\n  https://doi.org/10.3390/app15094991", "summary": "In robot navigation and manipulation, accurately determining the camera's\npose relative to the environment is crucial for effective task execution. In\nthis paper, we systematically prove that this problem corresponds to the\nPerspective-3-Point (P3P) formulation, where exactly three known 3D points and\ntheir corresponding 2D image projections are used to estimate the pose of a\nstereo camera. In image-based visual servoing (IBVS) control, the system\nbecomes overdetermined, as the 6 degrees of freedom (DoF) of the stereo camera\nmust align with 9 observed 2D features in the scene. When more constraints are\nimposed than available DoFs, global stability cannot be guaranteed, as the\ncamera may become trapped in a local minimum far from the desired configuration\nduring servoing. To address this issue, we propose a novel control strategy for\naccurately positioning a calibrated stereo camera. Our approach integrates a\nfeedforward controller with a Youla parameterization-based feedback controller,\nensuring robust servoing performance. Through simulations, we demonstrate that\nour method effectively avoids local minima and enables the camera to reach the\ndesired pose accurately and efficiently.", "AI": {"tldr": "论文提出了一种新的控制策略，结合前馈控制器和基于Youla参数化的反馈控制器，解决了立体相机在视觉伺服控制中因过约束导致的局部极小值问题。", "motivation": "在机器人导航和操作中，准确确定相机相对于环境的姿态是关键。然而，在基于图像的视觉伺服控制中，过约束问题可能导致相机陷入局部极小值，影响任务执行。", "method": "提出了一种集成前馈控制器和基于Youla参数化反馈控制器的策略，用于精确校准立体相机的姿态控制。", "result": "仿真结果表明，该方法有效避免了局部极小值，使相机能够准确高效地到达目标姿态。", "conclusion": "该方法为立体相机在视觉伺服控制中的姿态定位问题提供了有效的解决方案。"}}
{"id": "2506.10279", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.10279", "abs": "https://arxiv.org/abs/2506.10279", "authors": ["Alexandre Capone", "Ryan Cosner", "Aaaron Ames", "Sandra Hirche"], "title": "Learning Safe Control via On-the-Fly Bandit Exploration", "comment": "arXiv admin note: text overlap with arXiv:2311.02133", "summary": "Control tasks with safety requirements under high levels of model uncertainty\nare increasingly common. Machine learning techniques are frequently used to\naddress such tasks, typically by leveraging model error bounds to specify\nrobust constraint-based safety filters. However, if the learned model\nuncertainty is very high, the corresponding filters are potentially invalid,\nmeaning no control input satisfies the constraints imposed by the safety\nfilter. While most works address this issue by assuming some form of safe\nbackup controller, ours tackles it by collecting additional data on the fly\nusing a Gaussian process bandit-type algorithm. We combine a control barrier\nfunction with a learned model to specify a robust certificate that ensures\nsafety if feasible. Whenever infeasibility occurs, we leverage the control\nbarrier function to guide exploration, ensuring the collected data contributes\ntoward the closed-loop system safety. By combining a safety filter with\nexploration in this manner, our method provably achieves safety in a setting\nthat allows for a zero-mean prior dynamics model, without requiring a backup\ncontroller. To the best of our knowledge, it is the first safe learning-based\ncontrol method that achieves this.", "AI": {"tldr": "论文提出了一种结合高斯过程与控制屏障函数的方法，用于在模型不确定性高的情况下确保控制任务的安全性，无需依赖备用控制器。", "motivation": "在高模型不确定性的控制任务中，传统方法依赖备用控制器确保安全性，但可能导致效率低下或无效。本文旨在通过动态数据收集和探索解决这一问题。", "method": "结合高斯过程（GP）的bandit算法与控制屏障函数（CBF），动态收集数据以验证安全性，并在不可行时引导探索。", "result": "该方法在零均值先验动力学模型下可证明地确保安全性，且无需备用控制器。", "conclusion": "这是首个无需备用控制器的安全学习控制方法，为高不确定性任务提供了新解决方案。"}}
{"id": "2506.10383", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.10383", "abs": "https://arxiv.org/abs/2506.10383", "authors": ["Nidhi Homey Parayil", "Thierry Peynot", "Chris Lehnert"], "title": "RICE: Reactive Interaction Controller for Cluttered Canopy Environment", "comment": "This work has been submitted to the IEEE RAL for possible publication", "summary": "Robotic navigation in dense, cluttered environments such as agricultural\ncanopies presents significant challenges due to physical and visual occlusion\ncaused by leaves and branches. Traditional vision-based or model-dependent\napproaches often fail in these settings, where physical interaction without\ndamaging foliage and branches is necessary to reach a target. We present a\nnovel reactive controller that enables safe navigation for a robotic arm in a\ncontact-rich, cluttered, deformable environment using end-effector position and\nreal-time tactile feedback. Our proposed framework's interaction strategy is\nbased on a trade-off between minimizing disturbance by maneuvering around\nobstacles and pushing through them to move towards the target. We show that\nover 35 trials in 3 experimental plant setups with an occluded target, the\nproposed controller successfully reached the target in all trials without\nbreaking any branch and outperformed the state-of-the-art model-free controller\nin robustness and adaptability. This work lays the foundation for safe,\nadaptive interaction in cluttered, contact-rich deformable environments,\nenabling future agricultural tasks such as pruning and harvesting in plant\ncanopies.", "AI": {"tldr": "提出一种新型反应式控制器，用于机器臂在密集、遮挡的农业环境中安全导航，结合末端执行器位置和实时触觉反馈。", "motivation": "传统视觉或模型依赖方法在农业冠层等密集、遮挡环境中易失效，需物理交互但不损坏植物。", "method": "基于权衡绕过障碍和推动障碍的策略，结合位置和触觉反馈。", "result": "在35次实验中，控制器成功到达目标且未损坏植物，优于现有无模型控制器。", "conclusion": "为农业任务（如修剪和收获）提供了安全、自适应交互的基础。"}}
{"id": "2506.10287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10287", "abs": "https://arxiv.org/abs/2506.10287", "authors": ["Rohit Sonker", "Alexandre Capone", "Andrew Rothstein", "Hiro Josep Farre Kaga", "Egemen Kolemen", "Jeff Schneider"], "title": "Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks", "comment": null, "summary": "Machine learning algorithms often struggle to control complex real-world\nsystems. In the case of nuclear fusion, these challenges are exacerbated, as\nthe dynamics are notoriously complex, data is poor, hardware is subject to\nfailures, and experiments often affect dynamics beyond the experiment's\nduration. Existing tools like reinforcement learning, supervised learning, and\nBayesian optimization address some of these challenges but fail to provide a\ncomprehensive solution. To overcome these limitations, we present a multi-scale\nBayesian optimization approach that integrates a high-frequency data-driven\ndynamics model with a low-frequency Gaussian process. By updating the Gaussian\nprocess between experiments, the method rapidly adapts to new data, refining\nthe predictions of the less reliable dynamical model. We validate our approach\nby controlling tearing instabilities in the DIII-D nuclear fusion plant.\nOffline testing on historical data shows that our method significantly\noutperforms several baselines. Results on live experiments on the DIII-D\ntokamak, conducted under high-performance plasma scenarios prone to\ninstabilities, shows a 50% success rate, marking a 117% improvement over\nhistorical outcomes.", "AI": {"tldr": "提出了一种多尺度贝叶斯优化方法，用于控制核聚变中的撕裂不稳定性，显著优于现有基线方法。", "motivation": "核聚变系统动态复杂、数据质量差、硬件易故障，现有机器学习工具无法全面解决这些问题。", "method": "结合高频数据驱动动态模型与低频高斯过程，通过实验间更新高斯过程快速适应新数据。", "result": "在DIII-D托卡马克实验中，成功率提高117%，达到50%。", "conclusion": "多尺度贝叶斯优化方法在复杂动态系统中表现出色，为核聚变控制提供了有效解决方案。"}}
{"id": "2506.10317", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10317", "abs": "https://arxiv.org/abs/2506.10317", "authors": ["Akshar Tumu", "Henrik I. Christensen", "Marcell Vazquez-Chanlatte", "Chikao Tsuchiya", "Dhaval Bhanderi"], "title": "Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving", "comment": "4 pages, 3 figures, Accepted at RSS 2025 Workshop -\n  RobotEvaluation@RSS2025", "summary": "Lane-topology prediction is a critical component of safe and reliable\nautonomous navigation. An accurate understanding of the road environment aids\nthis task. We observe that this information often follows conventions encoded\nin natural language, through design codes that reflect the road structure and\nroad names that capture the road functionality. We augment this information in\na lightweight manner to SMERF, a map-prior-based online lane-topology\nprediction model, by combining structured road metadata from OSM maps and\nlane-width priors from Road design manuals with the road centerline encodings.\nWe evaluate our method on two geo-diverse complex intersection scenarios. Our\nmethod shows improvement in both lane and traffic element detection and their\nassociation. We report results using four topology-aware metrics to\ncomprehensively assess the model performance. These results demonstrate the\nability of our approach to generalize and scale to diverse topologies and\nconditions.", "AI": {"tldr": "论文提出了一种轻量级方法，通过结合OSM地图的结构化道路元数据和道路设计手册中的车道宽度先验，改进了SMERF模型的车道拓扑预测能力。", "motivation": "车道拓扑预测对自动驾驶导航至关重要，而道路环境信息通常遵循自然语言编码的惯例（如设计规范和道路名称）。", "method": "在SMERF模型中结合OSM地图的道路元数据和车道宽度先验，增强车道拓扑预测。", "result": "在两个地理多样化的复杂交叉口场景中，模型在车道和交通元素检测及其关联方面表现提升。", "conclusion": "该方法能够泛化并适应多样化的拓扑结构和条件。"}}
{"id": "2506.10359", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10359", "abs": "https://arxiv.org/abs/2506.10359", "authors": ["Che Wang", "Jeroen van Baar", "Chaitanya Mitash", "Shuai Li", "Dylan Randle", "Weiyao Wang", "Sumedh Sontakke", "Kostas E. Bekris", "Kapil Katyal"], "title": "Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success", "comment": "Accepted to Robotics: Science and Systems (RSS 2025), 15 pages", "summary": "This work demonstrates how autonomously learning aspects of robotic operation\nfrom sparsely-labeled, real-world data of deployed, engineered solutions at\nindustrial scale can provide with solutions that achieve improved performance.\nSpecifically, it focuses on multi-suction robot picking and performs a\ncomprehensive study on the application of multi-modal visual encoders for\npredicting the success of candidate robotic picks. Picking diverse items from\nunstructured piles is an important and challenging task for robot manipulation\nin real-world settings, such as warehouses. Methods for picking from clutter\nmust work for an open set of items while simultaneously meeting latency\nconstraints to achieve high throughput. The demonstrated approach utilizes\nmultiple input modalities, such as RGB, depth and semantic segmentation, to\nestimate the quality of candidate multi-suction picks. The strategy is trained\nfrom real-world item picking data, with a combination of multimodal pretrain\nand finetune. The manuscript provides comprehensive experimental evaluation\nperformed over a large item-picking dataset, an item-picking dataset targeted\nto include partial occlusions, and a package-picking dataset, which focuses on\ncontainers, such as boxes and envelopes, instead of unpackaged items. The\nevaluation measures performance for different item configurations, pick scenes,\nand object types. Ablations help to understand the effects of in-domain\npretraining, the impact of different modalities and the importance of\nfinetuning. These ablations reveal both the importance of training over\nmultiple modalities but also the ability of models to learn during pretraining\nthe relationship between modalities so that during finetuning and inference,\nonly a subset of them can be used as input.", "AI": {"tldr": "本文展示了如何通过从稀疏标记的工业规模真实数据中自主学习机器人操作，实现性能提升，重点研究了多模态视觉编码器在多吸盘机器人抓取中的应用。", "motivation": "解决从无序堆中抓取多样化物品的挑战，满足高吞吐量的实时性要求，适用于仓库等实际场景。", "method": "利用RGB、深度和语义分割等多模态输入预测抓取质量，结合多模态预训练和微调策略。", "result": "在大规模物品抓取数据集、部分遮挡数据集和包裹抓取数据集上进行了全面实验，验证了方法的有效性。", "conclusion": "多模态训练和预训练对性能至关重要，模型能在微调和推理时仅使用部分模态输入。"}}
{"id": "2506.10363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10363", "abs": "https://arxiv.org/abs/2506.10363", "authors": ["Daniel Betschinske", "Malte Schrimpf", "Steven Peters", "Kamil Klonecki", "Jan Peter Karch", "Moritz Lippert"], "title": "Towards more efficient quantitative safety validation of residual risk for assisted and automated driving", "comment": null, "summary": "The safety validation of Advanced Driver Assistance Systems (ADAS) and\nAutomated Driving Systems (ADS) increasingly demands efficient and reliable\nmethods to quantify residual risk while adhering to international standards\nsuch as ISO 21448. Traditionally, Field Operational Testing (FOT) has been\npivotal for macroscopic safety validation of automotive driving functions up to\nSAE automation level 2. However, state-of-the-art derivations for empirical\nsafety demonstrations using FOT often result in impractical testing efforts,\nparticularly at higher automation levels. Even at lower automation levels, this\nlimitation - coupled with the substantial costs associated with FOT - motivates\nthe exploration of approaches to enhance the efficiency of FOT-based\nmacroscopic safety validation. Therefore, this publication systematically\nidentifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT,\nincluding novel methods reported in the literature. Based on an analysis of ISO\n21448, two models are derived: a generic model capturing the argumentation\ncomponents of the standard, and a base model, exemplarily applied to Automatic\nEmergency Braking (AEB) systems, establishing a baseline for the real-world\ndriving requirement for a Quantitative Safety Validation of Residual Risk\n(QSVRR). Subsequently, the RAs are assessed using four criteria:\nquantifiability, threats to validity, missing links, and black box\ncompatibility, highlighting potential benefits, inherent limitations, and\nidentifying key areas for further research. Our evaluation reveals that, while\nseveral approaches offer potential, none are free from missing links or other\nsubstantial shortcomings. Moreover, no identified alternative can fully replace\nFOT, reflecting its crucial role in the safety validation of ADAS and ADS.", "AI": {"tldr": "论文探讨了如何通过减少现场操作测试（FOT）的负担来高效验证高级驾驶辅助系统（ADAS）和自动驾驶系统（ADS）的安全性，提出了两种模型并评估了现有方法的优缺点。", "motivation": "传统的FOT方法在高自动化级别下测试成本高且效率低，因此需要探索更高效的验证方法以满足国际标准（如ISO 21448）。", "method": "基于ISO 21448，论文提出了两种模型：通用模型和基础模型（以自动紧急制动系统为例），并评估了现有减少FOT负担的方法。", "result": "评估发现，现有方法虽有一定潜力，但均存在局限性，无法完全替代FOT在安全验证中的关键作用。", "conclusion": "FOT在ADAS和ADS的安全验证中仍不可或缺，未来研究需进一步优化现有方法以弥补其不足。"}}
{"id": "2506.10462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10462", "abs": "https://arxiv.org/abs/2506.10462", "authors": ["Ana Müller", "Sabina Jeschke", "Anja Richert"], "title": "Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions", "comment": "Accepted as a regular paper at the 2025 IEEE International Conference\n  on Robot and Human Interactive Communication (RO-MAN). \\c{opyright} IEEE.\n  This is the preprint version. The final version will appear in the IEEE\n  proceedings", "summary": "This paper investigates the impact of a group-adaptive conversation design in\ntwo socially interactive agents (SIAs) through two real-world studies. Both\nSIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped\nwith a conversational artificial intelligence (CAI) backend combining hybrid\nretrieval and generative models. The studies were carried out in an in-the-wild\nsetting with a total of $N = 188$ participants who interacted with the SIAs -\nin dyads, triads or larger groups - at a German museum. Although the results\ndid not reveal a significant effect of the group-sensitive conversation design\non perceived satisfaction, the findings provide valuable insights into the\nchallenges of adapting CAI for multi-party interactions and across different\nembodiments (robot vs.\\ virtual agent), highlighting the need for multimodal\nstrategies beyond linguistic pluralization. These insights contribute to the\nfields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and\nbroader Human-Machine Interaction (HMI), providing insights for future research\non effective dialogue adaptation in group settings.", "AI": {"tldr": "研究探讨了群体自适应对话设计对两种社交交互代理（SIAs）的影响，发现其对满意度无显著影响，但为多群体交互和不同体现形式的挑战提供了见解。", "motivation": "探索群体敏感的对话设计在社交交互代理（SIAs）中的实际效果，以及其在多群体交互和不同体现形式（机器人与虚拟代理）中的适应性。", "method": "通过两项真实世界研究，使用Furhat社交机器人和MetaHuman虚拟代理，结合混合检索和生成模型的对话AI后端，与188名参与者互动。", "result": "群体敏感对话设计对满意度无显著影响，但揭示了在多群体交互和不同体现形式中适应对话AI的挑战。", "conclusion": "研究为未来群体设置中有效对话适应的研究提供了见解，强调了超越语言多元化的多模态策略需求。"}}
{"id": "2506.10600", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10600", "abs": "https://arxiv.org/abs/2506.10600", "authors": ["Wang Xinjie", "Liu Liu", "Cao Yu", "Wu Ruiqi", "Qin Wenkang", "Wang Dehui", "Sui Wei", "Su Zhizhong"], "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence", "comment": null, "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.", "AI": {"tldr": "EmbodiedGen是一个用于生成高质量、可控且逼真的3D资产的基础平台，支持低成本生成具有物理属性的3D世界，适用于具身智能任务。", "motivation": "当前具身智能任务依赖传统3D图形资产，成本高且真实性有限，限制了数据驱动方法的扩展性。", "method": "EmbodiedGen通过六个关键模块（如图像到3D、文本到3D等）生成多样化的交互式3D世界，利用生成式AI技术。", "result": "生成的3D资产具有高真实性和物理准确性，可直接用于物理模拟引擎，支持下游任务。", "conclusion": "EmbodiedGen解决了具身智能研究中数据扩展性和真实性的挑战，是一个易用且功能全面的工具包。"}}
{"id": "2506.10686", "categories": ["cs.RO", "cs.SC", "math.GR", "math.OC", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2506.10686", "abs": "https://arxiv.org/abs/2506.10686", "authors": ["Andreas Mueller"], "title": "An $O(n$)-Algorithm for the Higher-Order Kinematics and Inverse Dynamics of Serial Manipulators using Spatial Representation of Twists", "comment": null, "summary": "Optimal control in general, and flatness-based control in particular, of\nrobotic arms necessitate to compute the first and second time derivatives of\nthe joint torques/forces required to achieve a desired motion. In view of the\nrequired computational efficiency, recursive $O(n)$-algorithms were proposed to\nthis end. Aiming at compact yet efficient formulations, a Lie group formulation\nwas recently proposed, making use of body-fixed and hybrid representation of\ntwists and wrenches. In this paper a formulation is introduced using the\nspatial representation. The second-order inverse dynamics algorithm is\naccompanied by a fourth-order forward and inverse kinematics algorithm. An\nadvantage of all Lie group formulations is that they can be parameterized in\nterms of vectorial quantities that are readily available. The method is\ndemonstrated for the 7 DOF Franka Emika Panda robot.", "AI": {"tldr": "本文提出了一种基于空间表示的二阶逆动力学算法，并配合四阶正向和逆向运动学算法，适用于机器人臂的最优控制。", "motivation": "为了提高计算效率，特别是在机器人臂的平坦性控制中，需要高效计算关节扭矩/力的时间导数。", "method": "采用空间表示法，结合Lie群理论，提出二阶逆动力学算法和四阶运动学算法。", "result": "该方法在7自由度Franka Emika Panda机器人上验证了其有效性。", "conclusion": "Lie群表示法能以矢量形式高效实现，适用于机器人控制。"}}
{"id": "2506.10756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10756", "abs": "https://arxiv.org/abs/2506.10756", "authors": ["Yuhang Zhang", "Haosheng Yu", "Jiaping Xiao", "Mir Feroskhan"], "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding", "comment": null, "summary": "Vision-and-language navigation (VLN) is a long-standing challenge in\nautonomous robotics, aiming to empower agents with the ability to follow human\ninstructions while navigating complex environments. Two key bottlenecks remain\nin this field: generalization to out-of-distribution environments and reliance\non fixed discrete action spaces. To address these challenges, we propose\nVision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles\n(UAVs) to execute language-guided flight. Without the requirement for\nlocalization or active ranging sensors, VLFly outputs continuous velocity\ncommands purely from egocentric observations captured by an onboard monocular\ncamera. The VLFly integrates three modules: an instruction encoder based on a\nlarge language model (LLM) that reformulates high-level language into\nstructured prompts, a goal retriever powered by a vision-language model (VLM)\nthat matches these prompts to goal images via vision-language similarity, and a\nwaypoint planner that generates executable trajectories for real-time UAV\ncontrol. VLFly is evaluated across diverse simulation environments without\nadditional fine-tuning and consistently outperforms all baselines. Moreover,\nreal-world VLN tasks in indoor and outdoor environments under direct and\nindirect instructions demonstrate that VLFly achieves robust open-vocabulary\ngoal understanding and generalized navigation capabilities, even in the\npresence of abstract language input.", "AI": {"tldr": "VLFly是一种为无人机设计的视觉语言导航框架，通过连续速度命令实现语言引导飞行，无需定位或测距传感器，在多样环境中表现优异。", "motivation": "解决视觉语言导航中的泛化问题和离散动作空间依赖，提升无人机在复杂环境中的导航能力。", "method": "结合大型语言模型（LLM）编码指令、视觉语言模型（VLM）匹配目标图像，以及路径规划模块生成连续轨迹。", "result": "在模拟和真实环境中均优于基线方法，支持开放词汇目标理解和抽象语言输入。", "conclusion": "VLFly展示了强大的泛化能力和实时控制潜力，适用于无人机导航任务。"}}
{"id": "2506.10787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10787", "abs": "https://arxiv.org/abs/2506.10787", "authors": ["Felix Nonnengießer", "Alap Kshirsagar", "Boris Belousov", "Jan Peters"], "title": "In-Hand Object Pose Estimation via Visual-Tactile Fusion", "comment": "8 pages", "summary": "Accurate in-hand pose estimation is crucial for robotic object manipulation,\nbut visual occlusion remains a major challenge for vision-based approaches.\nThis paper presents an approach to robotic in-hand object pose estimation,\ncombining visual and tactile information to accurately determine the position\nand orientation of objects grasped by a robotic hand. We address the challenge\nof visual occlusion by fusing visual information from a wrist-mounted RGB-D\ncamera with tactile information from vision-based tactile sensors mounted on\nthe fingertips of a robotic gripper. Our approach employs a weighting and\nsensor fusion module to combine point clouds from heterogeneous sensor types\nand control each modality's contribution to the pose estimation process. We use\nan augmented Iterative Closest Point (ICP) algorithm adapted for weighted point\nclouds to estimate the 6D object pose. Our experiments show that incorporating\ntactile information significantly improves pose estimation accuracy,\nparticularly when occlusion is high. Our method achieves an average pose\nestimation error of 7.5 mm and 16.7 degrees, outperforming vision-only\nbaselines by up to 20%. We also demonstrate the ability of our method to\nperform precise object manipulation in a real-world insertion task.", "AI": {"tldr": "提出了一种结合视觉和触觉信息的机器人手内物体姿态估计方法，显著提高了遮挡情况下的精度。", "motivation": "视觉遮挡是视觉方法的主要挑战，需要结合触觉信息提高姿态估计的准确性。", "method": "融合手腕RGB-D相机和指尖视觉触觉传感器的数据，使用加权点云的增强ICP算法估计6D姿态。", "result": "平均姿态估计误差为7.5毫米和16.7度，比纯视觉方法提升20%。", "conclusion": "结合触觉信息显著提高了遮挡情况下的姿态估计精度，适用于实际物体操作任务。"}}
{"id": "2506.10826", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10826", "abs": "https://arxiv.org/abs/2506.10826", "authors": ["Wenxuan Song", "Jiayi Chen", "Wenxue Li", "Xu He", "Han Zhao", "Pengxiang Ding Shiyan Su", "Feilong Tang", "Xuelian Cheng", "Donglin Wang", "Zongyuan Ge", "Xinhu Zheng", "Zhe Liu", "Hesheng Wang", "Yunhui Liu", "Haoang Li"], "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System", "comment": "14 pages", "summary": "A fundamental requirement for real-world robotic deployment is the ability to\nunderstand and respond to natural language instructions. Existing\nlanguage-conditioned manipulation tasks typically assume that instructions are\nperfectly aligned with the environment. This assumption limits robustness and\ngeneralization in realistic scenarios where instructions may be ambiguous,\nirrelevant, or infeasible. To address this problem, we introduce RAtional\nMAnipulation (RAMA), a new benchmark that challenges models with both unseen\nexecutable instructions and defective ones that should be rejected. In RAMA, we\nconstruct a dataset with over 14,000 samples, including diverse defective\ninstructions spanning six dimensions: visual, physical, semantic, motion,\nsafety, and out-of-context. We further propose the Rational\nVision-Language-Action model (RationalVLA). It is a dual system for robotic\narms that integrates the high-level vision-language model with the low-level\nmanipulation policy by introducing learnable latent space embeddings. This\ndesign enables RationalVLA to reason over instructions, reject infeasible\ncommands, and execute manipulation effectively. Experiments demonstrate that\nRationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher\nsuccess rate and 0.94 average task length, while maintaining competitive\nperformance on standard manipulation tasks. Real-world trials further validate\nits effectiveness and robustness in practical applications. Our project page is\nhttps://irpn-eai.github.io/rationalvla.", "AI": {"tldr": "论文提出了RAMA基准和RationalVLA模型，用于处理机器人执行自然语言指令时的模糊、无关或不可行问题，显著提升了任务成功率和效率。", "motivation": "现有语言条件操控任务假设指令与环境完美对齐，限制了实际场景中的鲁棒性和泛化能力。", "method": "构建包含14,000多样本的数据集，提出RationalVLA模型，结合高层视觉语言模型与低层操控策略，通过可学习潜在空间嵌入实现指令推理与拒绝。", "result": "RationalVLA在RAMA基准上比现有方法任务成功率提升14.5%，平均任务长度降低0.94，并在实际应用中验证了有效性。", "conclusion": "RAMA和RationalVLA为处理复杂指令提供了新方法，显著提升了机器人操控的鲁棒性和实用性。"}}
{"id": "2506.10850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10850", "abs": "https://arxiv.org/abs/2506.10850", "authors": ["Derek Benham", "Easton Potokar", "Joshua G. Mangelson"], "title": "Invariant Extended Kalman Filter for Autonomous Surface Vessels with Partial Orientation Measurements", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics. 8 pages,\n  4 figures, 2 tables", "summary": "Autonomous surface vessels (ASVs) are increasingly vital for marine science,\noffering robust platforms for underwater mapping and inspection. Accurate state\nestimation, particularly of vehicle pose, is paramount for precise seafloor\nmapping, as even small surface deviations can have significant consequences\nwhen sensing the seafloor below. To address this challenge, we propose an\nInvariant Extended Kalman Filter (InEKF) framework designed to integrate\npartial orientation measurements. While conventional estimation often relies on\nrelative position measurements to fixed landmarks, open ocean ASVs primarily\nobserve a receding horizon. We leverage forward-facing monocular cameras to\nestimate roll and pitch with respect to this horizon, which provides\nyaw-ambiguous partial orientation information. To effectively utilize these\nmeasurements within the InEKF, we introduce a novel framework for incorporating\nsuch partial orientation data. This approach contrasts with traditional InEKF\nimplementations that assume full orientation measurements and is particularly\nrelevant for planar vehicle motion constrained to a \"seafaring plane.\" This\npaper details the developed InEKF framework; its integration with horizon-based\nroll/pitch observations and dual-antenna GPS heading measurements for ASV state\nestimation; and provides a comparative analysis against the InEKF using full\norientation and a Multiplicative EKF (MEKF). Our results demonstrate the\nefficacy and robustness of the proposed partial orientation measurements for\naccurate ASV state estimation in open ocean environments.", "AI": {"tldr": "提出了一种基于不变扩展卡尔曼滤波（InEKF）的框架，用于整合部分方向测量，以提高自主水面船只（ASV）在开放海洋环境中的状态估计精度。", "motivation": "由于ASV在海洋科学中的重要性，精确的状态估计（尤其是姿态）对海底测绘至关重要。传统方法依赖固定地标的相对位置测量，而开放海洋中ASV主要观测地平线，因此需要新的解决方案。", "method": "利用前向单目相机估计地平线相关的滚转和俯仰角，结合双天线GPS航向测量，开发了一种新的InEKF框架以整合部分方向数据。", "result": "实验结果表明，提出的方法在开放海洋环境中对ASV状态估计具有高效性和鲁棒性。", "conclusion": "该框架为ASV在缺乏全方向测量时的状态估计提供了有效解决方案，优于传统InEKF和MEKF方法。"}}
{"id": "2506.10875", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.10875", "abs": "https://arxiv.org/abs/2506.10875", "authors": ["Guanjin Wang", "Xiangxue Zhao", "Shapour Azarm", "Balakumar Balachandran"], "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material", "comment": null, "summary": "An alternative data-driven modeling approach has been proposed and employed\nto gain fundamental insights into robot motion interaction with granular\nterrain at certain length scales. The approach is based on an integration of\ndimension reduction (Sequentially Truncated Higher-Order Singular Value\nDecomposition), surrogate modeling (Gaussian Process), and data assimilation\ntechniques (Reduced Order Particle Filter). This approach can be used online\nand is based on offline data, obtained from the offline collection of\nhigh-fidelity simulation data and a set of sparse experimental data. The\nresults have shown that orders of magnitude reduction in computational time can\nbe obtained from the proposed data-driven modeling approach compared with\nphysics-based high-fidelity simulations. With only simulation data as input,\nthe data-driven prediction technique can generate predictions that have\ncomparable accuracy as simulations. With both simulation data and sparse\nphysical experimental measurement as input, the data-driven approach with its\nembedded data assimilation techniques has the potential in outperforming only\nhigh-fidelity simulations for the long-horizon predictions. In addition, it is\ndemonstrated that the data-driven modeling approach can also reproduce the\nscaling relationship recovered by physics-based simulations for maximum\nresistive forces, which may indicate its general predictability beyond a\ncase-by-case basis. The results are expected to help robot navigation and\nexploration in unknown and complex terrains during both online and offline\nphases.", "AI": {"tldr": "提出了一种基于数据驱动的建模方法，结合降维、代理建模和数据同化技术，显著减少计算时间并保持高精度，适用于机器人导航和复杂地形探索。", "motivation": "研究机器人运动与颗粒地形交互的基本原理，提出高效且准确的建模方法。", "method": "整合降维（ST-HOSVD）、代理建模（高斯过程）和数据同化（降阶粒子滤波）技术，基于离线高保真模拟数据和稀疏实验数据。", "result": "计算时间显著减少，预测精度与模拟相当；结合实验数据后，长期预测优于纯模拟。", "conclusion": "该方法在机器人导航和复杂地形探索中具有潜力，展现了超越个案预测的通用性。"}}
{"id": "2506.10884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10884", "abs": "https://arxiv.org/abs/2506.10884", "authors": ["Dong Hae Mangalindan", "Karthik Kandikonda", "Ericka Rovira", "Vaibhav Srivastava"], "title": "Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies", "comment": null, "summary": "With increasing efficiency and reliability, autonomous systems are becoming\nvaluable assistants to humans in various tasks. In the context of\nrobot-assisted delivery, we investigate how robot performance and trust repair\nstrategies impact human trust. In this task, while handling a secondary task,\nhumans can choose to either send the robot to deliver autonomously or manually\ncontrol it. The trust repair strategies examined include short and long\nexplanations, apology and promise, and denial.\n  Using data from human participants, we model human behavior using an\nInput-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and\nhuman action probabilities. Our findings indicate that humans are more likely\nto deploy the robot autonomously when their trust is high. Furthermore, state\ntransition estimates show that long explanations are the most effective at\nrepairing trust following a failure, while denial is most effective at\npreventing trust loss.\n  We also demonstrate that the trust estimates generated by our model are\nisomorphic to self-reported trust values, making them interpretable. This model\nlays the groundwork for developing optimal policies that facilitate real-time\nadjustment of human trust in autonomous systems.", "AI": {"tldr": "研究探讨了机器人性能与信任修复策略对人类信任的影响，发现长解释最能修复信任，而否认最能防止信任流失。", "motivation": "随着自主系统效率与可靠性提升，研究其在机器人辅助交付任务中如何影响人类信任。", "method": "使用输入-输出隐马尔可夫模型（IOHMM）建模人类行为，分析信任动态与行动概率。", "result": "高信任时人类更倾向自主部署机器人；长解释修复信任效果最佳，否认防止信任流失最有效。", "conclusion": "模型信任估计与自报告值一致，为实时调整人类对自主系统信任的最优策略奠定基础。"}}
{"id": "2506.10923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10923", "abs": "https://arxiv.org/abs/2506.10923", "authors": ["Xili Yi", "Nima Fazeli"], "title": "Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations", "comment": "11 pages, 12 figures", "summary": "We introduce Vib2Move, a novel approach for in-hand object reconfiguration\nthat uses fingertip micro-vibrations and gravity to precisely reposition planar\nobjects. Our framework comprises three key innovations. First, we design a\nvibration-based actuator that dynamically modulates the effective finger-object\nfriction coefficient, effectively emulating changes in gripping force. Second,\nwe derive a sliding motion model for objects clamped in a parallel gripper with\ntwo symmetric, variable-friction contact patches. Third, we propose a motion\nplanner that coordinates end-effector finger trajectories and fingertip\nvibrations to achieve the desired object pose. In real-world trials, Vib2Move\nconsistently yields final positioning errors below 6 mm, demonstrating\nreliable, high-precision manipulation across a variety of planar objects. For\nmore results and information, please visit https://vib2move.github.io.", "AI": {"tldr": "Vib2Move是一种利用指尖微振动和重力精确重新定位平面物体的新方法，通过动态调节摩擦系数和协调运动规划，实现高精度操作。", "motivation": "研究动机是开发一种能够精确重新定位平面物体的方法，利用振动和重力实现高效操作。", "method": "方法包括设计振动执行器、推导滑动运动模型以及提出协调运动规划器。", "result": "实验结果显示，Vib2Move的最终定位误差低于6毫米，适用于多种平面物体。", "conclusion": "结论表明，Vib2Move是一种可靠且高精度的操作方法，适用于平面物体的重新配置。"}}
{"id": "2506.10966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.10966", "abs": "https://arxiv.org/abs/2506.10966", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation", "comment": null, "summary": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.", "AI": {"tldr": "GenManip是一个面向策略泛化研究的仿真平台，通过LLM驱动的任务场景图生成多样化任务，并引入GenManip-Bench基准评估策略泛化能力。", "motivation": "现有仿真平台在支持策略适应多样化指令和场景方面不足，无法满足对指令跟随基础模型（如LLMs）适应性研究的兴趣。", "method": "提出GenManip平台，利用LLM驱动的任务场景图自动生成大规模多样化任务，并开发GenManip-Bench基准进行系统评估。", "result": "模块化系统结合基础模型在多样化场景中泛化效果优于端到端策略，数据扩展对后者有帮助但有限。", "conclusion": "GenManip平台有望为现实条件下策略泛化研究提供关键支持。"}}
{"id": "2506.10968", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10968", "abs": "https://arxiv.org/abs/2506.10968", "authors": ["Justin Kerr", "Kush Hari", "Ethan Weber", "Chung Min Kim", "Brent Yi", "Tyler Bonnen", "Ken Goldberg", "Angjoo Kanazawa"], "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop", "comment": "Project page: https://www.eyerobot.net/", "summary": "Humans do not passively observe the visual world -- we actively look in order\nto act. Motivated by this principle, we introduce EyeRobot, a robotic system\nwith gaze behavior that emerges from the need to complete real-world tasks. We\ndevelop a mechanical eyeball that can freely rotate to observe its surroundings\nand train a gaze policy to control it using reinforcement learning. We\naccomplish this by first collecting teleoperated demonstrations paired with a\n360 camera. This data is imported into a simulation environment that supports\nrendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze\non top of robot demonstrations. We then introduce a BC-RL loop to train the\nhand and eye jointly: the hand (BC) agent is trained from rendered eye\nobservations, and the eye (RL) agent is rewarded when the hand produces correct\naction predictions. In this way, hand-eye coordination emerges as the eye looks\ntowards regions which allow the hand to complete the task. EyeRobot implements\na foveal-inspired policy architecture allowing high resolution with a small\ncompute budget, which we find also leads to the emergence of more stable\nfixation as well as improved ability to track objects and ignore distractors.\nWe evaluate EyeRobot on five panoramic workspace manipulation tasks requiring\nmanipulation in an arc surrounding the robot arm. Our experiments suggest\nEyeRobot exhibits hand-eye coordination behaviors which effectively facilitate\nmanipulation over large workspaces with a single camera. See project site for\nvideos: https://www.eyerobot.net/", "AI": {"tldr": "EyeRobot是一个通过强化学习训练机械眼球实现任务导向凝视行为的机器人系统，结合手眼协调完成大范围工作空间操作任务。", "motivation": "人类通过主动观察来完成任务，受此启发，开发具有任务导向凝视行为的机器人系统。", "method": "通过360度摄像头收集遥操作数据，在仿真环境中训练凝视策略，结合行为克隆（BC）和强化学习（RL）联合训练手和眼。", "result": "EyeRobot在手眼协调任务中表现出稳定的凝视行为和高效的操作能力，适用于大范围工作空间。", "conclusion": "EyeRobot展示了任务导向凝视行为的有效性，为机器人操作提供了新的解决方案。"}}
