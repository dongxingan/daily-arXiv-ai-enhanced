{"id": "2508.00097", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00097", "abs": "https://arxiv.org/abs/2508.00097", "authors": ["Zhigen Zhao", "Liuchuan Yu", "Ke Jing", "Ning Yang"], "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation", "comment": "6 pages, 6 figures, project link: https://github.com/XR-Robotics", "summary": "The rapid advancement of Vision-Language-Action models has created an urgent\nneed for large-scale, high-quality robot demonstration datasets. Although\nteleoperation is the predominant method for data collection, current approaches\nsuffer from limited scalability, complex setup procedures, and suboptimal data\nquality. This paper presents XRoboToolkit, a cross-platform framework for\nextended reality based robot teleoperation built on the OpenXR standard. The\nsystem features low-latency stereoscopic visual feedback, optimization-based\ninverse kinematics, and support for diverse tracking modalities including head,\ncontroller, hand, and auxiliary motion trackers. XRoboToolkit's modular\narchitecture enables seamless integration across robotic platforms and\nsimulation environments, spanning precision manipulators, mobile robots, and\ndexterous hands. We demonstrate the framework's effectiveness through precision\nmanipulation tasks and validate data quality by training VLA models that\nexhibit robust autonomous performance.", "AI": {"tldr": "XRoboToolkit是一个基于OpenXR标准的跨平台扩展现实机器人远程操作框架，解决了现有数据收集方法的可扩展性和数据质量问题。", "motivation": "当前机器人演示数据收集方法存在可扩展性差、设置复杂和数据质量低的问题，需要一种更高效的解决方案。", "method": "开发了XRoboToolkit框架，支持低延迟立体视觉反馈、优化逆运动学和多种跟踪方式，模块化设计支持多种机器人平台和仿真环境。", "result": "通过精确操作任务验证了框架的有效性，并训练出具有鲁棒自主性能的VLA模型。", "conclusion": "XRoboToolkit为大规模高质量机器人数据收集提供了一种高效、灵活的解决方案。"}}
{"id": "2508.00162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00162", "abs": "https://arxiv.org/abs/2508.00162", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "comment": null, "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "AI": {"tldr": "CHILD是一种紧凑可重构的遥操作系统，支持人形机器人的关节级控制，适用于全身控制和移动操作。", "motivation": "现有研究很少支持人形机器人的全身关节级遥操作，限制了任务的多样性。", "method": "CHILD系统设计为紧凑且可重构，适配标准婴儿背带，支持直接关节映射和自适应力反馈。", "result": "验证了系统在人形机器人和双臂系统上的移动操作和全身控制能力。", "conclusion": "CHILD系统提升了遥操作的灵活性和安全性，并开源硬件设计以促进可访问性和可重复性。"}}
{"id": "2508.00258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00258", "abs": "https://arxiv.org/abs/2508.00258", "authors": ["Zhiwei Wu", "Siyi Wei", "Jiahao Luo", "Jinhui Zhang"], "title": "Topology-Inspired Morphological Descriptor for Soft Continuum Robots", "comment": null, "summary": "This paper presents a topology-inspired morphological descriptor for soft\ncontinuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory\nto achieve a quantitative characterization of robot morphologies. By counting\ncritical points of directional projections, the proposed descriptor enables a\ndiscrete representation of multimodal configurations and facilitates\nmorphological classification. Furthermore, we apply the descriptor to\nmorphology control by formulating the target configuration as an optimization\nproblem to compute actuation parameters that generate equilibrium shapes with\ndesired topological features. The proposed framework provides a unified\nmethodology for quantitative morphology description, classification, and\ncontrol of soft continuum robots, with the potential to enhance their precision\nand adaptability in medical applications such as minimally invasive surgery and\nendovascular interventions.", "AI": {"tldr": "提出了一种基于拓扑学的形态描述符，结合伪刚体模型和莫尔斯理论，用于软体连续机器人的形态定量表征与控制。", "motivation": "提升软体连续机器人在医疗应用（如微创手术和血管内介入）中的精确性和适应性。", "method": "结合伪刚体模型和莫尔斯理论，通过计数方向投影的临界点实现形态的离散表示和分类。", "result": "实现了软体机器人形态的定量描述、分类及控制，并通过优化问题求解目标形态的驱动参数。", "conclusion": "该框架为软体连续机器人提供了一种统一的形态描述与控制方法，具有广泛的应用潜力。"}}
{"id": "2508.00288", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00288", "abs": "https://arxiv.org/abs/2508.00288", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "comment": "Accepted to ACM MM Dataset Track 2025", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "AI": {"tldr": "UAV-ON是一个用于无人机在开放环境中进行目标导航的基准测试，摆脱了对语言指令的依赖，强调语义目标驱动的自主导航。", "motivation": "现有研究多依赖语言指令（如VLN），限制了无人机导航的扩展性和自主性。UAV-ON旨在填补这一空白，推动无人机在复杂环境中的语义目标导航研究。", "method": "提出UAV-ON基准，包含14个高保真虚拟环境和1270个标注目标对象，提供语义目标描述。开发了AOA等基线方法，结合语义目标和自身观测进行导航。", "result": "实验显示基线方法在UAV-ON中表现不佳，突显了无人机导航与语义目标结合的挑战。", "conclusion": "UAV-ON为无人机在复杂环境中的语义目标导航研究提供了新方向，推动了自主导航技术的发展。"}}
{"id": "2508.00303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00303", "abs": "https://arxiv.org/abs/2508.00303", "authors": ["Zehui Xu", "Junhui Wang", "Yongliang Shi", "Chao Gao", "Guyue Zhou"], "title": "TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps", "comment": null, "summary": "This paper introduces TopoDiffuser, a diffusion-based framework for\nmultimodal trajectory prediction that incorporates topometric maps to generate\naccurate, diverse, and road-compliant future motion forecasts. By embedding\nstructural cues from topometric maps into the denoising process of a\nconditional diffusion model, the proposed approach enables trajectory\ngeneration that naturally adheres to road geometry without relying on explicit\nconstraints. A multimodal conditioning encoder fuses LiDAR observations,\nhistorical motion, and route information into a unified bird's-eye-view (BEV)\nrepresentation. Extensive experiments on the KITTI benchmark demonstrate that\nTopoDiffuser outperforms state-of-the-art methods, while maintaining strong\ngeometric consistency. Ablation studies further validate the contribution of\neach input modality, as well as the impact of denoising steps and the number of\ntrajectory samples. To support future research, we publicly release our code at\nhttps://github.com/EI-Nav/TopoDiffuser.", "AI": {"tldr": "TopoDiffuser是一种基于扩散的多模态轨迹预测框架，利用拓扑地图生成准确、多样且符合道路的未来运动预测。", "motivation": "现有方法在轨迹预测中难以同时保证准确性和道路几何一致性，TopoDiffuser旨在通过结合拓扑地图解决这一问题。", "method": "通过将拓扑地图的结构信息嵌入条件扩散模型的去噪过程，生成自然符合道路几何的轨迹。多模态编码器融合LiDAR观测、历史运动和路线信息为统一的BEV表示。", "result": "在KITTI基准测试中，TopoDiffuser优于现有方法，并保持几何一致性。消融研究验证了各输入模态和去噪步骤的贡献。", "conclusion": "TopoDiffuser通过结合拓扑地图和扩散模型，实现了准确、多样且道路合规的轨迹预测，为未来研究提供了新思路。"}}
{"id": "2508.00354", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00354", "abs": "https://arxiv.org/abs/2508.00354", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "comment": null, "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "AI": {"tldr": "Omni-Scan是一种利用双机械臂和静态相机生成高质量3D高斯溅射模型的流程，适用于物体缺陷检测。", "motivation": "传统3D物体扫描方法受限于工作空间和设备，Omni-Scan通过双机械臂旋转物体解决遮挡问题。", "method": "使用DepthAnything、Segment Anything和RAFT光流模型识别物体并去除背景，改进3DGS训练流程以处理遮挡。", "result": "在12种工业和家用物体上检测缺陷，平均准确率达83%。", "conclusion": "Omni-Scan提供了一种高效且灵活的3D物体建模方法，适用于多种应用场景。"}}
{"id": "2508.00355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00355", "abs": "https://arxiv.org/abs/2508.00355", "authors": ["Zhenghan Chen", "Haocheng Xu", "Haodong Zhang", "Liang Zhang", "He Li", "Dongqi Wang", "Jiyu Yu", "Yifei Yang", "Zhongxiang Zhou", "Rong Xiong"], "title": "TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots", "comment": null, "summary": "Humanoid robots have the potential capability to perform a diverse range of\nmanipulation tasks, but this is based on a robust and precise standing\ncontroller. Existing methods are either ill-suited to precisely control\nhigh-dimensional upper-body joints, or difficult to ensure both robustness and\naccuracy, especially when upper-body motions are fast. This paper proposes a\nnovel time optimization policy (TOP), to train a standing manipulation control\nmodel that ensures balance, precision, and time efficiency simultaneously, with\nthe idea of adjusting the time trajectory of upper-body motions but not only\nstrengthening the disturbance resistance of the lower-body. Our approach\nconsists of three parts. Firstly, we utilize motion prior to represent\nupper-body motions to enhance the coordination ability between the upper and\nlower-body by training a variational autoencoder (VAE). Then we decouple the\nwhole-body control into an upper-body PD controller for precision and a\nlower-body RL controller to enhance robust stability. Finally, we train TOP\nmethod in conjunction with the decoupled controller and VAE to reduce the\nbalance burden resulting from fast upper-body motions that would destabilize\nthe robot and exceed the capabilities of the lower-body RL policy. The\neffectiveness of the proposed approach is evaluated via both simulation and\nreal world experiments, which demonstrate the superiority on standing\nmanipulation tasks stably and accurately. The project page can be found at\nhttps://anonymous.4open.science/w/top-258F/.", "AI": {"tldr": "提出了一种新颖的时间优化策略（TOP），用于训练人形机器人的站立操纵控制模型，同时确保平衡、精确和时间效率。", "motivation": "现有方法难以同时满足高维上半身关节的精确控制和鲁棒性，尤其是在上半身动作快速时。", "method": "结合运动先验（VAE）、解耦控制（上半身PD控制器和下半身RL控制器）以及TOP方法，优化时间轨迹。", "result": "通过仿真和真实实验验证了方法的有效性，实现了稳定且精确的站立操纵任务。", "conclusion": "TOP方法显著提升了人形机器人在快速上半身动作时的平衡和精确控制能力。"}}
{"id": "2508.00362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00362", "abs": "https://arxiv.org/abs/2508.00362", "authors": ["Zhenghan Chen", "Haodong Zhang", "Dongqi Wang", "Jiyu Yu", "Haocheng Xu", "Yue Wang", "Rong Xiong"], "title": "A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot", "comment": null, "summary": "Motion imitation is a pivotal and effective approach for humanoid robots to\nachieve a more diverse range of complex and expressive movements, making their\nperformances more human-like. However, the significant differences in\nkinematics and dynamics between humanoid robots and humans present a major\nchallenge in accurately imitating motion while maintaining balance. In this\npaper, we propose a novel whole-body motion imitation framework for a full-size\nhumanoid robot. The proposed method employs contact-aware whole-body motion\nretargeting to mimic human motion and provide initial values for reference\ntrajectories, and the non-linear centroidal model predictive controller ensures\nthe motion accuracy while maintaining balance and overcoming external\ndisturbances in real time. The assistance of the whole-body controller allows\nfor more precise torque control. Experiments have been conducted to imitate a\nvariety of human motions both in simulation and in a real-world humanoid robot.\nThese experiments demonstrate the capability of performing with accuracy and\nadaptability, which validates the effectiveness of our approach.", "AI": {"tldr": "提出了一种用于全尺寸人形机器人的全身运动模仿框架，结合接触感知的运动重定向和非线性质心模型预测控制，实现高精度运动模仿与实时平衡。", "motivation": "人形机器人模仿人类运动时，由于动力学和运动学的差异，难以同时保持平衡和运动准确性。", "method": "采用接触感知的全身运动重定向生成参考轨迹，结合非线性质心模型预测控制器实现实时平衡与运动精度。", "result": "实验验证了该方法在仿真和真实机器人上的准确性和适应性。", "conclusion": "所提框架有效解决了人形机器人运动模仿中的平衡与精度问题。"}}
{"id": "2508.00384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00384", "abs": "https://arxiv.org/abs/2508.00384", "authors": ["Juanwu Lu", "Rohit Gupta", "Ahmadreza Moradipari", "Kyungtae Han", "Ruqi Zhang", "Ziran Wang"], "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva", "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.", "AI": {"tldr": "NIVA是一个基于分层贝叶斯模型的概率框架，用于多智能体交通模拟，通过自回归采样实现闭环、观察条件模拟。", "motivation": "随着自动驾驶车辆的快速迭代，需要构建真实且可扩展的多智能体交通模拟器以高效评估。", "method": "NIVA采用分层贝叶斯模型，通过自回归采样从潜在的高斯分布混合中生成模拟数据，统一了轨迹预测模型和闭环模拟模型。", "result": "在Waymo Open Motion数据集上的实验表明，NIVA性能与现有方法相当，同时提供了对意图和驾驶风格的精细控制。", "conclusion": "NIVA为多智能体交通模拟提供了一种高效且灵活的方法，具有潜在的实际应用价值。"}}
{"id": "2508.00467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00467", "abs": "https://arxiv.org/abs/2508.00467", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "AI": {"tldr": "提出了一种基于子集的集体决策方法（SubCDM），通过动态构建子集减少资源消耗，同时保持决策准确性。", "motivation": "现有集体决策策略需要所有机器人参与，资源消耗大且限制了其他任务分配。", "method": "动态构建子集，仅依赖局部信息，自适应调整子集大小以适应不同决策难度。", "result": "仿真实验显示，SubCDM在减少机器人参与数量的同时，保持了与全群决策相当的准确性。", "conclusion": "SubCDM是一种资源高效的集体决策方法，适用于机器人群体。"}}
{"id": "2508.00491", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00491", "abs": "https://arxiv.org/abs/2508.00491", "authors": ["Carlo Alessi", "Federico Vasile", "Federico Ceola", "Giulia Pasquale", "Nicolò Boccardo", "Lorenzo Natale"], "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning", "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation", "AI": {"tldr": "论文提出了一种基于模仿学习的HannesImitationPolicy方法，用于控制假肢手在非结构化环境中抓取物体，并展示了其优于传统分割视觉伺服控制器的性能。", "motivation": "通过模仿学习减少假肢手控制的认知负荷，提升灵活性和适应性，使其能在非结构化场景中学习任务。", "method": "采用模仿学习方法，利用HannesImitationDataset中的抓取演示数据训练扩散策略，预测手腕方向和手部闭合动作。", "result": "实验表明，该方法能成功抓取多种物体，并在非结构化场景中优于分割视觉伺服控制器。", "conclusion": "模仿学习为假肢手控制提供了新思路，增强了其在复杂环境中的适应性和灵活性。"}}
{"id": "2508.00580", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00580", "abs": "https://arxiv.org/abs/2508.00580", "authors": ["Raul Castilla-Arquillo", "Carlos Perez-del-Pulgar", "Levin Gerdes", "Alfonso Garcia-Cerezo", "Miguel A. Olivares-Mendez"], "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery", "comment": null, "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.", "AI": {"tldr": "OmniUnet是一种基于Transformer的神经网络架构，用于RGB-D-T图像语义分割，在火星探测任务中表现出色。", "motivation": "在非结构化环境中，机器人导航需要多模态感知系统以确保安全，而火星探测中热成像对地形安全评估尤为重要。", "method": "开发了OmniUnet网络和定制多模态传感器，并在模拟火星环境的半沙漠中收集数据集进行训练和评估。", "result": "模型像素准确率达80.37%，推理时间673毫秒，适合在资源受限设备上部署。", "conclusion": "OmniUnet和公开数据集为行星机器人多模态地形感知研究提供了重要支持。"}}
{"id": "2508.00584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00584", "abs": "https://arxiv.org/abs/2508.00584", "authors": ["Konstantinos Plotas", "Emmanouil Papadakis", "Drosakis Drosakis", "Panos Trahanias", "Dimitrios Papageorgiou"], "title": "A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup", "comment": "Please find the citation info @ Zenodo, ArXiv or Zenodo, as the\n  proceedings of ICRA are no longer sent to IEEE Xplore", "summary": "In this work, a control scheme for human-robot collaborative object\ntransportation is proposed, considering a quadruped robot equipped with the\nMIGHTY suction cup that serves both as a gripper for holding the object and a\nforce/torque sensor. The proposed control scheme is based on the notion of\nadmittance control, and incorporates a variable damping term aiming towards\nincreasing the controllability of the human and, at the same time, decreasing\nher/his effort. Furthermore, to ensure that the object is not detached from the\nsuction cup during the collaboration, an additional control signal is proposed,\nwhich is based on a barrier artificial potential. The proposed control scheme\nis proven to be passive and its performance is demonstrated through\nexperimental evaluations conducted using the Unitree Go1 robot equipped with\nthe MIGHTY suction cup.", "AI": {"tldr": "提出了一种基于导纳控制和可变阻尼项的人机协作物体运输控制方案，结合屏障人工势能确保物体不脱落，并通过实验验证了其被动性和性能。", "motivation": "解决人机协作运输中控制性和人类操作者负担的问题，同时确保物体在运输过程中不脱落。", "method": "采用导纳控制框架，引入可变阻尼项和基于屏障人工势能的额外控制信号。", "result": "实验证明该控制方案具有被动性，能有效减少人类操作者的负担并提高控制性。", "conclusion": "提出的控制方案在人机协作物体运输中表现出色，兼具控制性和安全性。"}}
{"id": "2508.00625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00625", "abs": "https://arxiv.org/abs/2508.00625", "authors": ["Bartosz Krawczyk", "Ahmed Elbary", "Robbie Cato", "Jagdish Patil", "Kaung Myat", "Anyeh Ndi-Tah", "Nivetha Sakthivel", "Mark Crampton", "Gautham Das", "Charles Fox"], "title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "comment": "6 pages, 4 figures, a TAROS2025 short paper", "summary": "OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.", "AI": {"tldr": "OpenScout v1.1是一款开源硬件移动机器人，升级了计算硬件、ROS2接口和Gazebo模拟，并报告了改进原因和方法。", "motivation": "为研究和工业提供更简化、更便宜且更强大的开源硬件移动机器人解决方案。", "method": "通过改进计算硬件、增加ROS2接口和Gazebo模拟功能，进行项目方法论和结果报告。", "result": "成功开发了OpenScout v1.1，提升了性能和功能。", "conclusion": "OpenScout v1.1为研究和工业提供了更高效的开源机器人平台。"}}
{"id": "2508.00691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00691", "abs": "https://arxiv.org/abs/2508.00691", "authors": ["Fabian C. Weigend", "Dabin K. Choe", "Santiago Canete", "Conor J. Walsh"], "title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "comment": "8 pages, 6 figures, 2 tables", "summary": "Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.", "AI": {"tldr": "该研究提出了一种基于数据驱动的方法，通过多任务时间卷积网络（TCN）为中风后行走者提供踝关节扭矩估计，以实现外骨骼的自适应控制。", "motivation": "尽管数据驱动方法在健康年轻人中表现出色，但应用于中风后步态缺陷人群时面临挑战，如高异质性和数据缺乏。研究旨在解决这些问题，推动外骨骼在非结构化社区环境中的应用。", "method": "使用四名中风后参与者的跑步机行走数据训练多任务TCN模型，并结合六名健康参与者的数据进行预训练。模型基于三个惯性测量单元（IMU）的数据。", "result": "模型在扭矩估计上表现出色（R²为0.74±0.13），并通过一名中风后参与者验证了实时传感、估计和驱动的可行性。", "conclusion": "该研究为中风后行走者的外骨骼自适应控制提供了初步解决方案，展示了数据驱动方法在此领域的潜力。"}}
{"id": "2508.00697", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00697", "abs": "https://arxiv.org/abs/2508.00697", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "comment": "ICCV 2025", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "AI": {"tldr": "LightDP通过压缩去噪模块和减少采样步骤，优化了Diffusion Policies在移动设备上的实时部署，性能接近现有最优方法。", "motivation": "Diffusion Policies在资源受限的移动平台上因计算效率低和内存占用大而难以应用。", "method": "LightDP采用网络压缩和采样步骤减少策略，结合统一剪枝与再训练流程及一致性蒸馏技术。", "result": "实验表明LightDP在多个标准数据集上实现实时预测，性能接近最优Diffusion Policies。", "conclusion": "LightDP为资源受限环境中扩散策略的实际部署迈出了重要一步。"}}
{"id": "2508.00795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00795", "abs": "https://arxiv.org/abs/2508.00795", "authors": ["Junbang Liang", "Pavel Tokmakov", "Ruoshi Liu", "Sruthi Sudhakar", "Paarth Shah", "Rares Ambrus", "Carl Vondrick"], "title": "Video Generators are Robot Policies", "comment": null, "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.", "AI": {"tldr": "论文提出Video Policy框架，通过视频生成作为机器人策略学习的代理，解决了感知和行为分布偏移的泛化问题，并减少对人类演示数据的依赖。", "motivation": "当前视觉运动策略在泛化和数据效率上存在局限，需要一种更高效的方法来提升机器人策略学习。", "method": "提出Video Policy框架，结合视频和动作生成，端到端训练，利用视频生成模型提取策略。", "result": "方法在仿真和现实世界中均表现出强泛化能力，显著提升样本效率和鲁棒性。", "conclusion": "通过大规模视频生成模型，实现了比传统行为克隆更优的性能，为机器人策略学习提供了更高效和可扩展的途径。"}}
