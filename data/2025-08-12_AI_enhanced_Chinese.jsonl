{"id": "2508.06518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06518", "abs": "https://arxiv.org/abs/2508.06518", "authors": ["Ray Wai Man Kong"], "title": "Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing", "comment": "13 pages, 9 figures", "summary": "The applied research is the design and development of an automated folding\nand sewing machine for pleated pants. It represents a significant advancement\nin addressing the challenges associated with manual sewing processes.\nTraditional methods for creating pleats are labour-intensive, prone to\ninconsistencies, and require high levels of skill, making automation a critical\nneed in the apparel industry. This research explores the technical feasibility\nand operational benefits of integrating advanced technologies into garment\nproduction, focusing on the creation of an automated machine capable of precise\nfolding and sewing operations and eliminating the marking operation.\n  The proposed machine incorporates key features such as a precision folding\nmechanism integrated into the automated sewing unit with real-time monitoring\ncapabilities. The results demonstrate remarkable improvements: the standard\nlabour time has been reduced by 93%, dropping from 117 seconds per piece to\njust 8 seconds with the automated system. Similarly, machinery time improved by\n73%, and the total output rate increased by 72%. These enhancements translate\ninto a cycle time reduction from 117 seconds per piece to an impressive 33\nseconds, enabling manufacturers to meet customer demand more swiftly. By\neliminating manual marking processes, the machine not only reduces labour costs\nbut also minimizes waste through consistent pleat formation. This automation\naligns with industry trends toward sustainability and efficiency, potentially\nreducing environmental impact by decreasing material waste and energy\nconsumption.", "AI": {"tldr": "该研究设计了一款自动化折叠和缝纫机，用于生产褶皱裤，显著提升了传统手工缝纫的效率。", "motivation": "传统褶皱制作方法劳动密集、易出错且技术要求高，自动化需求迫切。", "method": "集成精密折叠机制和实时监控功能的自动化缝纫单元。", "result": "劳动力时间减少93%，机器时间提升73%，总产出率增加72%。", "conclusion": "自动化机器提高了效率，降低了成本和浪费，符合可持续性趋势。"}}
{"id": "2508.06520", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06520", "abs": "https://arxiv.org/abs/2508.06520", "authors": ["Liwei Chen", "Tong Qin", "Zhenhua Huangfu", "Li Li", "Wei Wei"], "title": "Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator", "comment": null, "summary": "We propose a differentiable optimization framework for flip-and-landing\ntrajectory design of reusable spacecraft, exemplified by the Starship vehicle.\nA deep neural network surrogate, trained on high-fidelity CFD data, predicts\naerodynamic forces and moments, and is tightly coupled with a differentiable\nrigid-body dynamics solver. This enables end-to-end gradient-based trajectory\noptimization without linearization or convex relaxation. The framework handles\nactuator limits and terminal landing constraints, producing physically\nconsistent, optimized control sequences. Both standard automatic\ndifferentiation and Neural ODEs are applied to support long-horizon rollouts.\nResults demonstrate the framework's effectiveness in modeling and optimizing\ncomplex maneuvers with high nonlinearities. This work lays the groundwork for\nfuture extensions involving unsteady aerodynamics, plume interactions, and\nintelligent guidance design.", "AI": {"tldr": "提出了一种可微分优化框架，用于可重复使用航天器的翻转和着陆轨迹设计，结合深度学习与动力学求解器实现端到端优化。", "motivation": "解决传统轨迹优化方法中线性化或凸松弛的限制，实现高非线性复杂机动的建模与优化。", "method": "使用深度神经网络代理模型预测气动力和力矩，结合可微分刚体动力学求解器，支持端到端梯度优化。", "result": "框架成功处理执行器限制和终端着陆约束，生成物理一致的优化控制序列。", "conclusion": "为未来涉及非定常气动力学、羽流交互和智能制导设计的扩展奠定了基础。"}}
{"id": "2508.06521", "categories": ["cs.RO", "68T40, 93C85, 70E60"], "pdf": "https://arxiv.org/pdf/2508.06521", "abs": "https://arxiv.org/abs/2508.06521", "authors": ["H. Liu", "L. S. Moreu", "T. S. Andersen", "V. V. Puche", "M. Fumagalli"], "title": "Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments", "comment": "7 pages, submitted", "summary": "The increasing demand for critical raw materials has revitalized interest in\nabandoned underground mines, which pose extreme challenges for conventional\ndrilling machinery due to confined, unstructured, and infrastructure-less\nenvironments. This paper presents the Stinger Robot, a novel compact robotic\nplatform specifically designed for autonomous high-force drilling in such\nsettings. The robot features a mechanically self-locking tri-leg bracing\nmechanism that enables stable anchoring to irregular tunnel surfaces. A key\ninnovation lies in its force-aware, closed-loop control strategy, which enables\nforce interaction with unstructured environments during bracing and drilling.\nImplemented as a finite-state machine in ROS 2, the control policy dynamically\nadapts leg deployment based on real-time contact feedback and load thresholds,\nensuring stability without external supports. We demonstrate, through\nsimulation and preliminary hardware tests, that the Stinger Robot can\nautonomously stabilize and drill in conditions previously inaccessible to\nnowadays mining machines. This work constitutes the first validated robotic\narchitecture to integrate distributed force-bracing and autonomous drilling in\nunderground environments, laying the groundwork for future collaborative mining\noperations using modular robot systems.", "AI": {"tldr": "论文提出了一种名为Stinger Robot的新型紧凑型机器人平台，专为在废弃地下矿山中自主高力钻孔而设计，解决了传统钻机在狭窄、无结构环境中的挑战。", "motivation": "由于对关键原材料的需求增加，废弃地下矿山重新受到关注，但这些环境对传统钻机提出了极端挑战。", "method": "机器人采用机械自锁三腿支撑机制，结合力感知闭环控制策略，通过ROS 2中的有限状态机动态调整腿部部署。", "result": "模拟和初步硬件测试表明，Stinger Robot能在传统采矿机器无法工作的环境中自主稳定和钻孔。", "conclusion": "该研究首次验证了在地下环境中集成分布式力支撑和自主钻孔的机器人架构，为未来模块化机器人协作采矿奠定了基础。"}}
{"id": "2508.06534", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06534", "abs": "https://arxiv.org/abs/2508.06534", "authors": ["Aishan Liu", "Jiakai Wang", "Tianyuan Zhang", "Hainan Li", "Jiangfan Liu", "Siyuan Liang", "Yilong Ren", "Xianglong Liu", "Dacheng Tao"], "title": "MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving", "comment": "Accepted by ACM MM 2025 Demo/Videos track", "summary": "Evaluating and ensuring the adversarial robustness of autonomous driving (AD)\nsystems is a critical and unresolved challenge. This paper introduces MetAdv, a\nnovel adversarial testing platform that enables realistic, dynamic, and\ninteractive evaluation by tightly integrating virtual simulation with physical\nvehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical\nsandbox, within which we design a three-layer closed-loop testing environment\nwith dynamic adversarial test evolution. This architecture facilitates\nend-to-end adversarial evaluation, ranging from high-level unified adversarial\ngeneration, through mid-level simulation-based interaction, to low-level\nexecution on physical vehicles. Additionally, MetAdv supports a broad spectrum\nof AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,\nend-to-end learning, vision-language models). It supports flexible 3D vehicle\nmodeling and seamless transitions between simulated and physical environments,\nwith built-in compatibility for commercial platforms such as Apollo and Tesla.\nA key feature of MetAdv is its human-in-the-loop capability: besides flexible\nenvironmental configuration for more customized evaluation, it enables\nreal-time capture of physiological signals and behavioral feedback from\ndrivers, offering new insights into human-machine trust under adversarial\nconditions. We believe MetAdv can offer a scalable and unified framework for\nadversarial assessment, paving the way for safer AD.", "AI": {"tldr": "MetAdv是一个新型对抗测试平台，通过虚拟仿真与物理车辆反馈的紧密结合，实现了自动驾驶系统的动态、交互式对抗评估。", "motivation": "评估和确保自动驾驶系统的对抗鲁棒性是一个关键且未解决的挑战。", "method": "MetAdv建立了一个混合虚拟-物理沙盒，设计了三层闭环测试环境，支持端到端对抗评估，涵盖从高层对抗生成到低层物理车辆执行的整个过程。", "result": "MetAdv支持广泛的自动驾驶任务和算法范式，具有灵活的3D车辆建模和模拟-物理环境无缝切换能力，并具备人机交互功能。", "conclusion": "MetAdv为对抗评估提供了一个可扩展的统一框架，有助于实现更安全的自动驾驶。"}}
{"id": "2508.06538", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06538", "abs": "https://arxiv.org/abs/2508.06538", "authors": ["Gioele Buriani", "Jingyue Liu", "Maximilian Stölzle", "Cosimo Della Santina", "Jiatao Ding"], "title": "Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots", "comment": "8 pages, under review", "summary": "Reduced-order models are essential for motion planning and control of\nquadruped robots, as they simplify complex dynamics while preserving critical\nbehaviors. This paper introduces a novel methodology for deriving such\ninterpretable dynamic models, specifically for jumping. We capture the\nhigh-dimensional, nonlinear jumping dynamics in a low-dimensional latent space\nby proposing a learning architecture combining Sparse Identification of\nNonlinear Dynamics (SINDy) with physical structural priors on the jump\ndynamics. Our approach demonstrates superior accuracy to the traditional\nactuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through\nsimulation and hardware experiments across different jumping strategies.", "AI": {"tldr": "提出了一种结合SINDy与物理结构先验的学习架构，用于构建四足机器人跳跃的可解释降阶模型，其精度优于传统aSLIP模型。", "motivation": "降阶模型对四足机器人的运动规划与控制至关重要，但需在简化复杂动力学的同时保留关键行为。", "method": "结合Sparse Identification of Nonlinear Dynamics (SINDy)与跳跃动力学的物理结构先验，构建低维潜在空间模型。", "result": "新方法在仿真和硬件实验中表现出优于传统aSLIP模型的精度，适用于多种跳跃策略。", "conclusion": "该方法为四足机器人跳跃提供了高精度且可解释的降阶模型，验证了其有效性。"}}
{"id": "2508.06547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06547", "abs": "https://arxiv.org/abs/2508.06547", "authors": ["Heran Wu", "Zirun Zhou", "Jingfeng Zhang"], "title": "A tutorial note on collecting simulated data for vision-language-action models", "comment": "This is a tutorial note for educational purposes", "summary": "Traditional robotic systems typically decompose intelligence into independent\nmodules for computer vision, natural language processing, and motion control.\nVision-Language-Action (VLA) models fundamentally transform this approach by\nemploying a single neural network that can simultaneously process visual\nobservations, understand human instructions, and directly output robot actions\n-- all within a unified framework. However, these systems are highly dependent\non high-quality training datasets that can capture the complex relationships\nbetween visual observations, language instructions, and robotic actions. This\ntutorial reviews three representative systems: the PyBullet simulation\nframework for flexible customized data generation, the LIBERO benchmark suite\nfor standardized task definition and evaluation, and the RT-X dataset\ncollection for large-scale multi-robot data acquisition. We demonstrated\ndataset generation approaches in PyBullet simulation and customized data\ncollection within LIBERO, and provide an overview of the characteristics and\nroles of the RT-X dataset for large-scale multi-robot data acquisition.", "AI": {"tldr": "VLA模型通过单一神经网络统一处理视觉、语言和动作，但依赖高质量数据集。本文介绍了三种代表性系统：PyBullet、LIBERO和RT-X。", "motivation": "传统机器人系统将智能分解为独立模块，而VLA模型通过统一框架实现多任务处理，但需要高质量数据集支持。", "method": "介绍了三种系统：PyBullet用于灵活数据生成，LIBERO用于标准化任务评估，RT-X用于大规模多机器人数据采集。", "result": "展示了PyBullet中的数据生成方法和LIBERO中的定制数据收集，并概述了RT-X数据集的特点和作用。", "conclusion": "VLA模型依赖高质量数据集，PyBullet、LIBERO和RT-X为数据生成和评估提供了有效工具。"}}
{"id": "2508.06742", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06742", "abs": "https://arxiv.org/abs/2508.06742", "authors": ["Alejandro Murillo-Gonzalez", "Junhong Xu", "Lantao Liu"], "title": "Learning Causal Structure Distributions for Robust Planning", "comment": null, "summary": "Structural causal models describe how the components of a robotic system\ninteract. They provide both structural and functional information about the\nrelationships that are present in the system. The structural information\noutlines the variables among which there is interaction. The functional\ninformation describes how such interactions work, via equations or learned\nmodels. In this paper we find that learning the functional relationships while\naccounting for the uncertainty about the structural information leads to more\nrobust dynamics models which improves downstream planning, while using\nsignificantly lower computational resources. This in contrast with common\nmodel-learning methods that ignore the causal structure and fail to leverage\nthe sparsity of interactions in robotic systems. We achieve this by estimating\na causal structure distribution that is used to sample causal graphs that\ninform the latent-space representations in an encoder-multidecoder\nprobabilistic model. We show that our model can be used to learn the dynamics\nof a robot, which together with a sampling-based planner can be used to perform\nnew tasks in novel environments, provided an objective function for the new\nrequirement is available. We validate our method using manipulators and mobile\nrobots in both simulation and the real-world. Additionally, we validate the\nlearned dynamics' adaptability and increased robustness to corrupted inputs and\nchanges in the environment, which is highly desirable in challenging real-world\nrobotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.", "AI": {"tldr": "论文提出了一种通过学习功能关系并考虑结构信息不确定性的方法，提高了机器人动力学模型的鲁棒性，同时减少了计算资源消耗。", "motivation": "传统模型学习方法忽略了因果结构，未能利用机器人系统中交互的稀疏性。本文旨在通过结合结构信息的不确定性，提升模型的鲁棒性和效率。", "method": "通过估计因果结构分布，采样因果图以指导编码器-多解码器概率模型中的潜在空间表示。", "result": "模型能够学习机器人动力学，结合采样规划器完成新任务，并在仿真和实际环境中验证了其适应性和鲁棒性。", "conclusion": "该方法显著提升了机器人动力学模型的鲁棒性和适应性，适用于复杂现实场景。"}}
{"id": "2508.06554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06554", "abs": "https://arxiv.org/abs/2508.06554", "authors": ["Abdelhaleem Saad", "Waseem Akram", "Irfan Hussain"], "title": "AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance", "comment": null, "summary": "Inspection of aquaculture net pens is essential for ensuring the structural\nintegrity and sustainable operation of offshore fish farming systems.\nTraditional methods, typically based on manually operated or single-ROV\nsystems, offer limited adaptability to real-time constraints such as energy\nconsumption, hardware faults, and dynamic underwater conditions. This paper\nintroduces AquaChat++, a novel multi-ROV inspection framework that uses Large\nLanguage Models (LLMs) to enable adaptive mission planning, coordinated task\nexecution, and fault-tolerant control in complex aquaculture environments. The\nproposed system consists of a two-layered architecture. The high-level plan\ngeneration layer employs an LLM, such as ChatGPT-4, to translate natural\nlanguage user commands into symbolic, multi-agent inspection plans. A task\nmanager dynamically allocates and schedules actions among ROVs based on their\nreal-time status and operational constraints, including thruster faults and\nbattery levels. The low-level control layer ensures accurate trajectory\ntracking and integrates thruster fault detection and compensation mechanisms.\nBy incorporating real-time feedback and event-triggered replanning, AquaChat++\nenhances system robustness and operational efficiency. Simulated experiments in\na physics-based aquaculture environment demonstrate improved inspection\ncoverage, energy-efficient behavior, and resilience to actuator failures. These\nfindings highlight the potential of LLM-driven frameworks to support scalable,\nintelligent, and autonomous underwater robotic operations within the\naquaculture sector.", "AI": {"tldr": "AquaChat++ 是一个基于大型语言模型的多 ROV 检查框架，用于提高水产养殖网箱检查的适应性和效率。", "motivation": "传统的水产养殖网箱检查方法适应性差，无法应对实时约束（如能耗、硬件故障和动态水下环境）。", "method": "AquaChat++ 采用双层架构：高层计划生成层使用 LLM 将自然语言命令转换为多智能体检查计划；低层控制层确保轨迹跟踪和故障补偿。", "result": "模拟实验显示 AquaChat++ 提高了检查覆盖率、能效和故障恢复能力。", "conclusion": "LLM 驱动的框架有望支持水产养殖领域的智能自主水下机器人操作。"}}
{"id": "2508.06744", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06744", "abs": "https://arxiv.org/abs/2508.06744", "authors": ["Yunke Ao", "Manish Prajapat", "Yarden As", "Yassine Taoudi-Benchekroun", "Fabio Carrillo", "Hooman Esfandiari", "Benjamin F. Grewe", "Andreas Krause", "Philipp Fürnstahl"], "title": "Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery", "comment": null, "summary": "Safety-critical control using high-dimensional sensory feedback from optical\ndata (e.g., images, point clouds) poses significant challenges in domains like\nautonomous driving and robotic surgery. Control can rely on low-dimensional\nstates estimated from high-dimensional data. However, the estimation errors\noften follow complex, unknown distributions that standard probabilistic models\nfail to capture, making formal safety guarantees challenging. In this work, we\nintroduce a novel characterization of these general estimation errors using\nsub-Gaussian noise with bounded mean. We develop a new technique for\nuncertainty propagation of proposed noise characterization in linear systems,\nwhich combines robust set-based methods with the propagation of sub-Gaussian\nvariance proxies. We further develop a Model Predictive Control (MPC) framework\nthat provides closed-loop safety guarantees for linear systems under the\nproposed noise assumption. We apply this MPC approach in an\nultrasound-image-guided robotic spinal surgery pipeline, which contains\ndeep-learning-based semantic segmentation, image-based registration, high-level\noptimization-based planning, and low-level robotic control. To validate the\npipeline, we developed a realistic simulation environment integrating real\nhuman anatomy, robot dynamics, efficient ultrasound simulation, as well as\nin-vivo data of breathing motion and drilling force. Evaluation results in\nsimulation demonstrate the potential of our approach for solving complex\nimage-guided robotic surgery task while ensuring safety.", "AI": {"tldr": "论文提出了一种基于高维光学数据的安全关键控制方法，通过子高斯噪声模型处理估计误差，并结合MPC框架确保线性系统的闭环安全性，应用于机器人脊柱手术。", "motivation": "高维光学数据（如图像、点云）在自动驾驶和机器人手术等领域的安全控制面临挑战，传统概率模型难以捕捉复杂的估计误差分布，导致安全保证困难。", "method": "提出子高斯噪声模型描述估计误差，结合鲁棒集方法和子高斯方差传播技术，开发了具有安全保证的MPC框架。", "result": "在机器人脊柱手术的仿真环境中验证了方法的有效性，展示了其在复杂图像引导任务中的潜力。", "conclusion": "该方法为高维数据驱动的安全控制提供了新思路，尤其在医疗机器人领域具有应用前景。"}}
{"id": "2508.06568", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06568", "abs": "https://arxiv.org/abs/2508.06568", "authors": ["Amin Yazdanshenas", "Reza Faieghi"], "title": "Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control", "comment": null, "summary": "This paper presents a new adaptive sliding mode control (SMC) framework for\nquadrotors that achieves robust and agile flight under tight computational\nconstraints. The proposed controller addresses key limitations of prior SMC\nformulations, including (i) the slow convergence and almost-global stability of\n$\\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational\ndynamics in Euler-based controllers, (iii) the unwinding phenomenon in\nquaternion-based formulations, and (iv) the gain overgrowth problem in adaptive\nSMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous\nglobal stability proofs for both the nonsmooth attitude sliding dynamics\ndefined on $\\mathbb{S}^3$ and the position sliding dynamics. Our controller is\ncomputationally efficient and runs reliably on a resource-constrained nano\nquadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude\ncontrol, respectively. In an extensive set of hardware experiments with over\n130 flight trials, the proposed controller consistently outperforms three\nbenchmark methods, demonstrating superior trajectory tracking accuracy and\nrobustness with relatively low control effort. The controller enables\naggressive maneuvers such as dynamic throw launches, flip maneuvers, and\naccelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.\nThese results highlight promising potential for real-world applications,\nparticularly in scenarios requiring robust, high-performance flight control\nunder significant external disturbances and tight computational constraints.", "AI": {"tldr": "提出了一种新的自适应滑模控制框架，用于四旋翼飞行器，在计算资源受限下实现鲁棒和敏捷飞行。", "motivation": "解决现有滑模控制方法的局限性，如收敛慢、全局稳定性不足、旋转动力学简化、解绕问题和增益增长问题。", "method": "利用非光滑稳定性分析，设计了在S³上的姿态滑动动力学和位置滑动动力学，并证明了全局稳定性。", "result": "在硬件实验中，控制器在130多次飞行试验中表现优于基准方法，实现了高刷新率和低控制能耗。", "conclusion": "该控制器在外部干扰和计算资源受限下展现出高性能飞行控制的潜力。"}}
{"id": "2508.07045", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07045", "abs": "https://arxiv.org/abs/2508.07045", "authors": ["Dennis Benders", "Johannes Köhler", "Robert Babuška", "Javier Alonso-Mora", "Laura Ferranti"], "title": "From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline", "comment": "8 pages, 5 figures", "summary": "Model predictive control (MPC) is a powerful strategy for planning and\ncontrol in autonomous mobile robot navigation. However, ensuring safety in\nreal-world deployments remains challenging due to the presence of disturbances\nand measurement noise. Existing approaches often rely on idealized assumptions,\nneglect the impact of noisy measurements, and simply heuristically guess\nunrealistic bounds. In this work, we present an efficient and modular robust\nMPC design pipeline that systematically addresses these limitations. The\npipeline consists of an iterative procedure that leverages closed-loop\nexperimental data to estimate disturbance bounds and synthesize a robust\noutput-feedback MPC scheme. We provide the pipeline in the form of\ndeterministic and reproducible code to synthesize the robust output-feedback\nMPC from data. We empirically demonstrate robust constraint satisfaction and\nrecursive feasibility in quadrotor simulations using Gazebo.", "AI": {"tldr": "提出了一种模块化鲁棒MPC设计流程，通过闭环实验数据估计扰动边界，确保无人机导航中的安全性和可行性。", "motivation": "现有MPC方法在真实环境中因扰动和噪声难以保证安全性，通常依赖理想假设或启发式猜测。", "method": "采用迭代流程，利用闭环实验数据估计扰动边界，并设计鲁棒输出反馈MPC方案。", "result": "在Gazebo仿真中验证了鲁棒约束满足和递归可行性。", "conclusion": "该流程高效、模块化，能系统解决现有MPC方法的局限性。"}}
{"id": "2508.06575", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06575", "abs": "https://arxiv.org/abs/2508.06575", "authors": ["Rui Zhou"], "title": "Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios", "comment": null, "summary": "Ensuring the safety of autonomous vehicles (AVs) is paramount in their\ndevelopment and deployment. Safety-critical scenarios pose more severe\nchallenges, necessitating efficient testing methods to validate AVs safety.\nThis study focuses on designing an accelerated testing algorithm for AVs in\nsafety-critical scenarios, enabling swift recognition of their driving\ncapabilities. First, typical logical scenarios were extracted from real-world\ncrashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)\ndatabase, obtaining pre-crash features through reconstruction. Second, Baidu\nApollo, an advanced black-box automated driving system (ADS) is integrated to\ncontrol the behavior of the ego vehicle. Third, we proposed an adaptive\nlarge-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to\nexpedite the testing process. Experimental results demonstrate a significant\nenhancement in testing efficiency when utilizing ALVNS-SA. It achieves an\n84.00% coverage of safety-critical scenarios, with crash scenario coverage of\n96.83% and near-crash scenario coverage of 92.07%. Compared to genetic\nalgorithm (GA), adaptive large neighborhood-simulated annealing algorithm\n(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage\nin safety-critical scenarios.", "AI": {"tldr": "该研究提出了一种加速测试算法（ALVNS-SA），用于验证自动驾驶车辆在安全关键场景中的安全性，显著提高了测试效率和场景覆盖率。", "motivation": "自动驾驶车辆的安全性至关重要，尤其是在安全关键场景中，需要高效的测试方法来验证其驾驶能力。", "method": "研究从真实事故数据库中提取典型逻辑场景，结合百度Apollo自动驾驶系统，并提出了ALVNS-SA算法以加速测试过程。", "result": "ALVNS-SA算法在安全关键场景中实现了84.00%的覆盖率，其中碰撞场景覆盖率为96.83%，接近碰撞场景覆盖率为92.07%，显著优于其他方法。", "conclusion": "ALVNS-SA算法在测试效率和场景覆盖率方面表现出色，为自动驾驶车辆的安全验证提供了有效工具。"}}
{"id": "2508.07079", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07079", "abs": "https://arxiv.org/abs/2508.07079", "authors": ["Mohamed Parvez Aslam", "Bojan Derajic", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Jan Oliver Ringert"], "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction", "comment": null, "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for\nautonomous robots. This work evaluates the integration of a deep learning-based\nSocial-Implicit (SI) pedestrian trajectory predictor within a Model Predictive\nControl (MPC) framework on the physical Continental Corriere robot. Tested\nacross varied pedestrian densities, the SI-MPC system is compared to a\ntraditional Constant Velocity (CV) model in both open-loop prediction and\nclosed-loop navigation. Results show that SI improves trajectory prediction -\nreducing errors by up to 76% in low-density settings - and enhances safety and\nmotion smoothness in crowded scenes. Moreover, real-world deployment reveals\ndiscrepancies between open-loop metrics and closed-loop performance, as the SI\nmodel yields broader, more cautious predictions. These findings emphasize the\nimportance of system-level evaluation and highlight the SI-MPC framework's\npromise for safer, more adaptive navigation in dynamic, human-populated\nenvironments.", "AI": {"tldr": "论文研究了在行人密集环境中，将基于深度学习的Social-Implicit（SI）行人轨迹预测器与模型预测控制（MPC）框架结合，以提升自主机器人的导航安全性和适应性。", "motivation": "解决自主机器人在行人密集环境中的安全导航问题，传统方法（如恒定速度模型）在复杂场景中表现不佳。", "method": "将SI行人轨迹预测器集成到MPC框架中，并在物理机器人上测试，对比传统CV模型的开环预测和闭环导航性能。", "result": "SI模型在低密度场景中减少轨迹预测误差达76%，在拥挤场景中提升安全性和运动平滑性。", "conclusion": "SI-MPC框架在动态行人环境中展现出更高的安全性和适应性，强调系统级评估的重要性。"}}
{"id": "2508.06687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06687", "abs": "https://arxiv.org/abs/2508.06687", "authors": ["Sreeja Roy-Singh", "Vinay Ravindra", "Richard Levinson", "Mahta Moghaddam", "Jan Mandel", "Adam Kochanski", "Angel Farguell Caus", "Kurtis Nelson", "Samira Alkaee Taleghan", "Archana Kannan", "Amer Melebari"], "title": "Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation", "comment": null, "summary": "We propose a novel concept of operations using optimal planning methods and\nmachine learning (ML) to collect spaceborne data that is unprecedented for\nmonitoring wildfires, process it to create new or enhanced products in the\ncontext of wildfire danger or spread monitoring, and assimilate them to improve\nexisting, wildfire decision support tools delivered to firefighters within\nlatency appropriate for time-critical applications. The concept is studied with\nrespect to NASA's CYGNSS Mission, a constellation of passive microwave\nreceivers that measure specular GNSS-R reflections despite clouds and smoke.\nOur planner uses a Mixed Integer Program formulation to schedule joint\nobservation data collection and downlink for all satellites. Optimal solutions\nare found quickly that collect 98-100% of available observation opportunities.\nML-based fire predictions that drive the planner objective are greater than 40%\nmore correlated with ground truth than existing state-of-art. The presented\ncase study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025\nrepresents the first high-resolution data collected by CYGNSS of active fires.\nCreation of Burnt Area Maps (BAM) using ML applied to the data during active\nfires and BAM assimilation into NASA's Weather Research and Forecasting Model\nusing ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained\nsoil moisture are integrated for the first time into USGS fire danger maps.\nInclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,\nand inclusion of high-resolution data boosts ML recall by another 15%. The\nproposed workflow has an expected latency of 6-30h, improving on the current\ndelivery time of multiple days. All components in the proposed concept are\nshown to be computationally scalable and globally generalizable, with\nsustainability considerations such as edge efficiency and low latency on small\ndevices.", "AI": {"tldr": "提出一种结合最优规划和机器学习的方法，用于收集和处理太空数据以监测野火，提升现有决策支持工具的效率。", "motivation": "通过优化数据收集和处理流程，提高野火监测和预测的准确性和时效性，支持消防员的实时决策。", "method": "采用混合整数规划调度卫星观测和数据下传，结合机器学习预测野火，生成高分辨率数据产品。", "result": "优化方案能捕获98-100%的观测机会，机器学习预测准确率提升40%，数据整合使预测准确率再提升13-15%。", "conclusion": "该方法显著提升了野火监测的效率和准确性，具有计算可扩展性和全球适用性，适合实时应用。"}}
{"id": "2508.07323", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07323", "abs": "https://arxiv.org/abs/2508.07323", "authors": ["Adeetya Uppal", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)", "comment": null, "summary": "Robotic trajectory planning in dynamic and cluttered environments remains a\ncritical challenge, particularly when striving for both time efficiency and\nmotion smoothness under actuation constraints. Traditional path planner, such\nas Artificial Potential Field (APF), offer computational efficiency but suffer\nfrom local minima issue due to position-based potential field functions and\noscillatory motion near the obstacles due to Newtonian mechanics. To address\nthis limitation, an Energy-based Artificial Potential Field (APF) framework is\nproposed in this paper that integrates position and velocity-dependent\npotential functions. E-APF ensures dynamic adaptability and mitigates local\nminima, enabling uninterrupted progression toward the goal. The proposed\nframework integrates E-APF with a hybrid trajectory optimizer that jointly\nminimizes jerk and execution time under velocity and acceleration constraints,\nensuring geometric smoothness and time efficiency. The entire framework is\nvalidated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic\nmanipulator. The results demonstrate collision-free, smooth, time-efficient,\nand oscillation-free trajectory in the presence of obstacles, highlighting the\nefficacy of the combined trajectory optimization and real-time obstacle\navoidance approach. This work lays the foundation for future integration with\nreactive control strategies and physical hardware deployment in real-world\nmanipulation tasks.", "AI": {"tldr": "提出了一种基于能量的APF框架（E-APF），结合位置和速度依赖的势函数，解决了传统APF的局部最小值和振荡问题，并通过混合轨迹优化器实现了平滑且时间高效的轨迹规划。", "motivation": "动态和复杂环境中机器人轨迹规划面临时间效率和运动平滑性的挑战，传统APF方法存在局部最小值和振荡问题。", "method": "提出E-APF框架，结合位置和速度依赖的势函数，并与混合轨迹优化器联合优化，最小化急动和执行时间。", "result": "在7自由度Kinova Gen3机械臂仿真中验证，实现了无碰撞、平滑、高效且无振荡的轨迹。", "conclusion": "E-APF框架为未来与反应控制策略和实际硬件部署的集成奠定了基础。"}}
{"id": "2508.06722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06722", "abs": "https://arxiv.org/abs/2508.06722", "authors": ["Justin London"], "title": "Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC", "comment": null, "summary": "Obstacle avoidance enables autonomous agents and robots to operate safely and\nefficiently in dynamic and complex environments, reducing the risk of\ncollisions and damage. For a robot or autonomous system to successfully\nnavigate through obstacles, it must be able to detect such obstacles. While\nnumerous collision avoidance algorithms like the dynamic window approach (DWA),\ntimed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been\nproposed, they may lead to suboptimal paths due to fixed weights, be\ncomputationally expensive, or have limited adaptability to dynamic obstacles in\nmulti-agent environments. Optimal reciprocal collision avoidance (ORCA), which\nimproves on RVO, provides smoother trajectories and stronger collision\navoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy\nlogic controllers (FLCs) to better handle uncertainty and imprecision for\nobstacle avoidance in path planning. Numerous multi-agent experiments are\nconducted and it is shown that ORCA-FL can outperform ORCA in reducing the\nnumber of collision if the agent has a velocity that exceeds a certain\nthreshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy\nQ reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.", "AI": {"tldr": "ORCA-FL通过模糊逻辑控制器改进ORCA算法，以更好地处理路径规划中的不确定性和不精确性，在多智能体实验中表现优于ORCA。", "motivation": "现有避障算法（如DWA、TEB、RVO）存在路径次优、计算成本高或动态障碍适应性差的问题，ORCA虽改进RVO但仍需提升。", "method": "提出ORCA-FL，利用模糊逻辑控制器（FLCs）优化ORCA，并进一步通过模糊Q强化学习（FQL）调优FLCs。", "result": "实验表明，ORCA-FL在智能体速度超过阈值时能减少碰撞次数，优于ORCA。", "conclusion": "ORCA-FL结合模糊逻辑和强化学习，显著提升了动态多智能体环境中的避障性能。"}}
{"id": "2508.07885", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07885", "abs": "https://arxiv.org/abs/2508.07885", "authors": ["Shoaib Ahmmad", "Zubayer Ahmed Aditto", "Md Mehrab Hossain", "Noushin Yeasmin", "Shorower Hossain"], "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning", "comment": null, "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.", "AI": {"tldr": "论文提出了一种基于AI的感知系统，用于GPS缺失的室内环境中自主四轴飞行器导航，结合云计算和定制PCB，实现了高效导航和避障。", "motivation": "解决GPS缺失环境下四轴飞行器的自主导航问题，提升在狭小空间中的感知和决策能力。", "method": "系统整合了YOLOv11目标检测、Depth Anything V2深度估计、定制PCB（含ToF传感器和IMU）、云端LLM决策，以及虚拟安全边界和多线程架构。", "result": "实验显示，目标检测mAP50为0.6，深度估计MAE为7.2 cm，42次试验中仅16次安全边界突破，系统延迟低于1秒。", "conclusion": "该框架为GPS缺失环境下的无人机导航提供了高效辅助感知系统，补充了现有技术的不足。"}}
{"id": "2508.08144", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08144", "abs": "https://arxiv.org/abs/2508.08144", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Amjad Haider", "Daniel Görges"], "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models", "comment": "Submitted in: The 2026 IEEE/SICE International Symposium on System\n  Integration (SII 2026)", "summary": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments.", "AI": {"tldr": "本文提出了一种基于组件感知结构化剪枝的模型压缩方法，用于在资源受限设备上部署神经网络控制器（NNCs），同时保持稳定性和性能。", "motivation": "随着资源受限移动平台的快速发展，对高效神经网络控制器的需求增加，但传统深度神经网络的计算复杂性和内存需求限制了其实际应用。", "method": "采用组件感知结构化剪枝方法，结合数学稳定性保证（如Lyapunov准则），确定最优剪枝幅度，并在TD-MPC算法上进行验证。", "result": "实验表明，该方法在降低模型复杂度的同时保持了控制性能和稳定性，并确定了安全压缩比的理论边界。", "conclusion": "该方法为资源受限环境下压缩NNCs的部署提供了理论框架和实践指导。"}}
{"id": "2508.06779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06779", "abs": "https://arxiv.org/abs/2508.06779", "authors": ["Minku Kim", "Brian Acosta", "Pratik Chaudhari", "Michael Posa"], "title": "Learning a Vision-Based Footstep Planner for Hierarchical Walking Control", "comment": "8 pages, 8 figures, accepted to 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots", "summary": "Bipedal robots demonstrate potential in navigating challenging terrains\nthrough dynamic ground contact. However, current frameworks often depend solely\non proprioception or use manually designed visual pipelines, which are fragile\nin real-world settings and complicate real-time footstep planning in\nunstructured environments. To address this problem, we present a vision-based\nhierarchical control framework that integrates a reinforcement learning\nhigh-level footstep planner, which generates footstep commands based on a local\nelevation map, with a low-level Operational Space Controller that tracks the\ngenerated trajectories. We utilize the Angular Momentum Linear Inverted\nPendulum model to construct a low-dimensional state representation to capture\nan informative encoding of the dynamics while reducing complexity. We evaluate\nour method across different terrain conditions using the underactuated bipedal\nrobot Cassie and investigate the capabilities and challenges of our approach\nthrough simulation and hardware experiments.", "AI": {"tldr": "提出了一种基于视觉的分层控制框架，结合强化学习的高层脚步规划器和低层操作空间控制器，以解决双足机器人在非结构化环境中的实时脚步规划问题。", "motivation": "当前的双足机器人框架依赖本体感知或手动设计的视觉管道，在现实环境中脆弱且难以实时规划脚步。", "method": "采用基于强化学习的高层脚步规划器生成脚步命令，结合低层操作空间控制器跟踪轨迹，并使用角动量线性倒立摆模型简化状态表示。", "result": "在双足机器人Cassie上评估了不同地形条件下的性能，并通过仿真和硬件实验验证了方法的有效性。", "conclusion": "该框架在非结构化环境中表现出潜力，但仍需进一步解决实际挑战。"}}
{"id": "2508.06804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06804", "abs": "https://arxiv.org/abs/2508.06804", "authors": ["Shu-Ang Yu", "Feng Gao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning", "comment": null, "summary": "Diffusion policies excel at learning complex action distributions for robotic\nvisuomotor tasks, yet their iterative denoising process poses a major\nbottleneck for real-time deployment. Existing acceleration methods apply a\nfixed number of denoising steps per action, implicitly treating all actions as\nequally important. However, our experiments reveal that robotic tasks often\ncontain a mix of \\emph{crucial} and \\emph{routine} actions, which differ in\ntheir impact on task success. Motivated by this finding, we propose\n\\textbf{D}ynamic \\textbf{D}enoising \\textbf{D}iffusion \\textbf{P}olicy\n\\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising\nsteps across actions at test time. D3P uses a lightweight, state-aware adaptor\nto allocate the optimal number of denoising steps for each action. We jointly\noptimize the adaptor and base diffusion policy via reinforcement learning to\nbalance task performance and inference efficiency. On simulated tasks, D3P\nachieves an averaged 2.2$\\times$ inference speed-up over baselines without\ndegrading success. Furthermore, we demonstrate D3P's effectiveness on a\nphysical robot, achieving a 1.9$\\times$ acceleration over the baseline.", "AI": {"tldr": "D3P是一种动态去噪扩散策略，通过自适应分配去噪步骤提升实时性能，在保持任务成功率的同时显著加速推理。", "motivation": "现有方法对所有动作采用固定去噪步骤，忽略了动作对任务成功的重要性差异。实验表明，机器人任务中动作分为关键和常规两类，影响不同。", "method": "提出D3P，通过轻量级状态感知适配器动态分配去噪步骤，并结合强化学习联合优化适配器和基础扩散策略。", "result": "在模拟任务中，D3P平均加速2.2倍且成功率不降；在物理机器人上加速1.9倍。", "conclusion": "D3P通过动态去噪步骤分配，显著提升推理效率，适用于实时机器人任务。"}}
{"id": "2508.06921", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06921", "abs": "https://arxiv.org/abs/2508.06921", "authors": ["Zhongyu Chen", "Chenyang Li", "Xuesong Li", "Dianye Huang", "Zhongliang Jiang", "Stefanie Speidel", "Xiangyu Chu", "K. W. Samuel Au"], "title": "Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound", "comment": null, "summary": "Precise needle alignment is essential for percutaneous needle insertion in\nrobotic ultrasound-guided procedures. However, inherent challenges such as\nspeckle noise, needle-like artifacts, and low image resolution make robust\nneedle detection difficult, particularly when visibility is reduced or lost. In\nthis paper, we propose a method to restore needle alignment when the ultrasound\nimaging plane and the needle insertion plane are misaligned. Unlike many\nexisting approaches that rely heavily on needle visibility in ultrasound\nimages, our method uses a more robust feature by periodically vibrating the\nneedle using a mechanical system. Specifically, we propose a vibration-based\nenergy metric that remains effective even when the needle is fully out of\nplane. Using this metric, we develop a control strategy to reposition the\nultrasound probe in response to misalignments between the imaging plane and the\nneedle insertion plane in both translation and rotation. Experiments conducted\non ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided\nneedle insertion system demonstrate the effectiveness of the proposed approach.\nThe experimental results show the translational error of 0.41$\\pm$0.27 mm and\nthe rotational error of 0.51$\\pm$0.19 degrees.", "AI": {"tldr": "提出一种通过周期性振动针头来恢复超声成像平面与针插入平面对齐的方法，即使在针头完全不可见时仍有效。", "motivation": "在机器人超声引导的针插入过程中，针的精确对齐至关重要，但超声图像中的噪声和低分辨率等问题使得针的检测困难。", "method": "通过机械系统周期性振动针头，提出一种基于振动的能量度量，并开发控制策略以调整超声探头位置。", "result": "实验显示，平移误差为0.41±0.27 mm，旋转误差为0.51±0.19度。", "conclusion": "该方法在针头不可见时仍能有效恢复对齐，具有较高的精确性和鲁棒性。"}}
{"id": "2508.06969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06969", "abs": "https://arxiv.org/abs/2508.06969", "authors": ["Bingkun Huang", "Evgeniy Kotov", "Arkady Yuschenko"], "title": "Manipulator for people with limited abilities", "comment": "105 pages, in Russian language", "summary": "The topic of this final qualification work was chosen due to the importance\nof developing robotic systems designed to assist people with disabilities.\nAdvances in robotics and automation technologies have opened up new prospects\nfor creating devices that can significantly improve the quality of life for\nthese people. In this context, designing a robotic hand with a control system\nadapted to the needs of people with disabilities is a major scientific and\npractical challenge. This work addresses the problem of developing and\nmanufacturing a four-degree-of-freedom robotic hand suitable for practical\nmanipulation. Addressing this issue requires a comprehensive approach,\nencompassing the design of the hand's mechanical structure, the development of\nits control system, and its integration with a technical vision system and\nsoftware based on the Robot Operating System (ROS).", "AI": {"tldr": "开发一种四自由度机械手，用于辅助残障人士，结合机械设计、控制系统、视觉系统和ROS软件。", "motivation": "机器人技术的发展为改善残障人士生活质量提供了新机会，设计适配的机械手是重要挑战。", "method": "综合设计机械结构、开发控制系统，并集成视觉系统和ROS软件。", "result": "提出了一种适合实际操作的机械手设计方案。", "conclusion": "该研究为残障人士辅助设备的发展提供了实用解决方案。"}}
{"id": "2508.06990", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06990", "abs": "https://arxiv.org/abs/2508.06990", "authors": ["Yue Hu", "Junzhe Wu", "Ruihan Xu", "Hang Liu", "Avery Xi", "Henry X. Liu", "Ram Vasudevan", "Maani Ghaffari"], "title": "Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation", "comment": "23 pages", "summary": "Semantic navigation requires an agent to navigate toward a specified target\nin an unseen environment. Employing an imaginative navigation strategy that\npredicts future scenes before taking action, can empower the agent to find\ntarget faster. Inspired by this idea, we propose SGImagineNav, a novel\nimaginative navigation framework that leverages symbolic world modeling to\nproactively build a global environmental representation. SGImagineNav maintains\nan evolving hierarchical scene graphs and uses large language models to predict\nand explore unseen parts of the environment. While existing methods solely\nrelying on past observations, this imaginative scene graph provides richer\nsemantic context, enabling the agent to proactively estimate target locations.\nBuilding upon this, SGImagineNav adopts an adaptive navigation strategy that\nexploits semantic shortcuts when promising and explores unknown areas otherwise\nto gather additional context. This strategy continuously expands the known\nenvironment and accumulates valuable semantic contexts, ultimately guiding the\nagent toward the target. SGImagineNav is evaluated in both real-world scenarios\nand simulation benchmarks. SGImagineNav consistently outperforms previous\nmethods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and\ndemonstrating cross-floor and cross-room navigation in real-world environments,\nunderscoring its effectiveness and generalizability.", "AI": {"tldr": "SGImagineNav是一种新颖的导航框架，通过符号世界建模和场景图预测未来场景，提升语义导航效率。", "motivation": "现有方法仅依赖过去观察，缺乏对未来场景的预测能力，限制了导航效率。", "method": "SGImagineNav利用分层场景图和大型语言模型预测环境未知部分，采用自适应导航策略。", "result": "在HM3D和HSSD基准测试中，成功率分别提升至65.4%和66.8%，并在真实环境中展示了跨楼层和跨房间导航能力。", "conclusion": "SGImagineNav通过主动预测和自适应策略，显著提升了语义导航的成功率和通用性。"}}
{"id": "2508.07003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07003", "abs": "https://arxiv.org/abs/2508.07003", "authors": ["Siyu Chen", "Shenghai Yuan", "Thien-Minh Nguyen", "Zhuyu Huang", "Chenyang Shi", "Jin Jing", "Lihua Xie"], "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events", "comment": "Accepted by IEEE RAL", "summary": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over\ntraditional SLAM methods, enabling photorealistic 3D reconstruction that\nconventional approaches often struggle to achieve. However, existing GS-SLAM\nsystems perform poorly under persistent and severe motion blur commonly\nencountered in real-world scenarios, leading to significantly degraded tracking\naccuracy and compromised 3D reconstruction quality. To address this limitation,\nwe propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D\ninputs to simultaneously reduce motion blur in images and compensate for the\nsparse and discrete nature of event streams, enabling robust tracking and\nhigh-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system\nexplicitly models the camera's continuous trajectory during exposure,\nsupporting event- and blur-aware tracking and mapping on a unified 3D Gaussian\nSplatting scene. Furthermore, we introduce a learnable camera response function\nto align the dynamic ranges of events and images, along with a no-event loss to\nsuppress ringing artifacts during reconstruction. We validate our approach on a\nnew dataset comprising synthetic and real-world sequences with significant\nmotion blur. Extensive experimental results demonstrate that EGS-SLAM\nconsistently outperforms existing GS-SLAM systems in both trajectory accuracy\nand photorealistic 3D Gaussian Splatting reconstruction. The source code will\nbe available at https://github.com/Chensiyu00/EGS-SLAM.", "AI": {"tldr": "EGS-SLAM通过融合事件数据和RGB-D输入，解决了传统GS-SLAM在运动模糊下的性能问题，提升了跟踪精度和3D重建质量。", "motivation": "现有GS-SLAM系统在严重运动模糊下表现不佳，导致跟踪和重建质量下降。", "method": "提出EGS-SLAM框架，结合事件数据和RGB-D输入，建模相机连续轨迹，引入可学习的相机响应函数和无事件损失。", "result": "在合成和真实数据集上验证，EGS-SLAM在轨迹精度和3D重建质量上优于现有GS-SLAM系统。", "conclusion": "EGS-SLAM显著提升了在运动模糊场景下的SLAM性能，具有实际应用潜力。"}}
{"id": "2508.07033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07033", "abs": "https://arxiv.org/abs/2508.07033", "authors": ["Shengli Zhou", "Xiangchen Wang", "Jinrui Zhang", "Ruozai Tian", "Rongtao Xu", "Feng Zheng"], "title": "$\\mathcal{P}^3$: Toward Versatile Embodied Agents", "comment": "16 pages, 8 figures", "summary": "Embodied agents have shown promising generalization capabilities across\ndiverse physical environments, making them essential for a wide range of\nreal-world applications. However, building versatile embodied agents poses\ncritical challenges due to three key issues: dynamic environment perception,\nopen-ended tool usage, and complex multi-task planning. Most previous works\nrely solely on feedback from tool agents to perceive environmental changes and\ntask status, which limits adaptability to real-time dynamics, causes error\naccumulation, and restricts tool flexibility. Furthermore, multi-task\nscheduling has received limited attention, primarily due to the inherent\ncomplexity of managing task dependencies and balancing competing priorities in\ndynamic and complex environments. To overcome these challenges, we introduce\n$\\mathcal{P}^3$, a unified framework that integrates real-time perception and\ndynamic scheduling. Specifically, $\\mathcal{P}^3$ enables 1) \\textbf Perceive\nrelevant task information actively from the environment, 2) \\textbf Plug and\nutilize any tool without feedback requirement, and 3) \\textbf Plan multi-task\nexecution based on prioritizing urgent tasks and dynamically adjusting task\norder based on dependencies. Extensive real-world experiments show that our\napproach bridges the gap between benchmarks and practical deployment,\ndelivering highly transferable, general-purpose embodied agents. Code and data\nwill be released soon.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07080", "abs": "https://arxiv.org/abs/2508.07080", "authors": ["Haolin Liu", "Zijun Guo", "Yanbo Chen", "Jiaqi Chen", "Huilong Yu", "Junqiang Xi"], "title": "An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving", "comment": null, "summary": "Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),\nsince they have to proactively interact with surrounding vehicles to enter the\nmain road safely within limited time. However, existing decision-making\nalgorithms fail to adequately address dynamic complexities and social\nacceptance of AVs, leading to suboptimal or unsafe merging decisions. To\naddress this, we propose an evolutionary game-theoretic (EGT) merging\ndecision-making framework, grounded in the bounded rationality of human\ndrivers, which dynamically balances the benefits of both AVs and main-road\nvehicles (MVs). We formulate the cut-in decision-making process as an EGT\nproblem with a multi-objective payoff function that reflects human-like driving\npreferences. By solving the replicator dynamic equation for the evolutionarily\nstable strategy (ESS), the optimal cut-in timing is derived, balancing\nefficiency, comfort, and safety for both AVs and MVs. A real-time driving style\nestimation algorithm is proposed to adjust the game payoff function online by\nobserving the immediate reactions of MVs. Empirical results demonstrate that we\nimprove the efficiency, comfort and safety of both AVs and MVs compared with\nexisting game-theoretic and traditional planning approaches across multi-object\nmetrics.", "AI": {"tldr": "论文提出了一种基于进化博弈论（EGT）的决策框架，用于解决自动驾驶车辆在高速匝道合流时的动态复杂性和社会接受度问题。", "motivation": "现有决策算法未能充分应对动态复杂性和社会接受度，导致合流决策次优或不安全。", "method": "采用进化博弈论框架，结合多目标收益函数和实时驾驶风格估计算法，动态优化合流时机。", "result": "实验表明，该方法在效率、舒适性和安全性上优于现有博弈论和传统规划方法。", "conclusion": "提出的EGT框架有效平衡了自动驾驶车辆和主路车辆的需求，提升了合流决策的整体性能。"}}
{"id": "2508.07118", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07118", "abs": "https://arxiv.org/abs/2508.07118", "authors": ["Aiden Swann", "Alex Qiu", "Matthew Strong", "Angelina Zhang", "Samuel Morstein", "Kai Rayle", "Monroe Kennedy III"], "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit", "comment": "8 pages, 5 figures", "summary": "DexFruit is a robotic manipulation framework that enables gentle, autonomous\nhandling of fragile fruit and precise evaluation of damage. Many fruits are\nfragile and prone to bruising, thus requiring humans to manually harvest them\nwith care. In this work, we demonstrate by using optical tactile sensing,\nautonomous manipulation of fruit with minimal damage can be achieved. We show\nthat our tactile informed diffusion policies outperform baselines in both\nreduced bruising and pick-and-place success rate across three fruits:\nstrawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,\na novel technique to represent and quantify visual damage in high-resolution 3D\nrepresentation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring\ndamage lack quantitative rigor or require expensive equipment. With FruitSplat,\nwe distill a 2D strawberry mask as well as a 2D bruise segmentation mask into\nthe 3DGS representation. Furthermore, this representation is modular and\ngeneral, compatible with any relevant 2D model. Overall, we demonstrate a 92%\ngrasping policy success rate, up to a 20% reduction in visual bruising, and up\nto an 31% improvement in grasp success rate on challenging fruit compared to\nour baselines across our three tested fruits. We rigorously evaluate this\nresult with over 630 trials. Please checkout our website at\nhttps://dex-fruit.github.io .", "AI": {"tldr": "DexFruit是一个机器人操作框架，通过光学触觉感知实现脆弱水果的自主轻柔处理和损伤评估，显著减少损伤并提高成功率。", "motivation": "许多水果易碎且容易碰伤，需人工小心采摘，因此需要开发自动化解决方案以减少损伤。", "method": "使用光学触觉感知和触觉信息扩散策略，结合FruitSplat技术（基于3D高斯溅射的高分辨率3D损伤表示）。", "result": "在草莓、番茄和黑莓上，实现了92%的抓取成功率，视觉损伤减少20%，抓取成功率提升31%。", "conclusion": "DexFruit框架在减少水果损伤和提高操作成功率方面表现优异，具有实际应用潜力。"}}
{"id": "2508.07163", "categories": ["cs.RO", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07163", "abs": "https://arxiv.org/abs/2508.07163", "authors": ["Kamal Acharya", "Iman Sharifi", "Mehul Lad", "Liang Sun", "Houbing Song"], "title": "Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey", "comment": "9 pages, 4 figures, IJCAI-2025 (accepted)", "summary": "Neurosymbolic AI combines neural network adaptability with symbolic\nreasoning, promising an approach to address the complex regulatory,\noperational, and safety challenges in Advanced Air Mobility (AAM). This survey\nreviews its applications across key AAM domains such as demand forecasting,\naircraft design, and real-time air traffic management. Our analysis reveals a\nfragmented research landscape where methodologies, including Neurosymbolic\nReinforcement Learning, have shown potential for dynamic optimization but still\nface hurdles in scalability, robustness, and compliance with aviation\nstandards. We classify current advancements, present relevant case studies, and\noutline future research directions aimed at integrating these approaches into\nreliable, transparent AAM systems. By linking advanced AI techniques with AAM's\noperational demands, this work provides a concise roadmap for researchers and\npractitioners developing next-generation air mobility solutions.", "AI": {"tldr": "神经符号AI结合神经网络适应性和符号推理，为高级空中交通（AAM）的复杂挑战提供解决方案。本文综述了其在需求预测、飞机设计和实时空中交通管理等领域的应用，揭示了研究碎片化问题，并指出了未来研究方向。", "motivation": "高级空中交通（AAM）面临复杂的监管、运营和安全挑战，神经符号AI有望通过结合神经网络和符号推理的优势来解决这些问题。", "method": "本文综述了神经符号AI在AAM中的应用，包括神经符号强化学习等方法，并分析了其动态优化潜力及当前面临的挑战。", "result": "研究发现神经符号AI在AAM中显示出潜力，但仍需解决可扩展性、鲁棒性和航空标准合规性等问题。", "conclusion": "本文为研究人员和从业者提供了整合神经符号AI到可靠、透明的AAM系统中的路线图，并指明了未来研究方向。"}}
{"id": "2508.07182", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07182", "abs": "https://arxiv.org/abs/2508.07182", "authors": ["Xuesong Li", "Lars Petersson", "Vivien Rolland"], "title": "3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction", "comment": null, "summary": "This paper addresses the challenge of novel-view synthesis and motion\nreconstruction of dynamic scenes from monocular video, which is critical for\nmany robotic applications. Although Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have demonstrated remarkable success in rendering\nstatic scenes, extending them to reconstruct dynamic scenes remains\nchallenging. In this work, we introduce a novel approach that combines 3DGS\nwith a motion trajectory field, enabling precise handling of complex object\nmotions and achieving physically plausible motion trajectories. By decoupling\ndynamic objects from static background, our method compactly optimizes the\nmotion trajectory field. The approach incorporates time-invariant motion\ncoefficients and shared motion trajectory bases to capture intricate motion\npatterns while minimizing optimization complexity. Extensive experiments\ndemonstrate that our approach achieves state-of-the-art results in both\nnovel-view synthesis and motion trajectory recovery from monocular video,\nadvancing the capabilities of dynamic scene reconstruction.", "AI": {"tldr": "提出了一种结合3D高斯泼溅与运动轨迹场的新方法，用于动态场景的新视角合成和运动重建。", "motivation": "解决从单目视频中重建动态场景的挑战，尤其是复杂物体运动的精确处理。", "method": "结合3D高斯泼溅与运动轨迹场，解耦动态物体与静态背景，优化运动轨迹场。", "result": "在单目视频的新视角合成和运动轨迹恢复方面取得了最先进的结果。", "conclusion": "该方法提升了动态场景重建的能力，实现了物理上合理的运动轨迹。"}}
{"id": "2508.07244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07244", "abs": "https://arxiv.org/abs/2508.07244", "authors": ["Ayesha Jena", "Stefan Reitmann", "Elin Anna Topp"], "title": "Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks", "comment": null, "summary": "We present a user study analyzing head-gaze-based robot control and foveated\nvisual augmentation in a simulated search-and-rescue task. Results show that\nfoveated augmentation significantly improves task performance, reduces\ncognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns\nanalysed over both the entire task duration and shorter time segments show that\nnear and far attention capture is essential to better understand user intention\nin critical scenarios. Our findings highlight the potential of foveation as an\naugmentation technique and the need to further study gaze measures to leverage\nthem during critical tasks.", "AI": {"tldr": "研究分析了基于头部注视的机器人控制和焦点视觉增强在模拟搜救任务中的应用，发现焦点增强显著提升任务表现并降低认知负荷。", "motivation": "探索头部注视和焦点视觉增强在搜救任务中的效果，以优化人机交互。", "method": "通过用户研究，分析头部注视模式和焦点视觉增强对任务表现的影响。", "result": "焦点增强显著提升任务表现，降低认知负荷38%，缩短任务时间60%以上。", "conclusion": "焦点增强技术潜力巨大，需进一步研究注视模式以优化关键任务表现。"}}
{"id": "2508.07267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07267", "abs": "https://arxiv.org/abs/2508.07267", "authors": ["Daria de Tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "title": "Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics", "comment": "Conference ICCAS 2025 - accepted (in processing)", "summary": "Achieving fully autonomous exploration and navigation remains a critical\nchallenge in robotics, requiring integrated solutions for localisation,\nmapping, decision-making and motion planning. Existing approaches either rely\non strict navigation rules lacking adaptability or on pre-training, which\nrequires large datasets. These AI methods are often computationally intensive\nor based on static assumptions, limiting their adaptability in dynamic or\nunknown environments. This paper introduces a bio-inspired agent based on the\nActive Inference Framework (AIF), which unifies mapping, localisation, and\nadaptive decision-making for autonomous navigation, including exploration and\ngoal-reaching. Our model creates and updates a topological map of the\nenvironment in real-time, planning goal-directed trajectories to explore or\nreach objectives without requiring pre-training. Key contributions include a\nprobabilistic reasoning framework for interpretable navigation, robust\nadaptability to dynamic changes, and a modular ROS2 architecture compatible\nwith existing navigation systems. Our method was tested in simulated and\nreal-world environments. The agent successfully explores large-scale simulated\nenvironments and adapts to dynamic obstacles and drift, proving to be\ncomparable to other exploration strategies such as Gbplanner, FAEL and\nFrontiers. This approach offers a scalable and transparent approach for\nnavigating complex, unstructured environments.", "AI": {"tldr": "本文提出了一种基于主动推理框架（AIF）的生物启发智能体，用于自主导航，无需预训练即可实时构建环境拓扑图并规划目标导向轨迹。", "motivation": "现有自主导航方法依赖严格规则或预训练，适应性差且计算量大，难以应对动态或未知环境。", "method": "采用AIF框架，结合概率推理和模块化ROS2架构，实现实时拓扑图构建和自适应决策。", "result": "在仿真和真实环境中测试，智能体成功探索大规模环境并适应动态障碍，性能与现有方法相当。", "conclusion": "该方法为复杂非结构化环境提供了一种可扩展且透明的导航解决方案。"}}
{"id": "2508.07269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07269", "abs": "https://arxiv.org/abs/2508.07269", "authors": ["Daria de Tinguy", "Tim Verbelen", "Bart Dhoedt"], "title": "Navigation and Exploration with Active Inference: from Biology to Industry", "comment": "conference IWAI 2025 - accepted (in processing)", "summary": "By building and updating internal cognitive maps, animals exhibit\nextraordinary navigation abilities in complex, dynamic environments. Inspired\nby these biological mechanisms, we present a real time robotic navigation\nsystem grounded in the Active Inference Framework (AIF). Our model\nincrementally constructs a topological map, infers the agent's location, and\nplans actions by minimising expected uncertainty and fulfilling perceptual\ngoals without any prior training. Integrated into the ROS2 ecosystem, we\nvalidate its adaptability and efficiency across both 2D and 3D environments\n(simulated and real world), demonstrating competitive performance with\ntraditional and state of the art exploration approaches while offering a\nbiologically inspired navigation approach.", "AI": {"tldr": "论文提出了一种基于主动推理框架（AIF）的实时机器人导航系统，模仿动物认知地图构建能力，无需预先训练即可在复杂动态环境中导航。", "motivation": "受动物通过构建认知地图实现高效导航的生物机制启发，旨在开发一种无需训练的机器人导航系统。", "method": "采用主动推理框架，逐步构建拓扑地图，推断位置并通过最小化预期不确定性和实现感知目标来规划动作。", "result": "在ROS2生态系统中验证了系统在2D和3D环境（模拟和现实）中的适应性和高效性，性能与传统及前沿方法相当。", "conclusion": "该系统提供了一种生物启发的导航方法，展示了在复杂动态环境中的竞争力和潜力。"}}
{"id": "2508.07287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07287", "abs": "https://arxiv.org/abs/2508.07287", "authors": ["Liwen Zhang", "Dong Zhou", "Shibo Shao", "Zihao Su", "Guanghui Sun"], "title": "Multimodal Spiking Neural Network for Space Robotic Manipulation", "comment": null, "summary": "This paper presents a multimodal control framework based on spiking neural\nnetworks (SNNs) for robotic arms aboard space stations. It is designed to cope\nwith the constraints of limited onboard resources while enabling autonomous\nmanipulation and material transfer in space operations. By combining geometric\nstates with tactile and semantic information, the framework strengthens\nenvironmental awareness and contributes to more robust control strategies. To\nguide the learning process progressively, a dual-channel, three-stage\ncurriculum reinforcement learning (CRL) scheme is further integrated into the\nsystem. The framework was tested across a range of tasks including target\napproach, object grasping, and stable lifting with wall-mounted robotic arms,\ndemonstrating reliable performance throughout. Experimental evaluations\ndemonstrate that the proposed method consistently outperforms baseline\napproaches in both task success rate and energy efficiency. These findings\nhighlight its suitability for real-world aerospace applications.", "AI": {"tldr": "提出了一种基于脉冲神经网络（SNN）的多模态控制框架，用于空间站机械臂，旨在解决资源有限问题并实现自主操作。", "motivation": "解决空间站机械臂在资源有限条件下的自主操作问题，提升环境感知能力。", "method": "结合几何状态、触觉和语义信息，采用双通道三阶段课程强化学习（CRL）方案。", "result": "在目标接近、物体抓取和稳定提升等任务中表现可靠，成功率和能效均优于基线方法。", "conclusion": "该框架适用于实际航空航天应用，具有较高的实用价值。"}}
{"id": "2508.07319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07319", "abs": "https://arxiv.org/abs/2508.07319", "authors": ["Yanzhao Yu", "Haotian Yang", "Junbo Tan", "Xueqian Wang"], "title": "A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks", "comment": null, "summary": "Manipulating deformable linear objects (DLOs) such as wires and cables is\ncrucial in various applications like electronics assembly and medical\nsurgeries. However, it faces challenges due to DLOs' infinite degrees of\nfreedom, complex nonlinear dynamics, and the underactuated nature of the\nsystem. To address these issues, this paper proposes a hybrid force-position\nstrategy for DLO shape control. The framework, combining both force and\nposition representations of DLO, integrates state trajectory planning in the\nforce space and Model Predictive Control (MPC) in the position space. We\npresent a dynamics model with an explicit action encoder, a property extractor\nand a graph processor based on Graph Attention Networks. The model is used in\nthe MPC to enhance prediction accuracy. Results from both simulations and\nreal-world experiments demonstrate the effectiveness of our approach in\nachieving efficient and stable shape control of DLOs. Codes and videos are\navailable at https://sites.google.com/view/dlom.", "AI": {"tldr": "提出了一种混合力-位置策略，用于控制可变形线性物体（DLOs）的形状，结合了力空间的状态轨迹规划和位置空间的模型预测控制（MPC）。", "motivation": "DLOs（如电线和电缆）在电子组装和医疗手术中有广泛应用，但其无限自由度、复杂非线性动力学和系统欠驱动特性带来了挑战。", "method": "提出了一个框架，结合力和位置表示，包括力空间的状态轨迹规划和位置空间的MPC。模型包含动作编码器、属性提取器和基于图注意力网络的图处理器。", "result": "仿真和实际实验表明，该方法能高效稳定地控制DLOs的形状。", "conclusion": "该混合策略有效解决了DLO形状控制的挑战，模型和框架在实际应用中表现良好。"}}
{"id": "2508.07387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07387", "abs": "https://arxiv.org/abs/2508.07387", "authors": ["Basant Sharma", "Prajyot Jadhav", "Pranjal Paul", "K. Madhava Krishna", "Arun Kumar Singh"], "title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control", "comment": null, "summary": "Navigating unknown environments with a single RGB camera is challenging, as\nthe lack of depth information prevents reliable collision-checking. While some\nmethods use estimated depth to build collision maps, we found that depth\nestimates from vision foundation models are too noisy for zero-shot navigation\nin cluttered environments.\n  We propose an alternative approach: instead of using noisy estimated depth\nfor direct collision-checking, we use it as a rich context input to a learned\ncollision model. This model predicts the distribution of minimum obstacle\nclearance that the robot can expect for a given control sequence. At inference,\nthese predictions inform a risk-aware MPC planner that minimizes estimated\ncollision risk. Our joint learning pipeline co-trains the collision model and\nrisk metric using both safe and unsafe trajectories. Crucially, our\njoint-training ensures optimal variance in our collision model that improves\nnavigation in highly cluttered environments. Consequently, real-world\nexperiments show 9x and 7x improvements in success rates over NoMaD and the ROS\nstack, respectively. Ablation studies further validate the effectiveness of our\ndesign choices.", "AI": {"tldr": "论文提出了一种基于学习碰撞模型的方法，通过预测最小障碍物间隙分布来改进单RGB相机在未知环境中的导航性能。", "motivation": "单RGB相机缺乏深度信息，导致在未知环境中导航时碰撞检测不可靠。现有方法使用估计深度构建碰撞地图，但深度估计噪声大，无法直接用于零样本导航。", "method": "提出一种替代方法：将噪声深度估计作为丰富上下文输入，训练一个学习碰撞模型，预测机器人执行给定控制序列时的最小障碍物间隙分布。结合风险感知MPC规划器，最小化碰撞风险。", "result": "实验表明，该方法在真实环境中比NoMaD和ROS堆栈的成功率分别提高了9倍和7倍。消融研究验证了设计选择的有效性。", "conclusion": "通过联合学习碰撞模型和风险度量，显著提高了在高度杂乱环境中的导航性能。"}}
{"id": "2508.07406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07406", "abs": "https://arxiv.org/abs/2508.07406", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "comment": null, "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain.", "AI": {"tldr": "论文提出了农业场景下的视觉与语言导航基准A2A和基线方法AgriVLN，解决了现有方法在农业领域的不足，并通过指令分解模块提升了性能。", "motivation": "农业机器人依赖人工操作或固定轨道，移动性和适应性差。现有视觉与语言导航方法未针对农业场景设计，因此需要专门的研究。", "method": "提出A2A基准，包含1,560个农业场景片段。基于视觉语言模型（VLM）设计AgriVLN基线方法，并引入子任务列表（STL）模块分解指令。", "result": "AgriVLN在短指令上表现良好，但对长指令跟踪不足。加入STL后，成功率从0.33提升至0.47，优于现有方法。", "conclusion": "A2A和AgriVLN填补了农业场景导航的空白，STL模块显著提升了性能，为农业机器人导航提供了新思路。"}}
{"id": "2508.07421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07421", "abs": "https://arxiv.org/abs/2508.07421", "authors": ["Zixi Jia", "Hongbin Gao", "Fashe Li", "Jiqiang Liu", "Hexiao Li", "Qinghua Liu"], "title": "Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics", "comment": "Accepted to IROS 2025", "summary": "Leveraging Large Language Models (LLMs) to write policy code for controlling\nrobots has gained significant attention. However, in long-horizon implicative\ntasks, this approach often results in API parameter, comments and sequencing\nerrors, leading to task failure. To address this problem, we propose a\ncollaborative Triple-S framework that involves multiple LLMs. Through\nIn-Context Learning, different LLMs assume specific roles in a closed-loop\nSimplification-Solution-Summary process, effectively improving success rates\nand robustness in long-horizon implicative tasks. Additionally, a novel\ndemonstration library update mechanism which learned from success allows it to\ngeneralize to previously failed tasks. We validate the framework in the\nLong-horizon Desktop Implicative Placement (LDIP) dataset across various\nbaseline models, where Triple-S successfully executes 89% of tasks in both\nobservable and partially observable scenarios. Experiments in both simulation\nand real-world robot settings further validated the effectiveness of Triple-S.\nOur code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.", "AI": {"tldr": "提出Triple-S框架，通过多LLM协作解决长时隐式任务中的API参数、注释和顺序错误，显著提高任务成功率。", "motivation": "利用LLM编写机器人控制策略代码时，长时隐式任务中常出现API参数、注释和顺序错误，导致任务失败。", "method": "提出Triple-S框架，通过多LLM在简化-解决-总结闭环过程中扮演特定角色，结合演示库更新机制。", "result": "在LDIP数据集中，Triple-S在可观察和部分可观察场景下任务成功率达89%。", "conclusion": "Triple-S框架显著提高了长时隐式任务的执行成功率和鲁棒性，实验验证了其有效性。"}}
{"id": "2508.07502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07502", "abs": "https://arxiv.org/abs/2508.07502", "authors": ["Mateus Salomão", "Tianyü Ren", "Alexander König"], "title": "A Learning-Based Framework for Collision-Free Motion Planning", "comment": null, "summary": "This paper presents a learning-based extension to a Circular Field (CF)-based\nmotion planner for efficient, collision-free trajectory generation in cluttered\nenvironments. The proposed approach overcomes the limitations of hand-tuned\nforce field parameters by employing a deep neural network trained to infer\noptimal planner gains from a single depth image of the scene. The pipeline\nincorporates a CUDA-accelerated perception module, a predictive agent-based\nplanning strategy, and a dataset generated through Bayesian optimization in\nsimulation. The resulting framework enables real-time planning without manual\nparameter tuning and is validated both in simulation and on a Franka Emika\nPanda robot. Experimental results demonstrate successful task completion and\nimproved generalization compared to classical planners.", "AI": {"tldr": "论文提出了一种基于学习的运动规划方法，通过深度神经网络优化参数，实现高效无碰撞轨迹生成。", "motivation": "解决传统手动调整力场参数的局限性，提升在复杂环境中的运动规划效率。", "method": "结合CUDA加速的感知模块、基于预测的规划策略，以及贝叶斯优化生成的数据集，训练深度神经网络推断最优参数。", "result": "实验验证了实时规划能力，任务完成率高，且比传统规划器具有更好的泛化性能。", "conclusion": "该方法无需手动调参，适用于复杂环境，并在仿真和实际机器人上验证了有效性。"}}
{"id": "2508.07560", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07560", "abs": "https://arxiv.org/abs/2508.07560", "authors": ["Yan Gong", "Naibang Wang", "Jianli Lu", "Xinyu Zhang", "Yongsheng Gao", "Jie Zhao", "Zifan Huang", "Haozhi Bai", "Nanxin Zeng", "Nayu Su", "Lei Yang", "Ziying Song", "Xiaoxi Hu", "Xinmin Jiang", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey", "comment": null, "summary": "Bird's-Eye-View (BEV) perception has become a foundational paradigm in\nautonomous driving, enabling unified spatial representations that support\nrobust multi-sensor fusion and multi-agent collaboration. As autonomous\nvehicles transition from controlled environments to real-world deployment,\nensuring the safety and reliability of BEV perception in complex scenarios -\nsuch as occlusions, adverse weather, and dynamic traffic - remains a critical\nchallenge. This survey provides the first comprehensive review of BEV\nperception from a safety-critical perspective, systematically analyzing\nstate-of-the-art frameworks and implementation strategies across three\nprogressive stages: single-modality vehicle-side, multimodal vehicle-side, and\nmulti-agent collaborative perception. Furthermore, we examine public datasets\nencompassing vehicle-side, roadside, and collaborative settings, evaluating\ntheir relevance to safety and robustness. We also identify key open-world\nchallenges - including open-set recognition, large-scale unlabeled data, sensor\ndegradation, and inter-agent communication latency - and outline future\nresearch directions, such as integration with end-to-end autonomous driving\nsystems, embodied intelligence, and large language models.", "AI": {"tldr": "综述首次从安全关键视角全面回顾BEV感知，分析单模态、多模态及多代理协作感知框架，评估相关数据集，并指出开放世界挑战与未来研究方向。", "motivation": "随着自动驾驶从受控环境转向实际部署，确保BEV感知在复杂场景中的安全性和可靠性成为关键挑战。", "method": "系统分析BEV感知的三个阶段：单模态车端、多模态车端及多代理协作感知，并评估相关数据集。", "result": "识别开放世界挑战（如开放集识别、传感器退化等），并指出未来研究方向（如端到端系统集成、大语言模型等）。", "conclusion": "BEV感知在安全性和可靠性方面仍需突破，未来研究需关注开放世界挑战及新技术整合。"}}
{"id": "2508.07566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07566", "abs": "https://arxiv.org/abs/2508.07566", "authors": ["Conor K. Trygstad", "Cody R. Longwell", "Francisco M. F. R. Gonçalves", "Elijah K. Blankenship", "Néstor O. Pérez-Arancibia"], "title": "Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer", "comment": "To be presented at the 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "We present an evolved steerable version of the single-tail\nFish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg\nbiologically inspired swimmer, which is driven by a new shape-memory alloy\n(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the\ntwo-dimensional (2D) space, which enabled the first demonstration of\nfeedback-controlled trajectory tracking of a single-tail aquatic robot with\nonboard actuation at the subgram scale. These new capabilities are the result\nof a physics-informed design with an enlarged head and shortened tail relative\nto those of the original platform. Enhanced by its design, this new platform\nachieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over\nfour times that of the original platform. Furthermore, when following 2D\nreferences in closed loop, the tested FRISSHBot prototype attains forward\nswimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as\nlow as 2.6 mm, turning rates of up to 13.1 {\\deg}/s, and turning radii as small\nas 10 mm.", "AI": {"tldr": "改进版的FRISSHBot通过新型SMA双压电晶片驱动器实现二维控制，速度提升四倍，并首次实现亚克级单尾机器人的反馈控制轨迹跟踪。", "motivation": "开发一种更高效、可控的小型水生机器人，以改进原始FRISSHBot的性能和功能。", "method": "采用物理信息设计，增大头部并缩短尾部，结合新型SMA双压电晶片驱动器。", "result": "最高游泳速度达13.6 mm/s，闭环跟踪时速度为9.1 mm/s，跟踪误差低至2.6 mm，转弯半径最小10 mm。", "conclusion": "改进设计显著提升了FRISSHBot的性能，实现了亚克级机器人的精确控制。"}}
{"id": "2508.07606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07606", "abs": "https://arxiv.org/abs/2508.07606", "authors": ["Hongtao Li", "Ziyuan Jiao", "Xiaofeng Liu", "Hangxin Liu", "Zilong Zheng"], "title": "In-situ Value-aligned Human-Robot Interactions with Physical Constraints", "comment": "8 pages, 7 figures", "summary": "Equipped with Large Language Models (LLMs), human-centered robots are now\ncapable of performing a wide range of tasks that were previously deemed\nchallenging or unattainable. However, merely completing tasks is insufficient\nfor cognitive robots, who should learn and apply human preferences to future\nscenarios. In this work, we propose a framework that combines human preferences\nwith physical constraints, requiring robots to complete tasks while considering\nboth. Firstly, we developed a benchmark of everyday household activities, which\nare often evaluated based on specific preferences. We then introduced\nIn-Context Learning from Human Feedback (ICLHF), where human feedback comes\nfrom direct instructions and adjustments made intentionally or unintentionally\nin daily life. Extensive sets of experiments, testing the ICLHF to generate\ntask plans and balance physical constraints with preferences, have demonstrated\nthe efficiency of our approach.", "AI": {"tldr": "提出了一种结合人类偏好与物理约束的框架，通过上下文学习人类反馈（ICLHF），使机器人能够完成任务并适应未来场景。", "motivation": "尽管大语言模型（LLMs）使机器人能完成复杂任务，但仅完成任务不足以满足认知需求，需学习并应用人类偏好。", "method": "开发了日常家务活动基准，引入ICLHF框架，结合直接指令和日常调整的人类反馈。", "result": "实验证明ICLHF能高效生成任务计划并平衡物理约束与人类偏好。", "conclusion": "该框架为机器人学习人类偏好提供了有效方法，适用于未来场景。"}}
{"id": "2508.07611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07611", "abs": "https://arxiv.org/abs/2508.07611", "authors": ["Zifan Wang", "Xun Yang", "Jianzhuang Zhao", "Jiaming Zhou", "Teli Ma", "Ziyao Gao", "Arash Ajoudani", "Junwei Liang"], "title": "End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy", "comment": null, "summary": "The deployment of humanoid robots in unstructured, human-centric environments\nrequires navigation capabilities that extend beyond simple locomotion to\ninclude robust perception, provable safety, and socially aware behavior.\nCurrent reinforcement learning approaches are often limited by blind\ncontrollers that lack environmental awareness or by vision-based systems that\nfail to perceive complex 3D obstacles. In this work, we present an end-to-end\nlocomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to\nmotor commands, enabling robust navigation in cluttered dynamic scenes. We\nformulate the control problem as a Constrained Markov Decision Process (CMDP)\nto formally separate safety from task objectives. Our key contribution is a\nnovel methodology that translates the principles of Control Barrier Functions\n(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal\nPolicy Optimization (P3O) to enforce safety constraints during training.\nFurthermore, we introduce a set of comfort-oriented rewards, grounded in\nhuman-robot interaction research, to promote motions that are smooth,\npredictable, and less intrusive. We demonstrate the efficacy of our framework\nthrough a successful sim-to-real transfer to a physical humanoid robot, which\nexhibits agile and safe navigation around both static and dynamic 3D obstacles.", "AI": {"tldr": "提出了一种基于LiDAR点云的端到端运动策略，结合CMDP和CBFs，实现人形机器人在复杂动态环境中的安全导航。", "motivation": "人形机器人在非结构化环境中需要更强的导航能力，现有强化学习方法在环境感知和安全性上存在不足。", "method": "使用CMDP框架，将CBFs转化为成本函数，结合P3O算法训练，并引入舒适性奖励。", "result": "实现了从仿真到实体的成功迁移，机器人能在静态和动态障碍物中安全灵活导航。", "conclusion": "该方法有效提升了人形机器人在复杂环境中的导航安全性和社会适应性。"}}
{"id": "2508.07648", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07648", "abs": "https://arxiv.org/abs/2508.07648", "authors": ["Mehrshad Zandigohar", "Mallesham Dasari", "Gunar Schirner"], "title": "Grasp-HGN: Grasping the Unexpected", "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems", "summary": "For transradial amputees, robotic prosthetic hands promise to regain the\ncapability to perform daily living activities. To advance next-generation\nprosthetic hand control design, it is crucial to address current shortcomings\nin robustness to out of lab artifacts, and generalizability to new\nenvironments. Due to the fixed number of object to interact with in existing\ndatasets, contrasted with the virtually infinite variety of objects encountered\nin the real world, current grasp models perform poorly on unseen objects,\nnegatively affecting users' independence and quality of life.\n  To address this: (i) we define semantic projection, the ability of a model to\ngeneralize to unseen object types and show that conventional models like YOLO,\ndespite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose\nGrasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to\ninfer the suitable grasp type estimate based on the object's physical\ncharacteristics resulting in a significant 50.2% accuracy over unseen object\ntypes compared to 36.7% accuracy of an SOTA grasp estimation model.\n  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp\nNetwork (HGN), an edge-cloud deployment infrastructure enabling fast grasp\nestimation on edge and accurate cloud inference as a fail-safe, effectively\nexpanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)\nenables dynamic switching between edge and cloud models, improving semantic\nprojection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object\ntypes. Over a real-world sample mix, it reaches 86% average accuracy (12.2%\ngain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.", "AI": {"tldr": "论文提出Grasp-LLaVA和HGN方法，解决假肢手控制中对未见物体泛化能力差和性能延迟问题，显著提升准确性和速度。", "motivation": "当前假肢手控制模型对未见物体泛化能力差，影响用户独立性和生活质量。", "method": "提出Grasp-LLaVA（基于视觉语言模型）和HGN（混合边缘-云部署架构），结合语义投影和动态切换技术。", "result": "Grasp-LLaVA对未见物体准确率提升至50.2%，HGN进一步将准确率提升至42.3%，速度提升3.5倍。", "conclusion": "Grasp-LLaVA和HGN显著提升了假肢手控制的泛化能力和实时性能，为下一代假肢设计提供了有效解决方案。"}}
{"id": "2508.07650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07650", "abs": "https://arxiv.org/abs/2508.07650", "authors": ["Helong Huang", "Min Cen", "Kai Tan", "Xingyue Quan", "Guowei Huang", "Hong Zhang"], "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions", "comment": "10 pages, 6 figures", "summary": "Vision-language-action models have emerged as a crucial paradigm in robotic\nmanipulation. However, existing VLA models exhibit notable limitations in\nhandling ambiguous language instructions and unknown environmental states.\nFurthermore, their perception is largely constrained to static two-dimensional\nobservations, lacking the capability to model three-dimensional interactions\nbetween the robot and its environment. To address these challenges, this paper\nproposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's\nability to interpret ambiguous instructions and improve task planning, we\ndesign a structured Chain-of-Thought reasoning module that integrates\nhigh-level task understanding and planning, failed task feedback, and low-level\nimaginative reasoning about future object positions and robot actions.\nAdditionally, we construct a real-time updatable 3D Pose-Object graph, which\ncaptures the spatial configuration of robot joints and the topological\nrelationships between objects in 3D space, enabling the model to better\nunderstand and manipulate their interactions. We further integrates a dropout\nhybrid reasoning strategy to achieve efficient control outputs. Experimental\nresults across multiple real-world robotic tasks demonstrate that GraphCoT-VLA\nsignificantly outperforms existing methods in terms of task success rate and\nresponse speed, exhibiting strong generalization and robustness in open\nenvironments and under uncertain instructions.", "AI": {"tldr": "GraphCoT-VLA是一种高效的端到端视觉-语言-动作模型，通过结构化思维链推理和实时更新的3D姿态-物体图，解决了现有模型在模糊指令和未知环境状态下的局限性。", "motivation": "现有视觉-语言-动作模型在处理模糊指令和未知环境状态时表现不佳，且缺乏对三维交互的建模能力。", "method": "设计了结构化思维链推理模块和实时更新的3D姿态-物体图，结合混合推理策略。", "result": "在多个实际机器人任务中，GraphCoT-VLA在任务成功率和响应速度上显著优于现有方法。", "conclusion": "GraphCoT-VLA在开放环境和不确定指令下表现出强大的泛化能力和鲁棒性。"}}
{"id": "2508.07657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07657", "abs": "https://arxiv.org/abs/2508.07657", "authors": ["Zhuoli Tian", "Yuyang Zhang", "Jinsheng Wei", "Meng Guo"], "title": "MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication", "comment": "38 pages, 28 figures, Submitted to the International Journal of\n  Robotics Research (IJRR). Project website: https://zl-tian.github.io/MoRoCo/", "summary": "Fleets of autonomous robots are increasingly deployed alongside multiple\nhuman operators to explore unknown environments, identify salient features, and\nperform complex tasks in scenarios such as subterranean exploration,\nreconnaissance, and search-and-rescue missions. In these contexts,\ncommunication is often severely limited to short-range exchanges via ad-hoc\nnetworks, posing challenges to coordination. While recent studies have\naddressed multi-robot exploration under communication constraints, they largely\noverlook the essential role of human operators and their real-time interaction\nwith robotic teams. Operators may demand timely updates on the exploration\nprogress and robot status, reprioritize or cancel tasks dynamically, or request\nlive video feeds and control access. Conversely, robots may seek human\nconfirmation for anomalous events or require help recovering from motion or\nplanning failures. To enable such bilateral, context-aware interactions under\nrestricted communication, this work proposes MoRoCo, a unified framework for\nonline coordination and exploration in multi-operator, multi-robot systems.\nMoRoCo enables the team to adaptively switch among three coordination modes:\nspread mode for parallelized exploration with intermittent data sharing,\nmigrate mode for coordinated relocation, and chain mode for maintaining\nhigh-bandwidth connectivity through multi-hop links. These transitions are\nmanaged through distributed algorithms via only local communication. Extensive\nlarge-scale human-in-the-loop simulations and hardware experiments validate the\nnecessity of incorporating human robot interactions and demonstrate that MoRoCo\nenables efficient, reliable coordination under limited communication, marking a\nsignificant step toward robust human-in-the-loop multi-robot autonomy in\nchallenging environments.", "AI": {"tldr": "MoRoCo框架支持多操作员与多机器人在有限通信下的实时交互与协调，通过三种模式实现高效探索。", "motivation": "现有研究多忽略人类操作员与机器人团队的实时交互需求，而通信受限环境下亟需双边、上下文感知的协作。", "method": "提出MoRoCo框架，包含三种协调模式（spread、migrate、chain），通过分布式算法实现本地通信管理。", "result": "大规模仿真与硬件实验验证了MoRoCo在有限通信下的高效协调能力。", "conclusion": "MoRoCo为挑战性环境中的人机协作多机器人系统提供了可靠解决方案。"}}
{"id": "2508.07686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07686", "abs": "https://arxiv.org/abs/2508.07686", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jiaqi Ma", "Jia Hu"], "title": "Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning", "comment": null, "summary": "End-to-end paradigm has emerged as a promising approach to autonomous\ndriving. However, existing single-agent end-to-end pipelines are often\nconstrained by occlusion and limited perception range, resulting in hazardous\ndriving. Furthermore, their black-box nature prevents the interpretability of\nthe driving behavior, leading to an untrustworthiness system. To address these\nlimitations, we introduce Risk Map as Middleware (RiskMM) and propose an\ninterpretable cooperative end-to-end driving framework. The risk map learns\ndirectly from the driving data and provides an interpretable spatiotemporal\nrepresentation of the scenario from the upstream perception and the\ninteractions between the ego vehicle and the surrounding environment for\ndownstream planning. RiskMM first constructs a multi-agent spatiotemporal\nrepresentation with unified Transformer-based architecture, then derives\nrisk-aware representations by modeling interactions among surrounding\nenvironments with attention. These representations are subsequently fed into a\nlearning-based Model Predictive Control (MPC) module. The MPC planner\ninherently accommodates physical constraints and different vehicle types and\ncan provide interpretation by aligning learned parameters with explicit MPC\nelements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm\nthat RiskMM achieves superior and robust performance in risk-aware trajectory\nplanning, significantly enhancing the interpretability of the cooperative\nend-to-end driving framework. The codebase will be released to facilitate\nfuture research in this field.", "AI": {"tldr": "提出了一种基于风险地图的中间件（RiskMM），用于构建可解释的协作端到端自动驾驶框架，解决了现有单智能体系统的遮挡、感知范围有限和黑盒问题。", "motivation": "现有单智能体端到端自动驾驶系统因遮挡和感知范围受限导致危险驾驶，且黑盒特性缺乏可解释性。", "method": "通过风险地图学习驾驶数据，构建多智能体时空表示，利用注意力建模环境交互，并结合基于学习的模型预测控制（MPC）模块。", "result": "在V2XPnP-Seq数据集上验证，RiskMM在风险感知轨迹规划中表现优越且鲁棒，显著提升了框架的可解释性。", "conclusion": "RiskMM为协作端到端自动驾驶提供了可解释且高效的解决方案，未来将开源代码以促进研究。"}}
{"id": "2508.07689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07689", "abs": "https://arxiv.org/abs/2508.07689", "authors": ["Christian Eichmann", "Sabine Bellmann", "Nicolas Hügel", "Louis-Elias Enslin", "Carsten Plasberg", "Georg Heppner", "Arne Roennau", "Ruediger Dillmann"], "title": "LAURON VI: A Six-Legged Robot for Dynamic Walking", "comment": null, "summary": "Legged locomotion enables robotic systems to traverse extremely challenging\nterrains. In many real-world scenarios, the terrain is not that difficult and\nthese mixed terrain types introduce the need for flexible use of different\nwalking strategies to achieve mission goals in a fast, reliable, and\nenergy-efficient way. Six-legged robots have a high degree of flexibility and\ninherent stability that aids them in traversing even some of the most difficult\nterrains, such as collapsed buildings. However, their lack of fast walking\ngaits for easier surfaces is one reason why they are not commonly applied in\nthese scenarios.\n  This work presents LAURON VI, a six-legged robot platform for research on\ndynamic walking gaits as well as on autonomy for complex field missions. The\nrobot's 18 series elastic joint actuators offer high-frequency interfaces for\nCartesian impedance and pure torque control. We have designed, implemented, and\ncompared three control approaches: kinematic-based, model-predictive, and\nreinforcement-learned controllers. The robot hardware and the different control\napproaches were extensively tested in a lab environment as well as on a Mars\nanalog mission. The introduction of fast locomotion strategies for LAURON VI\nmakes six-legged robots vastly more suitable for a wide range of real-world\napplications.", "AI": {"tldr": "本文介绍了六足机器人LAURON VI，旨在通过动态步态和自主控制技术提升其在复杂地形中的适应性，尤其是快速行走能力。", "motivation": "六足机器人在复杂地形中表现出色，但在简单地形中缺乏快速行走能力，限制了其广泛应用。", "method": "设计了三种控制方法：基于运动学的、模型预测的和强化学习的控制器，并在实验室和火星模拟任务中测试。", "result": "LAURON VI通过引入快速行走策略，显著提升了六足机器人在多种实际场景中的适用性。", "conclusion": "六足机器人通过动态步态和自主控制技术的结合，能够更好地适应复杂和混合地形任务。"}}
{"id": "2508.07758", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07758", "abs": "https://arxiv.org/abs/2508.07758", "authors": ["Antonio Rosales", "Alaa Abderrahim", "Markku Suomalainen", "Mikael Haag", "Tapio Heikkilä"], "title": "Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation", "comment": null, "summary": "This paper presents a scheme to enhance payload manipulation using a robot\ncollaborating with an overhead crane. In the current industrial practice, when\nthe crane's payload has to be accurately manipulated and located in a desired\nposition, the task becomes laborious and risky since the operators have to\nguide the fine motions of the payload by hand. In the proposed collaborative\nscheme, the crane lifts the payload while the robot's end-effector guides it\ntoward the desired position. The only link between the robot and the crane is\nthe interaction force produced during the guiding of the payload. Two\nadmittance transfer functions are considered to accomplish harmless and smooth\ncontact with the payload. The first is used in a position-based admittance\ncontrol integrated with the robot. The second one adds compliance to the crane\nby processing the interaction force through the admittance transfer function to\ngenerate a crane's velocity command that makes the crane follow the payload.\nThen the robot's end-effector and the crane move collaboratively to guide the\npayload to the desired location. A method is presented to design the admittance\ncontrollers that accomplish a fluent robot-crane collaboration. Simulations and\nexperiments validating the scheme potential are shown.", "AI": {"tldr": "提出了一种通过机器人与起重机协作增强载荷操纵的方案，减少人工操作的风险和复杂性。", "motivation": "当前工业实践中，起重机载荷的精确定位和操纵需要人工引导，任务繁重且危险。", "method": "采用机器人末端执行器引导载荷，通过交互力实现机器人与起重机的协作，设计了两种导纳传递函数以实现平滑接触。", "result": "通过仿真和实验验证了方案的可行性，实现了流畅的机器人与起重机协作。", "conclusion": "该协作方案有效提升了载荷操纵的精确性和安全性，减少了人工干预的需求。"}}
{"id": "2508.07770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07770", "abs": "https://arxiv.org/abs/2508.07770", "authors": ["Yizheng Zhang", "Zhenjun Yu", "Jiaxin Lai", "Cewu Lu", "Lei Han"], "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation", "comment": "Accepted by Conference on Robot Learning 2025", "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/", "AI": {"tldr": "AgentWorld是一个交互式仿真平台，用于开发家庭移动操作能力，结合自动化场景构建和双模式遥操作系统，支持从基础动作到多阶段任务的数据收集，并通过模仿学习方法验证其有效性。", "motivation": "解决家庭环境中机器人技能的可扩展获取问题，缩小仿真训练与实际部署之间的差距。", "method": "平台结合自动化场景构建（布局生成、语义资产放置、视觉材料配置、物理模拟）和双模式遥操作系统（轮式基座和人形运动策略），收集多样化任务数据。", "result": "通过模仿学习方法（行为克隆、动作分块变换器、扩散策略、视觉-语言-动作模型）验证了数据集在仿真到现实转移中的有效性。", "conclusion": "AgentWorld为复杂家庭环境中的机器人技能获取提供了全面解决方案，并公开了代码和数据集。"}}
{"id": "2508.07814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07814", "abs": "https://arxiv.org/abs/2508.07814", "authors": ["Malaika Zafar", "Roohan Ahmed Khan", "Faryal Batool", "Yasheerah Yaqoot", "Ziang Guo", "Mikhail Litvinov", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing", "comment": null, "summary": "With the growing demand for efficient logistics, unmanned aerial vehicles\n(UAVs) are increasingly being paired with automated guided vehicles (AGVs).\nWhile UAVs offer the ability to navigate through dense environments and varying\naltitudes, they are limited by battery life, payload capacity, and flight\nduration, necessitating coordinated ground support.\n  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by\nenabling semantic collaboration between UAVs and ground robots through\nimpedance control. The system leverages the Vision Language Model (VLM) and the\nRetrieval-Augmented Generation (RAG) to adjust impedance control parameters in\nresponse to environmental changes. In this framework, the UAV acts as a leader\nusing Artificial Potential Field (APF) planning for real-time navigation, while\nthe ground robot follows via virtual impedance links with adaptive link\ntopology to avoid collisions with short obstacles.\n  The system demonstrated a 92% success rate across 12 real-world trials. Under\noptimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in\nobject detection and selection of impedance parameters. The mobile robot\nprioritized short obstacle avoidance, occasionally resulting in a lateral\ndeviation of up to 50 cm from the UAV path, which showcases safe navigation in\na cluttered setting.", "AI": {"tldr": "SwarmVLM通过语义协作和阻抗控制，解决了无人机与地面机器人在异构导航中的限制，实现了92%的成功率和安全导航。", "motivation": "随着物流需求增长，无人机与地面机器人协作的需求增加，但无人机受限于电池、负载和飞行时间，需要地面支持。", "method": "结合视觉语言模型（VLM）和检索增强生成（RAG）调整阻抗控制参数，无人机作为领导者使用APF规划导航，地面机器人通过虚拟阻抗链接跟随。", "result": "系统在12次真实试验中成功率92%，VLM-RAG在理想光照下物体检测和参数选择准确率8%，地面机器人能安全避开障碍。", "conclusion": "SwarmVLM展示了异构导航中无人机与地面机器人协作的潜力，尤其在复杂环境中表现优异。"}}
{"id": "2508.07839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07839", "abs": "https://arxiv.org/abs/2508.07839", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans", "comment": null, "summary": "Affective tactile interaction constitutes a fundamental component of human\ncommunication. In natural human-human encounters, touch is seldom experienced\nin isolation; rather, it is inherently multisensory. Individuals not only\nperceive the physical sensation of touch but also register the accompanying\nauditory cues generated through contact. The integration of haptic and auditory\ninformation forms a rich and nuanced channel for emotional expression. While\nextensive research has examined how robots convey emotions through facial\nexpressions and speech, their capacity to communicate social gestures and\nemotions via touch remains largely underexplored. To address this gap, we\ndeveloped a multimodal interaction system incorporating a 5*5 grid of 25\nvibration motors synchronized with audio playback, enabling robots to deliver\ncombined haptic-audio stimuli. In an experiment involving 32 Chinese\nparticipants, ten emotions and six social gestures were presented through\nvibration, sound, or their combination. Participants rated each stimulus on\narousal and valence scales. The results revealed that (1) the combined\nhaptic-audio modality significantly enhanced decoding accuracy compared to\nsingle modalities; (2) each individual channel-vibration or sound-effectively\nsupported certain emotions recognition, with distinct advantages depending on\nthe emotional expression; and (3) gestures alone were generally insufficient\nfor conveying clearly distinguishable emotions. These findings underscore the\nimportance of multisensory integration in affective human-robot interaction and\nhighlight the complementary roles of haptic and auditory cues in enhancing\nemotional communication.", "AI": {"tldr": "研究开发了一种多模态交互系统，结合触觉和听觉刺激，探索机器人通过触摸传达情感和社交手势的能力。实验表明，多模态显著提高了情感解码准确性，触觉和听觉各自对特定情感识别有优势。", "motivation": "探索机器人通过触觉和听觉结合的方式传达情感和社交手势的能力，填补现有研究中机器人情感表达方式的不足。", "method": "开发了一个包含25个振动马达和音频播放的多模态系统，通过振动、声音或其组合呈现十种情感和六种社交手势，32名参与者对刺激进行评分。", "result": "多模态显著提高了情感解码准确性；触觉和听觉各自对特定情感识别有优势；单独手势难以清晰传达情感。", "conclusion": "多感官整合在情感人机交互中至关重要，触觉和听觉线索在增强情感沟通中具有互补作用。"}}
{"id": "2508.07842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07842", "abs": "https://arxiv.org/abs/2508.07842", "authors": ["Yutong Shen", "Hangxu Liu", "Penghui Liu", "Ruizhe Xia", "Tianyi Yao", "Yitong Sun", "Tongtong Feng"], "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts", "comment": "14 pages,8 figures. Submitted to AAAI'26", "summary": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex\nmulti-step tasks that require continuous planning, sequential decision-making,\nand extended execution across domains to achieve the final goal. However,\nexisting methods heavily rely on skill chaining by concatenating pre-trained\nsubtasks, with environment observations and self-state tightly coupled, lacking\nthe ability to generalize to new combinations of environments and skills,\nfailing to complete various LH tasks across domains. To solve this problem,\nthis paper presents DETACH, a cross-domain learning framework for LH tasks via\nbiologically inspired dual-stream disentanglement. Inspired by the brain's\n\"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an\nenvironment learning module for spatial understanding, which captures object\nfunctions, spatial relationships, and scene semantics, achieving cross-domain\ntransfer through complete environment-self disentanglement; ii) a skill\nlearning module for task execution, which processes self-state information\nincluding joint degrees of freedom and motor patterns, enabling cross-skill\ntransfer through independent motor pattern encoding. We conducted extensive\nexperiments on various LH tasks in HSI scenes. Compared with existing methods,\nDETACH can achieve an average subtasks success rate improvement of 23% and\naverage execution efficiency improvement of 29%.", "AI": {"tldr": "DETACH是一种基于生物启发的双流解耦框架，用于解决跨领域长时程任务中的泛化问题，显著提升了任务成功率和执行效率。", "motivation": "现有方法依赖技能链式拼接，难以泛化到新环境和技能组合，无法完成跨领域的长时程任务。", "method": "DETACH通过环境学习模块（空间理解）和技能学习模块（任务执行）实现环境和自我的完全解耦，支持跨领域和跨技能迁移。", "result": "实验表明，DETACH在任务成功率和执行效率上分别平均提升了23%和29%。", "conclusion": "DETACH通过双流解耦机制有效解决了跨领域长时程任务的泛化问题，性能显著优于现有方法。"}}
{"id": "2508.07917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07917", "abs": "https://arxiv.org/abs/2508.07917", "authors": ["Jason Lee", "Jiafei Duan", "Haoquan Fang", "Yuquan Deng", "Shuo Liu", "Boyang Li", "Bohan Fang", "Jieyu Zhang", "Yi Ru Wang", "Sangho Lee", "Winson Han", "Wilbert Pumacay", "Angelica Wu", "Rose Hendrix", "Karen Farley", "Eli VanderBilt", "Ali Farhadi", "Dieter Fox", "Ranjay Krishna"], "title": "MolmoAct: Action Reasoning Models that can Reason in Space", "comment": "Appendix on Blogpost: https://allenai.org/blog/molmoact", "summary": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact", "AI": {"tldr": "MolmoAct是一种新型视觉-语言-动作模型（ARM），通过三阶段结构化推理提升机器人适应性、泛化能力和语义理解，在仿真和现实任务中表现优异。", "motivation": "现有机器人基础模型直接将感知和指令映射到控制，限制了适应性、泛化能力和语义理解。", "method": "MolmoAct采用三阶段结构化推理：深度感知编码、可编辑轨迹规划、精确动作预测。", "result": "在仿真和现实任务中表现优异，如70.5%零样本准确率（SimperEnv）、86.6%成功率（LIBERO），并显著优于基线模型。", "conclusion": "MolmoAct是当前最先进的机器人基础模型，通过结构化推理将感知转化为有目的的动作，并开源了模型和数据集。"}}
{"id": "2508.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07945", "abs": "https://arxiv.org/abs/2508.07945", "authors": ["En Yen Puang", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "We consider the problem of learning a common representation for dexterous\nmanipulation across manipulators of different morphologies. To this end, we\npropose PCHands, a novel approach for extracting hand postural synergies from a\nlarge set of manipulators. We define a simplified and unified description\nformat based on anchor positions for manipulators ranging from 2-finger\ngrippers to 5-finger anthropomorphic hands. This enables learning a\nvariable-length latent representation of the manipulator configuration and the\nalignment of the end-effector frame of all manipulators. We show that it is\npossible to extract principal components from this latent representation that\nis universal across manipulators of different structures and degrees of\nfreedom. To evaluate PCHands, we use this compact representation to encode\nobservation and action spaces of control policies for dexterous manipulation\ntasks learned with RL. In terms of learning efficiency and consistency, the\nproposed representation outperforms a baseline that learns the same tasks in\njoint space. We additionally show that PCHands performs robustly in RL from\ndemonstration, when demonstrations are provided from a different manipulator.\nWe further support our results with real-world experiments that involve a\n2-finger gripper and a 4-finger anthropomorphic hand. Code and additional\nmaterial are available at https://hsp-iit.github.io/PCHands/.", "AI": {"tldr": "PCHands提出了一种通用表示方法，用于不同形态机械手的灵巧操作学习，通过锚点位置统一描述格式，提取跨机械手的主成分，并在强化学习中验证其高效性和鲁棒性。", "motivation": "解决不同形态机械手在灵巧操作中的通用表示问题，以提高学习效率和跨机械手的任务一致性。", "method": "提出PCHands方法，基于锚点位置统一描述格式，学习可变长度潜在表示，并提取跨机械手的主成分。", "result": "PCHands在强化学习中表现优于关节空间基线，且在跨机械手的演示学习中表现鲁棒。", "conclusion": "PCHands为不同形态机械手的灵巧操作提供了一种高效且通用的表示方法。"}}
{"id": "2508.08046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08046", "abs": "https://arxiv.org/abs/2508.08046", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Wei Meng", "Lihua Xie"], "title": "Aerial Target Encirclement and Interception with Noisy Range Observations", "comment": "The paper has been accepted in Automatica", "summary": "This paper proposes a strategy to encircle and intercept a non-cooperative\naerial point-mass moving target by leveraging noisy range measurements for\nstate estimation. In this approach, the guardians actively ensure the\nobservability of the target by using an anti-synchronization (AS), 3D\n``vibrating string\" trajectory, which enables rapid position and velocity\nestimation based on the Kalman filter. Additionally, a novel anti-target\ncontroller is designed for the guardians to enable adaptive transitions from\nencircling a protected target to encircling, intercepting, and neutralizing a\nhostile target, taking into consideration the input constraints of the\nguardians. Based on the guaranteed uniform observability, the exponentially\nbounded stability of the state estimation error and the convergence of the\nencirclement error are rigorously analyzed. Simulation results and real-world\nUAV experiments are presented to further validate the effectiveness of the\nsystem design.", "AI": {"tldr": "提出一种利用噪声距离测量包围拦截非合作空中目标的策略，通过反同步3D轨迹确保目标可观测性，并设计新型控制器实现自适应拦截。", "motivation": "解决非合作空中目标的快速状态估计和拦截问题，确保目标可观测性并适应输入约束。", "method": "采用反同步3D轨迹和卡尔曼滤波进行状态估计，设计自适应控制器实现目标拦截。", "result": "仿真和无人机实验验证了系统设计的有效性，状态估计误差和包围误差收敛。", "conclusion": "该方法能有效拦截非合作目标，确保状态估计和拦截的稳定性。"}}
{"id": "2508.08108", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08108", "abs": "https://arxiv.org/abs/2508.08108", "authors": ["Wei Zhang", "Yinchuan Wang", "Wangtao Lu", "Pengyu Zhang", "Xiang Zhang", "Yue Wang", "Chaoqun Wang"], "title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain", "comment": null, "summary": "It is a challenging task for ground robots to autonomously navigate in harsh\nenvironments due to the presence of non-trivial obstacles and uneven terrain.\nThis requires trajectory planning that balances safety and efficiency. The\nprimary challenge is to generate a feasible trajectory that prevents robot from\ntip-over while ensuring effective navigation. In this paper, we propose a\ncapsizing-aware trajectory planner (CAP) to achieve trajectory planning on the\nuneven terrain. The tip-over stability of the robot on rough terrain is\nanalyzed. Based on the tip-over stability, we define the traversable\norientation, which indicates the safe range of robot orientations. This\norientation is then incorporated into a capsizing-safety constraint for\ntrajectory optimization. We employ a graph-based solver to compute a robust and\nfeasible trajectory while adhering to the capsizing-safety constraint.\nExtensive simulation and real-world experiments validate the effectiveness and\nrobustness of the proposed method. The results demonstrate that CAP outperforms\nexisting state-of-the-art approaches, providing enhanced navigation performance\non uneven terrains.", "AI": {"tldr": "提出了一种基于翻覆感知的轨迹规划方法（CAP），用于地面机器人在复杂地形中的安全高效导航。", "motivation": "地面机器人在复杂地形中导航时面临翻覆风险，需平衡安全与效率。", "method": "分析机器人翻覆稳定性，定义可穿越方向，并将其作为约束加入轨迹优化，使用图求解器生成轨迹。", "result": "仿真和实验表明，CAP方法优于现有技术，提升了导航性能。", "conclusion": "CAP方法有效解决了复杂地形中的导航问题，具有鲁棒性和实用性。"}}
{"id": "2508.08113", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08113", "abs": "https://arxiv.org/abs/2508.08113", "authors": ["Yinpei Dai", "Jayjun Lee", "Yichi Zhang", "Ziqiao Ma", "Jed Yang", "Amir Zadeh", "Chuan Li", "Nima Fazeli", "Joyce Chai"], "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies", "comment": "CoRL 2025", "summary": "In this paper, we propose AimBot, a lightweight visual augmentation technique\nthat provides explicit spatial cues to improve visuomotor policy learning in\nrobotic manipulation. AimBot overlays shooting lines and scope reticles onto\nmulti-view RGB images, offering auxiliary visual guidance that encodes the\nend-effector's state. The overlays are computed from depth images, camera\nextrinsics, and the current end-effector pose, explicitly conveying spatial\nrelationships between the gripper and objects in the scene. AimBot incurs\nminimal computational overhead (less than 1 ms) and requires no changes to\nmodel architectures, as it simply replaces original RGB images with augmented\ncounterparts. Despite its simplicity, our results show that AimBot consistently\nimproves the performance of various visuomotor policies in both simulation and\nreal-world settings, highlighting the benefits of spatially grounded visual\nfeedback.", "AI": {"tldr": "AimBot是一种轻量级视觉增强技术，通过叠加辅助视觉线索（如射击线和准星）到多视角RGB图像中，提升机器人操作中的视觉运动策略学习。", "motivation": "在机器人操作中，视觉运动策略学习通常缺乏明确的空间线索，AimBot旨在通过视觉增强提供空间引导，改善策略性能。", "method": "AimBot利用深度图像、相机外参和末端执行器姿态计算叠加内容，替换原始RGB图像，无需修改模型架构。", "result": "实验表明，AimBot在仿真和真实环境中均能显著提升多种视觉运动策略的性能，计算开销极低（小于1毫秒）。", "conclusion": "AimBot通过简单但有效的空间视觉反馈，显著提升了机器人操作的视觉运动策略学习效果。"}}
{"id": "2508.08226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08226", "abs": "https://arxiv.org/abs/2508.08226", "authors": ["Haiyue Chen", "Aniket Datar", "Tong Xu", "Francesco Cancelliere", "Harsh Rangwala", "Madhan Balaji Rao", "Daeun Song", "David Eichinger", "Xuesu Xiao"], "title": "Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy", "comment": "6 pages", "summary": "Off-road navigation is an important capability for mobile robots deployed in\nenvironments that are inaccessible or dangerous to humans, such as disaster\nresponse or planetary exploration. Progress is limited due to the lack of a\ncontrollable and standardized real-world testbed for systematic data collection\nand validation. To fill this gap, we introduce Verti-Arena, a reconfigurable\nindoor facility designed specifically for off-road autonomy. By providing a\nrepeatable benchmark environment, Verti-Arena supports reproducible experiments\nacross a variety of vertically challenging terrains and provides precise ground\ntruth measurements through onboard sensors and a motion capture system.\nVerti-Arena also supports consistent data collection and comparative evaluation\nof algorithms in off-road autonomy research. We also develop a web-based\ninterface that enables research groups worldwide to remotely conduct\nstandardized off-road autonomy experiments on Verti-Arena.", "AI": {"tldr": "论文介绍了Verti-Arena，一个可重构的室内设施，用于标准化和可重复的越野自主导航实验。", "motivation": "越野导航对移动机器人在危险或难以进入的环境中（如灾难响应或行星探索）至关重要，但缺乏可控且标准化的真实测试环境限制了研究进展。", "method": "通过Verti-Arena设施，提供可重复的基准环境，支持多种垂直挑战性地形的实验，并利用机载传感器和运动捕捉系统提供精确的地面真实数据。", "result": "Verti-Arena支持一致的数据收集和算法比较，并通过基于网络的界面实现全球研究团队远程进行标准化实验。", "conclusion": "Verti-Arena填补了越野自主导航研究中标准化测试环境的空白，促进了可重复性和比较性研究。"}}
{"id": "2508.08240", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08240", "abs": "https://arxiv.org/abs/2508.08240", "authors": ["Kaijun Wang", "Liqin Lu", "Mingyu Liu", "Jianuo Jiang", "Zeju Li", "Bolin Zhang", "Wancai Zheng", "Xinyi Yu", "Hao Chen", "Chunhua Shen"], "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks", "comment": null, "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/", "AI": {"tldr": "ODYSSEY是一个统一的移动操作框架，用于配备机械臂的四足机器人，结合了高级任务规划和低级全身控制。", "motivation": "解决语言引导的长时程移动操作中的三大挑战：现有方法局限于桌面场景、泛化能力不足、以及在高机动性和精确末端执行器控制之间的平衡问题。", "method": "引入分层规划器（基于视觉语言模型）和新型全身控制策略，实现任务分解和精确执行。", "result": "通过仿真到现实的迁移，展示了系统在真实环境中的泛化能力和鲁棒性。", "conclusion": "ODYSSEY推动了通用机器人助手在复杂动态任务中的可行性。"}}
{"id": "2508.08241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08241", "abs": "https://arxiv.org/abs/2508.08241", "authors": ["Takara E. Truong", "Qiayuan Liao", "Xiaoyu Huang", "Guy Tevet", "C. Karen Liu", "Koushil Sreenath"], "title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion", "comment": "9 pages, 1 figure", "summary": "Learning skills from human motions offers a promising path toward\ngeneralizable policies for whole-body humanoid control, yet two key\ncornerstones are missing: (1) a high-quality motion tracking framework that\nfaithfully transforms large-scale kinematic references into robust and\nextremely dynamic motions on real hardware, and (2) a distillation approach\nthat can effectively learn these motion primitives and compose them to solve\ndownstream tasks. We address these gaps with BeyondMimic, the first real-world\nframework to learn from human motions for versatile and naturalistic humanoid\ncontrol via guided diffusion. Our framework provides a motion tracking pipeline\ncapable of challenging skills such as jumping spins, sprinting, and cartwheels\nwith state-of-the-art motion quality. Moving beyond mimicking existing motions\nand synthesize novel ones, we further introduce a unified diffusion policy that\nenables zero-shot task-specific control at test time using simple cost\nfunctions. Deployed on hardware, BeyondMimic performs diverse tasks at test\ntime, including waypoint navigation, joystick teleoperation, and obstacle\navoidance, bridging sim-to-real motion tracking and flexible synthesis of human\nmotion primitives for whole-body control. https://beyondmimic.github.io/.", "AI": {"tldr": "BeyondMimic框架通过扩散策略从人类动作中学习，实现高动态运动跟踪和任务控制。", "motivation": "解决从人类动作中学习通用策略的两个关键问题：高质量运动跟踪和动作原语的合成与组合。", "method": "采用扩散策略，结合运动跟踪管道和任务特定控制。", "result": "实现了高动态运动（如跳跃、冲刺等）和零样本任务控制（如导航、避障）。", "conclusion": "BeyondMimic成功将模拟到真实的运动跟踪与动作原语合成结合，为全身控制提供灵活解决方案。"}}
