{"id": "2509.00054", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00054", "abs": "https://arxiv.org/abs/2509.00054", "authors": ["Haimei Pan", "Jiyun Zhang", "Qinxi Wei", "Xiongnan Jin", "Chen Xinkai", "Jie Cheng"], "title": "Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought", "comment": null, "summary": "Fire is a highly destructive disaster, but effective prevention can\nsignificantly reduce its likelihood of occurrence. When it happens, deploying\nemergency robots in fire-risk scenarios can help minimize the danger to human\nresponders. However, current research on pre-disaster warnings and\ndisaster-time rescue still faces significant challenges due to incomplete\nperception, inadequate fire situational awareness, and delayed response. To\nenhance intelligent perception and response planning for robots in fire\nscenarios, we first construct a knowledge graph (KG) by leveraging large\nlanguage models (LLMs) to integrate fire domain knowledge derived from fire\nprevention guidelines and fire rescue task information from robotic emergency\nresponse documents. We then propose a new framework called Insights-on-Graph\n(IOG), which integrates the structured fire information of KG and Large\nMultimodal Models (LMMs). The framework generates perception-driven risk graphs\nfrom real-time scene imagery to enable early fire risk detection and provide\ninterpretable emergency responses for task module and robot component\nconfiguration based on the evolving risk situation. Extensive simulations and\nreal-world experiments show that IOG has good applicability and practical\napplication value in fire risk detection and rescue decision-making.", "AI": {"tldr": "基于知识图谱和大语言模型构建的IOG框架，通过实时场景图像生成感知驱动的风险图谱，实现火灾早期风险检测和可解释的应急响应规划", "motivation": "当前火灾预防和救援研究面临感知不完整、态势认知不足和响应延迟等挑战，需要提升机器人在火灾场景中的智能感知和响应规划能力", "method": "首先利用大语言模型构建火灾知识图谱，整合防火指南和救援任务信息；然后提出IOG框架，结合知识图谱的结构化信息和大型多模态模型，从实时图像生成风险图谱", "result": "大量仿真和真实实验表明，IOG在火灾风险检测和救援决策方面具有良好的适用性和实际应用价值", "conclusion": "IOG框架有效提升了机器人在火灾场景中的智能感知能力和应急响应规划水平，为火灾预防和救援提供了新的技术解决方案"}}
{"id": "2509.00055", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.00055", "abs": "https://arxiv.org/abs/2509.00055", "authors": ["Tongtong Feng", "Xin Wang", "Feilin Han", "Leping Zhang", "Wenwu Zhu"], "title": "U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks", "comment": null, "summary": "Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for\nadvancing the low-altitude economy. However, existing methods focus only on\nspecific basic tasks due to dataset limitations, failing in real-world\ndeployment for LH tasks. LH tasks are not mere concatenations of basic tasks,\nrequiring handling long-term dependencies, maintaining persistent states, and\nadapting to dynamic goal shifts. This paper presents U2UData-2, the first\nlarge-scale swarm UAV autonomous flight dataset for LH tasks and the first\nscalable swarm UAV data online collection and algorithm closed-loop\nverification platform. The dataset is captured by 15 UAVs in autonomous\ncollaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120\nhours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames.\nThis dataset also includes brightness, temperature, humidity, smoke, and\nairflow values covering all flight routes. The platform supports the\ncustomization of simulators, UAVs, sensors, flight algorithms, formation modes,\nand LH tasks. Through a visual control window, this platform allows users to\ncollect customized datasets through one-click deployment online and to verify\nalgorithms by closed-loop simulation. U2UData-2 also introduces an LH task for\nwildlife conservation and provides comprehensive benchmarks with 9 SOTA models.\nU2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.", "AI": {"tldr": "U2UData-2是首个面向长时域任务的大规模无人机集群自主飞行数据集和平台，包含12个场景、720条轨迹、120小时数据，支持野生动物保护等LH任务验证。", "motivation": "现有方法受限于数据集，只能处理特定基础任务，无法应对真实世界长时域任务的长期依赖、状态保持和动态目标变化等挑战。", "method": "构建包含15架无人机自主协同飞行的数据集，开发可定制模拟器、无人机、传感器和算法的在线平台，支持一键部署和闭环验证。", "result": "创建了包含432万LiDAR帧和1296万RGB帧的大规模数据集，涵盖亮度、温度、湿度等多种环境参数，并提供9个SOTA模型的基准测试。", "conclusion": "U2UData-2填补了长时域无人机集群自主飞行数据集的空白，为真实世界部署提供了重要的数据集和验证平台支撑。"}}
{"id": "2509.00060", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00060", "abs": "https://arxiv.org/abs/2509.00060", "authors": ["Yingjun Tian", "Guoxin Fang", "Renbo Su", "Aoran Lyu", "Neelotpal Dutta", "Simeon Gill", "Andrew Weightman", "Charlie C. L. Wang"], "title": "Correspondence-Free, Function-Based Sim-to-Real Learning for Deformable Surface Control", "comment": null, "summary": "This paper presents a correspondence-free, function-based sim-to-real\nlearning method for controlling deformable freeform surfaces. Unlike\ntraditional sim-to-real transfer methods that strongly rely on marker points\nwith full correspondences, our approach simultaneously learns a deformation\nfunction space and a confidence map -- both parameterized by a neural network\n-- to map simulated shapes to their real-world counterparts. As a result, the\nsim-to-real learning can be conducted by input from either a 3D scanner as\npoint clouds (without correspondences) or a motion capture system as marker\npoints (tolerating missed markers). The resultant sim-to-real transfer can be\nseamlessly integrated into a neural network-based computational pipeline for\ninverse kinematics and shape control. We demonstrate the versatility and\nadaptability of our method on both vision devices and across four pneumatically\nactuated soft robots: a deformable membrane, a robotic mannequin, and two soft\nmanipulators.", "AI": {"tldr": "提出了一种基于函数的无对应点模拟到真实学习方法，用于控制可变形自由曲面，无需依赖标记点对应关系，通过神经网络学习变形函数空间和置信度映射。", "motivation": "传统模拟到真实转换方法严重依赖具有完整对应关系的标记点，限制了在无对应点或标记点缺失情况下的应用。", "method": "同时学习由神经网络参数化的变形函数空间和置信度映射，可将模拟形状映射到真实对应物，支持3D扫描点云或运动捕捉系统输入。", "result": "方法在视觉设备和四种气动软机器人上展示了多功能性和适应性，包括可变形膜、机器人模型和两个软操纵器。", "conclusion": "该方法实现了无缝的模拟到真实转换，可集成到基于神经网络的逆向运动学和形状控制计算流程中。"}}
{"id": "2509.00064", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00064", "abs": "https://arxiv.org/abs/2509.00064", "authors": ["Mingze Liu", "Sai Fan", "Haozhen Li", "Haobo Liang", "Yixing Yuan", "Yanke Wang"], "title": "OpenTie: Open-vocabulary Sequential Rebar Tying System", "comment": "This article is under its initial revision", "summary": "Robotic practices on the construction site emerge as an attention-attracting\nmanner owing to their capability of tackle complex challenges, especially in\nthe rebar-involved scenarios. Most of existing products and research are mainly\nfocused on flat rebar setting with model training demands. To fulfill this gap,\nwe propose OpenTie, a 3D training-free rebar tying framework utilizing a\nRGB-to-point-cloud generation and an open-vocabulary detection. We implements\nthe OpenTie via a robotic arm with a binocular camera and guarantees a high\naccuracy by applying the prompt-based object detection method on the image\nfiltered by our propose post-processing procedure based a image to point cloud\ngeneration framework. The system is flexible for horizontal and vertical rebar\ntying tasks and the experiments on the real-world rebar setting verifies that\nthe effectiveness of the system in practice.", "AI": {"tldr": "OpenTie是一个无需训练的3D钢筋绑扎框架，使用RGB到点云生成和开放词汇检测技术，通过双目相机和机械臂实现高精度水平与垂直钢筋绑扎。", "motivation": "现有钢筋绑扎产品和研究主要关注平面钢筋设置且需要模型训练，无法满足复杂场景需求，需要开发无需训练且能处理3D场景的解决方案。", "method": "采用RGB到点云生成技术和开放词汇检测方法，通过双目相机捕获图像，基于提示的目标检测方法在点云生成后处理的过滤图像上实现高精度检测。", "result": "系统在真实世界钢筋设置实验中验证了有效性，能够灵活处理水平和垂直钢筋绑扎任务。", "conclusion": "OpenTie框架成功解决了无需模型训练的3D钢筋绑扎问题，在实际应用中表现出良好的效果和灵活性。"}}
{"id": "2509.00319", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00319", "abs": "https://arxiv.org/abs/2509.00319", "authors": ["Chi Kit Ng", "Huxin Gao", "Tian-Ao Ren", "Jiewen Lai", "Hongliang Ren"], "title": "Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach", "comment": null, "summary": "Navigating a flexible robotic endoscope (FRE) through the gastrointestinal\ntract is critical for surgical diagnosis and treatment. However, navigation in\nthe dynamic stomach is particularly challenging because the FRE must learn to\neffectively use contact with the deformable stomach walls to reach target\nlocations. To address this, we introduce a deep reinforcement learning (DRL)\nbased Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact\nforce feedback to enhance motion stability and navigation precision. The\ntraining environment is established using a physics-based finite element method\n(FEM) simulation of a deformable stomach. Trained with the Proximal Policy\nOptimization (PPO) algorithm, our approach achieves high navigation success\nrates (within 3 mm error between the FRE's end-effector and target) and\nsignificantly outperforms baseline policies. In both static and dynamic stomach\nenvironments, the CAN agent achieved a 100% success rate with 1.6 mm average\nerror, and it maintained an 85% success rate in challenging unseen scenarios\nwith stronger external disturbances. These results validate that the DRL-based\nCAN strategy substantially enhances FRE navigation performance over prior\nmethods.", "AI": {"tldr": "基于深度强化学习的接触辅助导航策略，利用力反馈在可变形胃环境中实现柔性内窥镜的高精度导航", "motivation": "柔性内窥镜在动态胃环境中导航具有挑战性，需要学习利用与可变形胃壁的接触来达到目标位置", "method": "使用基于物理的有限元方法模拟可变形胃环境，采用近端策略优化算法训练接触辅助导航策略", "result": "在静态和动态胃环境中达到100%成功率（平均误差1.6mm），在具有更强外部干扰的未见场景中保持85%成功率", "conclusion": "基于深度强化学习的接触辅助导航策略显著提升了柔性内窥镜的导航性能，优于现有方法"}}
{"id": "2509.00065", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00065", "abs": "https://arxiv.org/abs/2509.00065", "authors": ["Zhitao Wang", "Yirong Xiong", "Roberto Horowitz", "Yanke Wang", "Yuxing Han"], "title": "Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying", "comment": "Accepted by The IEEE International Conference on Automation Science\n  and Engineering (CASE) 2025", "summary": "Rebar tying is a repetitive but critical task in reinforced concrete\nconstruction, typically performed manually at considerable ergonomic risk.\nRecent advances in robotic manipulation hold the potential to automate the\ntying process, yet face challenges in accurately estimating tying poses in\ncongested rebar nodes. In this paper, we introduce a hybrid perception and\nmotion planning approach that integrates geometry-based perception with\nEquivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust\nmulti-node rebar tying with minimal training data. Our perception module\nutilizes density-based clustering (DBSCAN), geometry-based node feature\nextraction, and principal component analysis (PCA) to segment rebar bars,\nidentify rebar nodes, and estimate orientation vectors for sequential ranking,\neven in complex, unstructured environments. The motion planner, based on\nDiffusion-EDFs, is trained on as few as 5-10 demonstrations to generate\nsequential end-effector poses that optimize collision avoidance and tying\nefficiency. The proposed system is validated on various rebar meshes, including\nsingle-layer, multi-layer, and cluttered configurations, demonstrating high\nsuccess rates in node detection and accurate sequential tying. Compared with\nconventional approaches that rely on large datasets or extensive manual\nparameter tuning, our method achieves robust, efficient, and adaptable\nmulti-node tying while significantly reducing data requirements. This result\nunderscores the potential of hybrid perception and diffusion-driven planning to\nenhance automation in on-site construction tasks, improving both safety and\nlabor efficiency.", "AI": {"tldr": "提出了一种结合几何感知和SE(3)等变去噪扩散的混合方法，用于自动化钢筋绑扎任务，仅需少量训练数据即可实现高效的多节点绑扎。", "motivation": "钢筋绑扎是钢筋混凝土施工中重复性高且具有人体工程学风险的关键任务，现有机器人方法在拥挤钢筋节点中准确估计绑扎姿态面临挑战。", "method": "结合密度聚类(DBSCAN)、几何特征提取和PCA进行钢筋分割和节点识别，使用Diffusion-EDFs运动规划器基于5-10个演示样本生成优化的末端执行器姿态序列。", "result": "在单层、多层和杂乱配置的钢筋网格上验证，展示了高成功率的节点检测和准确的顺序绑扎性能，显著降低了数据需求。", "conclusion": "混合感知和扩散驱动规划方法具有增强现场施工任务自动化的潜力，可同时提高安全性和劳动效率。"}}
{"id": "2509.00329", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00329", "abs": "https://arxiv.org/abs/2509.00329", "authors": ["Yu Tian", "Chi Kit Ng", "Hongliang Ren"], "title": "Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots", "comment": null, "summary": "Deformable continuum robots (DCRs) present unique planning challenges due to\nnonlinear deformation mechanics and partial state observability, violating the\nMarkov assumptions of conventional reinforcement learning (RL) methods. While\nJacobian-based approaches offer theoretical foundations for rigid manipulators,\ntheir direct application to DCRs remains limited by time-varying kinematics and\nunderactuated deformation dynamics. This paper proposes Jacobian Exploratory\nDual-Phase RL (JEDP-RL), a framework that decomposes planning into phased\nJacobian estimation and policy execution. During each training step, we first\nperform small-scale local exploratory actions to estimate the deformation\nJacobian matrix, then augment the state representation with Jacobian features\nto restore approximate Markovianity. Extensive SOFA surgical dynamic\nsimulations demonstrate JEDP-RL's three key advantages over proximal policy\noptimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy\nconvergence, 2) Navigation efficiency: requires 25% fewer steps to reach the\ntarget, and 3) Generalization ability: achieve 92% success rate under material\nproperty variations and achieve 83% (33% higher than PPO) success rate in the\nunseen tissue environment.", "AI": {"tldr": "提出了JEDP-RL框架，通过分阶段雅可比估计和策略执行来解决可变形连续体机器人的规划问题，相比PPO基线方法在收敛速度、导航效率和泛化能力方面都有显著提升", "motivation": "可变形连续体机器人(DCRs)由于非线性变形力学和部分状态可观测性，违反了传统强化学习方法的马尔可夫假设，而基于雅可比的方法虽然为刚性机械臂提供了理论基础，但直接应用于DCRs受到时变运动学和欠驱动变形动力学的限制", "method": "提出Jacobian Exploratory Dual-Phase RL (JEDP-RL)框架，将规划分解为分阶段的雅可比估计和策略执行。在每个训练步骤中，首先执行小规模局部探索动作来估计变形雅可比矩阵，然后用雅可比特征增强状态表示以恢复近似的马尔可夫性", "result": "在SOFA手术动态模拟中，JEDP-RL相比PPO基线表现出三个关键优势：1)收敛速度：策略收敛速度快3.2倍；2)导航效率：到达目标所需的步骤减少25%；3)泛化能力：在材料属性变化下达到92%的成功率，在未见过的组织环境中达到83%的成功率（比PPO高33%）", "conclusion": "JEDP-RL框架通过结合局部探索和雅可比特征增强，有效解决了DCRs的非马尔可夫规划问题，在多个性能指标上显著优于传统PPO方法，展示了在复杂变形环境中的优越性能"}}
{"id": "2509.00119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00119", "abs": "https://arxiv.org/abs/2509.00119", "authors": ["Jake Robbennolt", "Sirajum Munira", "Stephen D. Boyles"], "title": "A Comparative Study of Spline-Based Trajectory Reconstruction Methods Across Varying Automatic Vehicle Location Data Densities", "comment": null, "summary": "Automatic vehicle location (AVL) data offers insights into transit dynamics,\nbut its effectiveness is often hampered by inconsistent update frequencies,\nnecessitating trajectory reconstruction. This research evaluates 13 trajectory\nreconstruction methods, including several novel approaches, using\nhigh-resolution AVL data from Austin, Texas. We examine the interplay of four\ncritical factors -- velocity, position, smoothing, and data density -- on\nreconstruction performance. A key contribution of this study is evaluation of\nthese methods across sparse and dense datasets, providing insights into the\ntrade-off between accuracy and resource allocation. Our evaluation framework\ncombines traditional mathematical error metrics for positional and velocity\nwith practical considerations, such as physical realism (e.g., aligning\nvelocity and acceleration with stopped states, deceleration rates, and speed\nvariability). In addition, we provide insight into the relative value of each\nmethod in calculating realistic metrics for infrastructure evaluations. Our\nfindings indicate that velocity-aware methods consistently outperform\nposition-only approaches. Interestingly, we discovered that smoothing-based\nmethods can degrade overall performance in complex, congested urban\nenvironments, although enforcing monotonicity remains critical. The velocity\nconstrained Hermite interpolation with monotonicity enforcement (VCHIP-ME)\nyields optimal results, offering a balance between high accuracy and\ncomputational efficiency. Its minimal overhead makes it suitable for both\nhistorical analysis and real-time applications, providing significant\npredictive power when combined with dense datasets. These findings offer\npractical guidance for researchers and practitioners implementing trajectory\nreconstruction systems and emphasize the importance of investing in\nhigher-frequency AVL data collection for improved analysis.", "AI": {"tldr": "本研究评估了13种轨迹重建方法，发现速度感知方法优于仅基于位置的方法，其中VCHIP-ME方法在精度和计算效率方面表现最佳。", "motivation": "自动车辆定位(AVL)数据由于更新频率不一致，需要进行轨迹重建以提高数据有效性，但现有方法在复杂城市环境中的性能表现需要系统评估。", "method": "使用德克萨斯州奥斯汀的高分辨率AVL数据，评估13种轨迹重建方法（包括新方法），分析速度、位置、平滑和数据密度四个关键因素的影响，结合传统数学误差指标和物理现实性考量。", "result": "速度感知方法始终优于仅位置方法；平滑方法在复杂拥堵城市环境中可能降低性能；VCHIP-ME方法在精度和计算效率方面达到最优平衡。", "conclusion": "研究为轨迹重建系统的实施提供实用指导，强调投资更高频率AVL数据收集的重要性，VCHIP-ME方法适合历史分析和实时应用。"}}
{"id": "2509.00530", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00530", "abs": "https://arxiv.org/abs/2509.00530", "authors": ["Fanxin Wang", "Yikun Cheng", "Chuyuan Tao", "Rohit Bhargava", "Thenkurussi Kesavadas"], "title": "Needle Biopsy And Fiber-Optic Compatible Robotic Insertion Platform", "comment": "Presented in EMBC 2025", "summary": "Tissue biopsy is the gold standard for diagnosing many diseases, involving\nthe extraction of diseased tissue for histopathology analysis by expert\npathologists. However, this procedure has two main limitations: 1) Manual\nsampling through tissue biopsy is prone to inaccuracies; 2) The extraction\nprocess is followed by a time-consuming pathology test. To address these\nlimitations, we present a compact, accurate, and maneuverable robotic insertion\nplatform to overcome the limitations in traditional histopathology. Our\nplatform is capable of steering a variety of tools with different sizes,\nincluding needle for tissue extraction and optical fibers for vibrational\nspectroscopy applications. This system facilitates the guidance of end-effector\nto the tissue and assists surgeons in navigating to the biopsy target area for\nmulti-modal diagnosis. In this paper, we outline the general concept of our\ndevice, followed by a detailed description of its mechanical design and control\nscheme. We conclude with the validation of the system through a series of\ntests, including positioning accuracy, admittance performance, and tool\ninsertion efficacy.", "AI": {"tldr": "这篇论文提出了一种经济、准确且机动性好的机器人插入平台，用于改善传统组织检查的限制，包括精确指导重复性好的细针插入和多模态诊断。", "motivation": "传统组织检查存在两个主要问题：1）手工采样容易出错；2）病理检查耗时较长。需要一种更准确、高效的方法来改善诊断过程。", "method": "设计了一种经济、准确且机动性好的机器人插入平台，能够控制各种不同尺寸的工具，包括组织提取针和光学纤维。系统包含机械设计和控制方案，支持多模态诊断。", "result": "通过一系列测试验证了系统的性能，包括定位精度、导纳性能和工具插入效果，证明了该平台的可靠性和有效性。", "conclusion": "该机器人插入平台成功充分解决了传统组织检查的两大限制，提供了一种更准确、高效的诊断方案，为多模态医学诊断开启了新可能。"}}
{"id": "2509.00178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00178", "abs": "https://arxiv.org/abs/2509.00178", "authors": ["Marina Y. Aoyama", "Joao Moura", "Juan Del Aguila Ferrandis", "Sethu Vijayakumar"], "title": "Poke and Strike: Learning Task-Informed Exploration Policies", "comment": "8 pages (main paper), 27 pages (including references and appendices),\n  6 figures (main paper), 21 figures (including appendices), Conference of\n  Robot Learning 2025, For videos and the project website, see\n  https://marina-aoyama.github.io/poke-and-strike/", "summary": "In many dynamic robotic tasks, such as striking pucks into a goal outside the\nreachable workspace, the robot must first identify the relevant physical\nproperties of the object for successful task execution, as it is unable to\nrecover from failure or retry without human intervention. To address this\nchallenge, we propose a task-informed exploration approach, based on\nreinforcement learning, that trains an exploration policy using rewards\nautomatically generated from the sensitivity of a privileged task policy to\nerrors in estimated properties. We also introduce an uncertainty-based\nmechanism to determine when to transition from exploration to task execution,\nensuring sufficient property estimation accuracy with minimal exploration time.\nOur method achieves a 90% success rate on the striking task with an average\nexploration time under 1.2 seconds, significantly outperforming baselines that\nachieve at most 40% success or require inefficient querying and retraining in a\nsimulator at test time. Additionally, we demonstrate that our task-informed\nrewards capture the relative importance of physical properties in both the\nstriking task and the classical CartPole example. Finally, we validate our\napproach by demonstrating its ability to identify object properties and adjust\ntask execution in a physical setup using the KUKA iiwa robot arm.", "AI": {"tldr": "提出基于强化学习的任务信息探索方法，通过特权任务策略对属性估计误差的敏感性自动生成奖励，训练探索策略来识别物体物理属性，在击球任务中达到90%成功率，平均探索时间低于1.2秒。", "motivation": "在动态机器人任务中，机器人需要识别物体的相关物理属性才能成功执行任务，但无法从失败中恢复或重试，需要人类干预。", "method": "基于强化学习的任务信息探索方法，使用特权任务策略对属性估计误差的敏感性自动生成奖励来训练探索策略，并引入基于不确定性的机制来确定何时从探索过渡到任务执行。", "result": "在击球任务中达到90%成功率，平均探索时间低于1.2秒，显著优于基线方法（最多40%成功率），并在KUKA iiwa机械臂上验证了识别物体属性和调整任务执行的能力。", "conclusion": "该方法能够有效捕获物理属性的相对重要性，实现高效的属性估计和任务执行，在真实物理设置中验证了其有效性。"}}
{"id": "2509.00571", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00571", "abs": "https://arxiv.org/abs/2509.00571", "authors": ["Arman Javan Sekhavat Pishkhani"], "title": "Gray-Box Computed Torque Control for Differential-Drive Mobile Robot Tracking", "comment": null, "summary": "This study presents a learning-based nonlinear algorithm for tracking control\nof differential-drive mobile robots. The Computed Torque Method (CTM) suffers\nfrom inaccurate knowledge of system parameters, while Deep Reinforcement\nLearning (DRL) algorithms are known for sample inefficiency and weak stability\nguarantees. The proposed method replaces the black-box policy network of a DRL\nagent with a gray-box Computed Torque Controller (CTC) to improve sample\nefficiency and ensure closed-loop stability. This approach enables finding an\noptimal set of controller parameters for an arbitrary reward function using\nonly a few short learning episodes. The Twin-Delayed Deep Deterministic Policy\nGradient (TD3) algorithm is used for this purpose. Additionally, some\ncontroller parameters are constrained to lie within known value ranges,\nensuring the RL agent learns physically plausible values. A technique is also\napplied to enforce a critically damped closed-loop time response. The\ncontroller's performance is evaluated on a differential-drive mobile robot\nsimulated in the MuJoCo physics engine and compared against the raw CTC and a\nconventional kinematic controller.", "AI": {"tldr": "基于深度强化学习的计算转矩控制算法，提高差动轮移动机器人踪踪控制的样本效率和稳定性", "motivation": "解决计算转矩方法对系统参数准确性的依赖，以及深度强化学习算法样本效率低和稳定性保证弱的问题", "method": "将DRL策略网络替换为灰盒计算转矩控制器，使用TD3算法寻找最优控制器参数，并加入物理可行性约束和临界阻尼间响应要求", "result": "在MuJoCo模拟环境中评估控制器性能，与原始计算转矩控制器和传统运动学控制器进行对比", "conclusion": "提出的灰盒控制器结构在保持闭环系统稳定性的同时，显著提高了学习效率，只需少量短时间学习就能获得优称控制效果"}}
{"id": "2509.00215", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00215", "abs": "https://arxiv.org/abs/2509.00215", "authors": ["Joseph Amigo", "Rooholla Khorrambakht", "Elliot Chane-Sane", "Nicolas Mansard", "Ludovic Righetti"], "title": "First Order Model-Based RL through Decoupled Backpropagation", "comment": "CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/", "summary": "There is growing interest in reinforcement learning (RL) methods that\nleverage the simulator's derivatives to improve learning efficiency. While\nearly gradient-based approaches have demonstrated superior performance compared\nto derivative-free methods, accessing simulator gradients is often impractical\ndue to their implementation cost or unavailability. Model-based RL (MBRL) can\napproximate these gradients via learned dynamics models, but the solver\nefficiency suffers from compounding prediction errors during training rollouts,\nwhich can degrade policy performance. We propose an approach that decouples\ntrajectory generation from gradient computation: trajectories are unrolled\nusing a simulator, while gradients are computed via backpropagation through a\nlearned differentiable model of the simulator. This hybrid design enables\nefficient and consistent first-order policy optimization, even when simulator\ngradients are unavailable, as well as learning a critic from simulation\nrollouts, which is more accurate. Our method achieves the sample efficiency and\nspeed of specialized optimizers such as SHAC, while maintaining the generality\nof standard approaches like PPO and avoiding ill behaviors observed in other\nfirst-order MBRL methods. We empirically validate our algorithm on benchmark\ncontrol tasks and demonstrate its effectiveness on a real Go2 quadruped robot,\nacross both quadrupedal and bipedal locomotion tasks.", "AI": {"tldr": "提出了一种结合模拟器轨迹生成和学习模型梯度计算的混合方法，实现高效的一阶策略优化，无需模拟器梯度，在样本效率和泛化性方面表现优异", "motivation": "传统基于梯度的RL方法需要模拟器导数，但实际中往往难以获取；基于模型的RL方法存在预测误差累积问题，影响策略性能", "method": "将轨迹生成与梯度计算解耦：使用模拟器展开轨迹，同时通过学习的可微分模型进行反向传播计算梯度", "result": "在基准控制任务和真实Go2四足机器人上验证有效，实现了SHAC等专用优化器的样本效率和速度，同时保持了PPO等标准方法的泛化性", "conclusion": "该方法解决了模拟器梯度不可用时的策略优化问题，避免了其他一阶MBRL方法的不良行为，在仿真和真实机器人任务中都表现出色"}}
{"id": "2509.00582", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00582", "abs": "https://arxiv.org/abs/2509.00582", "authors": ["Rui Bai", "Rui Xu", "Teng Rui", "Jiale Liu", "Qi Wei Oung", "Hoi Leong Lee", "Zhen Tian", "Fujiang Yuan"], "title": "Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved Double Quintic Polynomial Approach with Time-to-Collision Evaluation", "comment": null, "summary": "Autonomous driving technology has made significant advancements in recent\nyears, yet challenges remain in ensuring safe and comfortable interactions with\nhuman-driven vehicles (HDVs), particularly during lane-changing maneuvers. This\npaper proposes an improved double quintic polynomial approach for safe and\nefficient lane-changing in mixed traffic environments. The proposed method\nintegrates a time-to-collision (TTC) based evaluation mechanism directly into\nthe trajectory optimization process, ensuring that the ego vehicle proactively\nmaintains a safe gap from surrounding HDVs throughout the maneuver. The\nframework comprises state estimation for both the autonomous vehicle (AV) and\nHDVs, trajectory generation using double quintic polynomials, real-time TTC\ncomputation, and adaptive trajectory evaluation. To the best of our knowledge,\nthis is the first work to embed an analytic TTC penalty directly into the\nclosed-form double-quintic polynomial solver, enabling real-time safety-aware\ntrajectory generation without post-hoc validation. Extensive simulations\nconducted under diverse traffic scenarios demonstrate the safety, efficiency,\nand comfort of the proposed approach compared to conventional methods such as\nquintic polynomials, Bezier curves, and B-splines. The results highlight that\nthe improved method not only avoids collisions but also ensures smooth\ntransitions and adaptive decision-making in dynamic environments. This work\nbridges the gap between model-based and adaptive trajectory planning\napproaches, offering a stable solution for real-world autonomous driving\napplications.", "AI": {"tldr": "提出了一种改进的双五次多项式方法，通过在轨迹优化过程中直接集成基于碰撞时间(TTC)的评估机制，实现混合交通环境中安全高效的车道变换。", "motivation": "自动驾驶技术虽然取得了显著进展，但在与人类驾驶车辆(HDVs)的安全舒适交互方面仍存在挑战，特别是在车道变换操作中需要确保安全。", "method": "采用双五次多项式进行轨迹生成，将解析TTC惩罚直接嵌入闭式双五次多项式求解器，包含状态估计、实时TTC计算和自适应轨迹评估。", "result": "在多种交通场景下的广泛仿真表明，相比传统的五次多项式、贝塞尔曲线和B样条方法，该方法在安全性、效率和舒适性方面表现更优，能够避免碰撞并确保平滑过渡。", "conclusion": "这项工作填补了基于模型和自适应轨迹规划方法之间的空白，为现实世界自动驾驶应用提供了稳定的解决方案，实现了实时安全感知的轨迹生成。"}}
{"id": "2509.00218", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00218", "abs": "https://arxiv.org/abs/2509.00218", "authors": ["Aleksandra Landowska", "Aislinn D Gomez Bergin", "Ayodeji O. Abioye", "Jayati Deshmukh", "Andriana Bouadouki", "Maria Wheadon", "Athina Georgara", "Dominic Price", "Tuyen Nguyen", "Shuang Ao", "Lokesh Singh", "Yi Long", "Raffaele Miele", "Joel E. Fischer", "Sarvapali D. Ramchurn"], "title": "Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting - UKAIRS 2025 (Copy)", "comment": null, "summary": "This paper introduces and overviews a multidisciplinary project aimed at\ndeveloping responsible and adaptive multi-human multi-robot (MHMR) systems for\ncomplex, dynamic settings. The project integrates co-design, ethical\nframeworks, and multimodal sensing to create AI-driven robots that are\nemotionally responsive, context-aware, and aligned with the needs of diverse\nusers. We outline the project's vision, methodology, and early outcomes,\ndemonstrating how embodied AI can support sustainable, ethical, and\nhuman-centred futures.", "AI": {"tldr": "开发负责任、自适应的多人类多机器人系统，整合协同设计、伦理框架和多模态感知，创建情感响应、情境感知的AI机器人", "motivation": "为复杂动态环境开发能够响应人类情感、符合伦理标准、满足多样化用户需求的多人类多机器人系统", "method": "采用多学科方法，整合协同设计、伦理框架和多模态感知技术，开发情感响应和情境感知的AI驱动机器人", "result": "项目展示了早期成果，证明具身AI可以支持可持续、符合伦理和以人为本的未来发展", "conclusion": "该项目为开发负责任、自适应且以人为中心的MHMR系统提供了有前景的框架，展示了具身AI在促进可持续和伦理未来方面的潜力"}}
{"id": "2509.00624", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00624", "abs": "https://arxiv.org/abs/2509.00624", "authors": ["Haochong Chen", "Xincheng Cao", "Bilin Aksun-Guvenc", "Levent Guvenc"], "title": "Vehicle-in-Virtual-Environment (VVE) Method for Developing and Evaluating VRU Safety of Connected and Autonomous Driving with Focus on Bicyclist Safety", "comment": null, "summary": "Extensive research has already been conducted in the autonomous driving field\nto help vehicles navigate safely and efficiently. At the same time, plenty of\ncurrent research on vulnerable road user (VRU) safety is performed which\nlargely concentrates on perception, localization, or trajectory prediction of\nVRUs. However, existing research still exhibits several gaps, including the\nlack of a unified planning and collision avoidance system for autonomous\nvehicles, limited investigation into delay tolerant control strategies, and the\nabsence of an efficient and standardized testing methodology. Ensuring VRU\nsafety remains one of the most pressing challenges in autonomous driving,\nparticularly in dynamic and unpredictable environments. In this two year\nproject, we focused on applying the Vehicle in Virtual Environment (VVE) method\nto develop, evaluate, and demonstrate safety functions for Vulnerable Road\nUsers (VRUs) using automated steering and braking of ADS. In this current\nsecond year project report, our primary focus was on enhancing the previous\nyear results while also considering bicyclist safety.", "AI": {"tldr": "该研究应用VVE方法开发自动驾驶系统的安全功能，重点关注弱势道路使用者（特别是自行车骑行者）的安全，通过自动转向和制动技术来评估和演示安全性能。", "motivation": "现有自动驾驶研究在弱势道路使用者安全方面存在多个空白，包括缺乏统一的规划避撞系统、延迟容忍控制策略研究不足，以及缺少高效标准化测试方法。VRU安全是自动驾驶领域最紧迫的挑战之一。", "method": "采用车辆在虚拟环境（VVE）方法，开发、评估和演示自动驾驶系统（ADS）的安全功能，重点使用自动转向和制动技术来保护弱势道路使用者。", "result": "项目第二年主要专注于增强第一年成果，并特别考虑了自行车骑行者的安全需求，但具体结果数据未在摘要中提供。", "conclusion": "VVE方法为开发自动驾驶系统的VRU安全功能提供了有效的测试和评估框架，特别是在动态不可预测环境中保护自行车骑行者等弱势道路使用者方面具有重要意义。"}}
{"id": "2509.00271", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00271", "abs": "https://arxiv.org/abs/2509.00271", "authors": ["Yishu Li", "Xinyi Mao", "Ying Yuan", "Kyutae Sim", "Ben Eisner", "David Held"], "title": "Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online", "comment": "CoRL 2025", "summary": "We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain\nscenarios online by leveraging past interactions. Robots frequently encounter\nvisually ambiguous objects whose manipulation outcomes remain uncertain until\nphysically interacted with. While generative models alone could theoretically\nadapt to such ambiguity, in practice they obtain suboptimal performance in\nambiguous cases, even when conditioned on action history. To address this, we\npropose explicitly decoupling action generation from verification: we use an\nunconditional diffusion-based generator to propose multiple candidate actions\nand employ our history-aware verifier to select the most promising action by\nreasoning about past interactions. Through theoretical analysis, we demonstrate\nthat employing a verifier significantly improves expected action quality.\nEmpirical evaluations and analysis across multiple simulated and real-world\nenvironments including articulated objects, multi-modal doors, and uneven\nobject pick-up confirm the effectiveness of our method and improvements over\nbaselines. Our project website is available at:\nhttps://liy1shu.github.io/HAVE_CoRL25/", "AI": {"tldr": "提出HAVE方法，通过历史感知验证器解决机器人操作中的视觉模糊问题，将动作生成与验证分离，在模拟和真实环境中验证了有效性", "motivation": "机器人在操作视觉模糊物体时，生成模型在模糊情况下表现不佳，即使考虑动作历史也难以获得最优性能", "method": "使用无条件扩散生成器提出多个候选动作，通过历史感知验证器基于过去交互选择最有前景的动作", "result": "理论分析表明验证器显著提升动作质量预期，在多个模拟和真实环境（铰接物体、多模态门、不平整物体抓取）中验证了方法有效性", "conclusion": "HAVE方法通过显式分离动作生成和验证，有效解决了机器人操作中的视觉模糊问题，优于基线方法"}}
{"id": "2509.00643", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00643", "abs": "https://arxiv.org/abs/2509.00643", "authors": ["Zhen Tian", "Zhihao Lin", "Dezong Zhao", "Christos Anagnostopoulos", "Qiyuan Wang", "Wenjing Zhao", "Xiaodan Wang", "Chongfeng Wei"], "title": "A Risk-aware Spatial-temporal Trajectory Planning Framework for Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields", "comment": null, "summary": "Trajectory planning is a critical component in ensuring the safety,\nstability, and efficiency of autonomous vehicles. While existing trajectory\nplanning methods have achieved progress, they often suffer from high\ncomputational costs, unstable performance in dynamic environments, and limited\nvalidation across diverse scenarios. To overcome these challenges, we propose\nan enhanced QP-MPC-based framework that incorporates three key innovations: (i)\na novel cost function designed with a dynamic hazard field, which explicitly\nbalances safety, efficiency, and comfort; (ii) seamless integration of this\ncost function into the QP-MPC formulation, enabling direct optimization of\ndesired driving behaviors; and (iii) extensive validation of the proposed\nframework across complex tasks. The spatial safe planning is guided by a\ndynamic hazard field (DHF) for risk assessment, while temporal safe planning is\nbased on a space-time graph. Besides, the quintic polynomial sampling and\nsub-reward of comforts are used to ensure comforts during lane-changing. The\nsub-reward of efficiency is used to maintain driving efficiency. Finally, the\nproposed DHF-enhanced objective function integrates multiple objectives,\nproviding a proper optimization tasks for QP-MPC. Extensive simulations\ndemonstrate that the proposed framework outperforms benchmark optimization\nmethods in terms of efficiency, stability, and comfort across a variety of\nscenarios likes lane-changing, overtaking, and crossing intersections.", "AI": {"tldr": "提出了一种基于QP-MPC的增强型轨迹规划框架，通过动态危险场(DHF)成本函数、时空安全规划和多目标优化，在自动驾驶车辆的各种复杂场景中实现了更好的效率、稳定性和舒适性。", "motivation": "现有轨迹规划方法存在计算成本高、动态环境下性能不稳定、多样化场景验证有限等问题，需要一种能够同时平衡安全性、效率和舒适性的解决方案。", "method": "采用QP-MPC框架，创新性地引入动态危险场(DHF)进行风险评估，结合时空图进行时间安全规划，使用五次多项式采样和舒适度子奖励确保换道舒适性，通过效率子奖励维持驾驶效率，最终将多目标整合到DHF增强的目标函数中。", "result": "大量仿真实验表明，该框架在换道、超车、交叉口通行等多种场景下，在效率、稳定性和舒适性方面均优于基准优化方法。", "conclusion": "所提出的DHF增强QP-MPC框架为自动驾驶车辆提供了有效的轨迹规划解决方案，能够同时优化安全性、效率和舒适性，在复杂动态环境中表现出色。"}}
{"id": "2509.00310", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00310", "abs": "https://arxiv.org/abs/2509.00310", "authors": ["Yuxuan Ding", "Shuangge Wang", "Tesca Fitzgerald"], "title": "TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization", "comment": null, "summary": "Robots often struggle to generalize from a single demonstration due to the\nlack of a transferable and interpretable spatial representation. In this work,\nwe introduce TReF-6, a method that infers a simplified, abstracted 6DoF\nTask-Relevant Frame from a single trajectory. Our approach identifies an\ninfluence point purely from the trajectory geometry to define the origin for a\nlocal frame, which serves as a reference for parameterizing a Dynamic Movement\nPrimitive (DMP). This influence point captures the task's spatial structure,\nextending the standard DMP formulation beyond start-goal imitation. The\ninferred frame is semantically grounded via a vision-language model and\nlocalized in novel scenes by Grounded-SAM, enabling functionally consistent\nskill generalization. We validate TReF-6 in simulation and demonstrate\nrobustness to trajectory noise. We further deploy an end-to-end pipeline on\nreal-world manipulation tasks, showing that TReF-6 supports one-shot imitation\nlearning that preserves task intent across diverse object configurations.", "AI": {"tldr": "TReF-6是一种从单次演示中推断6自由度任务相关框架的方法，通过轨迹几何识别影响点来定义局部坐标系，结合视觉语言模型实现语义理解和跨场景泛化。", "motivation": "机器人通常难以从单次演示中泛化，因为缺乏可迁移和可解释的空间表示。需要一种能够从轨迹中提取抽象空间结构的方法来实现一次性模仿学习。", "method": "从轨迹几何中识别影响点来定义局部坐标系的原点，作为动态运动基元(DMP)的参数化参考。通过视觉语言模型进行语义接地，使用Grounded-SAM在新场景中定位框架。", "result": "在仿真中验证了TReF-6的有效性，展示了对轨迹噪声的鲁棒性。在真实世界操作任务中部署端到端管道，支持一次性模仿学习并在不同物体配置下保持任务意图。", "conclusion": "TReF-6提供了一种可解释的空间表示方法，能够从单次演示中提取任务相关框架，实现功能一致的技能泛化，超越了传统的起点-终点模仿方法。"}}
{"id": "2509.01980", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.01980", "abs": "https://arxiv.org/abs/2509.01980", "authors": ["Luca Di Pierno", "Robert Hewitt", "Stephan Weiss", "Roland Brockers"], "title": "Hybrid Autonomy Framework for a Future Mars Science Helicopter", "comment": "8 pages, IEEE CASE 2025 Conference", "summary": "Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary\nsurface exploration beyond the reach of ground-based robots. Thus, NASA is\nstudying a Mars Science Helicopter (MSH), an advanced concept capable of\nperforming long-range science missions and autonomously navigating challenging\nMartian terrain. Given significant Earth-Mars communication delays and mission\ncomplexity, an advanced autonomy framework is required to ensure safe and\nefficient operation by continuously adapting behavior based on mission\nobjectives and real-time conditions, without human intervention. This study\npresents a deterministic high-level control framework for aerial exploration,\nintegrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a\nscalable, robust, and computationally efficient autonomy solution for critical\nscenarios like deep space exploration. In this paper we outline key\ncapabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework\nwhich orchestrates them to achieve the desired objectives. Monte Carlo\nsimulations and real field tests validate the framework, demonstrating its\nrobustness and adaptability to both discrete events and real-time system\nfeedback. These inputs trigger state transitions or dynamically adjust behavior\nexecution, enabling reactive and context-aware responses. The framework is\nmiddleware-agnostic, supporting integration with systems like F-Prime and\nextending beyond aerial robotics.", "AI": {"tldr": "NASA开发火星科学直升机自主控制框架，结合有限状态机和行为树，实现可扩展、鲁棒且计算高效的深空探索自主解决方案", "motivation": "解决地球-火星通信延迟问题，确保火星科学直升机在复杂任务中无需人工干预的安全高效运行", "method": "采用有限状态机(FSM)与行为树(BTs)混合的确定性高层控制框架，支持中间件无关集成", "result": "通过蒙特卡洛模拟和实地测试验证，框架对离散事件和实时系统反馈表现出鲁棒性和适应性", "conclusion": "该自主框架成功实现反应式和上下文感知响应，可扩展到航空机器人以外的应用领域"}}
{"id": "2509.00317", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00317", "abs": "https://arxiv.org/abs/2509.00317", "authors": ["Fulvio Mastrogiovanni", "Antony Thomas"], "title": "A Framework for Task and Motion Planning based on Expanding AND/OR Graphs", "comment": "Accepted for an oral presentation at ASTRA Conference, 2025", "summary": "Robot autonomy in space environments presents unique challenges, including\nhigh perception and motion uncertainty, strict kinematic constraints, and\nlimited opportunities for human intervention. Therefore, Task and Motion\nPlanning (TMP) may be critical for autonomous servicing, surface operations, or\neven in-orbit missions, just to name a few, as it models tasks as discrete\naction sequencing integrated with continuous motion feasibility assessments. In\nthis paper, we introduce a TMP framework based on expanding AND/OR graphs,\nreferred to as TMP-EAOG, and demonstrate its adaptability to different\nscenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,\nwhich expands iteratively as the plan is executed, and performs in-the-loop\nmotion planning assessments to ascertain their feasibility. As a consequence,\nTMP-EAOG is characterised by the desirable properties of (i) robustness to a\ncertain degree of uncertainty, because AND/OR graph expansion can accommodate\nfor unpredictable information about the robot environment, (ii) controlled\nautonomy, since an AND/OR graph can be validated by human experts, and (iii)\nbounded flexibility, in that unexpected events, including the assessment of\nunfeasible motions, can lead to different courses of action as alternative\npaths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We\nuse a simulated mobile manipulator as a proxy for space-grade autonomous\nrobots. Our evaluation shows that TMP-EAOG can deal with a wide range of\nchallenges in the benchmarks.", "AI": {"tldr": "提出了基于扩展AND/OR图的TMP-EAOG框架，用于空间机器人任务与运动规划，具有鲁棒性、可控自主性和有限灵活性。", "motivation": "空间环境中的机器人自主性面临感知和运动不确定性、严格运动学约束以及人类干预机会有限等独特挑战，需要能够处理这些问题的任务与运动规划方法。", "method": "TMP-EAOG框架在AND/OR图中编码任务级抽象，通过迭代扩展图结构并在循环中执行运动规划可行性评估。", "result": "在模拟移动机械臂上的评估表明，TMP-EAOG能够处理基准测试中的各种挑战，展现出良好的适应性。", "conclusion": "TMP-EAOG框架为空间自主机器人提供了一种有效的任务与运动规划解决方案，具有应对不确定性和意外事件的能力。"}}
{"id": "2509.02204", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.02204", "abs": "https://arxiv.org/abs/2509.02204", "authors": ["Dario Ruggiero", "Mauro Mancini", "Elisa Capello"], "title": "Adaptive Navigation Strategy for Low-Thrust Proximity Operations in Circular Relative Orbit", "comment": "This work has been accepted and presented at the 35th AAS/AIAA Space\n  Flight Mechanics Meeting, 2025, Kaua'i, Hawai", "summary": "This paper presents an adaptive observer-based navigation strategy for\nspacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in\nproximity operations like formation flight and uncooperative target inspection.\nThe proposed method adjusts observer gains based on the estimated state to\nachieve fast convergence and low noise sensitivity in state estimation. A\nLyapunov-based analysis ensures stability and accuracy, while simulations using\nvision-based sensor data validate the approach under realistic conditions.\nCompared to classical observers with time-invariant gains, the proposed method\nenhances trajectory tracking precision and reduces control input switching,\nmaking it a promising solution for autonomous spacecraft localization and\ncontrol.", "AI": {"tldr": "提出了一种基于自适应观测器的航天器导航策略，用于圆形相对轨道场景，通过动态调整观测器增益实现快速收敛和低噪声敏感性。", "motivation": "解决航天器近距离操作（如编队飞行和非合作目标检测）中的状态估计挑战，传统固定增益观测器在收敛速度和噪声敏感性方面存在局限。", "method": "采用自适应观测器方法，根据估计状态动态调整观测器增益，结合Lyapunov稳定性分析确保系统稳定性和估计精度。", "result": "仿真验证表明，相比传统固定增益观测器，该方法提高了轨迹跟踪精度，减少了控制输入切换，在视觉传感器数据下表现良好。", "conclusion": "该方法为自主航天器定位和控制提供了一种有前景的解决方案，特别适用于圆形相对轨道场景的导航需求。"}}
{"id": "2509.00328", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00328", "abs": "https://arxiv.org/abs/2509.00328", "authors": ["Bear Häon", "Kaylene Stocking", "Ian Chuang", "Claire Tomlin"], "title": "Mechanistic interpretability for steering vision-language-action models", "comment": "CoRL 2025. Project website: https://vla-mech-interp.github.io/", "summary": "Vision-Language-Action (VLA) models are a promising path to realizing\ngeneralist embodied agents that can quickly adapt to new tasks, modalities, and\nenvironments. However, methods for interpreting and steering VLAs fall far\nshort of classical robotics pipelines, which are grounded in explicit models of\nkinematics, dynamics, and control. This lack of mechanistic insight is a\ncentral challenge for deploying learned policies in real-world robotics, where\nrobustness and explainability are critical. Motivated by advances in\nmechanistic interpretability for large language models, we introduce the first\nframework for interpreting and steering VLAs via their internal\nrepresentations, enabling direct intervention in model behavior at inference\ntime. We project feedforward activations within transformer layers onto the\ntoken embedding basis, identifying sparse semantic directions - such as speed\nand direction - that are causally linked to action selection. Leveraging these\nfindings, we introduce a general-purpose activation steering method that\nmodulates behavior in real time, without fine-tuning, reward signals, or\nenvironment interaction. We evaluate this method on two recent open-source\nVLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in\nsimulation (LIBERO) and on a physical robot (UR5). This work demonstrates that\ninterpretable components of embodied VLAs can be systematically harnessed for\ncontrol - establishing a new paradigm for transparent and steerable foundation\nmodels in robotics.", "AI": {"tldr": "视觉-语言-动作模型的机制解释性研究，通过激活值导向识别和调节模型行为，实现无需微调或环境交互的实时控制", "motivation": "解决VLA模型在机器人实际部署中的稳健性和可解释性挑战，充分利用大语言模型机制解释性的进展", "method": "将transformer层内部激活值投影到令片嵌入基础，识别与动作选择因果相关的稀疏语义方向，并提出通用的激活值导向方法", "result": "在Pi0和OpenVLA两个开源VLA上验证，在模拟环境(LIBERO)和物理机器人(UR5)上实现了零样本行为控制", "conclusion": "体化VLA模型的可解释组件可以系统性地用于控制，为机器人领域的透明可控基础模型建立了新范式"}}
{"id": "2509.00339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00339", "abs": "https://arxiv.org/abs/2509.00339", "authors": ["Md. Taherul Islam Shawon", "Yuan Li", "Yincai Cai", "Junjie Niu", "Ting Peng"], "title": "Autonomous Aggregate Sorting in Construction and Mining via Computer Vision-Aided Robotic Arm Systems", "comment": null, "summary": "Traditional aggregate sorting methods, whether manual or mechanical, often\nsuffer from low precision, limited flexibility, and poor adaptability to\ndiverse material properties such as size, shape, and lithology. To address\nthese limitations, this study presents a computer vision-aided robotic arm\nsystem designed for autonomous aggregate sorting in construction and mining\napplications. The system integrates a six-degree-of-freedom robotic arm, a\nbinocular stereo camera for 3D perception, and a ROS-based control framework.\nCore techniques include an attention-augmented YOLOv8 model for aggregate\ndetection, stereo matching for 3D localization, Denavit-Hartenberg kinematic\nmodeling for arm motion control, minimum enclosing rectangle analysis for size\nestimation, and hand-eye calibration for precise coordinate alignment.\nExperimental validation with four aggregate types achieved an average grasping\nand sorting success rate of 97.5%, with comparable classification accuracy.\nRemaining challenges include the reliable handling of small aggregates and\ntexture-based misclassification. Overall, the proposed system demonstrates\nsignificant potential to enhance productivity, reduce operational costs, and\nimprove safety in aggregate handling, while providing a scalable framework for\nadvancing smart automation in construction, mining, and recycling industries.", "AI": {"tldr": "基于计算机视觉的机器人系统实现骨料自动分拣，平均成功率97.5%，解决了传统分拣方法精度低、灵活性差的问题", "motivation": "传统骨料分拣方法（人工或机械）存在精度低、灵活性有限、对不同材料特性适应性差的问题，需要更智能的自动化解决方案", "method": "集成六自由度机械臂、双目立体相机和ROS控制框架，采用注意力增强YOLOv8模型进行检测、立体匹配进行3D定位、DH运动学建模、最小外接矩形分析尺寸估计和手眼标定", "result": "在四种骨料类型上实现了平均97.5%的抓取分拣成功率，分类准确率相当", "conclusion": "该系统展示了提升生产效率、降低运营成本和改善安全性的巨大潜力，为建筑、采矿和回收行业的智能自动化提供了可扩展框架"}}
{"id": "2509.00361", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00361", "abs": "https://arxiv.org/abs/2509.00361", "authors": ["Chuye Zhang", "Xiaoxiong Zhang", "Wei Pan", "Linfang Zheng", "Wei Zhang"], "title": "Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-Top Manipulation", "comment": "9th Conference on Robot Learning (CoRL 2025), Seoul, Korea", "summary": "Robotic manipulation in unstructured environments requires systems that can\ngeneralize across diverse tasks while maintaining robust and reliable\nperformance. We introduce {GVF-TAPE}, a closed-loop framework that combines\ngenerative visual foresight with task-agnostic pose estimation to enable\nscalable robotic manipulation. GVF-TAPE employs a generative video model to\npredict future RGB-D frames from a single side-view RGB image and a task\ndescription, offering visual plans that guide robot actions. A decoupled pose\nestimation model then extracts end-effector poses from the predicted frames,\ntranslating them into executable commands via low-level controllers. By\niteratively integrating video foresight and pose estimation in a closed loop,\nGVF-TAPE achieves real-time, adaptive manipulation across a broad range of\ntasks. Extensive experiments in both simulation and real-world settings\ndemonstrate that our approach reduces reliance on task-specific action data and\ngeneralizes effectively, providing a practical and scalable solution for\nintelligent robotic systems.", "AI": {"tldr": "GVF-TAPE是一个结合生成式视觉预测和任务无关位姿估计的闭环框架，用于实现可扩展的机器人操作，减少对任务特定数据的依赖。", "motivation": "非结构化环境中的机器人操作需要能够跨多样化任务泛化同时保持鲁棒性的系统，现有方法往往依赖任务特定的动作数据，缺乏通用性。", "method": "使用生成式视频模型从单视角RGB图像和任务描述预测未来RGB-D帧，提供视觉规划；通过解耦的位姿估计模型从预测帧中提取末端执行器位姿，转换为可执行命令；通过闭环迭代整合视觉预测和位姿估计。", "result": "在仿真和真实环境的大量实验中，该方法实现了实时自适应操作，有效泛化到广泛任务范围，减少了对任务特定动作数据的依赖。", "conclusion": "GVF-TAPE提供了一个实用且可扩展的智能机器人系统解决方案，通过生成式视觉预测和任务无关位姿估计的闭环整合，实现了强大的泛化能力和实时性能。"}}
{"id": "2509.00465", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00465", "abs": "https://arxiv.org/abs/2509.00465", "authors": ["Jiading Fang"], "title": "Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning", "comment": null, "summary": "This thesis introduces \"Embodied Spatial Intelligence\" to address the\nchallenge of creating robots that can perceive and act in the real world based\non natural language instructions. To bridge the gap between Large Language\nModels (LLMs) and physical embodiment, we present contributions on two fronts:\nscene representation and spatial reasoning. For perception, we develop robust,\nscalable, and accurate scene representations using implicit neural models, with\ncontributions in self-supervised camera calibration, high-fidelity depth field\ngeneration, and large-scale reconstruction. For spatial reasoning, we enhance\nthe spatial capabilities of LLMs by introducing a novel navigation benchmark, a\nmethod for grounding language in 3D, and a state-feedback mechanism to improve\nlong-horizon decision-making. This work lays a foundation for robots that can\nrobustly perceive their surroundings and intelligently act upon complex,\nlanguage-based commands.", "AI": {"tldr": "本论文提出\"体验式空间智能\"概念，通过隐式神经场景表征和增强大语言模型空间推理能力，解决机器人根据自然语言指令在真实世界中感知和行动的挑战。", "motivation": "克服大语言模型(LLMs)与物理体验之间的差距，创造能够根据自然语言指令在真实世界中感知和行动的机器人。", "method": "在景观表征方面：使用隐式神经模型开发稳健、可扩展和准确的场景表征，包括自监督相机标定、高保真度深度场生成和大规模重建。在空间推理方面：通过引入新的导航标准、语言在3D环境中基础化方法和状态反馈机制来增强LLMs的空间能力。", "result": "为机器人能够稳健感知周围环境并智能地执行复杂语言命令奠定了基础。", "conclusion": "该研究成功构建了体验式空间智能的基础框架，有效缩小了语言模型与物理世界之间的差距，为未来智能机器人的发展提供了重要技术支撑。"}}
{"id": "2509.00491", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00491", "abs": "https://arxiv.org/abs/2509.00491", "authors": ["Masaki Saito", "Shunki Itadera", "Toshiyuki Murakami"], "title": "Extended Diffeomorphism for Real-Time Motion Replication in Workspaces with Different Spatial Arrangements", "comment": null, "summary": "This paper presents two types of extended diffeomorphism designs to\ncompensate for spatial placement differences between robot workspaces.\nTeleoperation of multiple robots is attracting attention to expand the\nutilization of the robot embodiment. Real-time reproduction of robot motion\nwould facilitate the efficient execution of similar tasks by multiple robots. A\nchallenge in the motion reproduction is compensating for the spatial\narrangement errors of target keypoints in robot workspaces. This paper proposes\na methodology for smooth mappings that transform primary robot poses into\nfollower robot poses based on the predefined key points in each workspace.\nThrough a picking task experiment using a dual-arm UR5 robot, this study\ndemonstrates that the proposed mapping generation method can balance lower\nmapping errors for precise operation and lower mapping gradients for smooth\nreplicated movement.", "AI": {"tldr": "提出两种扩展微分同胚设计来补偿机器人工作空间之间的空间位置差异，通过关键点映射实现多机器人遥操作中的平滑运动复制", "motivation": "多机器人遥操作需要实时复制机器人运动以提高相似任务的执行效率，但面临机器人工作空间空间布置误差补偿的挑战", "method": "基于预定义关键点的平滑映射方法，将主机器人姿态转换为从机器人姿态，使用扩展微分同胚设计", "result": "通过双UR5机械臂拾取任务实验证明，该方法能够在精确操作的较低映射误差和平滑复制的较低映射梯度之间取得平衡", "conclusion": "所提出的映射生成方法有效解决了多机器人运动复制中的空间布置补偿问题，实现了精确且平滑的运动传输"}}
{"id": "2509.00497", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00497", "abs": "https://arxiv.org/abs/2509.00497", "authors": ["Yiyang Chen", "Zhigang Wu", "Guohong Zheng", "Xuesong Wu", "Liwen Xu", "Haoyuan Tang", "Zhaocheng He", "Haipeng Zeng"], "title": "FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection Dataset of Dense Conflict Trajectories", "comment": "26 pages, 14 figures", "summary": "The trajectory data of traffic participants (TPs) is a fundamental resource\nfor evaluating traffic conditions and optimizing policies, especially at urban\nintersections. Although data acquisition using drones is efficient, existing\ndatasets still have limitations in scene representativeness, information\nrichness, and data fidelity. This study introduces FLUID, comprising a\nfine-grained trajectory dataset that captures dense conflicts at typical urban\nsignalized intersections, and a lightweight, full-pipeline framework for\ndrone-based trajectory processing. FLUID covers three distinct intersection\ntypes, with approximately 5 hours of recording time and featuring over 20,000\nTPs across 8 categories. Notably, the dataset averages two vehicle conflicts\nper minute, involving roughly 25% of all motor vehicles. FLUID provides\ncomprehensive data, including trajectories, traffic signals, maps, and raw\nvideos. Comparison with the DataFromSky platform and ground-truth measurements\nvalidates its high spatio-temporal accuracy. Through a detailed classification\nof motor vehicle conflicts and violations, FLUID reveals a diversity of\ninteractive behaviors, demonstrating its value for human preference mining,\ntraffic behavior modeling, and autonomous driving research.", "AI": {"tldr": "FLUID是一个基于无人机采集的精细交通轨迹数据集，包含三种典型城市信号交叉口的高密度冲突数据，提供超过2万个交通参与者的轨迹、信号灯、地图和原始视频数据，具有高时空精度。", "motivation": "现有无人机采集的交通数据集在场景代表性、信息丰富度和数据保真度方面存在局限，需要更全面的交叉口交通冲突数据来支持交通评估和政策优化。", "method": "使用无人机采集三个不同类型城市信号交叉口的交通数据，构建包含轨迹、交通信号、地图和原始视频的完整数据集，并通过与DataFromSky平台和地面真实测量对比验证准确性。", "result": "数据集包含约5小时记录时间，覆盖8类超过20,000个交通参与者，平均每分钟发生两次车辆冲突，涉及约25%的机动车辆，展示了多样化的交互行为。", "conclusion": "FLUID数据集为人类偏好挖掘、交通行为建模和自动驾驶研究提供了宝贵资源，展示了城市交叉口复杂交通互动的多样性。"}}
{"id": "2509.00499", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00499", "abs": "https://arxiv.org/abs/2509.00499", "authors": ["Dongwon Son", "Hojin Jung", "Beomjoon Kim"], "title": "NeuralSVCD for Efficient Swept Volume Collision Detection", "comment": "CoRL 2025", "summary": "Robot manipulation in unstructured environments requires efficient and\nreliable Swept Volume Collision Detection (SVCD) for safe motion planning.\nTraditional discrete methods potentially miss collisions between these points,\nwhereas SVCD continuously checks for collisions along the entire trajectory.\nExisting SVCD methods typically face a trade-off between efficiency and\naccuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a\nnovel neural encoder-decoder architecture tailored to overcome this trade-off.\nOur approach leverages shape locality and temporal locality through distributed\ngeometric representations and temporal optimization. This enhances\ncomputational efficiency without sacrificing accuracy. Comprehensive\nexperiments show that NeuralSVCD consistently outperforms existing\nstate-of-the-art SVCD methods in terms of both collision detection accuracy and\ncomputational efficiency, demonstrating its robust applicability across diverse\nrobotic manipulation scenarios. Code and videos are available at\nhttps://neuralsvcd.github.io/.", "AI": {"tldr": "NeuralSVCD是一种新颖的神经编码器-解码器架构，用于机器人操作中的扫描体积碰撞检测，通过分布式几何表示和时间优化来同时提高计算效率和准确性。", "motivation": "传统离散碰撞检测方法可能在采样点之间遗漏碰撞，而现有扫描体积碰撞检测方法在效率和准确性之间存在权衡，限制了实际应用。", "method": "提出神经编码器-解码器架构，利用形状局部性和时间局部性，通过分布式几何表示和时间优化来提升计算效率。", "result": "综合实验表明，NeuralSVCD在碰撞检测准确性和计算效率方面均优于现有最先进方法，适用于各种机器人操作场景。", "conclusion": "NeuralSVCD成功克服了效率与准确性之间的权衡，为无结构环境中的机器人操作提供了高效可靠的碰撞检测解决方案。"}}
{"id": "2509.00564", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00564", "abs": "https://arxiv.org/abs/2509.00564", "authors": ["Philip Lorimer", "Jack Saunders", "Alan Hunter", "Wenbin Li"], "title": "Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot", "comment": "Authors' accepted manuscript (IROS 2024, Abu Dhabi, Oct 14-18, 2024).\n  Please cite the version of record: DOI 10.1109/IROS58592.2024.10802717. 8\n  pages", "summary": "Free-roaming dollies enhance filmmaking with dynamic movement, but challenges\nin automated camera control remain unresolved. Our study advances this field by\napplying Reinforcement Learning (RL) to automate dolly-in shots using\nfree-roaming ground-based filming robots, overcoming traditional control\nhurdles. We demonstrate the effectiveness of combined control for precise film\ntasks by comparing it to independent control strategies. Our robust RL pipeline\nsurpasses traditional Proportional-Derivative controller performance in\nsimulation and proves its efficacy in real-world tests on a modified ROSBot 2.0\nplatform equipped with a camera turret. This validates our approach's\npracticality and sets the stage for further research in complex filming\nscenarios, contributing significantly to the fusion of technology with\ncinematic creativity. This work presents a leap forward in the field and opens\nnew avenues for research and development, effectively bridging the gap between\ntechnological advancement and creative filmmaking.", "AI": {"tldr": "使用强化学习自动化自由移动摄影机器人的推轨镜头拍摄，相比传统控制方法表现更优", "motivation": "解决自由移动摄影推车在自动化摄像机控制方面的挑战，推动技术与电影创作的融合", "method": "应用强化学习(RL)算法，在改进的ROSBot 2.0平台上结合摄像机云台进行联合控制", "result": "RL管道在仿真中超越传统比例微分控制器性能，并在真实世界测试中验证了有效性", "conclusion": "该方法为复杂拍摄场景研究奠定了基础，显著促进了技术进步与电影创意的结合"}}
{"id": "2509.00570", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00570", "abs": "https://arxiv.org/abs/2509.00570", "authors": ["Alessandro Leanza", "Angelo Moroncelli", "Giuseppe Vizzari", "Francesco Braghin", "Loris Roveda", "Blerina Spahiu"], "title": "ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph", "comment": null, "summary": "ConceptBot is a modular robotic planning framework that combines Large\nLanguage Models and Knowledge Graphs to generate feasible and risk-aware plans\ndespite ambiguities in natural language instructions and correctly analyzing\nthe objects present in the environment - challenges that typically arise from a\nlack of commonsense reasoning. To do that, ConceptBot integrates (i) an Object\nProperty Extraction (OPE) module that enriches scene understanding with\nsemantic concepts from ConceptNet, (ii) a User Request Processing (URP) module\nthat disambiguates and structures instructions, and (iii) a Planner that\ngenerates context-aware, feasible pick-and-place policies. In comparative\nevaluations against Google SayCan, ConceptBot achieved 100% success on explicit\ntasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),\nreached 76% on risk-aware tasks (versus 15%), and outperformed SayCan in\napplication-specific scenarios, including material classification (70% vs. 20%)\nand toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved an\noverall score of 80% (versus 46% for the next-best baseline). These results,\nvalidated in both simulation and laboratory experiments, demonstrate\nConceptBot's ability to generalize without domain-specific training and to\nsignificantly improve the reliability of robotic policies in unstructured\nenvironments. Website: https://sites.google.com/view/conceptbot", "AI": {"tldr": "ConceptBot是一个结合大语言模型和知识图谱的模块化机器人规划框架，能够处理自然语言指令的模糊性并生成可行且风险感知的规划策略", "motivation": "解决机器人规划中因缺乏常识推理而导致的自然语言指令模糊性和环境对象分析困难的问题", "method": "集成三个模块：(i)对象属性提取模块，用ConceptNet丰富场景理解；(ii)用户请求处理模块，消歧和结构化指令；(iii)规划器，生成上下文感知的拾放策略", "result": "在比较评估中，ConceptBot在显式任务上达到100%成功率，隐式任务87%准确率（SayCan为31%），风险感知任务76%（SayCan为15%），在材料分类和毒性检测等应用场景中均优于SayCan，在SafeAgentBench上总体得分80%", "conclusion": "ConceptBot能够无需领域特定训练即可泛化，并显著提高非结构化环境中机器人策略的可靠性"}}
{"id": "2509.00574", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00574", "abs": "https://arxiv.org/abs/2509.00574", "authors": ["Philip Lorimer", "Alan Hunter", "Wenbin Li"], "title": "Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot", "comment": "Preprint; under double-anonymous review. 6 pages", "summary": "Cinematic camera control demands a balance of precision and artistry -\nqualities that are difficult to encode through handcrafted reward functions.\nWhile reinforcement learning (RL) has been applied to robotic filmmaking, its\nreliance on bespoke rewards and extensive tuning limits creative usability. We\npropose a Learning from Demonstration (LfD) approach using Generative\nAdversarial Imitation Learning (GAIL) to automate dolly-in shots with a\nfree-roaming, ground-based filming robot. Expert trajectories are collected via\njoystick teleoperation in simulation, capturing smooth, expressive motion\nwithout explicit objective design.\n  Trained exclusively on these demonstrations, our GAIL policy outperforms a\nPPO baseline in simulation, achieving higher rewards, faster convergence, and\nlower variance. Crucially, it transfers directly to a real-world robot without\nfine-tuning, achieving more consistent framing and subject alignment than a\nprior TD3-based method. These results show that LfD offers a robust,\nreward-free alternative to RL in cinematic domains, enabling real-time\ndeployment with minimal technical effort. Our pipeline brings intuitive,\nstylized camera control within reach of creative professionals, bridging the\ngap between artistic intent and robotic autonomy.", "AI": {"tldr": "使用生成对抗模仿学习(GAIL)的从演示中学习方法，通过专家演示训练电影摄影机器人，无需手工设计奖励函数，实现了更好的拍摄效果和实时部署", "motivation": "电影摄影控制需要精确性和艺术性的平衡，传统强化学习方法依赖手工设计的奖励函数和大量调参，限制了创作可用性", "method": "采用从演示中学习(LfD)方法，使用生成对抗模仿学习(GAIL)。通过摇杆遥操作在模拟环境中收集专家轨迹，捕捉流畅、富有表现力的运动，无需显式目标设计", "result": "GAIL策略在模拟环境中优于PPO基线，获得更高奖励、更快收敛和更低方差。无需微调即可直接迁移到真实机器人，比之前的TD3方法获得更一致的构图和主体对齐", "conclusion": "LfD为电影领域提供了无需奖励函数的鲁棒替代方案，能够以最少的技术努力实现实时部署，使直观、风格化的相机控制触手可及，弥合艺术意图与机器人自主性之间的差距"}}
{"id": "2509.00576", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00576", "abs": "https://arxiv.org/abs/2509.00576", "authors": ["Tao Jiang", "Tianyuan Yuan", "Yicheng Liu", "Chenhao Lu", "Jianning Cui", "Xiao Liu", "Shuiqi Cheng", "Jiyang Gao", "Huazhe Xu", "Hang Zhao"], "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model", "comment": "https://opengalaxea.github.io/G0/", "summary": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of\nrobot behaviors recorded in authentic human living and working environments.\nAll demonstrations are gathered using a consistent robotic embodiment, paired\nwith precise subtask-level language annotations to facilitate both training and\nevaluation. Building on this dataset, we introduce G0, a dual-system framework\nthat couples a Vision-Language Model (VLM) for multimodal planning with a\nVision-Language-Action (VLA) model for fine-grained execution. G0 is trained\nusing a three-stage curriculum: cross-embodiment pre-training,\nsingle-embodiment pre-training, and task-specific post-training. A\ncomprehensive benchmark spanning tabletop manipulation, few-shot learning, and\nlong-horizon mobile manipulation, demonstrates the effectiveness of our\napproach. In particular, we find that the single-embodiment pre-training stage,\ntogether with the Galaxea Open-World Dataset, plays a critical role in\nachieving strong performance.", "AI": {"tldr": "提出了Galaxea开放世界数据集和G0双系统框架，通过三阶段课程学习在机器人任务中实现强性能", "motivation": "为了解决机器人在真实人类生活和工作环境中执行复杂任务的需求，需要大规模、多样化的数据集和有效的学习框架", "method": "构建Galaxea开放世界数据集，包含精确的子任务级语言标注；提出G0双系统框架，结合VLM进行多模态规划和VLA进行细粒度执行；采用三阶段课程学习：跨具身预训练、单具身预训练和任务特定后训练", "result": "在桌面操作、少样本学习和长时程移动操作等综合基准测试中表现出色，特别是单具身预训练阶段和Galaxea数据集对性能提升至关重要", "conclusion": "Galaxea数据集和G0框架为机器人在开放世界环境中的学习和执行提供了有效的解决方案，单具身预训练是关键成功因素"}}
{"id": "2509.00660", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.00660", "abs": "https://arxiv.org/abs/2509.00660", "authors": ["Felipe Arias-Russi", "Yuanchen Bai", "Angelique Taylor"], "title": "CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction", "comment": null, "summary": "The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz\n(WoZ) controlled robots to explore navigation, conversational dynamics,\nhuman-in-the-loop interactions, and more to explore appropriate robot behaviors\nin everyday settings. However, existing WoZ tools are often limited to one\ncontext, making them less adaptable across different settings, users, and\nrobotic platforms. To mitigate these issues, we introduce a Context-Adaptable\nRobot Interface System (CARIS) that combines advanced robotic capabilities such\nteleoperation, human perception, human-robot dialogue, and multimodal data\nrecording. Through pilot studies, we demonstrate the potential of CARIS to WoZ\ncontrol a robot in two contexts: 1) mental health companion and as a 2) tour\nguide. Furthermore, we identified areas of improvement for CARIS, including\nsmoother integration between movement and communication, clearer functionality\nseparation, recommended prompts, and one-click communication options to enhance\nthe usability wizard control of CARIS. This project offers a publicly\navailable, context-adaptable tool for the HRI community, enabling researchers\nto streamline data-driven approaches to intelligent robot behavior.", "AI": {"tldr": "开发了一个名为CARIS的情境自适应机器人界面系统，用于改进传统的Wizard-of-Oz控制方法，使其能够跨不同情境、用户和机器人平台使用。", "motivation": "传统Wizard-of-Oz工具通常局限于单一情境，难以适应不同设置、用户和机器人平台的需求，限制了人机交互研究的灵活性。", "method": "开发了CARIS系统，集成了远程操作、人类感知、人机对话和多模态数据记录等先进机器人功能，并通过在两个情境（心理健康伴侣和导游）中的试点研究进行验证。", "result": "试点研究表明CARIS具有在多情境下进行Wizard-of-Oz控制的潜力，同时识别了需要改进的领域，包括运动与通信的平滑集成、功能分离、推荐提示和一键通信选项。", "conclusion": "CARIS为HRI研究社区提供了一个公开可用的情境自适应工具，能够帮助研究人员更高效地开发数据驱动的智能机器人行为方法。"}}
{"id": "2509.00741", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00741", "abs": "https://arxiv.org/abs/2509.00741", "authors": ["Yi Liu", "Keyu Fan", "Bin Lan", "Houde Liu"], "title": "DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments", "comment": "Accepted by ICME 2025(Oral)", "summary": "Visual SLAM algorithms have been enhanced through the exploration of Gaussian\nSplatting representations, particularly in generating high-fidelity dense maps.\nWhile existing methods perform reliably in static environments, they often\nencounter camera tracking drift and fuzzy mapping when dealing with the\ndisturbances caused by moving objects. This paper presents DyPho-SLAM, a\nreal-time, resource-efficient visual SLAM system designed to address the\nchallenges of localization and photorealistic mapping in environments with\ndynamic objects. Specifically, the proposed system integrates prior image\ninformation to generate refined masks, effectively minimizing noise from mask\nmisjudgment. Additionally, to enhance constraints for optimization after\nremoving dynamic obstacles, we devise adaptive feature extraction strategies\nsignificantly improving the system's resilience. Experiments conducted on\npublicly dynamic RGB-D datasets demonstrate that the proposed system achieves\nstate-of-the-art performance in camera pose estimation and dense map\nreconstruction, while operating in real-time in dynamic scenes.", "AI": {"tldr": "DyPho-SLAM是一种实时视觉SLAM系统，针对动态环境中的移动物体干扰问题，通过集成先验图像信息生成精细化掩码，并采用自适应特征提取策略，在动态RGB-D数据集上实现了最先进的相机位姿估计和密集地图重建性能。", "motivation": "现有视觉SLAM算法在静态环境中表现可靠，但在处理动态物体干扰时经常遇到相机跟踪漂移和模糊映射问题，需要开发能够有效处理动态环境的实时SLAM系统。", "method": "系统集成了先验图像信息来生成精细化掩码，有效减少掩码误判带来的噪声；设计了自适应特征提取策略来增强移除动态障碍物后的优化约束，显著提高系统鲁棒性。", "result": "在公开动态RGB-D数据集上的实验表明，该系统在相机位姿估计和密集地图重建方面达到了最先进的性能，同时能够在动态场景中实时运行。", "conclusion": "DyPho-SLAM成功解决了动态环境中视觉SLAM的挑战，通过创新的掩码生成和特征提取策略，实现了实时、高效的定位和逼真地图重建。"}}
{"id": "2509.00823", "categories": ["cs.RO", "cs.SC", "math.AC", "68W30, 13P10, 13P25, 68U07, 68R10"], "pdf": "https://arxiv.org/pdf/2509.00823", "abs": "https://arxiv.org/abs/2509.00823", "authors": ["Takumu Okazaki", "Akira Terui", "Masahiko Mikawa"], "title": "Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using Comprehensive Gröbner Systems", "comment": "24 pages", "summary": "We propose an effective method for solving the inverse kinematic problem of a\nspecific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer\nalgebra. It is known that when the rotation axes of three consecutive\nrotational joints of a manipulator intersect at a single point, the inverse\nkinematics problem can be divided into determining position and orientation. We\nextend this method to more general manipulators in which the rotational axes of\ntwo consecutive joints intersect. This extension broadens the class of 6-DOF\nmanipulators for which the inverse kinematics problem can be solved, and is\nexpected to enable more efficient solutions. The inverse kinematic problem is\nsolved using the Comprehensive Gr\\\"obner System (CGS) with joint parameters of\nthe robot appearing as parameters in the coefficients to prevent repetitive\ncalculations of the Gr\\\"obner bases. The effectiveness of the proposed method\nis shown by experiments.", "AI": {"tldr": "使用计算机代数和维纳基CGS方法解决6自由度机器人逆运动学问题，扩展了可解的机器人类型范围", "motivation": "解决更广泛类型的6-DOF机器人逆运动学问题，提高解题效率", "method": "扩展了三轴交点方法到两轴交点情况，使用CGS技术将关节参数作为系数参数避免重复计算", "result": "实验证明方法有效，能够解决更广泛类型的逆运动学问题", "conclusion": "该方法扩展了可解逆运动学问题的6-DOF机器人类型范围，有助于提高解题效率"}}
{"id": "2509.00828", "categories": ["cs.RO", "cs.SC", "math.AC", "68W30, 13P10, 13P25, 68U07, 68R10"], "pdf": "https://arxiv.org/pdf/2509.00828", "abs": "https://arxiv.org/abs/2509.00828", "authors": ["Takumu Okazaki", "Akira Terui", "Masahiko Mikawa"], "title": "An Effective Trajectory Planning and an Optimized Path Planning for a 6-Degree-of-Freedom Robot Manipulator", "comment": "26 pages", "summary": "An effective method for optimizing path planning for a specific model of a\n6-degree-of-freedom (6-DOF) robot manipulator is presented as part of the\nmotion planning of the manipulator using computer algebra. We assume that we\nare given a path in the form of a set of line segments that the end-effector\nshould follow. We also assume that we have a method to solve the inverse\nkinematic problem of the manipulator at each via-point of the trajectory. The\nproposed method consists of three steps. First, we calculate the feasible\nregion of the manipulator under a specific configuration of the end-effector.\nNext, we aim to find a trajectory on the line segments and a sequence of joint\nconfigurations the manipulator should follow to move the end-effector along the\nspecified trajectory. Finally, we find the optimal combination of solutions to\nthe inverse kinematic problem at each via-point along the trajectory by\nreducing the problem to a shortest-path problem of the graph and applying\nDijkstra's algorithm. We show the effectiveness of the proposed method by\nexperiments.", "AI": {"tldr": "提出了一种基于计算机代数的6自由度机器人路径规划优化方法，通过逆运动学求解、可行区域计算和Dijkstra算法寻找最优关节配置序列", "motivation": "为了解决6自由度机器人末端执行器沿指定轨迹运动时，如何选择最优的逆运动学解序列以最小化关节运动量的问题", "method": "三步骤方法：1)计算末端执行器特定配置下的可行区域 2)在轨迹线段上寻找路径和关节配置序列 3)将问题转化为图的最短路径问题并应用Dijkstra算法", "result": "通过实验验证了所提方法的有效性", "conclusion": "该方法能够有效优化6自由度机器人的路径规划，通过代数计算和图搜索算法找到最优的关节配置序列"}}
{"id": "2509.00836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00836", "abs": "https://arxiv.org/abs/2509.00836", "authors": ["Yulin Li", "Tetsuro Miyazaki", "Kenji Kawashima"], "title": "One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields", "comment": null, "summary": "Motion planning for robotic manipulators is a fundamental problem in\nrobotics. Classical optimization-based methods typically rely on the gradients\nof signed distance fields (SDFs) to impose collision-avoidance constraints.\nHowever, these methods are susceptible to local minima and may fail when the\nSDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have\nbeen introduced, which directly model distances in the robot's configuration\nspace. Unlike workspace SDFs, CDFs are differentiable almost everywhere and\nthus provide reliable gradient information. On the other hand, gradient-free\napproaches such as Model Predictive Path Integral (MPPI) control leverage\nlong-horizon rollouts to achieve collision avoidance. While effective, these\nmethods are computationally expensive due to the large number of trajectory\nsamples, repeated collision checks, and the difficulty of designing cost\nfunctions with heterogeneous physical units. In this paper, we propose a\nframework that integrates CDFs with MPPI to enable direct navigation in the\nrobot's configuration space. Leveraging CDF gradients, we unify the MPPI cost\nin joint-space and reduce the horizon to one step, substantially cutting\ncomputation while preserving collision avoidance in practice. We demonstrate\nthat our approach achieves nearly 100% success rates in 2D environments and\nconsistently high success rates in challenging 7-DOF Franka manipulator\nsimulations with complex obstacles. Furthermore, our method attains control\nfrequencies exceeding 750 Hz, substantially outperforming both\noptimization-based and standard MPPI baselines. These results highlight the\neffectiveness and efficiency of the proposed CDF-MPPI framework for\nhigh-dimensional motion planning.", "AI": {"tldr": "提出CDF-MPPI框架，将配置空间距离场与模型预测路径积分控制结合，实现高效高维运动规划", "motivation": "传统优化方法依赖SDF梯度但易陷入局部极小值，梯度自由方法如MPPI计算昂贵且成本函数设计困难", "method": "利用CDF梯度统一MPPI成本函数在关节空间，将规划时域缩减到单步，大幅降低计算量", "result": "在2D环境中达到近100%成功率，7自由度机械臂仿真中保持高成功率，控制频率超过750Hz", "conclusion": "CDF-MPPI框架在保持避障性能的同时显著提升计算效率，适用于高维运动规划"}}
{"id": "2509.00981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.00981", "abs": "https://arxiv.org/abs/2509.00981", "authors": ["Liancheng Zheng", "Zhen Tian", "Yangfan He", "Shuo Liu", "Ke Gong", "Huilin Chen", "Zhihao Lin"], "title": "Enhanced Mean Field Game for Interactive Decision-Making with Varied Stylish Multi-Vehicles", "comment": null, "summary": "This paper presents an MFG-based decision-making framework for autonomous\ndriving in heterogeneous traffic. To capture diverse human behaviors, we\npropose a quantitative driving style representation that maps abstract traits\nto parameters such as speed, safety factors, and reaction time. These\nparameters are embedded into the MFG through a spatial influence field model.\nTo ensure safe operation in dense traffic, we introduce a safety-critical\nlane-changing algorithm that leverages dynamic safety margins,\ntime-to-collision analysis, and multi-layered constraints. Real-world NGSIM\ndata is employed for style calibration and empirical validation. Experimental\nresults demonstrate zero collisions across six style combinations, two\n15-vehicle scenarios, and NGSIM-based trials, consistently outperforming\nconventional game-theoretic baselines. Overall, our approach provides a\nscalable, interpretable, and behavior-aware planning framework for real-world\nautonomous driving applications.", "AI": {"tldr": "基于MFG的自动驾驶决策框架，通过量化驾驶风格表征和空间影响场模型处理异构交通，结合安全关键换道算法实现零碰撞性能", "motivation": "解决异构交通中多样化人类行为的建模问题，为自动驾驶提供可扩展、可解释且行为感知的规划框架", "method": "提出定量驾驶风格表征方法，将抽象特征映射到速度、安全因子等参数；通过空间影响场模型嵌入MFG；引入基于动态安全边际和碰撞时间分析的安全关键换道算法", "result": "在六种风格组合、两个15辆车场景和NGSIM数据试验中实现零碰撞， consistently优于传统博弈论基线方法", "conclusion": "该框架为现实世界自动驾驶应用提供了可扩展、可解释且行为感知的规划解决方案"}}
{"id": "2509.01010", "categories": ["cs.RO", "math.AG"], "pdf": "https://arxiv.org/pdf/2509.01010", "abs": "https://arxiv.org/abs/2509.01010", "authors": ["Hai-Jun Su"], "title": "A Robust Numerical Method for Solving Trigonometric Equations in Robotic Kinematics", "comment": null, "summary": "This paper presents a robust numerical method for solving systems of\ntrigonometric equations commonly encountered in robotic kinematics. Our\napproach employs polynomial substitution techniques combined with eigenvalue\ndecomposition to handle singular matrices and edge cases effectively. The\nmethod demonstrates superior numerical stability compared to traditional\napproaches and has been implemented as an open-source Python package. For\nnon-singular matrices, we employ Weierstrass substitution to transform the\nsystem into a quartic polynomial, ensuring all analytical solutions are found.\nFor singular matrices, we develop specialized geometric constraint methods\nusing SVD analysis. The solver demonstrates machine precision accuracy ($<\n10^{-15}$ error) with 100\\% success rate on extensive test cases, making it\nparticularly valuable for robotics applications such as inverse kinematics\nproblems.", "AI": {"tldr": "提出了一种用于求解机器人运动学中三角方程组的鲁棒数值方法，通过多项式替换和特征值分解处理奇异矩阵和边缘情况，具有优越的数值稳定性。", "motivation": "机器人运动学中经常遇到三角方程组求解问题，传统方法在数值稳定性和奇异矩阵处理方面存在不足，需要开发更鲁棒的求解方法。", "method": "采用多项式替换技术结合特征值分解：对非奇异矩阵使用Weierstrass替换将系统转化为四次多项式；对奇异矩阵使用基于SVD分析的专门几何约束方法。", "result": "求解器在大量测试案例中表现出机器精度准确性（误差<10^{-15}），成功率达到100%，已实现为开源Python包。", "conclusion": "该方法在数值稳定性和准确性方面优于传统方法，特别适用于机器人逆运动学等应用场景，为解决复杂三角方程组提供了可靠工具。"}}
{"id": "2509.01043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01043", "abs": "https://arxiv.org/abs/2509.01043", "authors": ["Thays Leach Mitre"], "title": "TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics Education", "comment": "6 pages, 5 figures. Preprint submission", "summary": "The high cost of robotic platforms limits students' ability to gain practical\nskills directly applicable in real-world scenarios. To address this challenge,\nthis paper presents TARA, a low-cost, 3D-printed robotic arm designed for\naccessible robotics education. TARA includes an open-source repository with\ndesign files, assembly instructions, and baseline code, enabling users to build\nand customize the platform. The system balances affordability and\nfunctionality, offering a highly capable robotic arm for approximately 200 USD,\nsignificantly lower than industrial systems that often cost thousands of\ndollars. Experimental validation confirmed accurate performance in basic\nmanipulation tasks. Rather than focusing on performance benchmarking, this work\nprioritizes educational reproducibility, providing a platform that students and\neducators can reliably replicate and extend.", "AI": {"tldr": "TARA是一个低成本3D打印机械臂，用于机器人教育，成本约200美元，提供开源设计和代码，注重教育可复现性而非性能基准测试", "motivation": "解决机器人平台高成本限制学生获得实践技能的问题，使机器人教育更加普及和可及", "method": "开发低成本3D打印机械臂TARA，包含开源设计文件、组装说明和基础代码，平衡经济性和功能性", "result": "实验验证显示在基本操作任务中具有准确性能，成本显著低于数千美元的工业系统", "conclusion": "TARA为教育提供了一个可可靠复现和扩展的平台，优先考虑教育可复现性而非性能基准测试"}}
{"id": "2509.01044", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01044", "abs": "https://arxiv.org/abs/2509.01044", "authors": ["Yonghyeon Lee", "Tzu-Yuan Lin", "Alexander Alexiev", "Sangbae Kim"], "title": "A Reactive Grasping Framework for Multi-DoF Grippers via Task Space Velocity Fields and Joint Space QP", "comment": "8 pages, 12 figures, under review", "summary": "We present a fast and reactive grasping framework for multi-DoF grippers that\ncombines task-space velocity fields with a joint-space Quadratic Program (QP)\nin a hierarchical structure. Reactive, collision-free global motion planning is\nparticularly challenging for high-DoF systems, since simultaneous increases in\nstate dimensionality and planning horizon trigger a combinatorial explosion of\nthe search space, making real-time planning intractable. To address this, we\nplan globally in a lower-dimensional task space, such as fingertip positions,\nand track locally in the full joint space while enforcing all constraints. This\napproach is realized by constructing velocity fields in multiple task-space\ncoordinates (or in some cases a subset of joint coordinates) and solving a\nweighted joint-space QP to compute joint velocities that track these fields\nwith appropriately assigned priorities. Through simulation experiments with\nprivileged knowledge and real-world tests using the recent pose-tracking\nalgorithm FoundationPose, we verify that our method enables high-DoF arm-hand\nsystems to perform real-time, collision-free reaching motions while adapting to\ndynamic environments and external disturbances.", "AI": {"tldr": "一种通过任务空间速度场和关节空间二次规划的层次结构，实现高自由度技器手系统的快速反应急抓的方法", "motivation": "高自由度系统的全局运动规划遇到状态维度和规划水平同时增加导致搜索空间组合爆烈的挑战，难以实现实时规划", "method": "在低维度任务空间（如指尖位置）进行全局规划，在完整关节空间进行局部跟踪，通过多个任务空间坐标的速度场构建和权重关节空间QP求解来计算关节速度", "result": "通过模拟实验和使用FoundationPose的实际测试，验证方法能够让高自由度手臂系统执行实时无碰撞到达运动，并适应动态环境和外部干扰", "conclusion": "层次结构结合任务空间规划和关节空间跟踪的方法有效解决了高自由度系统实时反应急抓的挑战"}}
{"id": "2509.01065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01065", "abs": "https://arxiv.org/abs/2509.01065", "authors": ["Sumitaka Honji", "Takahiro Wada"], "title": "Model Predictive Control for a Soft Robotic Finger with Stochastic Behavior based on Fokker-Planck Equation", "comment": "6 pages, 7 figures, presented/published at 2025 IEEE 8th\n  International Conference on Soft Robotics (RoboSoft)", "summary": "The inherent flexibility of soft robots offers numerous advantages, such as\nenhanced adaptability and improved safety. However, this flexibility can also\nintroduce challenges regarding highly uncertain and nonlinear motion. These\nchallenges become particularly problematic when using open-loop control\nmethods, which lack a feedback mechanism and are commonly employed in soft\nrobot control. Though one potential solution is model-based control, typical\ndeterministic models struggle with uncertainty as mentioned above. The idea is\nto use the Fokker-Planck Equation (FPE), a master equation of a stochastic\nprocess, to control not the state of soft robots but the probabilistic\ndistribution. In this study, we propose and implement a stochastic-based\ncontrol strategy, termed FPE-based Model Predictive Control (FPE-MPC), for a\nsoft robotic finger. Two numerical simulation case studies examine the\nperformance and characteristics of this control method, revealing its efficacy\nin managing the uncertainty inherent in soft robotic systems.", "AI": {"tldr": "提出基于Fokker-Planck方程的随机控制策略FPE-MPC，用于软体机器人手指的概率分布控制，有效处理柔性系统的不确定性", "motivation": "软体机器人的灵活性带来高度不确定性和非线性运动挑战，传统开环控制和确定性模型难以有效处理这些问题", "method": "使用Fokker-Planck方程（随机过程主方程）构建FPE-MPC模型预测控制方法，控制软体机器人的概率分布而非状态", "result": "通过两个数值模拟案例研究验证了该控制方法的性能和特性，证明其能有效管理软体机器人系统中的不确定性", "conclusion": "FPE-MPC方法为软体机器人控制提供了一种有效的随机控制策略，能够处理柔性系统固有的不确定性挑战"}}
{"id": "2509.01111", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01111", "abs": "https://arxiv.org/abs/2509.01111", "authors": ["Haolan Zhang", "Chenghao Li", "Thanh Nguyen Canh", "Lijun Wang", "Nak Young Chong"], "title": "SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments", "comment": "submitted", "summary": "Visual simultaneous localization and mapping (SLAM) plays a critical role in\nautonomous robotic systems, especially where accurate and reliable measurements\nare essential for navigation and sensing. In feature-based SLAM, the\nquantityand quality of extracted features significantly influence system\nperformance. Due to the variations in feature quantity and quality across\ndiverse environments, current approaches face two major challenges: (1) limited\nadaptability in dynamic feature culling and pose estimation, and (2)\ninsufficient environmental awareness in assessment and optimization strategies.\nTo address these issues, we propose SRR-SLAM, a scene-reliability based\nframework that enhances feature-based SLAM through environment-aware\nprocessing. Our method introduces a unified scene reliability assessment\nmechanism that incorporates multiple metrics and historical observations to\nguide system behavior. Based on this assessment, we develop: (i) adaptive\ndynamic region selection with flexible geometric constraints, (ii)\ndepth-assisted self-adjusting clustering for efficient dynamic feature removal\nin high-dimensional settings, and (iii) reliability-aware pose refinement that\ndynamically integrates direct methods when features are insufficient.\nFurthermore, we propose (iv) reliability-based keyframe selection and a\nweighted optimization scheme to reduce computational overhead while improving\nestimation accuracy. Extensive experiments on public datasets and real world\nscenarios show that SRR-SLAM outperforms state-of-the-art dynamic SLAM methods,\nachieving up to 90% improvement in accuracy and robustness across diverse\nenvironments. These improvements directly contribute to enhanced measurement\nprecision and reliability in autonomous robotic sensing systems.", "AI": {"tldr": "SRR-SLAM是一个基于场景可靠性的视觉SLAM框架，通过环境感知处理提升特征提取和位姿估计的适应性，在动态环境中实现高达90%的精度提升。", "motivation": "传统特征SLAM方法在动态环境中面临两个主要挑战：特征筛选和位姿估计的适应性不足，以及环境感知评估和优化策略不够充分。", "method": "提出统一的场景可靠性评估机制，包含自适应动态区域选择、深度辅助自调整聚类、可靠性感知位姿优化，以及基于可靠性的关键帧选择和加权优化方案。", "result": "在公开数据集和真实场景中的广泛实验表明，SRR-SLAM优于最先进的动态SLAM方法，在不同环境中实现了高达90%的精度和鲁棒性提升。", "conclusion": "SRR-SLAM显著提高了自主机器人传感系统的测量精度和可靠性，为动态环境下的视觉SLAM提供了有效的解决方案。"}}
{"id": "2509.01113", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01113", "abs": "https://arxiv.org/abs/2509.01113", "authors": ["Haiyun Zhang", "Kelvin HoLam Heung", "Gabrielle J. Naquila", "Ashwin Hingwe", "Ashish D. Deshpande"], "title": "A novel parameter estimation method for pneumatic soft hand control applying logarithmic decrement for pseudo rigid body modeling", "comment": null, "summary": "The rapid advancement in physical human-robot interaction (HRI) has\naccelerated the development of soft robot designs and controllers. Controlling\nsoft robots, especially soft hand grasping, is challenging due to their\ncontinuous deformation, motivating the use of reduced model-based controllers\nfor real-time dynamic performance. Most existing models, however, suffer from\ncomputational inefficiency and complex parameter identification, limiting their\nreal-time applicability. To address this, we propose a paradigm coupling\nPseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter\nestimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate\nPRBM plus LDM for predicting position and force output from pressure input and\nbenchmark its performance. We then implement PRBM plus LDM as the basis for\nclosed-loop position and force controllers. Compared to a simple PID\ncontroller, the PRBM plus LDM position controller achieves lower error (average\nmaximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force\ncontrol, PRBM plus LDM outperforms constant pressure grasping in pinching tasks\non delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70,\nbrass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a\ncomputationally efficient and accurate modeling technique for soft actuators,\nenabling stable and flexible grasping with precise force regulation.", "AI": {"tldr": "这篇论文提出了一种结合伪刚体模型和对数减少方法的新方法PRBM+LDM，用于软体机器人手的高效控制，在位置和力控制方面都取得了更好的性能。", "motivation": "软体机器人控制面临连续变形的挑战，现有模型存在计算效率低和参数识别复杂的问题，限制了实时应用。", "method": "提出PRBM+LDM方法，结合伪刚体模型和对数减少方法进行参数估计，并在软体机器人手平台上验证和实现闭环控制器。", "result": "位置控制器锐失从20.38度降到4.37度；力控制在脆弱物体摘取任务中表现优异：土豆片86对82.5，螺丝刀74.42对70，黄铜硬币64.75对35。", "conclusion": "PRBM+LDM是一种计算高效、准确的软体驱动器建模技术，能够实现稳定灵活的摘取和精确的力调节。"}}
{"id": "2509.01145", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01145", "abs": "https://arxiv.org/abs/2509.01145", "authors": ["Haiyun Zhang", "Gabrielle Naquila", "Jung Hyun Bae", "Zonghuan Wu", "Ashwin Hingwe", "Ashish Deshpande"], "title": "Novel bio-inspired soft actuators for upper-limb exoskeletons: design, fabrication and feasibility study", "comment": null, "summary": "Soft robots have been increasingly utilized as sophisticated tools in\nphysical rehabilitation, particularly for assisting patients with neuromotor\nimpairments. However, many soft robotics for rehabilitation applications are\ncharacterized by limitations such as slow response times, restricted range of\nmotion, and low output force. There are also limited studies on the precise\nposition and force control of wearable soft actuators. Furthermore, not many\nstudies articulate how bellow-structured actuator designs quantitatively\ncontribute to the robots' capability. This study introduces a paradigm of upper\nlimb soft actuator design. This paradigm comprises two actuators: the\nLobster-Inspired Silicone Pneumatic Robot (LISPER) for the elbow and the\nScallop-Shaped Pneumatic Robot (SCASPER) for the shoulder. LISPER is\ncharacterized by higher bandwidth, increased output force/torque, and high\nlinearity. SCASPER is characterized by high output force/torque and simplified\nfabrication processes. Comprehensive analytical models that describe the\nrelationship between pressure, bending angles, and output force for both\nactuators were presented so the geometric configuration of the actuators can be\nset to modify the range of motion and output forces. The preliminary test on a\ndummy arm is conducted to test the capability of the actuators.", "AI": {"tldr": "这篇论文提出了一种新的上肢软体机器人扩展器设计范式，包括受龙虾启发的肚部扩展器和受户发启发的肩部扩展器，以解决现有软体机器人在康复治疗中的应用限制。", "motivation": "现有软体机器人在康复治疗中存在响应速度慢、运动范围受限、输出力小等问题，且缺乏对粗结构扩展器设计的定量分析。", "method": "设计了两种新型扩展器：LISPER（龙虾叒发的肚部氧动机器人）用于肚部，SCASPER（户发形肩部氧动机器人）用于肩部。提供了完整的分析模型来描述压力、弯曲角度和输出力之间的关系，并在模拟手臂上进行初步测试。", "result": "LISPER具有更高的带宽、增强的输出力/扭矩和高线性度。SCASPER具有高输出力/扭矩和简化的制造过程。", "conclusion": "该研究提供了一种有前景的上肢软体机器人设计方案，通过精确的模型控制和优化设计，有望解决现有软体机器人在康复治疗中的技术限制。"}}
{"id": "2509.01228", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01228", "abs": "https://arxiv.org/abs/2509.01228", "authors": ["Jianyu Dou", "Yinan Deng", "Jiahui Wang", "Xingsi Tang", "Yi Yang", "Yufeng Yue"], "title": "OpenMulti: Open-Vocabulary Instance-Level Multi-Agent Distributed Implicit Mapping", "comment": "Accepted to IEEE Robotics and Automation Letters. Project website:\n  https://openmulti666.github.io/", "summary": "Multi-agent distributed collaborative mapping provides comprehensive and\nefficient representations for robots. However, existing approaches lack\ninstance-level awareness and semantic understanding of environments, limiting\ntheir effectiveness for downstream applications. To address this issue, we\npropose OpenMulti, an open-vocabulary instance-level multi-agent distributed\nimplicit mapping framework. Specifically, we introduce a Cross-Agent Instance\nAlignment module, which constructs an Instance Collaborative Graph to ensure\nconsistent instance understanding across agents. To alleviate the degradation\nof mapping accuracy due to the blind-zone optimization trap, we leverage Cross\nRendering Supervision to enhance distributed learning of the scene.\nExperimental results show that OpenMulti outperforms related algorithms in both\nfine-grained geometric accuracy and zero-shot semantic accuracy. In addition,\nOpenMulti supports instance-level retrieval tasks, delivering semantic\nannotations for downstream applications. The project website of OpenMulti is\npublicly available at https://openmulti666.github.io/.", "AI": {"tldr": "OpenMulti是一个开词汇实例级多智能体分布式隐式建图框架，通过跨智能体实例对齐和交叉渲染监督解决现有方法缺乏实例级感知和语义理解的问题。", "motivation": "现有的多智能体分布式协作建图方法缺乏实例级感知和环境语义理解，限制了其在下游应用中的有效性。", "method": "提出Cross-Agent Instance Alignment模块构建实例协作图确保跨智能体实例理解一致性；利用Cross Rendering Supervision缓解盲区优化陷阱导致的建图精度下降。", "result": "实验结果表明OpenMulti在细粒度几何精度和零样本语义精度方面均优于相关算法，并支持实例级检索任务。", "conclusion": "OpenMulti为下游应用提供语义标注，是一个有效的开词汇实例级多智能体分布式建图解决方案。"}}
{"id": "2509.01251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01251", "abs": "https://arxiv.org/abs/2509.01251", "authors": ["Pilar Bachiller-Burgos", "Ulysses Bernardet", "Luis V. Calderita", "Pranup Chhetri", "Anthony Francis", "Noriaki Hirose", "Noé Pérez", "Dhruv Shah", "Phani T. Singamaneni", "Xuesu Xiao", "Luis J. Manso"], "title": "Towards Data-Driven Metrics for Social Robot Navigation Benchmarking", "comment": null, "summary": "This paper presents a joint effort towards the development of a data-driven\nSocial Robot Navigation metric to facilitate benchmarking and policy\noptimization. We provide our motivations for our approach and describe our\nproposal for storing rated social navigation trajectory datasets. Following\nthese guidelines, we compiled a dataset with 4427 trajectories -- 182 real and\n4245 simulated -- and presented it to human raters, yielding a total of 4402\nrated trajectories after data quality assurance. We also trained an RNN-based\nbaseline metric on the dataset and present quantitative and qualitative\nresults. All data, software, and model weights are publicly available.", "AI": {"tldr": "开发数据驱动的社交机器人导航指标，用于基准测试和策略优化，包含4427条轨迹数据集和基于RNN的基线指标", "motivation": "需要建立标准化的社交机器人导航评估指标来促进算法比较和优化，缺乏公开可用的评分数据集", "method": "提出社交导航轨迹数据存储标准，收集4427条轨迹（182条真实+4245条模拟），人工评分后训练RNN基线模型", "result": "获得4402条评分轨迹数据集，训练出有效的RNN导航指标模型，所有数据和模型权重公开可用", "conclusion": "成功建立了社交机器人导航的标准化评估框架，为后续研究提供了基准数据集和评估工具"}}
{"id": "2509.01291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01291", "abs": "https://arxiv.org/abs/2509.01291", "authors": ["Nouhed Naidja", "Stéphane Font", "Marc Revilloud", "Guillaume Sandou"], "title": "Toward a Holistic Multi-Criteria Trajectory Evaluation Framework for Autonomous Driving in Mixed Traffic Environment", "comment": null, "summary": "This paper presents a unified framework for the evaluation and optimization\nof autonomous vehicle trajectories, integrating formal safety, comfort, and\nefficiency criteria. An innovative geometric indicator, based on the analysis\nof safety zones using adaptive ellipses, is used to accurately quantify\ncollision risks. Our method applies the Shoelace formula to compute the\nintersection area in the case of misaligned and time-varying configurations.\nComfort is modeled using indicators centered on longitudinal and lateral jerk,\nwhile efficiency is assessed by overall travel time. These criteria are\naggregated into a comprehensive objective function solved using a PSO based\nalgorithm. The approach was successfully validated under real traffic\nconditions via experiments conducted in an urban intersection involving an\nautonomous vehicle interacting with a human-operated vehicle, and in simulation\nusing data recorded from human driving in real traffic.", "AI": {"tldr": "提出一个统一框架来评估和优化自动驾驶车辆轨迹，整合安全性、舒适性和效率标准，使用几何指标量化碰撞风险，并通过PSO算法进行优化。", "motivation": "需要一种综合的方法来同时评估和优化自动驾驶车辆的多个关键性能指标（安全性、舒适性、效率），以提升自动驾驶系统的整体性能。", "method": "使用基于自适应椭圆的安全区域分析来量化碰撞风险，应用鞋带公式计算不对齐和时间变化配置的交集面积；舒适性通过纵向和横向加加速度指标建模；效率通过总行驶时间评估；最后使用PSO算法优化综合目标函数。", "result": "该方法在真实交通条件下成功验证，包括城市交叉路口自动驾驶车辆与人工驾驶车辆的交互实验，以及基于真实交通中人类驾驶数据的仿真。", "conclusion": "提出的统一框架能够有效评估和优化自动驾驶车辆轨迹，在安全性、舒适性和效率方面表现出良好的综合性能，适用于真实交通场景。"}}
{"id": "2509.01297", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01297", "abs": "https://arxiv.org/abs/2509.01297", "authors": ["Seonsoo Kim", "Jun-Gill Kang", "Taehong Kim", "Seongil Hong"], "title": "Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning", "comment": "Accepted to The Conference on Robot Learning (CoRL) 2025 Project\n  Page: seonsoo-p1.github.io/DMCM", "summary": "In meta-learning and its downstream tasks, many methods rely on implicit\nadaptation to task variations, where multiple factors are mixed together in a\nsingle entangled representation. This makes it difficult to interpret which\nfactors drive performance and can hinder generalization. In this work, we\nintroduce a disentangled multi-context meta-learning framework that explicitly\nassigns each task factor to a distinct context vector. By decoupling these\nvariations, our approach improves robustness through deeper task understanding\nand enhances generalization by enabling context vector sharing across tasks\nwith shared factors. We evaluate our approach in two domains. First, on a\nsinusoidal regression task, our model outperforms baselines on\nout-of-distribution tasks and generalizes to unseen sine functions by sharing\ncontext vectors associated with shared amplitudes or phase shifts. Second, in a\nquadruped robot locomotion task, we disentangle the robot-specific properties\nand the characteristics of the terrain in the robot dynamics model. By\ntransferring disentangled context vectors acquired from the dynamics model into\nreinforcement learning, the resulting policy achieves improved robustness under\nout-of-distribution conditions, surpassing the baselines that rely on a single\nunified context. Furthermore, by effectively sharing context, our model enables\nsuccessful sim-to-real policy transfer to challenging terrains with\nout-of-distribution robot-specific properties, using just 20 seconds of real\ndata from flat terrain, a result not achievable with single-task adaptation.", "AI": {"tldr": "提出了解耦多上下文元学习框架，通过为每个任务因素分配独立上下文向量来提升任务理解和泛化能力，在正弦回归和机器人运动任务中验证了方法的有效性。", "motivation": "现有元学习方法通常将多个任务因素混合在单一纠缠表示中，这导致难以解释性能驱动因素并阻碍泛化能力。", "method": "引入解耦多上下文元学习框架，为每个任务因素分配独立的上下文向量，通过解耦变化实现更深层次的任务理解和跨任务上下文向量共享。", "result": "在正弦回归任务中优于基线方法，能够泛化到未见过的正弦函数；在四足机器人运动任务中，通过将解耦上下文向量从动力学模型转移到强化学习，提升了分布外条件下的鲁棒性，并实现了仅用20秒真实数据就能成功进行sim-to-real策略迁移。", "conclusion": "解耦多上下文元学习框架通过显式分离任务因素，显著提升了模型的解释性、泛化能力和鲁棒性，在多个领域验证了其有效性。"}}
{"id": "2509.01364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01364", "abs": "https://arxiv.org/abs/2509.01364", "authors": ["Peiran Liu", "Qiang Zhang", "Daojie Peng", "Lingfeng Zhang", "Yihao Qin", "Hang Zhou", "Jun Ma", "Renjing Xu", "Yiding Ji"], "title": "TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation", "comment": null, "summary": "Object Navigation (ObjectNav) has made great progress with large language\nmodels (LLMs), but still faces challenges in memory management, especially in\nlong-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a\nnew framework that leverages topological structures as spatial memory. By\nbuilding and updating a topological graph that captures scene connections,\nadjacency, and semantic meaning, TopoNav helps agents accumulate spatial\nknowledge over time, retrieve key information, and reason effectively toward\ndistant goals. Our experiments show that TopoNav achieves state-of-the-art\nperformance on benchmark ObjectNav datasets, with higher success rates and more\nefficient paths. It particularly excels in diverse and complex environments, as\nit connects temporary visual inputs with lasting spatial understanding.", "AI": {"tldr": "TopoNav是一个利用拓扑结构作为空间记忆的新框架，通过构建和更新捕捉场景连接、邻近性和语义意义的拓扑图，在物体导航任务中实现了最先进的性能。", "motivation": "物体导航在大型语言模型推动下取得进展，但在长时程任务和动态场景中的记忆管理仍面临挑战，需要更好的空间记忆机制。", "method": "提出TopoNav框架，利用拓扑结构作为空间记忆，构建和更新包含场景连接、邻近关系和语义信息的拓扑图，帮助智能体积累空间知识、检索关键信息并进行有效推理。", "result": "在基准物体导航数据集上实现了最先进的性能，具有更高的成功率和更高效的路径规划，在多样复杂环境中表现尤为突出。", "conclusion": "TopoNav通过将临时视觉输入与持久空间理解相连接，有效解决了物体导航中的记忆管理问题，为长时程导航任务提供了有力解决方案。"}}
{"id": "2509.01450", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.01450", "abs": "https://arxiv.org/abs/2509.01450", "authors": ["Ane San Martin", "Michael Hagenow", "Julie Shah", "Johan Kildal", "Elena Lazkano"], "title": "Analyzing Reluctance to Ask for Help When Cooperating With Robots: Insights to Integrate Artificial Agents in HRC", "comment": "8 pages, 5 figures. Accepted for IEEE RO-MAN 2025", "summary": "As robot technology advances, collaboration between humans and robots will\nbecome more prevalent in industrial tasks. When humans run into issues in such\nscenarios, a likely future involves relying on artificial agents or robots for\naid. This study identifies key aspects for the design of future user-assisting\nagents. We analyze quantitative and qualitative data from a user study\nexamining the impact of on-demand assistance received from a remote human in a\nhuman-robot collaboration (HRC) assembly task. We study scenarios in which\nusers require help and we assess their experiences in requesting and receiving\nassistance. Additionally, we investigate participants' perceptions of future\nnon-human assisting agents and whether assistance should be on-demand or\nunsolicited. Through a user study, we analyze the impact that such design\ndecisions (human or artificial assistant, on-demand or unsolicited help) can\nhave on elicited emotional responses, productivity, and preferences of humans\nengaged in HRC tasks.", "AI": {"tldr": "这篇论文通过用户研究分析了人机协作任务中的远程帮助系统设计关键因素，包括帮助请求方式和帮助者类型对用户情感、生产力和偏好的影响", "motivation": "随着机器人技术的发展，人机协作在工业任务中将更加普遍。当人类遇到问题时，未来可能依赖人工智能组件或机器人获得帮助，因此需要研究设计未来用户帮助组件的关键方面", "method": "通过人机协作组装任务的用户研究，采集定量和定性数据，分析远程人类帮助的影响。研究包括帮助请求体验、帮助接收体验，以及用户对非人类帮助组件的看法和帮助方式偏好", "result": "研究分析了不同设计决策（人类或人工智能帮助者，需求帮助或主动帮助）对人机协作任务中用户情感响应、生产力和偏好的影响", "conclusion": "该研究为未来用户帮助组件的设计提供了重要参考，确定了帮助请求方式和帮助者类型等关键设计因素对人机协作效果的影响"}}
{"id": "2509.01547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01547", "abs": "https://arxiv.org/abs/2509.01547", "authors": ["Fan Zhu", "Yifan Zhao", "Ziyu Chen", "Biao Yu", "Hui Zhu"], "title": "FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field", "comment": "ICRA 2025", "summary": "Visual SLAM has regained attention due to its ability to provide perceptual\ncapabilities and simulation test data for Embodied AI. However, traditional\nSLAM methods struggle to meet the demands of high-quality scene reconstruction,\nand Gaussian SLAM systems, despite their rapid rendering and high-quality\nmapping capabilities, lack effective pose optimization methods and face\nchallenges in geometric reconstruction. To address these issues, we introduce\nFGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the\nscene representation to enhance geometric mapping performance. After initial\npose estimation, we apply global adjustment to optimize camera poses and sparse\npoint cloud, ensuring robust tracking of our approach. Additionally, we\nmaintain a globally consistent opacity radiance field based on 3D Gaussians and\nintroduce depth distortion and normal consistency terms to refine the scene\nrepresentation. Furthermore, after constructing tetrahedral grids, we identify\nlevel sets to directly extract surfaces from 3D Gaussians. Results across\nvarious real-world and large-scale synthetic datasets demonstrate that our\nmethod achieves state-of-the-art tracking accuracy and mapping performance.", "AI": {"tldr": "FGO-SLAM是一个基于高斯SLAM的系统，通过引入不透明度辐射场和全局优化方法，解决了传统SLAM在高质量场景重建方面的不足，实现了卓越的跟踪精度和建图性能。", "motivation": "传统SLAM方法难以满足高质量场景重建的需求，而现有的高斯SLAM系统缺乏有效的位姿优化方法，在几何重建方面面临挑战。", "method": "使用不透明度辐射场作为场景表示，采用全局调整优化相机位姿和稀疏点云，引入深度畸变和法线一致性项来精化场景表示，并通过四面体网格构建和水平集提取直接从3D高斯中提取表面。", "result": "在多个真实世界和大规模合成数据集上的实验表明，该方法在跟踪精度和建图性能方面达到了最先进的水平。", "conclusion": "FGO-SLAM通过创新的场景表示和优化策略，成功解决了高斯SLAM系统中的关键问题，为高质量场景重建提供了有效的解决方案。"}}
{"id": "2509.01583", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01583", "abs": "https://arxiv.org/abs/2509.01583", "authors": ["Thomas Jantos", "Stephan Weiss", "Jan Steinbrener"], "title": "Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for Object-relative State Estimation", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Deep Learning (DL) has become essential in various robotics applications due\nto excelling at processing raw sensory data to extract task specific\ninformation from semantic objects. For example, vision-based object-relative\nnavigation relies on a DL-based 6D object pose predictor to provide the\nrelative pose between the object and the robot as measurements to the robot's\nstate estimator. Accurately knowing the uncertainty inherent in such Deep\nNeural Network (DNN) based measurements is essential for probabilistic state\nestimators subsequently guiding the robot's tasks. Thus, in this letter, we\nshow that we can extend any existing DL-based object-relative pose predictor\nfor aleatoric uncertainty inference simply by including two multi-layer\nperceptrons detached from the translational and rotational part of the DL\npredictor. This allows for efficient training while freezing the existing\npre-trained predictor. We then use the inferred 6D pose and its uncertainty as\na measurement and corresponding noise covariance matrix in an extended Kalman\nfilter (EKF). Our approach induces minimal computational overhead such that the\nstate estimator can be deployed on edge devices while benefiting from the\ndynamically inferred measurement uncertainty. This increases the performance of\nthe object-relative state estimation task compared to a fix-covariance\napproach. We conduct evaluations on synthetic data and real-world data to\nunderline the benefits of aleatoric uncertainty inference for the\nobject-relative state estimation task.", "AI": {"tldr": "提出一种为深度学习6D位姿预测器添加认知不确定性的方法，通过附加两个多层感知器来动态推断测量不确定性，提升基于扩展卡尔曼滤波的状态估计性能", "motivation": "深度学习在机器人感知中广泛应用，但现有的6D物体位姿预测器缺乏不确定性量化，而概率状态估计器需要准确的测量不确定性信息才能有效工作", "method": "在现有预训练的位姿预测器基础上，附加两个独立的多层感知器（分别处理平移和旋转部分）来推断认知不确定性，冻结原有预测器参数仅训练新增部分", "result": "方法计算开销小，可在边缘设备部署，相比固定协方差方法显著提升了物体相对状态估计的性能，在合成数据和真实数据上都验证了有效性", "conclusion": "通过简单添加两个MLP来推断认知不确定性，能够有效提升基于深度学习的6D位姿预测在状态估计任务中的性能，且具有实际部署的可行性"}}
{"id": "2509.01611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01611", "abs": "https://arxiv.org/abs/2509.01611", "authors": ["Ziteng Gao", "Jiaqi Qu", "Chaoyu Chen"], "title": "A Hybrid Input based Deep Reinforcement Learning for Lane Change Decision-Making of Autonomous Vehicle", "comment": null, "summary": "Lane change decision-making for autonomous vehicles is a complex but\nhigh-reward behavior. In this paper, we propose a hybrid input based deep\nreinforcement learning (DRL) algorithm, which realizes abstract lane change\ndecisions and lane change actions for autonomous vehicles within traffic flow.\nFirstly, a surrounding vehicles trajectory prediction method is proposed to\nreduce the risk of future behavior of surrounding vehicles to ego vehicle, and\nthe prediction results are input into the reinforcement learning model as\nadditional information. Secondly, to comprehensively leverage environmental\ninformation, the model extracts feature from high-dimensional images and\nlow-dimensional sensor data simultaneously. The fusion of surrounding vehicle\ntrajectory prediction and multi-modal information are used as state space of\nreinforcement learning to improve the rationality of lane change decision.\nFinally, we integrate reinforcement learning macro decisions with end-to-end\nvehicle control to achieve a holistic lane change process. Experiments were\nconducted within the CARLA simulator, and the results demonstrated that the\nutilization of a hybrid state space significantly enhances the safety of\nvehicle lane change decisions.", "AI": {"tldr": "提出基于混合输入的深度强化学习算法，结合轨迹预测和多模态信息，实现自动驾驶车辆的安全换道决策与控制", "motivation": "自动驾驶车辆换道决策复杂但回报高，需要综合考虑周围车辆未来行为和多种环境信息来提升决策安全性", "method": "混合输入深度强化学习：1) 周围车辆轨迹预测作为额外输入；2) 同时提取高维图像和低维传感器特征；3) 融合轨迹预测和多模态信息作为状态空间；4) 结合强化学习宏观决策与端到端车辆控制", "result": "在CARLA模拟器中实验表明，混合状态空间的使用显著提升了车辆换道决策的安全性", "conclusion": "所提出的混合输入深度强化学习方法通过整合轨迹预测和多模态信息，有效提高了自动驾驶车辆换道决策的安全性和合理性"}}
{"id": "2509.01643", "categories": ["cs.RO", "cs.CY", "cs.HC", "I.2.9; J.5; K.4.2"], "pdf": "https://arxiv.org/pdf/2509.01643", "abs": "https://arxiv.org/abs/2509.01643", "authors": ["Minja Axelsson"], "title": "Speculative Design of Equitable Robotics: Queer Fictions and Futures", "comment": "Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures", "summary": "This paper examines the speculative topic of equitable robots through an\nexploratory essay format. It focuses specifically on robots by and for LGBTQ+\npopulations. It aims to provoke thought and conversations in the field about\nwhat aspirational queer robotics futures may look like, both in the arts and\nsciences. First, it briefly reviews the state-of-the-art of queer robotics in\nfiction and science, drawing together threads from each. Then, it discusses\nqueering robots through three speculative design proposals for queer robot\nroles: 1) reflecting the queerness of their ''in-group'' queer users, building\nand celebrating ''in-group'' identity, 2) a new kind of queer activism by\nimplementing queer robot identity performance to interact with ''out-group''\nusers, with a goal of reducing bigotry through familiarisation, and 3) a\nnetwork of queer-owned robots, through which the community could reach each\nother, and distribute and access important resources. The paper then questions\nwhether robots should be queered, and what ethical implications this raises.\nFinally, the paper makes suggestions for what aspirational queer robotics\nfutures may look like, and what would be required to get there.", "AI": {"tldr": "本文通过探索性论文形式探讨LGBTQ+群体专属机器人的公平性话题，提出了三种酷儿机器人设计提案，并讨论相关伦理问题", "motivation": "旨在激发关于酷儿机器人未来愿景的思考和对话，探讨如何通过机器人技术为LGBTQ+群体创造更公平的未来", "method": "采用探索性论文格式，首先回顾科幻和科学中的酷儿机器人现状，然后提出三种推测性设计提案：反映用户酷儿身份的机器人、酷儿行动主义机器人和酷儿拥有网络机器人", "result": "提出了三种具体的酷儿机器人角色设计方案，并引发了关于机器人是否应该被酷儿化以及相关伦理影响的讨论", "conclusion": "为酷儿机器人未来提出了愿景建议，并指出了实现这些愿景所需的条件，推动了该领域的思考和对话"}}
{"id": "2509.01657", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01657", "abs": "https://arxiv.org/abs/2509.01657", "authors": ["Amber Xie", "Rahul Chand", "Dorsa Sadigh", "Joey Hejna"], "title": "Data Retrieval with Importance Weights for Few-Shot Imitation Learning", "comment": "Conference on Robot Learning 2025", "summary": "While large-scale robot datasets have propelled recent progress in imitation\nlearning, learning from smaller task specific datasets remains critical for\ndeployment in new environments and unseen tasks. One such approach to few-shot\nimitation learning is retrieval-based imitation learning, which extracts\nrelevant samples from large, widely available prior datasets to augment a\nlimited demonstration dataset. To determine the relevant data from prior\ndatasets, retrieval-based approaches most commonly calculate a prior data\npoint's minimum distance to a point in the target dataset in latent space.\nWhile retrieval-based methods have shown success using this metric for data\nselection, we demonstrate its equivalence to the limit of a Gaussian kernel\ndensity (KDE) estimate of the target data distribution. This reveals two\nshortcomings of the retrieval rule used in prior work. First, it relies on\nhigh-variance nearest neighbor estimates that are susceptible to noise. Second,\nit does not account for the distribution of prior data when retrieving data. To\naddress these issues, we introduce Importance Weighted Retrieval (IWR), which\nestimates importance weights, or the ratio between the target and prior data\ndistributions for retrieval, using Gaussian KDEs. By considering the\nprobability ratio, IWR seeks to mitigate the bias of previous selection rules,\nand by using reasonable modeling parameters, IWR effectively smooths estimates\nusing all data points. Across both simulation environments and real-world\nevaluations on the Bridge dataset we find that our method, IWR, consistently\nimproves performance of existing retrieval-based methods, despite only\nrequiring minor modifications.", "AI": {"tldr": "本文提出了重要性加权检索(IWR)方法，通过高斯核密度估计计算目标数据与先验数据分布的概率比来改进基于检索的模仿学习，解决了传统最近邻检索方法的高方差和忽略先验数据分布的问题。", "motivation": "现有的基于检索的模仿学习方法使用最近邻距离作为检索标准，存在高方差估计和忽略先验数据分布的问题，限制了在小规模任务特定数据集上的学习效果。", "method": "提出重要性加权检索(IWR)方法，使用高斯核密度估计计算目标数据与先验数据分布的概率比作为重要性权重，通过考虑概率比来减少偏差，并利用所有数据点进行平滑估计。", "result": "在仿真环境和真实世界的Bridge数据集评估中，IWR方法持续提升了现有基于检索方法的性能，且只需要较小的修改。", "conclusion": "IWR方法通过概率比估计有效解决了传统检索方法的问题，在少样本模仿学习中表现出显著的性能提升，为检索式学习提供了更稳健的数据选择策略。"}}
{"id": "2509.01658", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01658", "abs": "https://arxiv.org/abs/2509.01658", "authors": ["Zhenyu Wu", "Angyuan Ma", "Xiuwei Xu", "Hang Yin", "Yinan Liang", "Ziwei Wang", "Jiwen Lu", "Haibin Yan"], "title": "MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation", "comment": "Accepted to CoRL 2025. Project Page: https://gary3410.github.io/MoTo/", "summary": "Mobile manipulation stands as a core challenge in robotics, enabling robots\nto assist humans across varied tasks and dynamic daily environments.\nConventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments due to the lack of large-scale training.\nHowever, recent advances in manipulation foundation models demonstrate\nimpressive generalization capability on a wide range of fixed-base manipulation\ntasks, which are still limited to a fixed setting. Therefore, we devise a\nplug-in module named MoTo, which can be combined with any off-the-shelf\nmanipulation foundation model to empower them with mobile manipulation ability.\nSpecifically, we propose an interaction-aware navigation policy to generate\nrobot docking points for generalized mobile manipulation. To enable zero-shot\nability, we propose an interaction keypoints framework via vision-language\nmodels (VLM) under multi-view consistency for both target object and robotic\narm following instructions, where fixed-base manipulation foundation models can\nbe employed. We further propose motion planning objectives for the mobile base\nand robot arm, which minimize the distance between the two keypoints and\nmaintain the physical feasibility of trajectories. In this way, MoTo guides the\nrobot to move to the docking points where fixed-base manipulation can be\nsuccessfully performed, and leverages VLM generation and trajectory\noptimization to achieve mobile manipulation in a zero-shot manner, without any\nrequirement on mobile manipulation expert data. Extensive experimental results\non OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%\nand 16.67% higher than the state-of-the-art mobile manipulation methods,\nrespectively, without requiring additional training data.", "AI": {"tldr": "MoTo是一个即插即用模块，可将任何现成的固定基座操作基础模型升级为移动操作能力，通过交互感知导航和视觉语言模型实现零样本移动操作", "motivation": "传统移动操作方法缺乏大规模训练数据而难以泛化，固定基座操作基础模型虽具有良好泛化能力但局限于固定设置，需要将其扩展到移动操作场景", "method": "提出交互感知导航策略生成机器人对接点，使用视觉语言模型在多视图一致性下生成交互关键点，结合运动规划目标最小化关键点距离并保持轨迹可行性", "result": "在OVMM和真实世界实验中，MoTo分别比最先进的移动操作方法成功率高出2.68%和16.67%，且无需额外训练数据", "conclusion": "MoTo成功实现了零样本移动操作，证明了将固定基座操作模型扩展到移动场景的可行性，为机器人辅助人类日常任务提供了有效解决方案"}}
{"id": "2509.01708", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01708", "abs": "https://arxiv.org/abs/2509.01708", "authors": ["Abdelrhman Werby", "Martin Büchner", "Adrian Röfer", "Chenguang Huang", "Wolfram Burgard", "Abhinav Valada"], "title": "Articulated Object Estimation in the Wild", "comment": "9th Conference on Robot Learning (CoRL), 2025", "summary": "Understanding the 3D motion of articulated objects is essential in robotic\nscene understanding, mobile manipulation, and motion planning. Prior methods\nfor articulation estimation have primarily focused on controlled settings,\nassuming either fixed camera viewpoints or direct observations of various\nobject states, which tend to fail in more realistic unconstrained environments.\nIn contrast, humans effortlessly infer articulation by watching others\nmanipulate objects. Inspired by this, we introduce ArtiPoint, a novel\nestimation framework that can infer articulated object models under dynamic\ncamera motion and partial observability. By combining deep point tracking with\na factor graph optimization framework, ArtiPoint robustly estimates articulated\npart trajectories and articulation axes directly from raw RGB-D videos. To\nfoster future research in this domain, we introduce Arti4D, the first\nego-centric in-the-wild dataset that captures articulated object interactions\nat a scene level, accompanied by articulation labels and ground-truth camera\nposes. We benchmark ArtiPoint against a range of classical and learning-based\nbaselines, demonstrating its superior performance on Arti4D. We make code and\nArti4D publicly available at https://artipoint.cs.uni-freiburg.de.", "AI": {"tldr": "ArtiPoint是一个新颖的3D关节物体运动估计框架，通过结合深度点跟踪和因子图优化，能够从原始RGB-D视频中鲁棒地估计关节部件轨迹和关节轴。", "motivation": "现有的关节估计方法主要关注受控环境，假设固定相机视角或直接观察不同物体状态，在无约束的真实环境中往往失效。受人类通过观察他人操作物体来推断关节的启发，需要开发能够在动态相机运动和部分可观测性下推断关节物体模型的方法。", "method": "结合深度点跟踪与因子图优化框架，直接从原始RGB-D视频中估计关节部件轨迹和关节轴。同时创建了首个以自我为中心的在野外场景级别捕获关节物体交互的数据集Arti4D。", "result": "在Arti4D数据集上对一系列经典和学习基线方法进行基准测试，证明了ArtiPoint的优越性能。", "conclusion": "ArtiPoint框架能够有效解决动态相机运动和部分可观测性下的关节物体建模问题，为未来研究提供了新的数据集和方法基准。"}}
{"id": "2509.01728", "categories": ["cs.RO", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.01728", "abs": "https://arxiv.org/abs/2509.01728", "authors": ["Parv Kapoor", "Akila Ganlath", "Changliu Liu", "Sebastian Scherer", "Eunsuk Kang"], "title": "Constrained Decoding for Robotics Foundation Models", "comment": null, "summary": "Recent advances in the development of robotic foundation models have led to\npromising end-to-end and general-purpose capabilities in robotic systems. These\nmodels are pretrained on vast datasets of robot trajectories to process multi-\nmodal inputs and directly output a sequence of action that the system then\nexecutes in the real world. Although this approach is attractive from the\nperspective of im- proved generalization across diverse tasks, these models are\nstill data-driven and, therefore, lack explicit notions of behavioral\ncorrectness and safety constraints. We address these limitations by introducing\na constrained decoding framework for robotics foundation models that enforces\nlogical constraints on action trajec- tories in dynamical systems. Our method\nensures that generated actions provably satisfy signal temporal logic (STL)\nspecifications at runtime without retraining, while remaining agnostic of the\nunderlying foundation model. We perform com- prehensive evaluation of our\napproach across state-of-the-art navigation founda- tion models and we show\nthat our decoding-time interventions are useful not only for filtering unsafe\nactions but also for conditional action-generation. Videos available on our\nwebsite: https://constrained-robot-fms.github.io", "AI": {"tldr": "提出了一种用于机器人基础模型的约束解码框架，通过在运行时强制执行信号时序逻辑（STL）规范来确保生成的动作满足安全约束，无需重新训练模型。", "motivation": "现有的机器人基础模型虽然具有端到端和通用能力，但缺乏对行为正确性和安全约束的显式考虑，存在安全隐患。", "method": "开发了约束解码框架，在解码时对动作轨迹施加逻辑约束，确保生成的动作满足STL规范，同时保持对底层基础模型的不可知性。", "result": "在多个最先进的导航基础模型上进行了全面评估，证明该方法不仅能过滤不安全动作，还能用于条件动作生成。", "conclusion": "该方法为机器人基础模型提供了运行时安全保障，无需重新训练即可确保动作满足安全约束，具有重要的实际应用价值。"}}
{"id": "2509.01746", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01746", "abs": "https://arxiv.org/abs/2509.01746", "authors": ["Yixuan Huang", "Novella Alvina", "Mohanraj Devendran Shanthi", "Tucker Hermans"], "title": "Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference", "comment": "Project page: sites.google.com/view/fail2progress. 25 pages, 8\n  figures. Accepted to the Conference on Robot Learning (CoRL) 2025", "summary": "Skill effect models for long-horizon manipulation tasks are prone to failures\nin conditions not covered by training data distributions. Therefore, enabling\nrobots to reason about and learn from failures is necessary. We investigate the\nproblem of efficiently generating a dataset targeted to observed failures.\nAfter fine-tuning a skill effect model on this dataset, we evaluate the extent\nto which the model can recover from failures and minimize future failures. We\npropose Fail2Progress, an approach that leverages Stein variational inference\nto generate multiple simulation environments in parallel, enabling efficient\ndata sample generation similar to observed failures. Our method is capable of\nhandling several challenging mobile manipulation tasks, including transporting\nmultiple objects, organizing a constrained shelf, and tabletop organization.\nThrough large-scale simulation and real-world experiments, we demonstrate that\nour approach excels at learning from failures across different numbers of\nobjects. Furthermore, we show that Fail2Progress outperforms several baselines.", "AI": {"tldr": "Fail2Progress利用Stein变分推理并行生成多个仿真环境，针对观察到的失败情况高效生成训练数据，从而提高技能效果模型在长时程操作任务中的失败恢复能力", "motivation": "长时程操作任务的技能效果模型在训练数据分布未覆盖的条件下容易失败，需要让机器人能够从失败中推理和学习", "method": "提出Fail2Progress方法，利用Stein变分推理并行生成多个仿真环境，针对观察到的失败高效生成数据集，然后对技能效果模型进行微调", "result": "方法能够处理多个具有挑战性的移动操作任务，包括运输多个物体、整理受限货架和桌面整理。通过大规模仿真和真实实验证明，该方法在不同物体数量下都能有效从失败中学习，且优于多个基线方法", "conclusion": "Fail2Progress通过高效生成针对失败的数据集，显著提高了技能效果模型的失败恢复能力，减少了未来失败的发生"}}
{"id": "2509.01765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01765", "abs": "https://arxiv.org/abs/2509.01765", "authors": ["Skand Peri", "Akhil Perincherry", "Bikram Pandit", "Stefan Lee"], "title": "Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control", "comment": "17 pages, 6 figures. Accepted as Oral presentation at Conference on\n  Robot Learning (CoRL) 2025", "summary": "Efficient robot control often requires balancing task performance with energy\nexpenditure. A common approach in reinforcement learning (RL) is to penalize\nenergy use directly as part of the reward function. This requires carefully\ntuning weight terms to avoid undesirable trade-offs where energy minimization\nharms task success. In this work, we propose a hyperparameter-free gradient\noptimization method to minimize energy expenditure without conflicting with\ntask performance. Inspired by recent works in multitask learning, our method\napplies policy gradient projection between task and energy objectives to derive\npolicy updates that minimize energy expenditure in ways that do not impact task\nperformance. We evaluate this technique on standard locomotion benchmarks of\nDM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage\nwhile maintaining comparable task performance. Further, we conduct experiments\non a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient\npolicies. Our method is easy to implement in standard RL pipelines with minimal\ncode changes, is applicable to any policy gradient method, and offers a\nprincipled alternative to reward shaping for energy efficient control policies.", "AI": {"tldr": "提出了一种无需超参数调优的梯度优化方法，通过策略梯度投影在任务目标和能耗目标之间进行优化，在保持任务性能的同时显著降低能耗", "motivation": "传统强化学习方法通过奖励函数惩罚能耗需要仔细调优权重，容易导致能耗最小化与任务性能之间的不良权衡", "method": "基于多任务学习的策略梯度投影方法，在任务目标和能耗目标之间推导策略更新，以不影响任务性能的方式最小化能耗", "result": "在DM-Control和HumanoidBench标准运动基准测试中实现了64%的能耗降低，同时保持可比较的任务性能，并在Unitree GO2四足机器人上展示了Sim2Real迁移", "conclusion": "该方法易于在标准RL流程中实现，适用于任何策略梯度方法，为节能控制策略提供了基于原理的替代方案"}}
{"id": "2509.01819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01819", "abs": "https://arxiv.org/abs/2509.01819", "authors": ["Ge Yan", "Jiyue Zhu", "Yuquan Deng", "Shiqi Yang", "Ri-Zhao Qiu", "Xuxin Cheng", "Marius Memmel", "Ranjay Krishna", "Ankit Goyal", "Xiaolong Wang", "Dieter Fox"], "title": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training", "comment": null, "summary": "This paper introduces ManiFlow, a visuomotor imitation learning policy for\ngeneral robot manipulation that generates precise, high-dimensional actions\nconditioned on diverse visual, language and proprioceptive inputs. We leverage\nflow matching with consistency training to enable high-quality dexterous action\ngeneration in just 1-2 inference steps. To handle diverse input modalities\nefficiently, we propose DiT-X, a diffusion transformer architecture with\nadaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained\nfeature interactions between action tokens and multi-modal observations.\nManiFlow demonstrates consistent improvements across diverse simulation\nbenchmarks and nearly doubles success rates on real-world tasks across\nsingle-arm, bimanual, and humanoid robot setups with increasing dexterity. The\nextensive evaluation further demonstrates the strong robustness and\ngeneralizability of ManiFlow to novel objects and background changes, and\nhighlights its strong scaling capability with larger-scale datasets. Our\nwebsite: maniflow-policy.github.io.", "AI": {"tldr": "ManiFlow是一个基于流匹配和一致性训练的视觉运动模仿学习策略，能够通过1-2次推理步骤生成高精度的机器人操作动作，支持多模态输入并显著提升各种机器人设置的成功率。", "motivation": "为了解决机器人操作中需要生成精确、高维动作的问题，同时处理多样化的视觉、语言和本体感觉输入，需要一个能够高效整合多模态信息并快速生成高质量动作的解决方案。", "method": "采用流匹配与一致性训练相结合的方法，提出DiT-X扩散变换器架构，具有自适应交叉注意力和AdaLN-Zero条件机制，实现动作令牌与多模态观察之间的细粒度特征交互。", "result": "在多样化仿真基准测试中表现一致提升，在真实世界任务中成功率几乎翻倍，涵盖单臂、双臂和人形机器人设置，展现出对新颖物体和背景变化的强鲁棒性和泛化能力。", "conclusion": "ManiFlow通过创新的架构设计和训练方法，显著提升了机器人操作的性能和效率，展示了强大的扩展能力，为通用机器人操作提供了有效的解决方案。"}}
{"id": "2509.01836", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01836", "abs": "https://arxiv.org/abs/2509.01836", "authors": ["Md Mahbub Alam", "Jose F. Rodrigues-Jr", "Gabriel Spadon"], "title": "Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment", "comment": null, "summary": "Accurate vessel trajectory prediction is essential for enhancing situational\nawareness and preventing collisions. Still, existing data-driven models are\nconstrained mainly to single-vessel forecasting, overlooking vessel\ninteractions, navigation rules, and explicit collision risk assessment. We\npresent a transformer-based framework for multi-vessel trajectory prediction\nwith integrated collision risk analysis. For a given target vessel, the\nframework identifies nearby vessels. It jointly predicts their future\ntrajectories through parallel streams encoding kinematic and derived physical\nfeatures, causal convolutions for temporal locality, spatial transformations\nfor positional encoding, and hybrid positional embeddings that capture both\nlocal motion patterns and long-range dependencies. Evaluated on large-scale\nreal-world AIS data using joint multi-vessel metrics, the model demonstrates\nsuperior forecasting capabilities beyond traditional single-vessel displacement\nerrors. By simulating interactions among predicted trajectories, the framework\nfurther quantifies potential collision risks, offering actionable insights to\nstrengthen maritime safety and decision support.", "AI": {"tldr": "提出基于Transformer的多船舶轨迹预测框架，集成碰撞风险分析，通过并行流编码运动特征、因果卷积处理时序、空间变换进行位置编码，在真实AIS数据上表现优于传统单船预测方法。", "motivation": "现有数据驱动模型主要局限于单船预测，忽略了船舶交互、航行规则和显式碰撞风险评估，需要开发能够同时处理多船交互和碰撞风险分析的预测框架。", "method": "基于Transformer的框架，通过并行流编码运动学和物理特征，使用因果卷积处理时序局部性，空间变换进行位置编码，混合位置嵌入捕获局部运动模式和长程依赖关系。", "result": "在大规模真实AIS数据上评估，使用联合多船指标，模型展现出超越传统单船位移误差的优越预测能力，能够通过预测轨迹的交互模拟量化潜在碰撞风险。", "conclusion": "该框架为加强海上安全和决策支持提供了可行的见解，通过集成多船轨迹预测和碰撞风险分析，显著提升了海上态势感知和防碰撞能力。"}}
{"id": "2509.01878", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01878", "abs": "https://arxiv.org/abs/2509.01878", "authors": ["Scarlett Raine", "Tobias Fischer"], "title": "AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring", "comment": "9 pages, 3 figures", "summary": "Marine ecosystems face increasing pressure due to climate change, driving the\nneed for scalable, AI-powered monitoring solutions. This paper examines the\nrapid emergence of underwater AI as a major research frontier and analyzes the\nfactors that have transformed marine perception from a niche application into a\ncatalyst for AI innovation. We identify three convergent drivers: environmental\nnecessity for ecosystem-scale monitoring, democratization of underwater\ndatasets through citizen science platforms, and researcher migration from\nsaturated terrestrial computer vision domains. Our analysis reveals how unique\nunderwater challenges - turbidity, cryptic species detection, expert annotation\nbottlenecks, and cross-ecosystem generalization - are driving fundamental\nadvances in weakly supervised learning, open-set recognition, and robust\nperception under degraded conditions. We survey emerging trends in datasets,\nscene understanding and 3D reconstruction, highlighting the paradigm shift from\npassive observation toward AI-driven, targeted intervention capabilities. The\npaper demonstrates how underwater constraints are pushing the boundaries of\nfoundation models, self-supervised learning, and perception, with\nmethodological innovations that extend far beyond marine applications to\nbenefit general computer vision, robotics, and environmental monitoring.", "AI": {"tldr": "本文分析了水下AI作为新兴研究前沿的快速发展，探讨了海洋感知从利基应用转变为AI创新催化剂的三大驱动因素：环境监测需求、数据民主化和研究人员迁移，并展示了水下挑战如何推动弱监督学习、开放集识别等基础AI技术进步。", "motivation": "应对气候变化对海洋生态系统的压力，需要可扩展的AI监测解决方案。水下AI的快速发展为海洋感知提供了新的技术路径，从被动观测转向AI驱动的主动干预能力。", "method": "通过分析三大驱动因素（环境必要性、数据民主化、研究人员迁移）和独特的水下挑战（浑浊度、隐秘物种检测、专家标注瓶颈、跨生态系统泛化），系统研究水下AI如何推动弱监督学习、开放集识别和鲁棒感知等基础AI技术进步。", "result": "研究发现水下约束正在推动基础模型、自监督学习和感知技术的边界拓展，这些方法论创新不仅适用于海洋应用，还惠及通用计算机视觉、机器人和环境监测领域。", "conclusion": "水下AI已成为AI创新的重要催化剂，其独特挑战推动了基础AI技术的突破性发展，这些技术进步具有超越海洋应用的广泛影响和价值。"}}
{"id": "2509.01944", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01944", "abs": "https://arxiv.org/abs/2509.01944", "authors": ["Zhenlong Yuan", "Jing Tang", "Jinguo Luo", "Rui Chen", "Chengxuan Qian", "Lei Sun", "Xiangxiang Chu", "Yujun Cai", "Dapeng Zhang", "Shuo Li"], "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving", "comment": null, "summary": "Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.", "AI": {"tldr": "AutoDrive-R²是一个新的视觉-语言-动作模型框架，通过思维链推理和强化学习提升自动驾驶系统的决策可解释性和动作合理性", "motivation": "当前自动驾驶VLA模型在决策过程可解释性和动作序列合理性方面存在不足，需要增强推理和自我反思能力", "method": "提出四步逻辑链的思维链数据集nuScenesR²-6K进行监督微调，并在强化学习阶段使用GRPO算法结合物理基础奖励框架", "result": "在nuScenes和Waymo数据集上实现了最先进的性能，展现出强大的泛化能力", "conclusion": "AutoDrive-R²通过思维链推理和强化学习的结合，显著提升了自动驾驶系统的决策质量和可解释性"}}
{"id": "2509.01985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01985", "abs": "https://arxiv.org/abs/2509.01985", "authors": ["Eduardo Espindola", "Yu Tang"], "title": "Geometric Control of Mechanical Systems with Symmetries Based on Sliding Modes", "comment": "32 pages, 3 figures, journal submission", "summary": "In this paper, we propose a framework for designing sliding mode controllers\nfor a class of mechanical systems with symmetry, both unconstrained and\nconstrained, that evolve on principal fiber bundles. Control laws are developed\nbased on the reduced motion equations by exploring symmetries, leading to a\nsliding mode control strategy where the reaching stage is executed on the base\nspace, and the sliding stage is performed on the structure group. Thus, design\ncomplexity is reduced, and difficult choices for coordinate representations\nwhen working with a particular Lie group are avoided. For this purpose, a\nsliding subgroup is constructed on the structure group based on a kinematic\ncontroller, and the sliding variable will converge to the identity of the state\nmanifold upon reaching the sliding subgroup. A reaching law based on a general\nsliding vector field is then designed on the base space using the local form of\nthe mechanical connection to drive the sliding variable to the sliding\nsubgroup, and its time evolution is given according to the appropriate\ncovariant derivative. Almost global asymptotic stability and local exponential\nstability are demonstrated using a Lyapunov analysis. We apply the results to a\nfully actuated system (a rigid spacecraft actuated by reaction wheels) and a\nsubactuated nonholonomic system (unicycle mobile robot actuated by wheels),\nwhich is also simulated for illustration.", "AI": {"tldr": "提出了一个基于主纤维丛的机械系统滑模控制框架，利用对称性简化设计，在基空间执行趋近阶段，在结构群执行滑动阶段，避免了复杂的坐标选择问题。", "motivation": "针对具有对称性的机械系统（包括无约束和约束系统），传统滑模控制在特定李群上选择坐标表示时存在困难，设计复杂度高，需要开发更简化的控制策略。", "method": "基于运动方程的降阶形式，在结构群上构建滑动子群，在基空间设计基于滑动向量场的趋近律，利用机械连接的局部形式驱动滑动变量到滑动子群，并通过协变导数给出时间演化。", "result": "通过Lyapunov分析证明了几乎全局渐近稳定性和局部指数稳定性，并将方法应用于刚性航天器（全驱动系统）和独轮车移动机器人（欠驱动非完整系统）的仿真验证。", "conclusion": "该框架成功降低了滑模控制的设计复杂度，避免了特定李群坐标表示的困难选择，为具有对称性的机械系统提供了一种有效的控制方法。"}}
{"id": "2509.01996", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.01996", "abs": "https://arxiv.org/abs/2509.01996", "authors": ["Chi Sun", "Xian Wang", "Abhishek Kumar", "Chengbin Cui", "Lik-Hang Lee"], "title": "MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation", "comment": "Accepted by ISMAR 2025", "summary": "Effective human-robot interaction (HRI) in multi-object teleoperation tasks\nfaces significant challenges due to perceptual ambiguities in virtual reality\n(VR) environments and the limitations of single-modality intention recognition.\nThis paper proposes a shared control framework that combines a virtual\nadmittance (VA) model with a Multimodal-CNN-based Human Intention Perception\nNetwork (MMIPN) to enhance teleoperation performance and user experience. The\nVA model employs artificial potential fields to guide operators toward target\nobjects by adjusting admittance force and optimizing motion trajectories. MMIPN\nprocesses multimodal inputs, including gaze movement, robot motions, and\nenvironmental context, to estimate human grasping intentions, helping to\novercome depth perception challenges in VR. Our user study evaluated four\nconditions across two factors, and the results showed that MMIPN significantly\nimproved grasp success rates, while the VA model enhanced movement efficiency\nby reducing path lengths. Gaze data emerged as the most crucial input modality.\nThese findings demonstrate the effectiveness of combining multimodal cues with\nimplicit guidance in VR-based teleoperation, providing a robust solution for\nmulti-object grasping tasks and enabling more natural interactions across\nvarious applications in the future.", "AI": {"tldr": "提出结合虚拟导纳模型和多模态CNN意图感知网络的共享控制框架，通过多模态输入和人工势场引导，显著提升VR遥操作中的抓取成功率和运动效率", "motivation": "解决VR环境中多目标遥操作任务的感知模糊性和单模态意图识别限制，提升人机交互效果和用户体验", "method": "虚拟导纳模型使用人工势场引导操作者向目标物体移动，优化运动轨迹；多模态CNN网络处理凝视、机器人运动和环境上下文等多模态输入来估计抓取意图", "result": "用户研究表明，多模态意图感知网络显著提高了抓取成功率，虚拟导纳模型通过缩短路径长度提升了运动效率，凝视数据是最重要的输入模态", "conclusion": "多模态线索与隐式引导相结合在VR遥操作中效果显著，为多目标抓取任务提供了强大解决方案，有望实现更自然的人机交互"}}
{"id": "2509.02011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02011", "abs": "https://arxiv.org/abs/2509.02011", "authors": ["Beibei Zhou", "Zhiyuan Zhang", "Zhenbo Song", "Jianhui Guo", "Hui Kong"], "title": "Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions", "comment": null, "summary": "Deep learning-based LiDAR odometry is crucial for autonomous driving and\nrobotic navigation, yet its performance under adverse weather, especially\nsnowfall, remains challenging. Existing models struggle to generalize across\nconditions due to sensitivity to snow-induced noise, limiting real-world use.\nIn this work, we present an unsupervised LiDAR odometry model to close the gap\nbetween clear and snowy weather conditions. Our approach focuses on effective\ndenoising to mitigate the impact of snowflake noise and outlier points on pose\nestimation, while also maintaining computational efficiency for real-time\napplications.\n  To achieve this, we introduce a Patch Spatial Measure (PSM) module that\nevaluates the dispersion of points within each patch, enabling effective\ndetection of sparse and discrete noise.\n  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive\npoint-wise weights, enhancing their discriminative capacity within local\nregions. To support real-time performance, we first apply an intensity\nthreshold mask to quickly suppress dense snowflake clusters near the LiDAR, and\nthen perform multi-modal feature fusion to refine the point-wise weight\nprediction, improving overall robustness under adverse weather. Our model is\ntrained in clear weather conditions and rigorously tested across various\nscenarios, including snowy and dynamic. Extensive experimental results confirm\nthe effectiveness of our method, demonstrating robust performance in both clear\nand snowy weather. This advancement enhances the model's generalizability and\npaves the way for more reliable autonomous systems capable of operating across\na wider range of environmental conditions.", "AI": {"tldr": "提出了一种无监督激光雷达里程计模型，通过PSM模块和PPWP模块有效处理雪天噪声，在保持实时性能的同时提升在恶劣天气下的鲁棒性。", "motivation": "现有激光雷达里程计模型对雪天噪声敏感，难以在不同天气条件下泛化，限制了实际应用。需要解决雪天噪声对位姿估计的影响。", "method": "使用Patch Spatial Measure模块检测离散噪声，Patch Point Weight Predictor分配自适应点权重，结合强度阈值掩码和多模态特征融合实现实时去噪。", "result": "模型在晴朗和雪天条件下都表现出鲁棒性能，经过各种场景的严格测试验证了有效性。", "conclusion": "该方法增强了模型在恶劣天气下的泛化能力，为更可靠的自动驾驶系统在更广泛环境条件下运行奠定了基础。"}}
{"id": "2509.02055", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02055", "abs": "https://arxiv.org/abs/2509.02055", "authors": ["Yang Zhang", "Chenwei Wang", "Ouyang Lu", "Yuan Zhao", "Yunfei Ge", "Zhenglong Sun", "Xiu Li", "Chi Zhang", "Chenjia Bai", "Xuelong Li"], "title": "Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance", "comment": "The first three authors contributed equally", "summary": "Vision-Language-Action (VLA) models pre-trained on large, diverse datasets\nshow remarkable potential for general-purpose robotic manipulation. However, a\nprimary bottleneck remains in adapting these models to downstream tasks,\nespecially when the robot's embodiment or the task itself differs from the\npre-training data. This discrepancy leads to a significant mismatch in action\ndistributions, demanding extensive data and compute for effective fine-tuning.\nTo address this challenge, we introduce \\textbf{Align-Then-stEer\n(\\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation\nframework. \\texttt{ATE} first aligns disparate action spaces by constructing a\nunified latent space, where a variational autoencoder constrained by reverse KL\ndivergence embeds adaptation actions into modes of the pre-training action\nlatent distribution. Subsequently, it steers the diffusion- or flow-based VLA's\ngeneration process during fine-tuning via a guidance mechanism that pushes the\nmodel's output distribution towards the target domain. We conduct extensive\nexperiments on cross-embodiment and cross-task manipulation in both simulation\nand real world. Compared to direct fine-tuning of representative VLAs, our\nmethod improves the average multi-task success rate by up to \\textbf{9.8\\%} in\nsimulation and achieves a striking \\textbf{32\\% success rate gain} in a\nreal-world cross-embodiment setting. Our work presents a general and\nlightweight solution that greatly enhances the practicality of deploying VLA\nmodels to new robotic platforms and tasks.", "AI": {"tldr": "提出了ATE框架，通过动作空间对齐和引导机制，高效适应VLA模型到不同的机器人平台和任务，显著提升跨本体和跨任务的操作性能。", "motivation": "VLA模型在预训练后适应下游任务时存在动作分布不匹配的问题，需要大量数据和计算资源进行微调，限制了实际部署的实用性。", "method": "ATE框架包含两个步骤：首先使用变分自编码器构建统一潜在空间对齐动作分布，然后通过引导机制在微调过程中将模型输出推向目标域。", "result": "在仿真环境中平均多任务成功率提升9.8%，在真实世界跨本体设置中获得32%的成功率增益。", "conclusion": "ATE提供了一个通用且轻量级的解决方案，大大增强了VLA模型在新机器人平台和任务上的部署实用性。"}}
{"id": "2509.02071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02071", "abs": "https://arxiv.org/abs/2509.02071", "authors": ["Guangzhen Sun", "Ye Ding", "Xiangyang Zhu"], "title": "A Geometric Method for Base Parameter Analysis in Robot Inertia Identification Based on Projective Geometric Algebra", "comment": "20 pages, 10 figures", "summary": "This paper proposes a novel geometric method for analytically determining the\nbase inertial parameters of robotic systems. The rigid body dynamics is\nreformulated using projective geometric algebra, leading to a new\nidentification model named ``tetrahedral-point (TP)\" model. Based on the rigid\nbody TP model, coefficients in the regresoor matrix of the identification model\nare derived in closed-form, exhibiting clear geometric interpretations.\nBuilding directly from the dynamic model, three foundational principles for\nbase parameter analysis are proposed: the shared points principle, fixed points\nprinciple, and planar rotations principle. With these principles, algorithms\nare developed to automatically determine all the base parameters. The core\nalgorithm, referred to as Dynamics Regressor Nullspace Generator (DRNG),\nachieves $O(1)$-complexity theoretically following an $O(N)$-complexity\npreprocessing stage, where $N$ is the number of rigid bodies. The proposed\nmethod and algorithms are validated across four robots: Puma560, Unitree Go2, a\n2RRU-1RRS parallel kinematics mechanism (PKM), and a 2PRS-1PSR PKM. In all\ncases, the algorithms successfully identify the complete set of base\nparameters. Notably, the approach demonstrates high robustness and\ncomputational efficiency, particularly in the cases of PKMs. Through the\ncomprehensive demonstrations, the method is shown to be general, robust, and\nefficient.", "AI": {"tldr": "提出了一种基于投影几何代数的新型几何方法，用于分析确定机器人系统的基础惯性参数，开发了具有几何解释的封闭形式识别模型和高效算法。", "motivation": "传统机器人系统基础惯性参数识别方法缺乏几何直观性且计算效率有限，特别是在并联机构(PKM)中表现不佳，需要一种具有清晰几何解释和高计算效率的新方法。", "method": "使用投影几何代数重构刚体动力学，提出\"四面体点(TP)\"识别模型，基于三个基本原理(共享点、固定点、平面旋转)开发算法，核心算法DRNG在O(N)预处理后达到O(1)复杂度。", "result": "在Puma560、Unitree Go2、2RRU-1RRS和2PRS-1PSR四种机器人上成功验证，算法能完整识别所有基础参数，特别是在PKM中表现出高鲁棒性和计算效率。", "conclusion": "该方法具有通用性、鲁棒性和高效性，为机器人系统基础参数识别提供了几何直观且计算高效的解决方案，特别适用于复杂并联机构。"}}
{"id": "2509.02134", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02134", "abs": "https://arxiv.org/abs/2509.02134", "authors": ["Andrea Eirale", "Matteo Leonetti", "Marcello Chiaberge"], "title": "Learning Social Heuristics for Human-Aware Path Planning", "comment": null, "summary": "Social robotic navigation has been at the center of numerous studies in\nrecent years. Most of the research has focused on driving the robotic agent\nalong obstacle-free trajectories, respecting social distances from humans, and\npredicting their movements to optimize navigation. However, in order to really\nbe socially accepted, the robots must be able to attain certain social norms\nthat cannot arise from conventional navigation, but require a dedicated\nlearning process. We propose Heuristic Planning with Learned Social Value\n(HPLSV), a method to learn a value function encapsulating the cost of social\nnavigation, and use it as an additional heuristic in heuristic-search path\nplanning. In this preliminary work, we apply the methodology to the common\nsocial scenario of joining a queue of people, with the intention of\ngeneralizing to further human activities.", "AI": {"tldr": "提出HPLSV方法，通过学习的社交价值函数作为启发式搜索路径规划的额外启发项，解决机器人社交导航中的队列加入问题", "motivation": "传统导航方法无法满足真正的社交接受度，机器人需要学习特定的社交规范，而不仅仅是避障和保持社交距离", "method": "Heuristic Planning with Learned Social Value (HPLSV) - 学习封装社交导航成本的价值函数，并将其作为启发式搜索路径规划的额外启发项", "result": "初步应用于加入人群队列的常见社交场景，并计划推广到更多人类活动", "conclusion": "该方法为机器人实现真正的社交接受度提供了一种新的学习框架，通过专门的社交价值学习过程来获得社交规范"}}
{"id": "2509.02146", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02146", "abs": "https://arxiv.org/abs/2509.02146", "authors": ["G. de Mathelin", "C. Hartl-Nesic", "A. Kugi"], "title": "Systematic Evaluation of Trade-Offs in Motion Planning Algorithms for Optimal Industrial Robotic Work Cell Design", "comment": "This work has been accepted to IFAC for publication under a Creative\n  Commons Licence CC-BY-NC-ND", "summary": "The performance of industrial robotic work cells depends on optimizing\nvarious hyperparameters referring to the cell layout, such as robot base\nplacement, tool placement, and kinematic design. Achieving this requires a\nbilevel optimization approach, where the high-level optimization adjusts these\nhyperparameters, and the low-level optimization computes robot motions.\nHowever, computing the optimal robot motion is computationally infeasible,\nintroducing trade-offs in motion planning to make the problem tractable. These\ntrade-offs significantly impact the overall performance of the bilevel\noptimization, but their effects still need to be systematically evaluated. In\nthis paper, we introduce metrics to assess these trade-offs regarding\noptimality, time gain, robustness, and consistency. Through extensive\nsimulation studies, we investigate how simplifications in motion-level\noptimization affect the high-level optimization outcomes, balancing\ncomputational complexity with solution quality. The proposed algorithms are\napplied to find the time-optimal kinematic design for a modular robot in two\npalletization scenarios.", "AI": {"tldr": "本文提出了评估工业机器人工作单元双层优化中运动规划权衡的指标，通过仿真研究分析运动级简化对高层优化的影响，并在模块化机器人码垛场景中应用", "motivation": "工业机器人工作单元性能优化需要双层优化方法，但底层运动规划的计算复杂性引入了各种权衡，这些权衡对整体性能的影响尚未被系统评估", "method": "引入评估运动规划权衡的指标（最优性、时间增益、鲁棒性、一致性），通过广泛的仿真研究分析运动级优化简化对高层优化结果的影响", "result": "提出了系统的评估框架，在模块化机器人的两个码垛场景中成功应用所提算法找到了时间最优的运动学设计", "conclusion": "运动级优化的简化策略显著影响双层优化的整体性能，需要仔细平衡计算复杂度和解的质量，提出的评估指标为这种权衡提供了系统分析工具"}}
{"id": "2509.02163", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02163", "abs": "https://arxiv.org/abs/2509.02163", "authors": ["Wenxiao Zhang", "Xiangrui Kong", "Conan Dewitt", "Thomas Bräunl", "Jin B. Hong"], "title": "Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety", "comment": null, "summary": "Integrating large language models (LLMs) into robotic systems has\nrevolutionised embodied artificial intelligence, enabling advanced\ndecision-making and adaptability. However, ensuring reliability, encompassing\nboth security against adversarial attacks and safety in complex environments,\nremains a critical challenge. To address this, we propose a unified framework\nthat mitigates prompt injection attacks while enforcing operational safety\nthrough robust validation mechanisms. Our approach combines prompt assembling,\nstate management, and safety validation, evaluated using both performance and\nsecurity metrics. Experiments show a 30.8% improvement under injection attacks\nand up to a 325% improvement in complex environment settings under adversarial\nconditions compared to baseline scenarios. This work bridges the gap between\nsafety and security in LLM-based robotic systems, offering actionable insights\nfor deploying reliable LLM-integrated mobile robots in real-world settings. The\nframework is open-sourced with simulation and physical deployment demos at\nhttps://llmeyesim.vercel.app/", "AI": {"tldr": "提出了一个统一框架来提升LLM机器人系统的可靠性和安全性，通过提示组装、状态管理和安全验证机制，在对抗攻击和复杂环境下显著提升性能。", "motivation": "虽然大语言模型在机器人系统中实现了先进的决策和适应性，但确保系统可靠性（包括对抗攻击的安全性和复杂环境下的安全性）仍是一个关键挑战。", "method": "提出了一个统一框架，结合提示组装、状态管理和安全验证机制，通过鲁棒的验证机制来缓解提示注入攻击并强制执行操作安全性。", "result": "实验显示在注入攻击下性能提升30.8%，在复杂环境对抗条件下相比基线场景提升高达325%。", "conclusion": "这项工作弥合了基于LLM的机器人系统中安全性和安全性之间的差距，为在现实世界环境中部署可靠的LLM集成移动机器人提供了可行的见解，框架已开源。"}}
{"id": "2509.02275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02275", "abs": "https://arxiv.org/abs/2509.02275", "authors": ["Fengyi Wang", "Xiangyu Fu", "Nitish Thakor", "Gordon Cheng"], "title": "Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object and Pose Recognition Using Multimodal Signals", "comment": null, "summary": "The human somatosensory system integrates multimodal sensory feedback,\nincluding tactile, proprioceptive, and thermal signals, to enable comprehensive\nperception and effective interaction with the environment. Inspired by the\nbiological mechanism, we present a sensorized soft anthropomorphic hand\nequipped with diverse sensors designed to emulate the sensory modalities of the\nhuman hand. This system incorporates biologically inspired encoding schemes\nthat convert multimodal sensory data into spike trains, enabling\nhighly-efficient processing through Spiking Neural Networks (SNNs). By\nutilizing these neuromorphic signals, the proposed framework achieves 97.14%\naccuracy in object recognition across varying poses, significantly\noutperforming previous studies on soft hands. Additionally, we introduce a\nnovel differentiator neuron model to enhance material classification by\ncapturing dynamic thermal responses. Our results demonstrate the benefits of\nmultimodal sensory fusion and highlight the potential of neuromorphic\napproaches for achieving efficient, robust, and human-like perception in\nrobotic systems.", "AI": {"tldr": "本文提出了一种仿生多模态传感器化软体人手，采用脉冲神经网络处理多模态感官数据，在物体识别和材料分类方面取得了优异性能。", "motivation": "受人类体感系统整合触觉、本体感觉和温度信号的多模态感知机制启发，旨在为机器人系统实现高效、鲁棒且类人的感知能力。", "method": "开发了配备多种传感器的仿生软体人手，采用生物启发编码方案将多模态感官数据转换为脉冲序列，通过脉冲神经网络进行处理，并引入了新型微分神经元模型来捕捉动态热响应。", "result": "系统在不同姿态下的物体识别准确率达到97.14%，显著优于以往软体手研究，同时在材料分类方面也表现出色。", "conclusion": "多模态感官融合和神经形态方法在实现机器人系统高效、鲁棒和类人感知方面具有巨大潜力。"}}
{"id": "2509.02283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02283", "abs": "https://arxiv.org/abs/2509.02283", "authors": ["Ruibin Zhang", "Fei Gao"], "title": "Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments", "comment": null, "summary": "Accurate and robust environmental perception is crucial for robot autonomous\nnavigation. While current methods typically adopt optical sensors (e.g.,\ncamera, LiDAR) as primary sensing modalities, their susceptibility to visual\nocclusion often leads to degraded performance or complete system failure. In\nthis paper, we focus on agricultural scenarios where robots are exposed to the\nrisk of onboard sensor contamination. Leveraging radar's strong penetration\ncapability, we introduce a radar-based 3D environmental perception framework as\na viable alternative. It comprises three core modules designed for dense and\naccurate semantic perception: 1) Parallel frame accumulation to enhance\nsignal-to-noise ratio of radar raw data. 2) A diffusion model-based\nhierarchical learning framework that first filters radar sidelobe artifacts\nthen generates fine-grained 3D semantic point clouds. 3) A specifically\ndesigned sparse 3D network optimized for processing large-scale radar raw data.\nWe conducted extensive benchmark comparisons and experimental evaluations on a\nself-built dataset collected in real-world agricultural field scenes. Results\ndemonstrate that our method achieves superior structural and semantic\nprediction performance compared to existing methods, while simultaneously\nreducing computational and memory costs by 51.3% and 27.5%, respectively.\nFurthermore, our approach achieves complete reconstruction and accurate\nclassification of thin structures such as poles and wires-which existing\nmethods struggle to perceive-highlighting its potential for dense and accurate\n3D radar perception.", "AI": {"tldr": "提出基于雷达的3D环境感知框架，用于农业场景中机器人导航，通过扩散模型和并行帧积累技术解决光学传感器易受遮挡污染的问题，在精度和效率上均优于现有方法。", "motivation": "当前机器人导航主要依赖光学传感器（相机、LiDAR），但在农业场景中易受传感器污染和视觉遮挡影响导致性能下降或系统失效。雷达具有较强的穿透能力，可作为替代方案。", "method": "1) 并行帧积累增强雷达信噪比；2) 基于扩散模型的分层学习框架，先滤除雷达旁瓣伪影，再生成细粒度3D语义点云；3) 专门设计的稀疏3D网络处理大规模雷达原始数据。", "result": "在真实农业场景数据集上测试，结构性和语义预测性能优于现有方法，计算和内存成本分别降低51.3%和27.5%，能够完整重建和准确分类电线杆等细薄结构。", "conclusion": "该方法展示了密集准确3D雷达感知的潜力，特别适用于存在传感器污染风险的农业环境，为机器人自主导航提供了可靠的感知替代方案。"}}
{"id": "2509.02324", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02324", "abs": "https://arxiv.org/abs/2509.02324", "authors": ["Changshi Zhou", "Haichuan Xu", "Ningquan Gu", "Zhipeng Wang", "Bin Cheng", "Pengpeng Zhang", "Yanchao Dong", "Mitsuhiro Hayashibe", "Yanmin Zhou", "Bin He"], "title": "Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception", "comment": null, "summary": "Language-guided long-horizon manipulation of deformable objects presents\nsignificant challenges due to high degrees of freedom, complex dynamics, and\nthe need for accurate vision-language grounding. In this work, we focus on\nmulti-step cloth folding, a representative deformable-object manipulation task\nthat requires both structured long-horizon planning and fine-grained visual\nperception. To this end, we propose a unified framework that integrates a Large\nLanguage Model (LLM)-based planner, a Vision-Language Model (VLM)-based\nperception system, and a task execution module. Specifically, the LLM-based\nplanner decomposes high-level language instructions into low-level action\nprimitives, bridging the semantic-execution gap, aligning perception with\naction, and enhancing generalization. The VLM-based perception module employs a\nSigLIP2-driven architecture with a bidirectional cross-attention fusion\nmechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to\nachieve language-conditioned fine-grained visual grounding. Experiments in both\nsimulation and real-world settings demonstrate the method's effectiveness. In\nsimulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3\non seen instructions, unseen instructions, and unseen tasks, respectively. On a\nreal robot, it robustly executes multi-step folding sequences from language\ninstructions across diverse cloth materials and configurations, demonstrating\nstrong generalization in practical scenarios. Project page:\nhttps://language-guided.netlify.app/", "AI": {"tldr": "这是一个使用大语言模型和视觉-语言模型的统一框架，用于语言指导的长时序变形物体操纵任务，特别是多步衣物折叠任务。", "motivation": "解决语言指导的长时序变形物体操纵面临的挑战，包括高约束度、复杂动力学和准确的视觉-语言基础。多步衣物折叠作为代表性任务，需要结构化的长时序规划和细粒度视觉感知。", "method": "提出了一个统一框架，集成了LLM基于的规划器、VLM基于的感知系统和任务执行模块。LLM规划器将高级语言指令分解为低级动作原语；VLM感知模块采用SigLIP2驱动的架构，包含双向交叉注意力融合机制和DoRA精调。", "result": "在模拟环境中，方法在见过的指令、未见的指令和未见的任务上分别超过最先进基线2.23、1.87和33.3。在真实机器人上，能够稳健执行来自语言指令的多步折叠序列，适应不同衣物材料和配置，展现了强大的实际场景演绎能力。", "conclusion": "该统一框架通过集成LLM和VLM技术，有效解决了语言指导长时序变形物体操纵的挑战，在模拟和真实环境中都展现了良好的性能和演绎能力。"}}
{"id": "2509.02343", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02343", "abs": "https://arxiv.org/abs/2509.02343", "authors": ["Lan Wei", "Lou Genoud", "Dandan Zhang"], "title": "Physics-Informed Machine Learning with Adaptive Grids for Optical Microrobot Depth Estimation", "comment": "2025 IEEE International Conference on Cyborg and Bionic Systems (CBS\n  2025)", "summary": "Optical microrobots actuated by optical tweezers (OT) offer great potential\nfor biomedical applications such as cell manipulation and microscale assembly.\nThese tasks demand accurate three-dimensional perception to ensure precise\ncontrol in complex and dynamic biological environments. However, the\ntransparent nature of microrobots and low-contrast microscopic imaging\nchallenge conventional deep learning methods, which also require large\nannotated datasets that are costly to obtain. To address these challenges, we\npropose a physics-informed, data-efficient framework for depth estimation of\noptical microrobots. Our method augments convolutional feature extraction with\nphysics-based focus metrics, such as entropy, Laplacian of Gaussian, and\ngradient sharpness, calculated using an adaptive grid strategy. This approach\nallocates finer grids over microrobot regions and coarser grids over background\nareas, enhancing depth sensitivity while reducing computational complexity. We\nevaluate our framework on multiple microrobot types and demonstrate significant\nimprovements over baseline models. Specifically, our approach reduces mean\nsquared error (MSE) by over 60% and improves the coefficient of determination\n(R^2) across all test cases. Notably, even when trained on only 20% of the\navailable data, our model outperforms ResNet50 trained on the full dataset,\nhighlighting its robustness under limited data conditions. Our code is\navailable at: https://github.com/LannWei/CBS2025.", "AI": {"tldr": "基于物理信息的深度估计框架，通过物理重点指标与卷积神经网络结合，实现了光学微结构器人的高精度三维感知，在数据有限条件下显著提升性能", "motivation": "光学微结构器人在生物医学应用中需要准确的三维感知，但透明性质和低对比度显微镜成像给传统深度学习方法带来挑战，同时大规模标注数据成本较高", "method": "提出物理信息驱动的深度估计框架，将卷积特征提取与基于物理的重点指标（如熵、拉普拉斯和梯度销利度）相结合，采用适应性网格策略在微结构器人区域分配更细细的网格以提高深度敏感性", "result": "在多种微结构器人类型上评估，平均方差锐减60%以上，决定系数R^2在所有测试案例中都有提升。仅使用20%数据训练的模型依然超过了使用全部数据训练的ResNet50模型", "conclusion": "该方法为光学微结构器人的三维感知提供了一种数据高效、计算效率高的解决方案，在数据有限条件下保持了漂亮的性能，对于复杂生物环境中的精确控制具有重要意义"}}
{"id": "2509.02425", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.02425", "abs": "https://arxiv.org/abs/2509.02425", "authors": ["Yifan Xu", "Qianwei Wang", "Vineet Kamat", "Carol Menassa"], "title": "OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments", "comment": "32 pages, 6 figures", "summary": "Indoor built environments like homes and offices often present complex and\ncluttered layouts that pose significant challenges for individuals who are\nblind or visually impaired, especially when performing tasks that involve\nlocating and gathering multiple objects. While many existing assistive\ntechnologies focus on basic navigation or obstacle avoidance, few systems\nprovide scalable and efficient multi-object search capabilities in real-world,\npartially observable settings. To address this gap, we introduce OpenGuide, an\nassistive mobile robot system that combines natural language understanding with\nvision-language foundation models (VLM), frontier-based exploration, and a\nPartially Observable Markov Decision Process (POMDP) planner. OpenGuide\ninterprets open-vocabulary requests, reasons about object-scene relationships,\nand adaptively navigates and localizes multiple target items in novel\nenvironments. Our approach enables robust recovery from missed detections\nthrough value decay and belief-space reasoning, resulting in more effective\nexploration and object localization. We validate OpenGuide in simulated and\nreal-world experiments, demonstrating substantial improvements in task success\nrate and search efficiency over prior methods. This work establishes a\nfoundation for scalable, human-centered robotic assistance in assisted living\nenvironments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.02437", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02437", "abs": "https://arxiv.org/abs/2509.02437", "authors": ["Yanwen Zou", "Zhaoye Zhou", "Chenyang Shi", "Zewei Ye", "Junda Huang", "Yan Ding", "Bo Zhao"], "title": "U-ARM : Ultra low-cost general teleoperation interface for robot manipulation", "comment": null, "summary": "We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\$50.5 for the 6-DoF leader arm and\n\\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.", "AI": {"tldr": "U-Arm是一个低成本、快速适配的领导者-跟随者遥操作框架，可与大多数商用机械臂兼容，通过3D打印的领导者臂实现，成本仅50-57美元，数据收集效率比Joycon高39%", "motivation": "为了解决现有开源遥操作接口成本高、兼容性差的问题，开发一个低成本、快速适配的领导者-跟随者遥操作框架，使其能够与大多数商用机械臂兼容", "method": "设计三种结构不同的3D打印领导者臂，共享一致的控制逻辑；优化机械设计和伺服选择；通过机械和控制优化解决冗余自由度控制难题", "result": "6自由度版本成本50.5美元，7自由度版本56.8美元；数据收集效率比Joycon高39%；在多种操作场景中达到相当的任务成功率", "conclusion": "U-Arm是一个成功的低成本遥操作解决方案，具有优秀的兼容性和实用性，已开源所有CAD模型和仿真支持，并提供真实世界操作数据集"}}
{"id": "2509.02453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02453", "abs": "https://arxiv.org/abs/2509.02453", "authors": ["Steven Swanbeck", "Mitch Pryor"], "title": "Coral: A Unifying Abstraction Layer for Composable Robotics Software", "comment": null, "summary": "Despite the multitude of excellent software components and tools available in\nthe robotics and broader software engineering communities, successful\nintegration of software for robotic systems remains a time-consuming and\nchallenging task for users of all knowledge and skill levels. And with robotics\nsoftware often being built into tightly coupled, monolithic systems, even minor\nalterations to improve performance, adjust to changing task requirements, or\ndeploy to new hardware can require significant engineering investment. To help\nsolve this problem, this paper presents Coral, an abstraction layer for\nbuilding, deploying, and coordinating independent software components that\nmaximizes composability to allow for rapid system integration without modifying\nlow-level code. Rather than replacing existing tools, Coral complements them by\nintroducing a higher-level abstraction that constrains the integration process\nto semantically meaningful choices, reducing the configuration burden without\nlimiting adaptability to diverse domains, systems, and tasks. We describe Coral\nin detail and demonstrate its utility in integrating software for scenarios of\nincreasing complexity, including LiDAR-based SLAM and multi-robot corrosion\nmitigation tasks. By enabling practical composability in robotics software,\nCoral offers a scalable solution to a broad range of robotics system\nintegration challenges, improving component reusability, system\nreconfigurability, and accessibility to both expert and non-expert users. We\nrelease Coral open source.", "AI": {"tldr": "Coral是一个机器人软件抽象层，通过最大化组件可组合性来解决系统集成难题，无需修改底层代码即可快速集成独立软件组件。", "motivation": "机器人软件集成耗时且困难，现有系统通常紧密耦合，即使是小的改动也需要大量工程投入，阻碍了性能改进、任务调整和新硬件部署。", "method": "Coral作为抽象层，通过引入高层语义约束来补充现有工具，限制集成过程到有意义的语义选择，减少配置负担同时保持对不同领域、系统和任务的适应性。", "result": "在LiDAR SLAM和多机器人腐蚀缓解等复杂场景中验证了Coral的实用性，展示了其在提高组件可重用性、系统可重构性和用户可访问性方面的效果。", "conclusion": "Coral通过实现机器人软件的实际可组合性，为广泛的系统集成挑战提供了可扩展解决方案，并已开源发布。"}}
{"id": "2509.02478", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02478", "abs": "https://arxiv.org/abs/2509.02478", "authors": ["Haoran Li", "Yijiong Lin", "Chenghua Lu", "Max Yang", "Efi Psomopoulou", "Nathan F Lepora"], "title": "Classification of Vision-Based Tactile Sensors: A Review", "comment": "15 pages", "summary": "Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.", "AI": {"tldr": "本文提出了一种新的视觉触觉传感器分类方法，将其分为基于标记的转换原理和基于强度的转换原理两大类，并进一步细分为四种机制，提供了硬件特性的比较研究和未来研究方向。", "motivation": "视觉触觉传感器在机器人领域应用广泛，但现有技术存在多种不同的传感原理、材料组成和多模态方法，缺乏统一的分类框架来系统比较和分析这些技术。", "method": "提出基于接触转换为触觉图像的转换原理进行分类：标记基转换（检测标记位移和密度变化）和强度基转换（像素值变化映射外部干扰）。标记基进一步分为简单标记基和形态标记基，强度基分为反射层基和透明层基。", "result": "建立了系统的视觉触觉传感器分类框架，对四种传感器类型的硬件特性进行了比较研究，包括各种组合类型和常用的触觉信息解释方法。", "conclusion": "该分类方法揭示了视觉触觉传感器技术当前面临的挑战，并为未来研究提供了方向指导，有助于推动该领域的标准化和发展。"}}
{"id": "2509.02527", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02527", "abs": "https://arxiv.org/abs/2509.02527", "authors": ["Raphael Stöckner", "Pedro Roque", "Maria Charitidou", "Dimos V. Dimarogonas"], "title": "Fault-tolerant Model Predictive Control for Spacecraft", "comment": "The paper has been submitted to CDC2025", "summary": "Given the cost and critical functions of satellite constellations, ensuring\nmission longevity and safe decommissioning is essential for space\nsustainability. This article presents a Model Predictive Control for spacecraft\ntrajectory and setpoint stabilization under multiple actuation failures. The\nproposed solution allows us to efficiently control the faulty spacecraft\nenabling safe navigation towards servicing or collision-free trajectories. The\nproposed scheme ensures closed-loop asymptotic stability and is shown to be\nrecursively feasible. We demonstrate its efficacy through open-source numerical\nresults and realistic experiments using the ATMOS platform.", "AI": {"tldr": "提出基于模型预测控制的航天器轨迹控制方法，可在多执行器故障情况下实现稳定控制和安全导航", "motivation": "卫星星座成本高昂且功能关键，需要确保任务寿命和安全退役，这对空间可持续性至关重要", "method": "采用模型预测控制(MPC)方法处理航天器轨迹和设定点稳定问题，支持多执行器故障情况下的控制", "result": "该方法能够高效控制故障航天器，实现安全导航至服务或避碰轨迹，确保闭环渐近稳定性和递归可行性", "conclusion": "通过开源数值结果和ATMOS平台的真实实验验证了该方案的有效性，为航天器故障控制提供了可靠解决方案"}}
{"id": "2509.02530", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02530", "abs": "https://arxiv.org/abs/2509.02530", "authors": ["Minghuan Liu", "Zhengbang Zhu", "Xiaoshen Han", "Peng Hu", "Haotong Lin", "Xinyao Li", "Jingxiao Chen", "Jiafeng Xu", "Yichu Yang", "Yunfeng Lin", "Xinghang Li", "Yong Yu", "Weinan Zhang", "Tao Kong", "Bingyi Kang"], "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots", "comment": "32 pages, 18 figures, project page:\n  https://manipulation-as-in-simulation.github.io/", "summary": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.", "AI": {"tldr": "这篇论文提出了相机深度模型(CDMs)，通过模拟深度摄像头的噪声模式生成高质量数据，实现了从模拟到现实的无缝转换，使得仅使用模拟深度训练的策略能够在现实任务中实现良好的性能。", "motivation": "现代机器人操作主要依赖于2D视觉观测，但普遍存在性能不佳和缺乏演绎性的问题。人类在3D世界中更依赖于物理属性而非纹理，因此希望通过深度摄像头为机器人提供类似的3D感知能力。", "method": "提出相机深度模型(CDMs)，作为普通深度摄像头的插件，输入RGB图像和原始深度信号，输出去噪后的准确深度值。开发了神经网络数据引擎，通过模拟深度摄像头的噪声模式来生成高质量的配对数据。", "result": "CDMs实现了接近模拟级别的深度预测精度，有效缩小了模拟与现实之间的差距。在两个具有挑战性的长期限任务中，仅使用模拟深度训练的策略能够无缝演绎到现实机器人，并处理关节、反射性和细长物体，性能减退微小或无减退。", "conclusion": "该研究为利用模拟数据和3D信息来开发更具演绎性的机器人策略提供了新的思路，通过准确模拟深度摄像头的噪声特性，实现了模拟到现实的高效转换。"}}
