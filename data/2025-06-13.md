<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Leveraging LLMs for Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10093)
*Marcos Abel Zuzuárregui,Stefano Carpin*

Main category: cs.RO

TL;DR: 本文提出了一种基于大型语言模型（如ChatGPT）的端到端系统，允许用户通过自然语言指令为自主机器人分配复杂的数据收集任务，并通过IEEE任务规范标准和ROS2节点实现任务执行。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人系统已在精准农业中成功应用，但适应多样化任务仍具挑战性，尤其是终端用户通常缺乏技术专长。

Method: 利用大型语言模型（如ChatGPT）将自然语言指令转化为任务计划，采用IEEE任务规范标准编码任务，并通过ROS2节点与现有ROS库对接执行。

Result: 实验验证了大型语言模型在空间推理和复杂路径规划中的优势与局限，并展示了所提系统的解决方案。

Conclusion: 该系统通过自然语言交互降低了技术门槛，为精准农业中的机器人任务分配提供了可行方案。

Abstract: Robotics and artificial intelligence hold significant potential for advancing
precision agriculture. While robotic systems have been successfully deployed
for various tasks, adapting them to perform diverse missions remains
challenging, particularly because end users often lack technical expertise. In
this paper, we present an end-to-end system that leverages large language
models (LLMs), specifically ChatGPT, to enable users to assign complex data
collection tasks to autonomous robots using natural language instructions. To
enhance reusability, mission plans are encoded using an existing IEEE task
specification standard, and are executed on robots via ROS2 nodes that bridge
high-level mission descriptions with existing ROS libraries. Through extensive
experiments, we highlight the strengths and limitations of LLMs in this
context, particularly regarding spatial reasoning and solving complex routing
challenges, and show how our proposed implementation overcomes them.

</details>


### [2] [Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models](https://arxiv.org/abs/2506.10098)
*Christian Reichenbächer,Philipp Rank,Jochen Hipp,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本文首次将高斯混合Copula模型应用于自动驾驶系统的安全验证中，用于驾驶场景的统计建模。该模型结合了高斯混合模型的多模态表达能力和Copula的灵活性，能够分别建模边缘分布和依赖关系。


<details>
  <summary>Details</summary>
Motivation: 在基于场景的安全评估中，了解场景参数的联合概率分布至关重要，因为风险量化依赖于具体参数组合的可能性。

Method: 使用高斯混合Copula模型，并与高斯混合模型和高斯Copula模型进行比较，基于联合国第157号法规定义的场景中的真实驾驶数据进行评估。

Result: 在1800万个场景实例中，高斯混合Copula模型在似然和Sinkhorn距离方面均表现出更好的数据拟合效果。

Conclusion: 高斯混合Copula模型为未来基于场景的验证框架提供了有力的基础。

Abstract: This paper presents the first application of Gaussian Mixture Copula Models
to the statistical modeling of driving scenarios for the safety validation of
automated driving systems. Knowledge of the joint probability distribution of
scenario parameters is essential for scenario-based safety assessment, where
risk quantification depends on the likelihood of concrete parameter
combinations. Gaussian Mixture Copula Models bring together the multimodal
expressivity of Gaussian Mixture Models and the flexibility of copulas,
enabling separate modeling of marginal distributions and dependencies. We
benchmark Gaussian Mixture Copula Models against previously proposed approaches
- Gaussian Mixture Models and Gaussian Copula Models - using real-world driving
data drawn from scenarios defined in United Nations Regulation No. 157. Our
evaluation across 18 million scenario instances demonstrates that Gaussian
Mixture Copula Models provide a better fit to the data in terms of both
likelihood and Sinkhorn distance. These results suggest that Gaussian Mixture
Copula Models are a compelling foundation for future scenario-based validation
frameworks.

</details>


### [3] [One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10106)
*Marcos Abel Zuzuárregui,Mustafa Melih Toslak,Stefano Carpin*

Main category: cs.RO

TL;DR: 论文提出了一种基于自然语言的机器人任务规划器，利用大语言模型和预定义原语，使非技术用户能通过简单语言控制异构机器人，无需编程。


<details>
  <summary>Details</summary>
Motivation: 人工智能在精准农业中的应用虽提升效率，但对非技术用户来说复杂且学习曲线陡峭。本文旨在降低技术门槛，使机器人自动化更易用。

Method: 通过大语言模型和预定义原语，将人类语言转化为中间描述，供不同机器人平台执行。扩展了之前轮式机器人任务规划系统，支持更多任务类型。

Result: 系统能支持多种机器人，并能执行复杂任务请求，验证了其通用性和强大功能。

Conclusion: 该研究为精准农业中的机器人自动化提供了更易用的解决方案，为非技术用户带来了便利。

Abstract: Artificial intelligence is transforming precision agriculture, offering
farmers new tools to streamline their daily operations. While these
technological advances promise increased efficiency, they often introduce
additional complexity and steep learning curves that are particularly
challenging for non-technical users who must balance tech adoption with
existing workloads. In this paper, we present a natural language (NL) robotic
mission planner that enables non-specialists to control heterogeneous robots
through a common interface. By leveraging large language models (LLMs) and
predefined primitives, our architecture seamlessly translates human language
into intermediate descriptions that can be executed by different robotic
platforms. With this system, users can formulate complex agricultural missions
without writing any code. In the work presented in this paper, we extend our
previous system tailored for wheeled robot mission planning through a new class
of experiments involving robotic manipulation and computer vision tasks. Our
results demonstrate that the architecture is both general enough to support a
diverse set of robots and powerful enough to execute complex mission requests.
This work represents a significant step toward making robotic automation in
precision agriculture more accessible to non-technical users.

</details>


### [4] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: 提出了一种模块化的视觉语言导航框架，通过解耦视觉语言理解和动作规划，结合轻量级逻辑和冻结模型，实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言导航中计算成本高和实时部署的挑战。

Method: 使用冻结的视觉语言模型Qwen2.5-VL-7B-Instruct，结合轻量级规划逻辑，采用提示工程、结构化历史管理和双帧视觉输入策略。

Result: 在Room-to-Room基准测试中初步结果显示出泛化能力有限，但模块化设计为未来改进奠定了基础。

Conclusion: 模块化框架为高效导航系统提供了可扩展的基础，未来可通过增强环境先验和扩展多模态输入进一步优化。

Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied
AI, requiring agents to interpret natural language instructions and navigate
through visually rich, unfamiliar environments. Recent advances in large
vision-language models (LVLMs), such as CLIP and Flamingo, have significantly
improved multimodal understanding but introduced new challenges related to
computational cost and real-time deployment. In this project, we propose a
modular, plug-and-play navigation framework that decouples vision-language
understanding from action planning. By integrating a frozen vision-language
model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to
achieve flexible, fast, and adaptable navigation without extensive model
fine-tuning. Our framework leverages prompt engineering, structured history
management, and a two-frame visual input strategy to enhance decision-making
continuity across navigation steps. We evaluate our system on the Room-to-Room
benchmark within the VLN-CE setting using the Matterport3D dataset and
Habitat-Lab simulation environment. Although our initial results reveal
challenges in generalizing to unseen environments under strict evaluation
settings, our modular approach lays a foundation for scalable and efficient
navigation systems, highlighting promising directions for future improvement
through enhanced environmental priors and expanded multimodal input
integration.

</details>


### [5] [Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators](https://arxiv.org/abs/2506.10240)
*Rongfei Li,Francis Assadian*

Main category: cs.RO

TL;DR: 提出了一种基于Youla参数化的前馈-反馈自适应控制算法，用于解决3D点特征在视野外时的稳定快速运动控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有IBVS方法大多依赖视野内的3D点特征，限制了应用范围。

Method: 结合前馈-反馈自适应控制与Youla参数化，设计特征估计环和自适应控制器。

Result: 算法在视野内外均能实现稳定、精确的位姿控制，适应性强且易于工业实现。

Conclusion: 所提方法在仿真中验证了其有效性和鲁棒性，适用于多种工业机器人系统。

Abstract: Image-based visual servoing (IBVS) methods have been well developed and used
in many applications, especially in pose (position and orientation) alignment.
However, most research papers focused on developing control solutions when 3D
point features can be detected inside the field of view. This work proposes an
innovative feedforward-feedback adaptive control algorithm structure with the
Youla Parameterization method. A designed feature estimation loop ensures
stable and fast motion control when point features are outside the field of
view. As 3D point features move inside the field of view, the IBVS feedback
loop preserves the precision of the pose at the end of the control period.
Also, an adaptive controller is developed in the feedback loop to stabilize the
system in the entire range of operations. The nonlinear camera and robot
manipulator model is linearized and decoupled online by an adaptive algorithm.
The adaptive controller is then computed based on the linearized model
evaluated at current linearized point. The proposed solution is robust and easy
to implement in different industrial robotic systems. Various scenarios are
used in simulations to validate the effectiveness and robust performance of the
proposed controller.

</details>


### [6] [A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures](https://arxiv.org/abs/2506.10239)
*Maximilian Mühlbauer,Freek Stulp,Sylvain Calinon,Alin Albu-Schäffer,João Silvério*

Main category: cs.RO

TL;DR: 提出了一种统一的概率虚拟夹具框架，支持手动、半自动和全自动模式切换，结合几何感知和阻抗优化，提升任务效率和精度。


<details>
  <summary>Details</summary>
Motivation: 在保持人在环的同时，通过自适应选择最适合的触觉反馈，提高任务效率和精度。

Method: 引入基于概率动态系统的虚拟夹具，支持粗引导和精确引导，结合几何感知和阻抗优化。

Result: 实验验证了框架在多机器人上的有效性，展示了多种操作模式和编程便捷性。

Conclusion: 该框架能灵活适应不同任务需求，提升人机协作的效率和精度。

Abstract: Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the
most suitable haptic feedback for each phase of a task, based on learned or
perceived uncertainty. While keeping the human in the loop remains essential,
for instance, to ensure high precision, partial automation of certain task
phases is critical for productivity. We present a unified framework for
probabilistic VFs that seamlessly switches between manual fixtures,
semi-automated fixtures (with the human handling precise tasks), and full
autonomy. We introduce a novel probabilistic Dynamical System-based VF for
coarse guidance, enabling the robot to autonomously complete certain task
phases while keeping the human operator in the loop. For tasks requiring
precise guidance, we extend probabilistic position-based trajectory fixtures
with automation allowing for seamless human interaction as well as
geometry-awareness and optimal impedance gains. For manual tasks requiring very
precise guidance, we also extend visual servoing fixtures with the same
geometry-awareness and impedance behaviour. We validate our approach
experimentally on different robots, showcasing multiple operation modes and the
ease of programming fixtures.

</details>


### [7] [A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control](https://arxiv.org/abs/2506.10252)
*Rongfei Li,Francis Assadian*

Main category: cs.RO

TL;DR: 论文提出了一种新的控制策略，结合前馈控制器和基于Youla参数化的反馈控制器，解决了立体相机在视觉伺服控制中因过约束导致的局部极小值问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人导航和操作中，相机姿态的准确估计对任务执行至关重要。然而，视觉伺服控制中因过约束可能导致全局稳定性无法保证，相机可能陷入远离目标配置的局部极小值。

Method: 提出了一种集成前馈控制器和基于Youla参数化的反馈控制器的控制策略。

Result: 仿真结果表明，该方法能有效避免局部极小值，使相机准确高效地达到目标姿态。

Conclusion: 该方法为立体相机的精确控制提供了有效解决方案，解决了过约束问题并提升了伺服性能。

Abstract: In robot navigation and manipulation, accurately determining the camera's
pose relative to the environment is crucial for effective task execution. In
this paper, we systematically prove that this problem corresponds to the
Perspective-3-Point (P3P) formulation, where exactly three known 3D points and
their corresponding 2D image projections are used to estimate the pose of a
stereo camera. In image-based visual servoing (IBVS) control, the system
becomes overdetermined, as the 6 degrees of freedom (DoF) of the stereo camera
must align with 9 observed 2D features in the scene. When more constraints are
imposed than available DoFs, global stability cannot be guaranteed, as the
camera may become trapped in a local minimum far from the desired configuration
during servoing. To address this issue, we propose a novel control strategy for
accurately positioning a calibrated stereo camera. Our approach integrates a
feedforward controller with a Youla parameterization-based feedback controller,
ensuring robust servoing performance. Through simulations, we demonstrate that
our method effectively avoids local minima and enables the camera to reach the
desired pose accurately and efficiently.

</details>


### [8] [Learning Safe Control via On-the-Fly Bandit Exploration](https://arxiv.org/abs/2506.10279)
*Alexandre Capone,Ryan Cosner,Aaaron Ames,Sandra Hirche*

Main category: cs.RO

TL;DR: 论文提出了一种结合高斯过程与屏障函数的安全学习方法，无需备用控制器即可确保模型不确定性下的安全性。


<details>
  <summary>Details</summary>
Motivation: 在高模型不确定性下，传统安全过滤器可能失效，需要备用控制器。本文旨在通过动态数据收集解决这一问题。

Method: 使用高斯过程bandit算法动态收集数据，结合控制屏障函数确保安全性。

Result: 方法在零均值先验动力学模型下可证明安全，无需备用控制器。

Conclusion: 这是首个无需备用控制器的安全学习控制方法。

Abstract: Control tasks with safety requirements under high levels of model uncertainty
are increasingly common. Machine learning techniques are frequently used to
address such tasks, typically by leveraging model error bounds to specify
robust constraint-based safety filters. However, if the learned model
uncertainty is very high, the corresponding filters are potentially invalid,
meaning no control input satisfies the constraints imposed by the safety
filter. While most works address this issue by assuming some form of safe
backup controller, ours tackles it by collecting additional data on the fly
using a Gaussian process bandit-type algorithm. We combine a control barrier
function with a learned model to specify a robust certificate that ensures
safety if feasible. Whenever infeasibility occurs, we leverage the control
barrier function to guide exploration, ensuring the collected data contributes
toward the closed-loop system safety. By combining a safety filter with
exploration in this manner, our method provably achieves safety in a setting
that allows for a zero-mean prior dynamics model, without requiring a backup
controller. To the best of our knowledge, it is the first safe learning-based
control method that achieves this.

</details>


### [9] [RICE: Reactive Interaction Controller for Cluttered Canopy Environment](https://arxiv.org/abs/2506.10383)
*Nidhi Homey Parayil,Thierry Peynot,Chris Lehnert*

Main category: cs.RO

TL;DR: 提出了一种新型反应式控制器，用于机器臂在密集、遮挡的农业环境中安全导航，结合末端执行器位置和实时触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 传统视觉或模型依赖方法在农业冠层等密集、遮挡环境中表现不佳，需要一种能安全交互且不损伤植物的导航方法。

Method: 基于末端执行器位置和实时触觉反馈，设计了一种权衡绕过障碍和推动障碍以接近目标的交互策略。

Result: 在3种实验植物设置中，控制器成功完成所有35次试验，未损伤任何分支，且在鲁棒性和适应性上优于现有无模型控制器。

Conclusion: 该研究为密集、接触丰富的可变形环境中的安全交互奠定了基础，支持未来农业任务如修剪和收获。

Abstract: Robotic navigation in dense, cluttered environments such as agricultural
canopies presents significant challenges due to physical and visual occlusion
caused by leaves and branches. Traditional vision-based or model-dependent
approaches often fail in these settings, where physical interaction without
damaging foliage and branches is necessary to reach a target. We present a
novel reactive controller that enables safe navigation for a robotic arm in a
contact-rich, cluttered, deformable environment using end-effector position and
real-time tactile feedback. Our proposed framework's interaction strategy is
based on a trade-off between minimizing disturbance by maneuvering around
obstacles and pushing through them to move towards the target. We show that
over 35 trials in 3 experimental plant setups with an occluded target, the
proposed controller successfully reached the target in all trials without
breaking any branch and outperformed the state-of-the-art model-free controller
in robustness and adaptability. This work lays the foundation for safe,
adaptive interaction in cluttered, contact-rich deformable environments,
enabling future agricultural tasks such as pruning and harvesting in plant
canopies.

</details>


### [10] [Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks](https://arxiv.org/abs/2506.10287)
*Rohit Sonker,Alexandre Capone,Andrew Rothstein,Hiro Josep Farre Kaga,Egemen Kolemen,Jeff Schneider*

Main category: cs.RO

TL;DR: 提出一种多尺度贝叶斯优化方法，结合高频数据驱动模型与低频高斯过程，用于控制核聚变中的撕裂不稳定性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 核聚变系统动态复杂、数据质量差、硬件易故障，现有机器学习方法无法全面解决这些问题。

Method: 多尺度贝叶斯优化，整合高频数据驱动模型与低频高斯过程，通过实验间更新高斯过程快速适应新数据。

Result: 在DIII-D托卡马克实验中，成功率达到50%，比历史结果提升117%。

Conclusion: 该方法为复杂动态系统的控制提供了有效解决方案，尤其在核聚变领域表现突出。

Abstract: Machine learning algorithms often struggle to control complex real-world
systems. In the case of nuclear fusion, these challenges are exacerbated, as
the dynamics are notoriously complex, data is poor, hardware is subject to
failures, and experiments often affect dynamics beyond the experiment's
duration. Existing tools like reinforcement learning, supervised learning, and
Bayesian optimization address some of these challenges but fail to provide a
comprehensive solution. To overcome these limitations, we present a multi-scale
Bayesian optimization approach that integrates a high-frequency data-driven
dynamics model with a low-frequency Gaussian process. By updating the Gaussian
process between experiments, the method rapidly adapts to new data, refining
the predictions of the less reliable dynamical model. We validate our approach
by controlling tearing instabilities in the DIII-D nuclear fusion plant.
Offline testing on historical data shows that our method significantly
outperforms several baselines. Results on live experiments on the DIII-D
tokamak, conducted under high-performance plasma scenarios prone to
instabilities, shows a 50% success rate, marking a 117% improvement over
historical outcomes.

</details>


### [11] [Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving](https://arxiv.org/abs/2506.10317)
*Akshar Tumu,Henrik I. Christensen,Marcell Vazquez-Chanlatte,Chikao Tsuchiya,Dhaval Bhanderi*

Main category: cs.RO

TL;DR: 论文提出了一种轻量级方法，通过结合OSM地图的结构化道路元数据和道路设计手册中的车道宽度先验，改进了基于地图先验的车道拓扑预测模型SMERF，提升了车道和交通元素的检测及其关联性能。


<details>
  <summary>Details</summary>
Motivation: 车道拓扑预测是自动驾驶导航的关键，而道路环境信息的准确理解对此任务至关重要。论文观察到这些信息通常遵循自然语言编码的惯例，如反映道路结构的设计规范和体现道路功能的路名。

Method: 通过结合OSM地图的结构化道路元数据和道路设计手册中的车道宽度先验，对SMERF模型进行轻量级增强，改进车道拓扑预测。

Result: 在两个地理多样化的复杂交叉口场景中，该方法在车道和交通元素检测及其关联方面均显示出改进。使用四种拓扑感知指标全面评估模型性能。

Conclusion: 结果表明，该方法能够泛化并适应多样化的拓扑结构和条件。

Abstract: Lane-topology prediction is a critical component of safe and reliable
autonomous navigation. An accurate understanding of the road environment aids
this task. We observe that this information often follows conventions encoded
in natural language, through design codes that reflect the road structure and
road names that capture the road functionality. We augment this information in
a lightweight manner to SMERF, a map-prior-based online lane-topology
prediction model, by combining structured road metadata from OSM maps and
lane-width priors from Road design manuals with the road centerline encodings.
We evaluate our method on two geo-diverse complex intersection scenarios. Our
method shows improvement in both lane and traffic element detection and their
association. We report results using four topology-aware metrics to
comprehensively assess the model performance. These results demonstrate the
ability of our approach to generalize and scale to diverse topologies and
conditions.

</details>


### [12] [Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success](https://arxiv.org/abs/2506.10359)
*Che Wang,Jeroen van Baar,Chaitanya Mitash,Shuai Li,Dylan Randle,Weiyao Wang,Sumedh Sontakke,Kostas E. Bekris,Kapil Katyal*

Main category: cs.RO

TL;DR: 该研究展示了如何从稀疏标记的工业规模真实数据中自主学习机器人操作，以提升性能。重点研究了多吸盘机器人抓取，并通过多模态视觉编码器预测抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 在仓库等真实环境中，从杂乱堆中抓取多样物品是机器人操作的重要挑战。现有方法需满足低延迟和高吞吐量要求。

Method: 利用RGB、深度和语义分割等多模态输入评估抓取质量，结合多模态预训练和微调进行训练。

Result: 实验评估表明，多模态训练和预训练对性能至关重要，模型能在微调和推理时仅使用部分模态。

Conclusion: 多模态学习和预训练能显著提升机器人抓取性能，尤其在复杂场景下表现优异。

Abstract: This work demonstrates how autonomously learning aspects of robotic operation
from sparsely-labeled, real-world data of deployed, engineered solutions at
industrial scale can provide with solutions that achieve improved performance.
Specifically, it focuses on multi-suction robot picking and performs a
comprehensive study on the application of multi-modal visual encoders for
predicting the success of candidate robotic picks. Picking diverse items from
unstructured piles is an important and challenging task for robot manipulation
in real-world settings, such as warehouses. Methods for picking from clutter
must work for an open set of items while simultaneously meeting latency
constraints to achieve high throughput. The demonstrated approach utilizes
multiple input modalities, such as RGB, depth and semantic segmentation, to
estimate the quality of candidate multi-suction picks. The strategy is trained
from real-world item picking data, with a combination of multimodal pretrain
and finetune. The manuscript provides comprehensive experimental evaluation
performed over a large item-picking dataset, an item-picking dataset targeted
to include partial occlusions, and a package-picking dataset, which focuses on
containers, such as boxes and envelopes, instead of unpackaged items. The
evaluation measures performance for different item configurations, pick scenes,
and object types. Ablations help to understand the effects of in-domain
pretraining, the impact of different modalities and the importance of
finetuning. These ablations reveal both the importance of training over
multiple modalities but also the ability of models to learn during pretraining
the relationship between modalities so that during finetuning and inference,
only a subset of them can be used as input.

</details>


### [13] [Towards more efficient quantitative safety validation of residual risk for assisted and automated driving](https://arxiv.org/abs/2506.10363)
*Daniel Betschinske,Malte Schrimpf,Steven Peters,Kamil Klonecki,Jan Peter Karch,Moritz Lippert*

Main category: cs.RO

TL;DR: 论文探讨了如何通过减少现场操作测试（FOT）的负担来提高高级驾驶辅助系统（ADAS）和自动驾驶系统（ADS）的安全验证效率，并评估了现有方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 传统FOT方法在高自动化级别下测试成本高且效率低，需要探索更高效的方法以满足国际标准（如ISO 21448）的要求。

Method: 系统识别并评估了FOT的减少方法（RAs），基于ISO 21448推导了两个模型（通用模型和基础模型），并应用于自动紧急制动（AEB）系统。

Result: 评估发现现有方法虽有潜力，但均存在不足，且无法完全替代FOT在安全验证中的关键作用。

Conclusion: FOT在ADAS和ADS安全验证中仍不可或缺，未来需进一步研究以弥补现有方法的局限性。

Abstract: The safety validation of Advanced Driver Assistance Systems (ADAS) and
Automated Driving Systems (ADS) increasingly demands efficient and reliable
methods to quantify residual risk while adhering to international standards
such as ISO 21448. Traditionally, Field Operational Testing (FOT) has been
pivotal for macroscopic safety validation of automotive driving functions up to
SAE automation level 2. However, state-of-the-art derivations for empirical
safety demonstrations using FOT often result in impractical testing efforts,
particularly at higher automation levels. Even at lower automation levels, this
limitation - coupled with the substantial costs associated with FOT - motivates
the exploration of approaches to enhance the efficiency of FOT-based
macroscopic safety validation. Therefore, this publication systematically
identifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT,
including novel methods reported in the literature. Based on an analysis of ISO
21448, two models are derived: a generic model capturing the argumentation
components of the standard, and a base model, exemplarily applied to Automatic
Emergency Braking (AEB) systems, establishing a baseline for the real-world
driving requirement for a Quantitative Safety Validation of Residual Risk
(QSVRR). Subsequently, the RAs are assessed using four criteria:
quantifiability, threats to validity, missing links, and black box
compatibility, highlighting potential benefits, inherent limitations, and
identifying key areas for further research. Our evaluation reveals that, while
several approaches offer potential, none are free from missing links or other
substantial shortcomings. Moreover, no identified alternative can fully replace
FOT, reflecting its crucial role in the safety validation of ADAS and ADS.

</details>


### [14] [Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions](https://arxiv.org/abs/2506.10462)
*Ana Müller,Sabina Jeschke,Anja Richert*

Main category: cs.RO

TL;DR: 研究探讨了群体自适应对话设计对两种社交交互代理（SIAs）的影响，发现设计对满意度无显著影响，但为多群体交互提供了宝贵见解。


<details>
  <summary>Details</summary>
Motivation: 探索群体敏感的对话设计在社交交互代理中的效果，以提升多群体交互体验。

Method: 通过两项真实世界研究，使用Furhat社交机器人和MetaHuman虚拟代理，结合混合检索和生成模型，与188名参与者互动。

Result: 群体敏感对话设计对满意度无显著影响，但揭示了多群体交互和不同代理形态的挑战。

Conclusion: 研究为多群体交互和跨形态代理的对话设计提供了新见解，需探索多模态策略。

Abstract: This paper investigates the impact of a group-adaptive conversation design in
two socially interactive agents (SIAs) through two real-world studies. Both
SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped
with a conversational artificial intelligence (CAI) backend combining hybrid
retrieval and generative models. The studies were carried out in an in-the-wild
setting with a total of $N = 188$ participants who interacted with the SIAs -
in dyads, triads or larger groups - at a German museum. Although the results
did not reveal a significant effect of the group-sensitive conversation design
on perceived satisfaction, the findings provide valuable insights into the
challenges of adapting CAI for multi-party interactions and across different
embodiments (robot vs.\ virtual agent), highlighting the need for multimodal
strategies beyond linguistic pluralization. These insights contribute to the
fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and
broader Human-Machine Interaction (HMI), providing insights for future research
on effective dialogue adaptation in group settings.

</details>


### [15] [EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence](https://arxiv.org/abs/2506.10600)
*Wang Xinjie,Liu Liu,Cao Yu,Wu Ruiqi,Qin Wenkang,Wang Dehui,Sui Wei,Su Zhizhong*

Main category: cs.RO

TL;DR: EmbodiedGen是一个用于生成高质量、可控且逼真的3D资产的平台，支持低成本生成物理准确的3D世界，用于具身智能任务的训练与评估。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能任务依赖传统3D图形资产，成本高且真实性有限，限制了数据驱动方法的扩展性。EmbodiedGen旨在解决这些问题。

Method: EmbodiedGen包含六个模块：图像转3D、文本转3D、纹理生成、关节物体生成、场景生成和布局生成，利用生成式AI生成多样化的交互式3D世界。

Result: 生成的3D资产具有高真实性和物理准确性，可直接导入物理模拟引擎，支持下游任务的训练与评估。

Conclusion: EmbodiedGen为具身智能研究提供了低成本、高扩展性的3D世界生成工具，解决了传统方法的局限性。

Abstract: Constructing a physically realistic and accurately scaled simulated 3D world
is crucial for the training and evaluation of embodied intelligence tasks. The
diversity, realism, low cost accessibility and affordability of 3D data assets
are critical for achieving generalization and scalability in embodied AI.
However, most current embodied intelligence tasks still rely heavily on
traditional 3D computer graphics assets manually created and annotated, which
suffer from high production costs and limited realism. These limitations
significantly hinder the scalability of data driven approaches. We present
EmbodiedGen, a foundational platform for interactive 3D world generation. It
enables the scalable generation of high-quality, controllable and
photorealistic 3D assets with accurate physical properties and real-world scale
in the Unified Robotics Description Format (URDF) at low cost. These assets can
be directly imported into various physics simulation engines for fine-grained
physical control, supporting downstream tasks in training and evaluation.
EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key
modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object
Generation, Scene Generation and Layout Generation. EmbodiedGen generates
diverse and interactive 3D worlds composed of generative 3D assets, leveraging
generative AI to address the challenges of generalization and evaluation to the
needs of embodied intelligence related research. Code is available at
https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.

</details>


### [16] [An $O(n$)-Algorithm for the Higher-Order Kinematics and Inverse Dynamics of Serial Manipulators using Spatial Representation of Twists](https://arxiv.org/abs/2506.10686)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种基于空间表示的二阶逆动力学算法，并配合四阶正向和逆向运动学算法，用于机器人臂的优化控制，特别是基于平坦性的控制。


<details>
  <summary>Details</summary>
Motivation: 为了在机器人臂的控制中高效计算关节扭矩/力的时间导数，需要紧凑且高效的算法。

Method: 采用空间表示法，结合Lie群理论，提出二阶逆动力学算法和四阶运动学算法。

Result: 该方法在7自由度Franka Emika Panda机器人上得到验证。

Conclusion: Lie群表示法能以向量化参数实现高效计算，适用于机器人控制。

Abstract: Optimal control in general, and flatness-based control in particular, of
robotic arms necessitate to compute the first and second time derivatives of
the joint torques/forces required to achieve a desired motion. In view of the
required computational efficiency, recursive $O(n)$-algorithms were proposed to
this end. Aiming at compact yet efficient formulations, a Lie group formulation
was recently proposed, making use of body-fixed and hybrid representation of
twists and wrenches. In this paper a formulation is introduced using the
spatial representation. The second-order inverse dynamics algorithm is
accompanied by a fourth-order forward and inverse kinematics algorithm. An
advantage of all Lie group formulations is that they can be parameterized in
terms of vectorial quantities that are readily available. The method is
demonstrated for the 7 DOF Franka Emika Panda robot.

</details>


### [17] [Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding](https://arxiv.org/abs/2506.10756)
*Yuhang Zhang,Haosheng Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: VLFly框架通过结合大型语言模型和视觉语言模型，实现了无人机在复杂环境中基于语言指令的连续速度控制导航，显著提升了泛化能力和实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉与语言导航（VLN）中的泛化问题和固定离散动作空间的局限性，特别是在无人机（UAV）导航中。

Method: VLFly整合了指令编码器（基于LLM）、目标检索器（基于VLM）和路径规划器，通过单目摄像头实现连续速度控制。

Result: 在多样化仿真和真实环境中，VLFly无需微调即超越基线方法，展示了强大的开放词汇目标理解和泛化导航能力。

Conclusion: VLFly为无人机语言导航提供了高效、泛化的解决方案，适用于复杂环境和抽象语言指令。

Abstract: Vision-and-language navigation (VLN) is a long-standing challenge in
autonomous robotics, aiming to empower agents with the ability to follow human
instructions while navigating complex environments. Two key bottlenecks remain
in this field: generalization to out-of-distribution environments and reliance
on fixed discrete action spaces. To address these challenges, we propose
Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles
(UAVs) to execute language-guided flight. Without the requirement for
localization or active ranging sensors, VLFly outputs continuous velocity
commands purely from egocentric observations captured by an onboard monocular
camera. The VLFly integrates three modules: an instruction encoder based on a
large language model (LLM) that reformulates high-level language into
structured prompts, a goal retriever powered by a vision-language model (VLM)
that matches these prompts to goal images via vision-language similarity, and a
waypoint planner that generates executable trajectories for real-time UAV
control. VLFly is evaluated across diverse simulation environments without
additional fine-tuning and consistently outperforms all baselines. Moreover,
real-world VLN tasks in indoor and outdoor environments under direct and
indirect instructions demonstrate that VLFly achieves robust open-vocabulary
goal understanding and generalized navigation capabilities, even in the
presence of abstract language input.

</details>


### [18] [In-Hand Object Pose Estimation via Visual-Tactile Fusion](https://arxiv.org/abs/2506.10787)
*Felix Nonnengießer,Alap Kshirsagar,Boris Belousov,Jan Peters*

Main category: cs.RO

TL;DR: 该论文提出了一种结合视觉和触觉信息的机器人手内物体姿态估计方法，通过融合RGB-D相机和触觉传感器的数据，显著提高了在遮挡情况下的估计精度。


<details>
  <summary>Details</summary>
Motivation: 视觉遮挡是视觉方法在机器人手内物体姿态估计中的主要挑战，需要结合多模态传感器数据以提高准确性。

Method: 采用加权和传感器融合模块结合异构传感器的点云数据，并使用改进的加权点云ICP算法估计6D物体姿态。

Result: 实验表明，加入触觉信息显著提高了姿态估计精度，平均误差为7.5毫米和16.7度，比纯视觉方法提升20%。

Conclusion: 该方法在遮挡情况下表现优异，并能支持精确的物体操作任务。

Abstract: Accurate in-hand pose estimation is crucial for robotic object manipulation,
but visual occlusion remains a major challenge for vision-based approaches.
This paper presents an approach to robotic in-hand object pose estimation,
combining visual and tactile information to accurately determine the position
and orientation of objects grasped by a robotic hand. We address the challenge
of visual occlusion by fusing visual information from a wrist-mounted RGB-D
camera with tactile information from vision-based tactile sensors mounted on
the fingertips of a robotic gripper. Our approach employs a weighting and
sensor fusion module to combine point clouds from heterogeneous sensor types
and control each modality's contribution to the pose estimation process. We use
an augmented Iterative Closest Point (ICP) algorithm adapted for weighted point
clouds to estimate the 6D object pose. Our experiments show that incorporating
tactile information significantly improves pose estimation accuracy,
particularly when occlusion is high. Our method achieves an average pose
estimation error of 7.5 mm and 16.7 degrees, outperforming vision-only
baselines by up to 20%. We also demonstrate the ability of our method to
perform precise object manipulation in a real-world insertion task.

</details>


### [19] [RationalVLA: A Rational Vision-Language-Action Model with Dual System](https://arxiv.org/abs/2506.10826)
*Wenxuan Song,Jiayi Chen,Wenxue Li,Xu He,Han Zhao,Pengxiang Ding Shiyan Su,Feilong Tang,Xuelian Cheng,Donglin Wang,Zongyuan Ge,Xinhu Zheng,Zhe Liu,Hesheng Wang,Yunhui Liu,Haoang Li*

Main category: cs.RO

TL;DR: 论文提出了RAMA基准和RationalVLA模型，用于处理机器人执行自然语言指令时的模糊、无关或不可行问题，显著提升了任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有语言条件操纵任务假设指令与环境完美对齐，限制了在现实场景中的鲁棒性和泛化能力。

Method: 构建包含14,000多样本的数据集，提出RationalVLA模型，结合高层视觉语言模型与低层操纵策略，通过可学习潜在空间嵌入实现指令推理与拒绝。

Result: RationalVLA在RAMA基准上比现有方法成功率提高14.5%，任务长度缩短0.94，同时在标准任务中保持竞争力。

Conclusion: RationalVLA在现实应用中表现出高效性和鲁棒性，为机器人自然语言操纵提供了新解决方案。

Abstract: A fundamental requirement for real-world robotic deployment is the ability to
understand and respond to natural language instructions. Existing
language-conditioned manipulation tasks typically assume that instructions are
perfectly aligned with the environment. This assumption limits robustness and
generalization in realistic scenarios where instructions may be ambiguous,
irrelevant, or infeasible. To address this problem, we introduce RAtional
MAnipulation (RAMA), a new benchmark that challenges models with both unseen
executable instructions and defective ones that should be rejected. In RAMA, we
construct a dataset with over 14,000 samples, including diverse defective
instructions spanning six dimensions: visual, physical, semantic, motion,
safety, and out-of-context. We further propose the Rational
Vision-Language-Action model (RationalVLA). It is a dual system for robotic
arms that integrates the high-level vision-language model with the low-level
manipulation policy by introducing learnable latent space embeddings. This
design enables RationalVLA to reason over instructions, reject infeasible
commands, and execute manipulation effectively. Experiments demonstrate that
RationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher
success rate and 0.94 average task length, while maintaining competitive
performance on standard manipulation tasks. Real-world trials further validate
its effectiveness and robustness in practical applications. Our project page is
https://irpn-eai.github.io/rationalvla.

</details>


### [20] [Invariant Extended Kalman Filter for Autonomous Surface Vessels with Partial Orientation Measurements](https://arxiv.org/abs/2506.10850)
*Derek Benham,Easton Potokar,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种基于不变扩展卡尔曼滤波器（InEKF）的框架，用于整合部分方向测量，以提升自主水面船（ASV）在开放海洋环境中的状态估计精度。


<details>
  <summary>Details</summary>
Motivation: 由于ASV在开放海洋中主要观测后退的地平线，传统依赖固定地标的相对位置测量方法不适用，因此需要一种新的方法来利用部分方向信息（如滚动和俯仰）。

Method: 利用前向单目相机估计滚动和俯仰，结合双天线GPS航向测量，提出了一种新的InEKF框架，专门处理部分方向数据。

Result: 实验表明，所提出的方法在开放海洋环境中对ASV状态估计具有高效性和鲁棒性。

Conclusion: 该框架为ASV在缺乏完整方向信息的环境中的状态估计提供了有效解决方案。

Abstract: Autonomous surface vessels (ASVs) are increasingly vital for marine science,
offering robust platforms for underwater mapping and inspection. Accurate state
estimation, particularly of vehicle pose, is paramount for precise seafloor
mapping, as even small surface deviations can have significant consequences
when sensing the seafloor below. To address this challenge, we propose an
Invariant Extended Kalman Filter (InEKF) framework designed to integrate
partial orientation measurements. While conventional estimation often relies on
relative position measurements to fixed landmarks, open ocean ASVs primarily
observe a receding horizon. We leverage forward-facing monocular cameras to
estimate roll and pitch with respect to this horizon, which provides
yaw-ambiguous partial orientation information. To effectively utilize these
measurements within the InEKF, we introduce a novel framework for incorporating
such partial orientation data. This approach contrasts with traditional InEKF
implementations that assume full orientation measurements and is particularly
relevant for planar vehicle motion constrained to a "seafaring plane." This
paper details the developed InEKF framework; its integration with horizon-based
roll/pitch observations and dual-antenna GPS heading measurements for ASV state
estimation; and provides a comparative analysis against the InEKF using full
orientation and a Multiplicative EKF (MEKF). Our results demonstrate the
efficacy and robustness of the proposed partial orientation measurements for
accurate ASV state estimation in open ocean environments.

</details>


### [21] [Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material](https://arxiv.org/abs/2506.10875)
*Guanjin Wang,Xiangxue Zhao,Shapour Azarm,Balakumar Balachandran*

Main category: cs.RO

TL;DR: 提出了一种基于数据驱动的建模方法，用于分析机器人与颗粒地形在特定尺度下的运动交互，结合降维、代理建模和数据同化技术，显著减少计算时间并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 研究机器人与复杂颗粒地形的交互行为，传统物理模拟计算成本高，需一种高效且准确的数据驱动方法。

Method: 集成降维（ST-HOSVD）、代理建模（高斯过程）和数据同化技术（降阶粒子滤波），利用离线高保真模拟数据和稀疏实验数据。

Result: 计算时间大幅减少，预测精度与模拟相当；结合实验数据时，长期预测优于纯模拟。还能复现物理模拟的尺度关系。

Conclusion: 该方法为机器人在未知复杂地形中的导航和探索提供了高效工具，适用于在线和离线场景。

Abstract: An alternative data-driven modeling approach has been proposed and employed
to gain fundamental insights into robot motion interaction with granular
terrain at certain length scales. The approach is based on an integration of
dimension reduction (Sequentially Truncated Higher-Order Singular Value
Decomposition), surrogate modeling (Gaussian Process), and data assimilation
techniques (Reduced Order Particle Filter). This approach can be used online
and is based on offline data, obtained from the offline collection of
high-fidelity simulation data and a set of sparse experimental data. The
results have shown that orders of magnitude reduction in computational time can
be obtained from the proposed data-driven modeling approach compared with
physics-based high-fidelity simulations. With only simulation data as input,
the data-driven prediction technique can generate predictions that have
comparable accuracy as simulations. With both simulation data and sparse
physical experimental measurement as input, the data-driven approach with its
embedded data assimilation techniques has the potential in outperforming only
high-fidelity simulations for the long-horizon predictions. In addition, it is
demonstrated that the data-driven modeling approach can also reproduce the
scaling relationship recovered by physics-based simulations for maximum
resistive forces, which may indicate its general predictability beyond a
case-by-case basis. The results are expected to help robot navigation and
exploration in unknown and complex terrains during both online and offline
phases.

</details>


### [22] [Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies](https://arxiv.org/abs/2506.10884)
*Dong Hae Mangalindan,Karthik Kandikonda,Ericka Rovira,Vaibhav Srivastava*

Main category: cs.RO

TL;DR: 研究探讨了机器人辅助配送中机器人表现和信任修复策略对人类信任的影响，发现长解释最能修复信任，而否认最能防止信任流失。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统效率和可靠性的提升，其在任务中成为人类的有力助手。研究旨在了解机器人表现和信任修复策略如何影响人类信任。

Method: 使用输入-输出隐马尔可夫模型（IOHMM）建模人类行为，分析信任动态和人类行动概率。

Result: 高信任时人类更倾向于自主部署机器人；长解释最能修复信任，否认最能防止信任流失。模型估计的信任值与自我报告值一致。

Conclusion: 模型为开发实时调整人类对自主系统信任的最优策略奠定了基础。

Abstract: With increasing efficiency and reliability, autonomous systems are becoming
valuable assistants to humans in various tasks. In the context of
robot-assisted delivery, we investigate how robot performance and trust repair
strategies impact human trust. In this task, while handling a secondary task,
humans can choose to either send the robot to deliver autonomously or manually
control it. The trust repair strategies examined include short and long
explanations, apology and promise, and denial.
  Using data from human participants, we model human behavior using an
Input-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and
human action probabilities. Our findings indicate that humans are more likely
to deploy the robot autonomously when their trust is high. Furthermore, state
transition estimates show that long explanations are the most effective at
repairing trust following a failure, while denial is most effective at
preventing trust loss.
  We also demonstrate that the trust estimates generated by our model are
isomorphic to self-reported trust values, making them interpretable. This model
lays the groundwork for developing optimal policies that facilitate real-time
adjustment of human trust in autonomous systems.

</details>


### [23] [Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations](https://arxiv.org/abs/2506.10923)
*Xili Yi,Nima Fazeli*

Main category: cs.RO

TL;DR: Vib2Move是一种利用指尖微振动和重力精确重新定位平面物体的新方法，通过动态调节摩擦系数和协调运动规划实现高精度操作。


<details>
  <summary>Details</summary>
Motivation: 解决平面物体在夹持器中的精确重新定位问题，利用振动和重力实现高精度操作。

Method: 设计振动执行器动态调节摩擦系数，建立滑动运动模型，提出协调运动规划器。

Result: 实际试验中，最终定位误差低于6毫米，适用于多种平面物体。

Conclusion: Vib2Move是一种可靠且高精度的平面物体操作方法。

Abstract: We introduce Vib2Move, a novel approach for in-hand object reconfiguration
that uses fingertip micro-vibrations and gravity to precisely reposition planar
objects. Our framework comprises three key innovations. First, we design a
vibration-based actuator that dynamically modulates the effective finger-object
friction coefficient, effectively emulating changes in gripping force. Second,
we derive a sliding motion model for objects clamped in a parallel gripper with
two symmetric, variable-friction contact patches. Third, we propose a motion
planner that coordinates end-effector finger trajectories and fingertip
vibrations to achieve the desired object pose. In real-world trials, Vib2Move
consistently yields final positioning errors below 6 mm, demonstrating
reliable, high-precision manipulation across a variety of planar objects. For
more results and information, please visit https://vib2move.github.io.

</details>


### [24] [GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation](https://arxiv.org/abs/2506.10966)
*Ning Gao,Yilun Chen,Shuai Yang,Xinyi Chen,Yang Tian,Hao Li,Haifeng Huang,Hanqing Wang,Tai Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: GenManip是一个用于研究策略泛化的仿真平台，通过LLM驱动的任务场景图生成多样化任务，并引入GenManip-Bench基准测试。结果表明，基于基础模型的模块化系统在泛化能力上优于端到端方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有仿真平台在策略泛化研究中的不足，尤其是对指令跟随基础模型（如LLMs）的适应性探索不足的问题。

Method: 开发GenManip平台，利用LLM驱动的任务场景图自动生成多样化任务，并创建GenManip-Bench基准测试。评估模块化系统和端到端策略的泛化能力。

Result: 模块化系统结合基础模型在多样化场景中表现更优，而端到端方法通过数据扩展也能提升性能。

Conclusion: GenManip平台为研究策略泛化提供了重要工具，展示了基础模型在提升泛化能力中的潜力。

Abstract: Robotic manipulation in real-world settings remains challenging, especially
regarding robust generalization. Existing simulation platforms lack sufficient
support for exploring how policies adapt to varied instructions and scenarios.
Thus, they lag behind the growing interest in instruction-following foundation
models like LLMs, whose adaptability is crucial yet remains underexplored in
fair comparisons. To bridge this gap, we introduce GenManip, a realistic
tabletop simulation platform tailored for policy generalization studies. It
features an automatic pipeline via LLM-driven task-oriented scene graph to
synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To
systematically assess generalization, we present GenManip-Bench, a benchmark of
200 scenarios refined via human-in-the-loop corrections. We evaluate two policy
types: (1) modular manipulation systems integrating foundation models for
perception, reasoning, and planning, and (2) end-to-end policies trained
through scalable data collection. Results show that while data scaling benefits
end-to-end methods, modular systems enhanced with foundation models generalize
more effectively across diverse scenarios. We anticipate this platform to
facilitate critical insights for advancing policy generalization in realistic
conditions. Project Page: https://genmanip.axi404.top/.

</details>


### [25] [Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop](https://arxiv.org/abs/2506.10968)
*Justin Kerr,Kush Hari,Ethan Weber,Chung Min Kim,Brent Yi,Tyler Bonnen,Ken Goldberg,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: EyeRobot是一个通过强化学习训练机械眼球实现任务导向凝视行为的机器人系统，结合手眼协调完成大范围工作空间的任务。


<details>
  <summary>Details</summary>
Motivation: 人类通过主动观察来完成任务，受此启发，开发具有任务导向凝视行为的机器人系统。

Method: 使用360度摄像头收集遥操作数据，在模拟环境中训练凝视策略，通过BC-RL循环联合训练手和眼。

Result: EyeRobot在五个全景工作空间任务中表现出有效的手眼协调行为，能够用单一摄像头完成大范围操作。

Conclusion: EyeRobot通过任务导向凝视行为实现了高效的手眼协调，适用于大范围工作空间的任务。

Abstract: Humans do not passively observe the visual world -- we actively look in order
to act. Motivated by this principle, we introduce EyeRobot, a robotic system
with gaze behavior that emerges from the need to complete real-world tasks. We
develop a mechanical eyeball that can freely rotate to observe its surroundings
and train a gaze policy to control it using reinforcement learning. We
accomplish this by first collecting teleoperated demonstrations paired with a
360 camera. This data is imported into a simulation environment that supports
rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze
on top of robot demonstrations. We then introduce a BC-RL loop to train the
hand and eye jointly: the hand (BC) agent is trained from rendered eye
observations, and the eye (RL) agent is rewarded when the hand produces correct
action predictions. In this way, hand-eye coordination emerges as the eye looks
towards regions which allow the hand to complete the task. EyeRobot implements
a foveal-inspired policy architecture allowing high resolution with a small
compute budget, which we find also leads to the emergence of more stable
fixation as well as improved ability to track objects and ignore distractors.
We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring
manipulation in an arc surrounding the robot arm. Our experiments suggest
EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate
manipulation over large workspaces with a single camera. See project site for
videos: https://www.eyerobot.net/

</details>
