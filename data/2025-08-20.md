<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters](https://arxiv.org/abs/2508.13303)
*Yingfan Zhou,Philip Sanderink,Sigurd Jager Lemming,Cheng Fang*

Main category: cs.RO

TL;DR: 提出可微分肌肉骨骼模型(Diff-MSM)，使用端到端自动微分技术同时识别肌肉和骨骼参数，无需测量内部关节扭矩，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 高保真个性化人体肌肉骨骼模型对于人机交互系统仿真和安全验证至关重要，但传统方法难以直接测量内部生物力学变量特别是关节扭矩

Method: 使用可微分肌肉骨骼模型，通过自动微分技术从可测量的肌肉激活到可观察的运动进行端到端参数识别

Result: 在模拟实验中显著优于现有基线方法，肌肉参数估计误差低至0.05%

Conclusion: 该方法不仅在肌肉骨骼建模方面表现出色，还有望在肌肉健康监测、康复和运动科学等领域开启新的应用

Abstract: High-fidelity personalized human musculoskeletal models are crucial for
simulating realistic behavior of physically coupled human-robot interactive
systems and verifying their safety-critical applications in simulations before
actual deployment, such as human-robot co-transportation and rehabilitation
through robotic exoskeletons. Identifying subject-specific Hill-type muscle
model parameters and bone dynamic parameters is essential for a personalized
musculoskeletal model, but very challenging due to the difficulty of measuring
the internal biomechanical variables in vivo directly, especially the joint
torques. In this paper, we propose using Differentiable MusculoSkeletal Model
(Diff-MSM) to simultaneously identify its muscle and bone parameters with an
end-to-end automatic differentiation technique differentiating from the
measurable muscle activation, through the joint torque, to the resulting
observable motion without the need to measure the internal joint torques.
Through extensive comparative simulations, the results manifested that our
proposed method significantly outperformed the state-of-the-art baseline
methods, especially in terms of accurate estimation of the muscle parameters
(i.e., initial guess sampled from a normal distribution with the mean being the
ground truth and the standard deviation being 10% of the ground truth could end
up with an average of the percentage errors of the estimated values as low as
0.05%). In addition to human musculoskeletal modeling and simulation, the new
parameter identification technique with the Diff-MSM has great potential to
enable new applications in muscle health monitoring, rehabilitation, and sports
science.

</details>


### [2] [A Surveillance Based Interactive Robot](https://arxiv.org/abs/2508.13319)
*Kshitij Kavimandan,Pooja Mangal,Devanshi Mehta*

Main category: cs.RO

TL;DR: 建立了一个基于树莓派Raspberry Pi 4的移动监控机器人，支持实时视频流、语音控制和多语言交互，可通过手机或浏览器远程监控和控制


<details>
  <summary>Details</summary>
Motivation: 开发一个使用商业硬件和开源软件的易复现监控机器人系统，支持实时视频流、语音控制和多语言交互功能

Method: 使用2个Raspberry Pi 4单元：前端单元负责差动基座、摄像头、麦克风和扬声器，中央单元负责实时流和视觉处理。使用FFmpeg传输视频，YOLOv3进行对象检测，Python语音库进行语音识别、多语言翻译和语音合成，Kinect RGB-D传感器提供视觉输入和障碍物提示

Result: 在室内测试中，机器人在CPU上以交互帧率检测常见物体，可靠识别命令，并将其转换为动作而无需手动控制

Conclusion: 设计依靠商业硬件和开源软件，易于复现。讨论了限制和实用扩展，包括与超声波距离数据的传感器融合、GPU加速以及添加人脸和文本识别功能

Abstract: We build a mobile surveillance robot that streams video in real time and
responds to speech so a user can monitor and steer it from a phone or browser.
The system uses two Raspberry Pi 4 units: a front unit on a differential drive
base with camera, mic, and speaker, and a central unit that serves the live
feed and runs perception. Video is sent with FFmpeg. Objects in the scene are
detected using YOLOv3 to support navigation and event awareness. For voice
interaction, we use Python libraries for speech recognition, multilingual
translation, and text-to-speech, so the robot can take spoken commands and read
back responses in the requested language. A Kinect RGB-D sensor provides visual
input and obstacle cues. In indoor tests the robot detects common objects at
interactive frame rates on CPU, recognises commands reliably, and translates
them to actions without manual control. The design relies on off-the-shelf
hardware and open software, making it easy to reproduce. We discuss limits and
practical extensions, including sensor fusion with ultrasonic range data, GPU
acceleration, and adding face and text recognition.

</details>


### [3] [Incremental Generalized Hybrid A*](https://arxiv.org/abs/2508.13392)
*Sidharth Talia,Oren Salzman,Siddhartha Srinivasa*

Main category: cs.RO

TL;DR: 提出IGHA*算法，一种增量式广义混合A*方法，通过动态组织顶点扩展而非刚性剪枝，在复杂动力学规划中比传统HA*减少6倍扩展次数，实现实时性能


<details>
  <summary>Details</summary>
Motivation: 解决在自动驾驶和越野自主性等应用中，复杂动力学下的实时规划问题。传统混合A*方法在网格分辨率选择上存在困难：太粗可能导致失败，太细则导致扩展过多和规划缓慢

Method: 提出增量式广义混合A*（IGHA*）框架，采用动态顶点扩展组织方式，避免刚性剪枝，可证明匹配或优于传统HA*

Result: 在汽车类机器人的道路运动学和越野动力学规划查询中，IGHA*变体比优化版HA*减少6倍扩展到最佳解的扩展次数。在高保真模拟器中，IGHA*在模型预测控制循环中优于HA*M

Conclusion: IGHA*在仿真和小规模越野车辆上展示了实时性能，能够在复杂动力学下实现快速、鲁棒的规划

Abstract: We address the problem of efficiently organizing search over very large
trees, which arises in many applications ranging from autonomous driving to
aerial vehicles. Here, we are motivated by off-road autonomy, where real-time
planning is essential. Classical approaches use graphs of motion primitives and
exploit dominance to mitigate the curse of dimensionality and prune expansions
efficiently. However, for complex dynamics, repeatedly solving two-point
boundary-value problems makes graph construction too slow for fast kinodynamic
planning. Hybrid A* (HA*) addressed this challenge by searching over a tree of
motion primitives and introducing approximate pruning using a grid-based
dominance check. However, choosing the grid resolution is difficult: too coarse
risks failure, while too fine leads to excessive expansions and slow planning.
We propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search
framework that dynamically organizes vertex expansions without rigid pruning.
IGHA* provably matches or outperforms HA*. For both on-road kinematic and
off-road kinodynamic planning queries for a car-like robot, variants of IGHA*
use 6x fewer expansions to the best solution compared to an optimized version
of HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*
outperforms HA*M when both are used in the loop with a model predictive
controller. We demonstrate real-time performance both in simulation and on a
small-scale off-road vehicle, enabling fast, robust planning under complex
dynamics. Code: https://github.com/personalrobotics/IGHAStar

</details>


### [4] [Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition](https://arxiv.org/abs/2508.13407)
*Jiming Ren,Xuan Lin,Roman Mineyev,Karen M. Feigh,Samuel Coogan,Ye Zhao*

Main category: cs.RO

TL;DR: 基于Benders分解的方法来解决双足式行走任务与动作规划中的混合整数规划问题，通过分解为主问题和子问题来提高计算效率


<details>
  <summary>Details</summary>
Motivation: 双足式行走中的任务与动作规划问题在混合整数规划中引入非凸约束后计算复杂度更高，单一优化问题解决困难

Method: 采用Benders分解技术，将问题分解为主问题（任务规划）和一系列子问题（动力学和运动学可行性检查），通过迭代切割平面技术进行解决

Result: 实验结果显示，该方法比其他解决非线性约束优化问题的算法更快完成规划

Conclusion: Benders分解技术能够有效地处理双足式行走中的复杂混合整数规划问题，提高规划效率

Abstract: Task and motion planning under Signal Temporal Logic constraints is known to
be NP-hard. A common class of approaches formulates these hybrid problems,
which involve discrete task scheduling and continuous motion planning, as
mixed-integer programs (MIP). However, in applications for bipedal locomotion,
introduction of non-convex constraints such as kinematic reachability and
footstep rotation exacerbates the computational complexity of MIPs. In this
work, we present a method based on Benders Decomposition to address scenarios
where solving the entire monolithic optimization problem is prohibitively
intractable. Benders Decomposition proposes an iterative cutting-plane
technique that partitions the problem into a master problem to prototype a plan
that meets the task specification, and a series of subproblems for kinematics
and dynamics feasibility checks. Our experiments demonstrate that this method
achieves faster planning compared to alternative algorithms for solving the
resulting optimization program with nonlinear constraints.

</details>


### [5] [Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory](https://arxiv.org/abs/2508.12681)
*Johann Licher,Max Bartholdt,Henrik Krauss,Tim-Lukas Habich,Thomas Seel,Moritz Schappler*

Main category: cs.RO

TL;DR: 基于域解耦物理信息神经网络的软连续体机器人实时非线性模型预测控制框架，实现了高精度动态轨迹跟踪


<details>
  <summary>Details</summary>
Motivation: 软连续体机器人的动态控制具有广阔应用前景，但精确动态模型计算复杂度高，现有数据驱动方法缺乏适应性且无法完整描述机器人形状

Method: 使用域解耦物理信息神经网络(DD-PINN)作为动态Cosserat柱模型的代理模型，速度提升国44000倍，并结合无病态卡尔滤波估计模型状态和弯曲顺度，实现70Hz的非线性进化模型预测控制

Result: 在模拟中实现了精确的动态轨迹跟踪，端执行器位置误差低于3mm(2.3%行程)；在实际实验中达到类似精度，最高加速度达3.55m/s²

Conclusion: 该框架为软连续体机器人提供了一种高效、准确且实时的动态控制方案，解决了传统模型计算复杂和现有方法适应性不足的问题

Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for
expanding their applications, but remains a challenging problem due to the high
computational demands of accurate dynamic models. While data-driven approaches
like Koopman-operator-based methods have been proposed, they typically lack
adaptability and cannot capture the full robot shape, limiting their
applicability. This work introduces a real-time-capable nonlinear
model-predictive control (MPC) framework for SCRs based on a domain-decoupled
physics-informed neural network (DD-PINN) with adaptable bending stiffness. The
DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a
speed-up factor of 44000. It is also used within an unscented Kalman filter for
estimating the model states and bending compliance from end-effector position
measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the
GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories
and setpoint control with end-effector position errors below 3 mm (2.3% of the
actuator's length). In real-world experiments, the controller achieves similar
accuracy and accelerations up to 3.55 m/s2.

</details>


### [6] [Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics](https://arxiv.org/abs/2508.13444)
*Tianyu Li,Jeonghwan Kim,Wontaek Kim,Donghoon Baek,Seungeun Rho,Sehoon Ha*

Main category: cs.RO

TL;DR: Switch4EAI是一个利用体感游戏（如Just Dance）来评估全身机器人控制策略的低成本系统，通过捕捉和重定向游戏中的舞蹈动作到机器人执行，为人形机器人性能评估提供量化基准。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏在真实环境中评估人形机器人运动性能并与人类进行直接比较的标准化基准，需要一种低成本、易部署的评估方法。

Method: 利用Nintendo Switch的体感游戏Just Dance，开发了一个包含动作捕捉、重建和重定向的流水线系统，在Unitree G1人形机器人上使用开源全身控制器进行验证。

Result: 系统成功实现了机器人对游戏舞蹈动作的执行，建立了机器人相对于人类玩家性能的量化基准，证明了商业游戏平台作为物理基准的可行性。

Conclusion: 商业体感游戏平台可以作为评估具身AI的有效物理基准，为未来机器人性能评估提供了新的方向和方法。

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged
robots to execute increasingly agile and coordinated movements. However,
standardized benchmarks for evaluating robotic athletic performance in
real-world settings and in direct comparison to humans remain scarce. We
present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable
pipeline that leverages motion-sensing console games to evaluate whole-body
robot control policies. Using Just Dance on the Nintendo Switch as a
representative example, our system captures, reconstructs, and retargets
in-game choreography for robotic execution. We validate the system on a Unitree
G1 humanoid with an open-source whole-body controller, establishing a
quantitative baseline for the robot's performance against a human player. In
the paper, we discuss these results, which demonstrate the feasibility of using
commercial games platform as physically grounded benchmarks and motivate future
work to for benchmarking embodied AI.

</details>


### [7] [Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle](https://arxiv.org/abs/2508.13457)
*Xu Yang,Jun Ni,Hengyang Feng,Feiyu Wang,Tiezhen Wang*

Main category: cs.RO

TL;DR: 基于新的轮轮转向半径角和侧滑角表示法，开发了全轮全向独立转向车辆的动态模型和FT-LTVMPC控制策略，实现了高精度位置和航向角同时跟随


<details>
  <summary>Details</summary>
Motivation: 解决全轮全向独立转向车辆(AWOISV)在多种运动模式下的平滑转换和精确控制问题，特别是同时跟随位置和任意航向角的挑战

Method: 提出理论轮轮转向半径角和侧滑角表示法，建立广义v-β-r动态模型，设计筛波管基线性时变MPC策略(FT-LTVMPC)

Result: 实验验证FT-LTVMPC能够实现高精度位置和航向角跟随，具有强壁性和良好的实时性能

Conclusion: 所提方法有效解决了AWOISV在全部运动模式下的精确控制问题，为类似车辆控制提供了新思路

Abstract: An all-wheel omni-directional independent steering vehicle (AWOISV) is a
specialized all-wheel independent steering vehicle with each wheel capable of
steering up to 90{\deg}, enabling unique maneuvers like yaw and diagonal
movement. This paper introduces a theoretical steering radius angle and
sideslip angle (\( \theta_R \)-\(\beta_R \)) representation, based on the
position of the instantaneous center of rotation relative to the wheel rotation
center, defining the motion modes and switching criteria for AWOISVs. A
generalized \( v\)-\(\beta\)-\(r \) dynamic model is developed with forward
velocity \(v\), sideslip angle \(\beta\), and yaw rate \(r\) as states, and
\(\theta_R\) and \(\beta_R\) as control inputs. This model decouples
longitudinal and lateral motions into forward and rotational motions, allowing
seamless transitions across all motion modes under specific conditions. A
filtered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,
achieving simultaneous tracking of lateral position and arbitrary heading
angles, with robustness to model inaccuracies and parameter uncertainties.
Co-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC
enables high-precision control of both position and heading while ensuring
excellent real-time performance.

</details>


### [8] [CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models](https://arxiv.org/abs/2508.13446)
*Catherine Glossop,William Chen,Arjun Bhorkar,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: 通过视觉语言模型生成假想标签，提升机器人数据集语言基础的细粒度和多样性，从而显著提高VLA模型的指令追随能力


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在追随细粒度指令方面遇到困难，主要原因是现有机器人数据集缺乏语言基础和语义多样性，尤其是对类似观测的细粒度任务多样性

Method: 提出一种新的数据增帽方法，利用视觉语言模型为现有机器人数据集生成假想标签，通过生成假想语言和动作来提高语言基础的多样性和细粒度

Result: 在3种不同室内外环境中进行视觉语言导航实验，结果显示假想重标注在不需额外数据收集的情况下，显著提高了VLA策略的指令追随能力，导航任务成功率提高27%

Conclusion: 假想标签方法能够有效提升VLA模型的语言指令理解和执行能力，使其在性能上可与最先进方法竞争，为通用机器人的语言理解提供了有效解决方案

Abstract: Generalist robots should be able to understand and follow user instructions,
but current vision-language-action (VLA) models struggle with following
fine-grained commands despite providing a powerful architecture for mapping
open-vocabulary natural language instructions to robot actions. One cause for
this is a lack of semantic diversity and language grounding in existing robot
datasets and, specifically, a lack of fine-grained task diversity for similar
observations. To address this, we present a novel method to augment existing
robot datasets by leveraging vision language models to create counterfactual
labels. Our method improves the language-following capabilities of VLAs by
increasing the diversity and granularity of language grounding for robot
datasets by generating counterfactual language and actions. We evaluate the
resulting model's ability to follow language instructions, ranging from simple
object-centric commands to complex referential tasks, by conducting visual
language navigation experiments in 3 different indoor and outdoor environments.
Our experiments demonstrate that counterfactual relabeling, without any
additional data collection, significantly improves instruction-following in VLA
policies, making them competitive with state-of-the-art methods and increasing
success rate by 27% on navigation tasks.

</details>


### [9] [Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms](https://arxiv.org/abs/2508.13459)
*Rohan Chandra,Shubham Singh,Abhishek Jha,Dannon Andrade,Hriday Sainathuni,Katia Sycara*

Main category: cs.RO

TL;DR: 本文是首个对社交小游戏(SMGs)导航方法的系统调研，通过统一分类体系解决领域中偏差偏差和比较困难。


<details>
  <summary>Details</summary>
Motivation: 社交小游戏(SMGs)作为自主车达到最后一公里的关键挑战，传统导航方法表现不佳，但现有研究偏差偏差严重且缺乏统一评估标准。

Method: 构建了一个统一的分类体系和定义，对现有SMG解决方法进行系统分类和评估。

Result: 完整地整理了领域内的研究状况，为不同偏差偏差和目标的方法提供了明确的分类标准。

Conclusion: 该研究为SMG导航领域建立了标准化的研究框架，有助于推动领域的可比性研究和实践应用。

Abstract: The ``Last Mile Challenge'' has long been considered an important, yet
unsolved, challenge for autonomous vehicles, public service robots, and
delivery robots. A central issue in this challenge is the ability of robots to
navigate constrained and cluttered environments (e.g., doorways, hallways,
corridor intersections), often while competing for space with other robots and
humans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs
are tightly coupled, high-agency interactions that arise within general
multi-robot navigation (MRN) scenarios. They are identified through certain
distinct characteristics and require specialized metrics to evaluate them.
Traditional navigation approaches designed for MRN do not perform well in SMGs,
which has led to focused research on dedicated SMG solvers (navigation methods
specialized to navigate in SMGs), which has flourished in recent years.
However, publications on SMG navigation research make different assumptions (on
centralized versus decentralized, observability, communication, cooperation,
etc.), and have different objective functions (safety versus liveness). These
assumptions and objectives are sometimes implicitly assumed or described
informally. This makes it difficult to establish appropriate baselines for
comparison in research papers, as well as making it difficult for practitioners
to find the papers relevant to their concrete application. Such ad-hoc
representation of the field also presents a barrier to new researchers wanting
to start research in this area. SMG navigation research requires its own
taxonomy, definitions, and evaluation protocols to guide effective research
moving forward. This survey is the first to catalog SMG solvers using a
well-defined and unified taxonomy and to classify existing methods accordingly.

</details>


### [10] [ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments](https://arxiv.org/abs/2508.13488)
*Jingwen Yu,Jiayi Yang,Anjun Hu,Jiankun Wang,Ping Tan,Hong Zhang*

Main category: cs.RO

TL;DR: ROVER是一种利用历史轨迹作为先验约束来验证闭环检测的方法，专门针对重复环境中的误检问题，通过轨迹优化和评分机制来拒绝错误的闭环候选。


<details>
  <summary>Details</summary>
Motivation: 在重复性环境中，基于外观特征的闭环检测容易产生误检，而现有方法忽略了机器人的时空运动轨迹这一重要先验知识。误检的闭环对SLAM系统可能是致命的。

Method: 提出ROVER方法：对于每个闭环候选，首先通过位姿图优化估计机器人轨迹，然后通过评分方案评估该轨迹与无闭环时的轨迹先验的符合程度，从而决定是否接受该闭环候选。

Result: 基准测试和真实世界实验证明了该方法的有效性，集成到最先进的SLAM系统中验证了其鲁棒性和效率。

Conclusion: 利用轨迹先验知识作为约束可以有效解决重复环境中闭环验证的挑战，ROVER方法在避免误检方面表现出色，代码和数据集已开源。

Abstract: Loop closure detection is important for simultaneous localization and mapping
(SLAM), which associates current observations with historical keyframes,
achieving drift correction and global relocalization. However, a falsely
detected loop can be fatal, and this is especially difficult in repetitive
environments where appearance-based features fail due to the high similarity.
Therefore, verification of a loop closure is a critical step in avoiding false
positive detections. Existing works in loop closure verification predominantly
focus on learning invariant appearance features, neglecting the prior knowledge
of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,
we propose ROVER, a loop closure verification method that leverages the
historical trajectory as a prior constraint to reject false loops in
challenging repetitive environments. For each loop candidate, it is first used
to estimate the robot trajectory with pose-graph optimization. This trajectory
is then submitted to a scoring scheme that assesses its compliance with the
trajectory without the loop, which we refer to as the trajectory prior, to
determine if the loop candidate should be accepted. Benchmark comparisons and
real-world experiments demonstrate the effectiveness of the proposed method.
Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify
its robustness and efficiency. Our source code and self-collected dataset are
available at https://github.com/jarvisyjw/ROVER.

</details>


### [11] [Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies](https://arxiv.org/abs/2508.13513)
*Maolin Lei,Edoardo Romiti,Arturo Laurenzi,Cheng Zhou,Wanli Xing,Liang Lu,Nikos G. Tsagarakis*

Main category: cs.RO

TL;DR: 统一的层次模型预测控制方法，通过高级和低级MPC的分层结构，实现多种机械手槍形态的适配性控制，无需深度参数调整


<details>
  <summary>Details</summary>
Motivation: 解决模块化机械手槍在不同形态下的控制适配性问题，避免每次更改配置都需要重新调整控制器参数

Method: 采用两层模型预测控制结构：高级MPC预测未来状态和提供轨迹信息，低级MPC根据高级信息更新预测模型并精炼控制动作，包含二次线性化技术

Result: 在多种机械手槍形态下进行了广泛评估，并在实际场景中演示了摘放任务的执行，验证了控制策略的有效性

Conclusion: 该层次H-MPC方法不仅保持了线性控制模型的简洁性，还提高了运动学表示的准确性，显著提升了整体控制精度和可靠性

Abstract: This work proposes a unified Hierarchical Model Predictive Control (H-MPC)
for modular manipulators across various morphologies, as the controller can
adapt to different configurations to execute the given task without extensive
parameter tuning in the controller. The H-MPC divides the control process into
two levels: a high-level MPC and a low-level MPC. The high-level MPC predicts
future states and provides trajectory information, while the low-level MPC
refines control actions by updating the predictive model based on this
high-level information. This hierarchical structure allows for the integration
of kinematic constraints and ensures smooth joint-space trajectories, even near
singular configurations. Moreover, the low-level MPC incorporates secondary
linearization by leveraging predictive information from the high-level MPC,
effectively capturing the second-order Taylor expansion information of the
kinematic model while still maintaining a linearized model formulation. This
approach not only preserves the simplicity of a linear control model but also
enhances the accuracy of the kinematic representation, thereby improving
overall control precision and reliability. To validate the effectiveness of the
control policy, we conduct extensive evaluations across different manipulator
morphologies and demonstrate the execution of pick-and-place tasks in
real-world scenarios.

</details>


### [12] [A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots](https://arxiv.org/abs/2508.13531)
*Bolin Li,Gewei Zuo,Zhixiang Wang,Xiaotian Ke,Lijun Zhu,Han Ding*

Main category: cs.RO

TL;DR: 提出一种三层次全身干扰抱抗控制框架(T-WB-DRC)，通过新颖的移动水平扩展状态观测器(MH-ESO)估计不确定性，提高有脚机器人的稳定性和弹性能力。


<details>
  <summary>Details</summary>
Motivation: 解决有脚机器人面临的模型不确定性、外部干扰和故障等挑战，提升运载运输、干扰抱抗和故障宽容性能力。

Method: 设计移动水平扩展状态观测器(MH-ESO)估测不确定性并降低噪声，构建三层全身干扰抱抗控制框架(T-WB-DRC)，同时考虑有无不确定性的动力学规划。

Result: 在Gazebo模拟环境中对人形和四足机器人进行了模拟，并在四足机器人上进行了实验室验证，证明框架在各种干扰条件下具有良好的稳定性和强声性。

Conclusion: T-WB-DRC框架通过加入不确定性估计和补偿机制，显著提升了有脚机器人的弹性能力，为复杂环境下的可靠运行提供了有效解决方案。

Abstract: This paper presents a control framework designed to enhance the stability and
robustness of legged robots in the presence of uncertainties, including model
uncertainties, external disturbances, and faults. The framework enables the
full-state feedback estimator to estimate and compensate for uncertainties in
whole-body dynamics of the legged robots. First, we propose a novel moving
horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate
noise in legged systems, which can be integrated into the framework for
disturbance compensation. Second, we introduce a three-level whole-body
disturbance rejection control framework (T-WB-DRC). Unlike the previous
two-level approach, this three-level framework considers both the plan based on
whole-body dynamics without uncertainties and the plan based on dynamics with
uncertainties, significantly improving payload transportation, external
disturbance rejection, and fault tolerance. Third, simulations of both humanoid
and quadruped robots in the Gazebo simulator demonstrate the effectiveness and
versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped
robot validate the robustness and stability of the system when using T-WB-DRC
under various disturbance conditions.

</details>


### [13] [MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence](https://arxiv.org/abs/2508.13534)
*Chao Tang,Anxing Xiao,Yuhong Deng,Tianrun Hu,Wenlong Dong,Hanbo Zhang,David Hsu,Hong Zhang*

Main category: cs.RO

TL;DR: MimicFunc是一个通过功能帧建立功能对应关系的框架，使机器人能够从单个人类RGB-D视频中学习工具操作技能，并泛化到新工具上，无需大量遥操作数据收集。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过观察一次工具操作就模仿并泛化到功能等效的不同工具上，而现有机器人难以实现这种级别的泛化能力。主要挑战在于处理功能相似工具之间的几何差异（功能内变异）。

Method: 提出MimicFunc框架，使用基于关键点的抽象构建功能中心局部坐标系（功能帧），建立功能级别的对应关系来模仿工具操作技能。

Result: 实验表明MimicFunc能有效使机器人从单个人类RGB-D视频泛化技能到操作新工具执行功能等效任务，生成的轨迹可用于训练视觉运动策略。

Conclusion: 该框架提供了一种直观且可扩展的方法来教授机器人工具操作技能，避免了劳动密集型的遥操作数据收集需求。

Abstract: Imitating tool manipulation from human videos offers an intuitive approach to
teaching robots, while also providing a promising and scalable alternative to
labor-intensive teleoperation data collection for visuomotor policy learning.
While humans can mimic tool manipulation behavior by observing others perform a
task just once and effortlessly transfer the skill to diverse tools for
functionally equivalent tasks, current robots struggle to achieve this level of
generalization. A key challenge lies in establishing function-level
correspondences, considering the significant geometric variations among
functionally similar tools, referred to as intra-function variations. To
address this challenge, we propose MimicFunc, a framework that establishes
functional correspondences with function frame, a function-centric local
coordinate frame constructed with keypoint-based abstraction, for imitating
tool manipulation skills. Experiments demonstrate that MimicFunc effectively
enables the robot to generalize the skill from a single RGB-D human video to
manipulating novel tools for functionally equivalent tasks. Furthermore,
leveraging MimicFunc's one-shot generalization capability, the generated
rollouts can be used to train visuomotor policies without requiring
labor-intensive teleoperation data collection for novel objects. Our code and
video are available at https://sites.google.com/view/mimicfunc.

</details>


### [14] [Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation](https://arxiv.org/abs/2508.13699)
*Maren Raab,Linda Miller,Zhe Zeng,Pascal Jansen,Martin Baumann,Johannes Kraus*

Main category: cs.RO

TL;DR: 研究探讨了机器人类型和移动模式对分心和未分心行人行为的影响，发现分心状态对行人行为无显著差异，但机器人尺寸和移动模式显著影响行人的横向适应行为。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人在公共空间的普及，需要开发能增强瞬时透明度、减少关键情况发生概率的通信策略，但目前缺乏对行人在自主机器人存在时分心行为的研究。

Method: 在实地环境中，对498名不知情的行人进行录像观察，让他们经过两个工作的自主清洁机器人，分析分心（使用手机）和未分心行人的移动行为差异。

Result: 约8%的行人因手机分心，但分心与未分心行人在机器人周围的行为无显著差异。较大的扫地机器人和偏移矩形移动模式显著增加了横向适应次数，偏移矩形模式还导致更多近距离横向适应。

Conclusion: 机器人类型和移动模式比行人分心状态更能影响行人行为，这为公共空间中人机交互研究提供了重要见解，有助于设计更安全的机器人移动策略。

Abstract: As autonomous robots become more common in public spaces, spontaneous
encounters with laypersons are more frequent. For this, robots need to be
equipped with communication strategies that enhance momentary transparency and
reduce the probability of critical situations. Adapting these robotic
strategies requires consideration of robot movements, environmental conditions,
and user characteristics and states. While numerous studies have investigated
the impact of distraction on pedestrians' movement behavior, limited research
has examined this behavior in the presence of autonomous robots. This research
addresses the impact of robot type and robot movement pattern on distracted and
undistracted pedestrians' movement behavior. In a field setting, unaware
pedestrians were videotaped while moving past two working, autonomous cleaning
robots. Out of N=498 observed pedestrians, approximately 8% were distracted by
smartphones. Distracted and undistracted pedestrians did not exhibit
significant differences in their movement behaviors around the robots. Instead,
both the larger sweeping robot and the offset rectangular movement pattern
significantly increased the number of lateral adaptations compared to the
smaller cleaning robot and the circular movement pattern. The offset
rectangular movement pattern also led to significantly more close lateral
adaptations. Depending on the robot type, the movement patterns led to
differences in the distances of lateral adaptations. The study provides initial
insights into pedestrian movement behavior around an autonomous cleaning robot
in public spaces, contributing to the growing field of HRI research.

</details>


### [15] [Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot](https://arxiv.org/abs/2508.13785)
*Liyang Liu,Ehsan Mihankhah,Nathan Wallace,Javier Martinez,Andrew J. Hill*

Main category: cs.RO

TL;DR: 开采矿山爆破孔自主检测机器人DIPPeR，通过LiDAR点云数据处理和虚拟深度图像分割技术，实现了精确的爆破孔寻找和位置定位


<details>
  <summary>Details</summary>
Motivation: 传统人工爆破孔检测速度慢、成本高，在揭示孔洞几何和地质特性方面有显著局限性，下游材料处理成本较高

Method: 使用LiDAR采集点云数据，提取地面上方钻孔废料锥体空间，将3D锥体点投影到虚拟深度图像进行2D分割，识别孔洞中心并压制非最大倾向候选者，自动调整投影参数以适应点云稀疏度和孔漏大小变化

Result: 在高保真模拟环境和现场野外测试中验证了导航和感知系统的有效性

Conclusion: 该自主检测机器人系统能够提供精确的爆破孔检测和位置定位，有助于降低开采矿山的材料处理成本

Abstract: In open-pit mining, holes are drilled into the surface of the excavation site
and detonated with explosives to facilitate digging. These blast holes need to
be inspected internally for investigation of downhole material types and
properties. Knowing these properties can lead to significant savings in
material handling costs in downstream processes. Manual hole inspection is slow
and expensive, with major limitations in revealing the geometric and geological
properties of the holes and their contents. This has been the motivation for
the development of our autonomous mine-site inspection robot - "DIPPeR". In
this paper, the automation aspect of the project is explained. We present a
robust blast hole seeking and detection framework that enables target-based
navigation and accurate down-hole sensor positioning. The pipeline first
processes point-cloud data collected by the on-board LiDAR sensors, extracting
the cone-shaped volume of drill-waste above the ground. By projecting the 3D
cone points into a virtual depth image, segmentation is achieved in the 2D
domain, yielding a circular hole at the image centre and a collared cone face.
We then identify the hole centre using a robust detection module while
suppressing non-maximum candidates, ensuring precise sensor placement for
down-hole inspection and avoiding collisions with the cavity wall. To enable
autonomous hole-seeking, the pipeline automatically adjusts its projection
parameters during robot navigation to account for variations in point sparsity
and hole opening size, ensuring a consistent hole appearance in 2D images. This
allows continuous tracking of the target hole as the robot approaches the goal
point. We demonstrate the effectiveness of our navigation and perception system
in both high-fidelity simulation environments and on-site field tests. A
demonstration video is available at
"https://www.youtube.com/watch?v=fRNbcBcaSqE".

</details>


### [16] [Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control](https://arxiv.org/abs/2508.13795)
*Haitham El-Hussieny*

Main category: cs.RO

TL;DR: 深度Koopman算子结合模型预测控制的四旋翼控制框架，通过线性化潜在空间实现高效控制


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼非线性动力学导致的控制复杂性问题，提高控制精度和降低计算时间

Method: 使甦深度Koopman算子学习飞行数据，在高维潜在空间构建线性动力学模型，然后集成模型预测控制（MPC）进行有限预测路径优化

Result: 在轨迹跟随和点稳定实验中，方法显示出更高的跟踪精度和显著更低的计算时间，较传统非线性MPC更优

Conclusion: Koopman基础学习方法能够处理复杂四旋翼动力学，满足嵌入式飞行控制的实时要求，具有很好的应用前景

Abstract: This paper presents a data-driven control framework for quadrotor systems
that integrates a deep Koopman operator with model predictive control (DK-MPC).
The deep Koopman operator is trained on sampled flight data to construct a
high-dimensional latent representation in which the nonlinear quadrotor
dynamics are approximated by linear models. This linearization enables the
application of MPC to efficiently optimize control actions over a finite
prediction horizon, ensuring accurate trajectory tracking and stabilization.
The proposed DK-MPC approach is validated through a series of
trajectory-following and point-stabilization numerical experiments, where it
demonstrates superior tracking accuracy and significantly lower computation
time compared to conventional nonlinear MPC. These results highlight the
potential of Koopman-based learning methods to handle complex quadrotor
dynamics while meeting the real-time requirements of embedded flight control.
Future work will focus on extending the framework to more agile flight
scenarios and improving robustness against external disturbances.

</details>


### [17] [Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer](https://arxiv.org/abs/2508.13877)
*Rathnam Vidushika Rasanji,Jin Wei-Kocsis,Jiansong Zhang,Dongming Gan,Ragu Athinarayanan,Paul Asunda*

Main category: cs.RO

TL;DR: 提出了SGDT框架，结合神经符号机制和因果变换器，用于多机器人协作操作任务


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人操作中数据密集且依赖MDP假设，难以处理复杂动态和长期依赖的多机器人操作任务。决策变换器作为离线替代方案，但在多机器人操作中的应用尚未充分探索

Method: SGDT框架包含神经符号规划器生成符号子目标的高级任务导向计划，以及目标条件决策变换器进行低级序列决策的层次架构

Result: 在零样本和少样本场景下评估了SGDT的性能，表明该框架能够实现结构化、可解释和可泛化的多机器人协作决策

Conclusion: 这是首个探索基于决策变换器技术在多机器人操作中的应用工作，SGDT为解决复杂多机器人协作任务提供了有效解决方案

Abstract: Reinforcement learning (RL) has demonstrated great potential in robotic
operations. However, its data-intensive nature and reliance on the Markov
Decision Process (MDP) assumption limit its practical deployment in real-world
scenarios involving complex dynamics and long-term temporal dependencies, such
as multi-robot manipulation. Decision Transformers (DTs) have emerged as a
promising offline alternative by leveraging causal transformers for sequence
modeling in RL tasks. However, their applications to multi-robot manipulations
still remain underexplored. To address this gap, we propose a novel framework,
Symbolically-Guided Decision Transformer (SGDT), which integrates a
neuro-symbolic mechanism with a causal transformer to enable deployable
multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic
planner generates a high-level task-oriented plan composed of symbolic
subgoals. Guided by these subgoals, a goal-conditioned decision transformer
(GCDT) performs low-level sequential decision-making for multi-robot
manipulation. This hierarchical architecture enables structured, interpretable,
and generalizable decision making in complex multi-robot collaboration tasks.
We evaluate the performance of SGDT across a range of task scenarios, including
zero-shot and few-shot scenarios. To our knowledge, this is the first work to
explore DT-based technology for multi-robot manipulation.

</details>


### [18] [Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models](https://arxiv.org/abs/2508.13881)
*Zhaokun Chen,Chaopeng Zhang,Xiaohan Li,Wenshuo Wang,Gentiane Venture,Junqiang Xi*

Main category: cs.RO

TL;DR: 通过大语言模型生成语义特权信息，将人类专家的理解模式融入驾驶风格识别系统，在保持计算效率的同时显著提升识别准确性


<details>
  <summary>Details</summary>
Motivation: 现有驾驶风格识别系统依赖低级传感器特征，缺乏人类专家的语义理解能力，导致算法分类与专家判断之间的差异

Method: 首先开发DriBehavGPT模块用于生成自然语言驾驶行为描述，通过文本嵌入和降维抄码为机器学习可处理表征，然后作为特权信息融入SVM+进行训练

Result: 在多种真实驾驶场景中，该框架在F1分数上比传统方法提升7.6%（跟车）和7.9%（变道），训练后推理仅需传感器数据

Conclusion: 语义行为表征在提高识别准确性和推进可解释性方面发挥关键作用，为构建人本中心的驾驶系统提供新方向

Abstract: Existing driving style recognition systems largely depend on low-level
sensor-derived features for training, neglecting the rich semantic reasoning
capability inherent to human experts. This discrepancy results in a fundamental
misalignment between algorithmic classifications and expert judgments. To
bridge this gap, we propose a novel framework that integrates Semantic
Privileged Information (SPI) derived from large language models (LLMs) to align
recognition outcomes with human-interpretable reasoning. First, we introduce
DriBehavGPT, an interactive LLM-based module that generates natural-language
descriptions of driving behaviors. These descriptions are then encoded into
machine learning-compatible representations via text embedding and
dimensionality reduction. Finally, we incorporate them as privileged
information into Support Vector Machine Plus (SVM+) for training, enabling the
model to approximate human-like interpretation patterns. Experiments across
diverse real-world driving scenarios demonstrate that our SPI-enhanced
framework outperforms conventional methods, achieving F1-score improvements of
7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively
used during training, while inference relies solely on sensor data, ensuring
computational efficiency without sacrificing performance. These results
highlight the pivotal role of semantic behavioral representations in improving
recognition accuracy while advancing interpretable, human-centric driving
systems.

</details>


### [19] [Multimodal Data Storage and Retrieval for Embodied AI: A Survey](https://arxiv.org/abs/2508.13901)
*Yihao Lu,Hao Tang*

Main category: cs.RO

TL;DR: 本调研调查了体现式AI系统的数据管理挑战，系统评估五种存储架构和五种检索范式，识别了关键瓶颈并提出了未来研究议程。


<details>
  <summary>Details</summary>
Motivation: 体现式AI以不断产生巨量、异构的多模态数据流，传统数据管理系统无法有效处理这些数据的存储和检索需求。

Method: 系统性评估五种存储架构（图数据库、多模型数据库、数据湖、向量数据库、时间序列数据库）和五种检索范式，基于对180多份相关研究的全面评审。

Result: 发现了实现长期语义一致性与保持实时响应能力之间的根本强张力，识别了从基础物理基准差距到系统性挑战的关键瓶颈。

Conclusion: 提出了包含物理感知数据模型、适应性存储-检索协同优化和标准化测试基准在内的前瞻性研究议程，为下一代自主体现系统的数据管理框架提供了严谨的路线图。

Abstract: Embodied AI (EAI) agents continuously interact with the physical world,
generating vast, heterogeneous multimodal data streams that traditional
management systems are ill-equipped to handle. In this survey, we first
systematically evaluate five storage architectures (Graph Databases,
Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series
Databases), focusing on their suitability for addressing EAI's core
requirements, including physical grounding, low-latency access, and dynamic
scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based
Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based
Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based
Optimization), revealing a fundamental tension between achieving long-term
semantic coherence and maintaining real-time responsiveness. Based on this
comprehensive analysis, we identify key bottlenecks, spanning from the
foundational Physical Grounding Gap to systemic challenges in cross-modal
integration, dynamic adaptation, and open-world generalization. Finally, we
outline a forward-looking research agenda encompassing physics-aware data
models, adaptive storage-retrieval co-optimization, and standardized
benchmarking, to guide future research toward principled data management
solutions for EAI. Our survey is based on a comprehensive review of more than
180 related studies, providing a rigorous roadmap for designing the robust,
high-performance data management frameworks essential for the next generation
of autonomous embodied systems.

</details>


### [20] [Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation](https://arxiv.org/abs/2508.13964)
*Martijn Cramer,Yanming Wu,David De Schepper,Eric Demeester*

Main category: cs.RO

TL;DR: 通过3D物体识别与定位技术提升协作机器人的机动性和重配能力，应对小批量多种类的生产挑战


<details>
  <summary>Details</summary>
Motivation: 中小企业面临小批量多种类生产带来的挑战，标准自动化方案无法满足需求，导致人工劳动成本高和技能人才浪费

Method: 集成现有技术（包括3D物体识别和定位）将协作机器人转化为移动和可重配的生产助手，并通过与产业合作伙伴的实际项目进行验证

Result: 提出了在工业环境中增强协作机器人系统的具体方法和关键步骤，并提供了实际实施案例

Conclusion: 通过技术集成和重配能力的提升，协作机器人可以成为有效解决中小企业小批量多种类生产挑战的方案

Abstract: Due to high-mix-low-volume production, sheet-metal workshops today are
challenged by small series and varying orders. As standard automation solutions
tend to fall short, SMEs resort to repetitive manual labour impacting
production costs and leading to tech-skilled workforces not being used to their
full potential. The COOCK+ ROBUST project aims to transform cobots into mobile
and reconfigurable production assistants by integrating existing technologies,
including 3D object recognition and localisation. This article explores both
the opportunities and challenges of enhancing cobotic systems with these
technologies in an industrial setting, outlining the key steps involved in the
process. Additionally, insights from a past project, carried out by the ACRO
research unit in collaboration with an industrial partner, serves as a concrete
implementation example throughout.

</details>


### [21] [Toward an Interaction-Centered Approach to Robot Trustworthiness](https://arxiv.org/abs/2508.13976)
*Carlo Mazzola,Hassan Ali,Kristína Malinovská,Igor Farkaš*

Main category: cs.RO

TL;DR: 通过互动框架建立人机信任，强调人类意识和透明性两大支枱，以避免错误信任和过度信任带来的安全风险


<details>
  <summary>Details</summary>
Motivation: 随着机器人更多积成到人类环境中，建立可信赖的体现型机器人成为致命关键，以保证有效而安全的人机交互

Method: 提出一种基于互相理解的交互框架，包含人类意识（机器人准确解释人类行为的能力）和透明性（清晰传达机器人意图和目标）两大核心支枱

Result: 让机器人能够按照人类期望和需求行动，同时为人类伙伴提供对机器人行为的理解和控制权，缩小人类感知信任与机器人真实能力之间的差距

Conclusion: 通过人类意识和透明性的集成，可以建立一种基于相互理解的人机信任关系，这对于实现有效、安全的人机交互至关重要

Abstract: As robots get more integrated into human environments, fostering
trustworthiness in embodied robotic agents becomes paramount for an effective
and safe human-robot interaction (HRI). To achieve that, HRI applications must
promote human trust that aligns with robot skills and avoid misplaced trust or
overtrust, which can pose safety risks and ethical concerns. To achieve that,
HRI applications must promote human trust that aligns with robot skills and
avoid misplaced trust or overtrust, which can pose safety risks and ethical
concerns. In this position paper, we outline an interaction-based framework for
building trust through mutual understanding between humans and robots. We
emphasize two main pillars: human awareness and transparency, referring to the
robot ability to interpret human actions accurately and to clearly communicate
its intentions and goals, respectively. By integrating these two pillars,
robots can behave in a manner that aligns with human expectations and needs
while providing their human partners with both comprehension and control over
their actions. We also introduce four components that we think are important
for bridging the gap between a human-perceived sense of trust and a robot true
capabilities.

</details>


### [22] [The Social Context of Human-Robot Interactions](https://arxiv.org/abs/2508.13982)
*Sydney Thompson,Kate Candon,Marynel Vázquez*

Main category: cs.RO

TL;DR: 这篇论文针对HRI领域中"社会情境"术语使用混乱的问题，提出了一个概念模型来统一描述人机交互的社会情境，并讨论了相关属性和开放研究问题。


<details>
  <summary>Details</summary>
Motivation: HRI研究中对"社会情境"的定义和使用方式不一致，导致沟通困难和研究成果难以整合，需要建立统一的概念框架。

Method: 通过文献调查分析现有"社会情境"定义，提出概念模型来描述人机交互的社会情境，并将该模型应用于现有研究工作。

Result: 开发了一个能够帮助研究人员规划交互、开发机器人行为模型以及分析交互后洞察的社会情境概念模型，并识别了相关的社会情境属性。

Conclusion: 论文为解决HRI领域社会情境术语混乱问题提供了概念框架，并指出了未来在理解和建模人机交互社会情境方面的开放研究问题。

Abstract: The Human-Robot Interaction (HRI) community often highlights the social
context of an interaction as a key consideration when designing, implementing,
and evaluating robot behavior. Unfortunately, researchers use the term "social
context" in varied ways. This can lead to miscommunication, making it
challenging to draw connections between related work on understanding and
modeling the social contexts of human-robot interactions. To address this gap,
we survey the HRI literature for existing definitions and uses of the term
"social context". Then, we propose a conceptual model for describing the social
context of a human-robot interaction. We apply this model to existing work, and
we discuss a range of attributes of social contexts that can help researchers
plan for interactions, develop behavior models for robots, and gain insights
after interactions have taken place. We conclude with a discussion of open
research questions in relation to understanding and modeling the social
contexts of human-robot interactions.

</details>


### [23] [Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/abs/2508.13998)
*Yifu Yuan,Haiqin Cui,Yaoting Huang,Yibin Chen,Fei Ni,Zibin Dong,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: 通过统一的指点表示作为中间表征，设计Embodied-R1 3B视觉-语言模型，使用两阶段强化学习诞练训练，在11个体现空间指点测试中达到最佳性能，并显示出凶强的零样本沿生能力。


<details>
  <summary>Details</summary>
Motivation: 解决体现AI中的"看到到做到"问题，该问题来自于数据稀缺和体现异质性。

Method: 提出指点作为统一的体现无关中间表示，建立Embodied-Points-200K大规模数据集，使用两阶段强化学习诞练(RFT)训练Embodied-R1 3B VLM模型。

Result: 在11个体现空间指点测试中达到最佳性能，在SIMPLEREnv中达到56.2%成功率，在8个实际XArm任务中达到87.5%成功率，比基线提升62%，并显示出对视觉干扰的高稳健性。

Conclusion: 指点为中心的表示结合RFT训练范式，为闭合机器人感知-动作间隔提供了有效且可普遍化的路径。

Abstract: Generalization in embodied AI is hindered by the "seeing-to-doing gap," which
stems from data scarcity and embodiment heterogeneity. To address this, we
pioneer "pointing" as a unified, embodiment-agnostic intermediate
representation, defining four core embodied pointing abilities that bridge
high-level vision-language comprehension with low-level action primitives. We
introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed
for embodied reasoning and pointing. We use a wide range of embodied and
general visual reasoning datasets as sources to construct a large-scale
dataset, Embodied-Points-200K, which supports key embodied pointing
capabilities. We then train Embodied-R1 using a two-stage Reinforced
Fine-tuning (RFT) curriculum with a specialized multi-task reward design.
Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and
pointing benchmarks. Critically, it demonstrates robust zero-shot
generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%
across 8 real-world XArm tasks without any task-specific fine-tuning,
representing a 62% improvement over strong baselines. Furthermore, the model
exhibits high robustness against diverse visual disturbances. Our work shows
that a pointing-centric representation, combined with an RFT training paradigm,
offers an effective and generalizable pathway to closing the perception-action
gap in robotics.

</details>


### [24] [Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation](https://arxiv.org/abs/2508.14042)
*Zhuoling Li,Xiaoyang Wu,Zhenhua Xu,Hengshuang Zhao*

Main category: cs.RO

TL;DR: 提出GEM系统，通过基于熵的理论框架实现仅需少量演示就能在动态物体操作中实现强泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决动态物体操作中演示数据收集成本高的问题，探索仅用少量演示就能实现强泛化的可能性

Method: 开发基于熵的理论框架来量化模仿学习的优化，并基于此提出GEM系统

Result: 在仿真和真实任务中，GEM能够泛化到不同的环境背景、机器人形态、运动动力学和物体几何形状。在真实食堂餐具收集任务中，无需场景内演示就实现了超过97%的成功率，完成超过10,000次操作

Conclusion: GEM系统证明了仅用少量演示就能在动态物体操作中实现强泛化能力，为提升制造效率提供了有效解决方案

Abstract: Realizing generalizable dynamic object manipulation is important for
enhancing manufacturing efficiency, as it eliminates specialized engineering
for various scenarios. To this end, imitation learning emerges as a promising
paradigm, leveraging expert demonstrations to teach a policy manipulation
skills. Although the generalization of an imitation learning policy can be
improved by increasing demonstrations, demonstration collection is
labor-intensive. To address this problem, this paper investigates whether
strong generalization in dynamic object manipulation is achievable with only a
few demonstrations. Specifically, we develop an entropy-based theoretical
framework to quantify the optimization of imitation learning. Based on this
framework, we propose a system named Generalizable Entropy-based Manipulation
(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM
can generalize across diverse environment backgrounds, robot embodiments,
motion dynamics, and object geometries. Notably, GEM has been deployed in a
real canteen for tableware collection. Without any in-scene demonstration, it
achieves a success rate of over 97% across more than 10,000 operations.

</details>
