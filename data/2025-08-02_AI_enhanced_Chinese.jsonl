{"id": "2507.23270", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23270", "abs": "https://arxiv.org/abs/2507.23270", "authors": ["Loris Schneider", "Marc Ungen", "Elias Huber", "Jan-Felix Klein"], "title": "Simulation-based planning of Motion Sequences for Automated Procedure Optimization in Multi-Robot Assembly Cells", "comment": null, "summary": "Reconfigurable multi-robot cells offer a promising approach to meet\nfluctuating assembly demands. However, the recurrent planning of their\nconfigurations introduces new challenges, particularly in generating optimized,\ncoordinated multi-robot motion sequences that minimize the assembly duration.\nThis work presents a simulation-based method for generating such optimized\nsequences. The approach separates assembly steps into task-related core\noperations and connecting traverse operations. While core operations are\nconstrained and predetermined, traverse operations offer substantial\noptimization potential. Scheduling the core operations is formulated as an\noptimization problem, requiring feasible traverse operations to be integrated\nusing a decomposition-based motion planning strategy. Several solution\ntechniques are explored, including a sampling heuristic, tree-based search and\ngradient-free optimization. For motion planning, a decomposition method is\nproposed that identifies specific areas in the schedule, which can be solved\nindependently with modified centralized path planning algorithms. The proposed\nmethod generates efficient and collision-free multi-robot assembly procedures\nthat outperform a baseline relying on decentralized, robot-individual motion\nplanning. Its effectiveness is demonstrated through simulation experiments.", "AI": {"tldr": "提出了一种基于仿真的方法，用于优化多机器人装配单元的协调运动序列，以减少装配时间。", "motivation": "可重构多机器人单元能够适应波动的装配需求，但其配置的反复规划带来了新的挑战，尤其是如何生成优化的、协调的多机器人运动序列。", "method": "将装配步骤分为任务相关的核心操作和连接的遍历操作，核心操作是预定的，而遍历操作具有优化潜力。通过分解式运动规划策略，将核心操作调度为优化问题，并探索了多种求解技术。", "result": "提出的方法生成了高效且无碰撞的多机器人装配流程，优于基于分散式机器人个体运动规划的基线方法。", "conclusion": "仿真实验证明了该方法的有效性，能够显著减少装配时间。"}}
{"id": "2507.23339", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23339", "abs": "https://arxiv.org/abs/2507.23339", "authors": ["Yihan Zhou", "Yiwen Lu", "Bo Yang", "Jiayun Li", "Yilin Mo"], "title": "Learning to Drift with Individual Wheel Drive: Maneuvering Autonomous Vehicle at the Handling Limits", "comment": null, "summary": "Drifting, characterized by controlled vehicle motion at high sideslip angles,\nis crucial for safely handling emergency scenarios at the friction limits.\nWhile recent reinforcement learning approaches show promise for drifting\ncontrol, they struggle with the significant simulation-to-reality gap, as\npolicies that perform well in simulation often fail when transferred to\nphysical systems. In this paper, we present a reinforcement learning framework\nwith GPU-accelerated parallel simulation and systematic domain randomization\nthat effectively bridges the gap. The proposed approach is validated on both\nsimulation and a custom-designed and open-sourced 1/10 scale Individual Wheel\nDrive (IWD) RC car platform featuring independent wheel speed control.\nExperiments across various scenarios from steady-state circular drifting to\ndirection transitions and variable-curvature path following demonstrate that\nour approach achieves precise trajectory tracking while maintaining controlled\nsideslip angles throughout complex maneuvers in both simulated and real-world\nenvironments.", "AI": {"tldr": "本文提出了一种基于强化学习的漂移控制框架，通过GPU加速并行仿真和系统域随机化，有效缩小了仿真与现实的差距，并在仿真和真实平台上验证了其性能。", "motivation": "解决强化学习在漂移控制中仿真与现实差距大的问题，提升在紧急情况下的车辆控制能力。", "method": "采用GPU加速的并行仿真和系统域随机化技术，结合强化学习框架，开发了一种漂移控制方法。", "result": "在仿真和真实1/10比例IWD RC车平台上验证了方法的有效性，实现了复杂操作中的精确轨迹跟踪和可控侧滑角。", "conclusion": "该方法成功缩小了仿真与现实的差距，为漂移控制提供了一种有效的解决方案。"}}
{"id": "2507.23350", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23350", "abs": "https://arxiv.org/abs/2507.23350", "authors": ["Mahmoud Ghorab", "Matthias Lorenzen"], "title": "Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications", "comment": "6 pages", "summary": "There is a growing demand for autonomous mobile robots capable of navigating\nunstructured agricultural environments. Tasks such as weed control in meadows\nrequire efficient path planning through an unordered set of coordinates while\nminimizing travel distance and adhering to curvature constraints to prevent\nsoil damage and protect vegetation. This paper presents an integrated\nnavigation framework combining a global path planner based on the Dubins\nTraveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control\n(NMPC) strategy for local path planning and control. The DTSP generates a\nminimum-length, curvature-constrained path that efficiently visits all targets,\nwhile the NMPC leverages this path to compute control signals to accurately\nreach each waypoint. The system's performance was validated through comparative\nsimulation analysis on real-world field datasets, demonstrating that the\ncoupled DTSP-based planner produced smoother and shorter paths, with a\nreduction of about 16% in the provided scenario, compared to decoupled methods.\nBased thereon, the NMPC controller effectively steered the robot to the desired\nwaypoints, while locally optimizing the trajectory and ensuring adherence to\nconstraints. These findings demonstrate the potential of the proposed framework\nfor efficient autonomous navigation in agricultural environments.", "AI": {"tldr": "提出了一种结合DTSP全局路径规划和NMPC局部控制的导航框架，用于农业环境中自主移动机器人的高效导航。", "motivation": "满足农业环境中自主移动机器人的导航需求，特别是在无序坐标中规划路径时需最小化距离并满足曲率约束。", "method": "结合Dubins旅行商问题（DTSP）生成全局路径，并采用非线性模型预测控制（NMPC）进行局部路径规划和控制。", "result": "仿真验证显示，该方法比解耦方法路径更平滑且缩短约16%，NMPC控制器能准确导航并优化轨迹。", "conclusion": "该框架在农业环境中展示了高效自主导航的潜力。"}}
{"id": "2507.23445", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23445", "abs": "https://arxiv.org/abs/2507.23445", "authors": ["Yuta Kawachi"], "title": "Quantifying and Visualizing Sim-to-Real Gaps: Physics-Guided Regularization for Reproducibility", "comment": null, "summary": "Simulation-to-real transfer using domain randomization for robot control\noften relies on low-gear-ratio, backdrivable actuators, but these approaches\nbreak down when the sim-to-real gap widens. Inspired by the traditional PID\ncontroller, we reinterpret its gains as surrogates for complex, unmodeled plant\ndynamics. We then introduce a physics-guided gain regularization scheme that\nmeasures a robot's effective proportional gains via simple real-world\nexperiments. Then, we penalize any deviation of a neural controller's local\ninput-output sensitivities from these values during training. To avoid the\noverly conservative bias of naive domain randomization, we also condition the\ncontroller on the current plant parameters. On an off-the-shelf two-wheeled\nbalancing robot with a 110:1 gearbox, our gain-regularized,\nparameter-conditioned RNN achieves angular settling times in hardware that\nclosely match simulation. At the same time, a purely domain-randomized policy\nexhibits persistent oscillations and a substantial sim-to-real gap. These\nresults demonstrate a lightweight, reproducible framework for closing\nsim-to-real gaps on affordable robotic hardware.", "AI": {"tldr": "论文提出了一种基于PID控制器增益的物理引导增益正则化方法，结合参数条件化的RNN，显著缩小了仿真到现实的差距。", "motivation": "解决传统域随机化方法在仿真与现实差距较大时失效的问题，特别是在高齿轮比非反向驱动执行器上的表现不佳。", "method": "通过重新解释PID增益作为复杂未建模动力学的替代，引入物理引导的增益正则化方案，并结合参数条件化的RNN控制器。", "result": "在110:1齿轮比的平衡机器人上，该方法实现了仿真与硬件中角稳定时间的匹配，而纯域随机化策略则表现出持续振荡和显著差距。", "conclusion": "该方法为低成本机器人硬件提供了一种轻量级、可重复的框架，有效缩小了仿真到现实的差距。"}}
{"id": "2507.23015", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23015", "abs": "https://arxiv.org/abs/2507.23015", "authors": ["Abhinav Jain", "Cindy Grimm", "Stefan Lee"], "title": "Learning to Prune Branches in Modern Tree-Fruit Orchards", "comment": null, "summary": "Dormant tree pruning is labor-intensive but essential to maintaining modern\nhighly-productive fruit orchards. In this work we present a closed-loop\nvisuomotor controller for robotic pruning. The controller guides the cutter\nthrough a cluttered tree environment to reach a specified cut point and ensures\nthe cutters are perpendicular to the branch. We train the controller using a\nnovel orchard simulation that captures the geometric distribution of branches\nin a target apple orchard configuration. Unlike traditional methods requiring\nfull 3D reconstruction, our controller uses just optical flow images from a\nwrist-mounted camera. We deploy our learned policy in simulation and the\nreal-world for an example V-Trellis envy tree with zero-shot transfer,\nachieving a 30% success rate -- approximately half the performance of an oracle\nplanner.", "AI": {"tldr": "提出了一种基于视觉反馈的闭环机器人修剪控制器，用于高效修剪果树，无需完整3D重建。", "motivation": "传统果树修剪劳动密集且效率低，现代高产量果园需要自动化解决方案。", "method": "使用手腕摄像头的光流图像训练控制器，通过模拟果园环境学习修剪策略。", "result": "在仿真和现实环境中实现30%的成功率，约为理想规划器性能的一半。", "conclusion": "该方法展示了机器人修剪的潜力，但性能仍需提升。"}}
{"id": "2507.23592", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23592", "abs": "https://arxiv.org/abs/2507.23592", "authors": ["Haiyun Zhang", "Stefano Dalla Gasperina", "Saad N. Yousaf", "Toshimitsu Tsuboi", "Tetsuya Narita", "Ashish D. Deshpande"], "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "comment": "8 pages, 10 figures, submitted to RA-L", "summary": "Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications.", "AI": {"tldr": "提出了一种基于冗余关节传感和残差加权优化的手部外骨骼校准框架，显著提高了跟踪精度。", "motivation": "由于用户解剖学差异和穿戴不一致，手部外骨骼的跟踪精度受限，影响精确任务的应用。", "method": "使用冗余关节传感和残差加权优化策略估计虚拟链接参数，并通过运动捕捉数据驱动调整成本函数权重。", "result": "在七名受试者中，该方法显著降低了关节和指尖跟踪误差，提高了运动保真度。", "conclusion": "该框架适用于闭环运动学和最小传感的外骨骼设计，为高保真远程操作和学习演示应用奠定了基础。"}}
{"id": "2507.23045", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23045", "abs": "https://arxiv.org/abs/2507.23045", "authors": ["Emmett Wise", "Pushyami Kaveti", "Qilong Chen", "Wenhao Wang", "Hanumant Singh", "Jonathan Kelly", "David M. Rosen", "Matthew Giamou"], "title": "A Certifably Correct Algorithm for Generalized Robot-World and Hand-Eye Calibration", "comment": "25 pages, 10 figures, submitted to the International Journal of\n  Robotics Research", "summary": "Automatic extrinsic sensor calibration is a fundamental problem for\nmulti-sensor platforms. Reliable and general-purpose solutions should be\ncomputationally efficient, require few assumptions about the structure of the\nsensing environment, and demand little effort from human operators. Since the\nengineering effort required to obtain accurate calibration parameters increases\nwith the number of sensors deployed, robotics researchers have pursued methods\nrequiring few assumptions about the sensing environment and minimal effort from\nhuman operators. In this work, we introduce a fast and certifiably globally\noptimal algorithm for solving a generalized formulation of the\n$\\textit{robot-world and hand-eye calibration}$ (RWHEC) problem. The\nformulation of RWHEC presented is \"generalized\" in that it supports the\nsimultaneous estimation of multiple sensor and target poses, and permits the\nuse of monocular cameras that, alone, are unable to measure the scale of their\nenvironments. In addition to demonstrating our method's superior performance\nover existing solutions, we derive novel identifiability criteria and establish\n$\\textit{a priori}$ guarantees of global optimality for problem instances with\nbounded measurement errors. We also introduce a complementary Lie-algebraic\nlocal solver for RWHEC and compare its performance with our global method and\nprior art. Finally, we provide a free and open-source implementation of our\nalgorithms and experiments.", "AI": {"tldr": "提出了一种快速且全局最优的算法，用于解决广义的机器人-世界和手眼标定（RWHEC）问题，支持多传感器和目标姿态的同时估计，并适用于单目相机。", "motivation": "多传感器平台的自动外参标定需要高效、通用且少人工干预的解决方案。现有方法在传感器数量增加时工程成本高，因此需要更优的方法。", "method": "引入了一种广义RWHEC问题求解算法，支持多传感器和目标姿态估计，适用于单目相机，并提供了全局最优性保证和局部求解器。", "result": "新方法性能优于现有解决方案，并提供了全局最优性保证和新的可识别性标准。", "conclusion": "提出的算法高效且通用，适用于复杂传感器配置，并开源了实现代码。"}}
{"id": "2507.23053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23053", "abs": "https://arxiv.org/abs/2507.23053", "authors": ["Yuanhao Chen", "Liu Zhao", "Ji Ma", "Peng Lu"], "title": "In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion", "comment": null, "summary": "Quadruped robots face persistent challenges in achieving versatile locomotion\ndue to limitations in reference motion data diversity. To address these\nchallenges, this approach introduces an in-between motion generation based\nmulti-style quadruped robot locomotion framework, integrating synergistic\nadvances in motion generation and imitation learning. Our approach establishes\na unified pipeline addressing two fundamental aspects: First, we propose a CVAE\nbased motion generator, synthesizing multi-style dynamically feasible\nlocomotion sequences between arbitrary start and end states. By embedding\nphysical constraints and leveraging joint poses based phase manifold\ncontinuity, this component produces physically plausible motions spanning\nmultiple gait modalities while ensuring kinematic compatibility with robotic\nmorphologies. Second, we adopt the adversarial motion priors algorithm. We\nvalidate the effectiveness of generated motion data in enhancing controller\nstability and improving velocity tracking performance. The proposed framework\ndemonstrates significant improvements in velocity tracking and deployment\nstability. We successfully deploy the framework on a real-world quadruped\nrobot, and the experimental validation confirms the framework's capability to\ngenerate and execute complex motion profiles, including gallop, tripod,\ntrotting and pacing.", "AI": {"tldr": "提出了一种基于中间运动生成的多风格四足机器人运动框架，结合运动生成和模仿学习，提升了运动多样性和控制器性能。", "motivation": "解决四足机器人因参考运动数据多样性不足而导致的运动能力受限问题。", "method": "1. 使用CVAE生成多风格动态可行运动序列；2. 采用对抗运动先验算法验证数据有效性。", "result": "显著提升了速度跟踪性能和部署稳定性，成功在真实机器人上验证了复杂运动能力。", "conclusion": "该框架能有效生成和执行多样化运动，为四足机器人运动控制提供了新思路。"}}
{"id": "2507.23088", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.23088", "abs": "https://arxiv.org/abs/2507.23088", "authors": ["Lalithkumar Seenivasan", "Jiru Xu", "Roger D. Soberanis Mukul", "Hao Ding", "Grayson Byrd", "Yu-Chun Ku", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance", "comment": null, "summary": "Emerging surgical data science and robotics solutions, especially those\ndesigned to provide assistance in situ, require natural human-machine\ninterfaces to fully unlock their potential in providing adaptive and intuitive\naid. Contemporary AI-driven solutions remain inherently rigid, offering limited\nflexibility and restricting natural human-machine interaction in dynamic\nsurgical environments. These solutions rely heavily on extensive task-specific\npre-training, fixed object categories, and explicit manual-prompting. This work\nintroduces a novel Perception Agent that leverages speech-integrated\nprompt-engineered large language models (LLMs), segment anything model (SAM),\nand any-point tracking foundation models to enable a more natural human-machine\ninteraction in real-time intraoperative surgical assistance. Incorporating a\nmemory repository and two novel mechanisms for segmenting unseen elements,\nPerception Agent offers the flexibility to segment both known and unseen\nelements in the surgical scene through intuitive interaction. Incorporating the\nability to memorize novel elements for use in future surgeries, this work takes\na marked step towards human-machine symbiosis in surgical procedures. Through\nquantitative analysis on a public dataset, we show that the performance of our\nagent is on par with considerably more labor-intensive manual-prompting\nstrategies. Qualitatively, we show the flexibility of our agent in segmenting\nnovel elements (instruments, phantom grafts, and gauze) in a custom-curated\ndataset. By offering natural human-machine interaction and overcoming rigidity,\nour Perception Agent potentially brings AI-based real-time assistance in\ndynamic surgical environments closer to reality.", "AI": {"tldr": "提出了一种新型感知代理，结合语音提示、大语言模型和基础跟踪模型，提升手术中人机交互的自然性和灵活性。", "motivation": "现有AI辅助手术方案缺乏灵活性，限制了自然交互，需任务特定预训练和固定对象类别。", "method": "利用语音集成提示工程、大语言模型（LLMs）、分割模型（SAM）和点跟踪基础模型，结合记忆库和两种新机制分割未知元素。", "result": "定量分析显示性能与手动提示策略相当，定性分析展示了对新元素的分割灵活性。", "conclusion": "感知代理通过自然交互和灵活性，推动了动态手术环境中AI实时辅助的实现。"}}
{"id": "2507.23172", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23172", "abs": "https://arxiv.org/abs/2507.23172", "authors": ["Vira Joshi", "Zifan Xu", "Bo Liu", "Peter Stone", "Amy Zhang"], "title": "Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks", "comment": "RLC 2025", "summary": "Multi-task Reinforcement Learning (MTRL) has emerged as a critical training\nparadigm for applying reinforcement learning (RL) to a set of complex\nreal-world robotic tasks, which demands a generalizable and robust policy. At\nthe same time, \\emph{massively parallelized training} has gained popularity,\nnot only for significantly accelerating data collection through GPU-accelerated\nsimulation but also for enabling diverse data collection across multiple tasks\nby simulating heterogeneous scenes in parallel. However, existing MTRL research\nhas largely been limited to off-policy methods like SAC in the\nlow-parallelization regime. MTRL could capitalize on the higher asymptotic\nperformance of on-policy algorithms, whose batches require data from the\ncurrent policy, and as a result, take advantage of massive parallelization\noffered by GPU-accelerated simulation. To bridge this gap, we introduce a\nmassively parallelized $\\textbf{M}$ulti-$\\textbf{T}$ask $\\textbf{Bench}$mark\nfor robotics (MTBench), an open-sourced benchmark featuring a broad\ndistribution of 50 manipulation tasks and 20 locomotion tasks, implemented\nusing the GPU-accelerated simulator IsaacGym. MTBench also includes four base\nRL algorithms combined with seven state-of-the-art MTRL algorithms and\narchitectures, providing a unified framework for evaluating their performance.\nOur extensive experiments highlight the superior speed of evaluating MTRL\napproaches using MTBench, while also uncovering unique challenges that arise\nfrom combining massive parallelism with MTRL. Code is available at\n$\\href{https://github.com/Viraj-Joshi/MTBench}{\nhttps://github.com/Viraj-Joshi/MTBench}$", "AI": {"tldr": "MTBench是一个用于多任务强化学习（MTRL）的大规模并行基准测试，包含50个操作任务和20个运动任务，支持GPU加速模拟，并评估了多种MTRL算法。", "motivation": "现有MTRL研究主要局限于低并行化的离线策略方法，而大规模并行化可以提升在线策略算法的性能，但缺乏相关基准测试。", "method": "开发了MTBench基准测试，使用GPU加速模拟器IsaacGym实现70个任务，并整合了四种基础RL算法和七种MTRL算法。", "result": "实验表明MTBench能高效评估MTRL方法，同时揭示了大规模并行化与MTRL结合时的独特挑战。", "conclusion": "MTBench为MTRL研究提供了统一的评估框架，并展示了大规模并行化的潜力与挑战。"}}
{"id": "2507.23203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23203", "abs": "https://arxiv.org/abs/2507.23203", "authors": ["Chenghao Wang", "Eric Sihite", "Kaushik Venkatesh Krishnamurthy", "Shreyansh Pitroda", "Adarsh Salagame", "Alireza Ramezani", "Morteza Gharib"], "title": "Quadratic Programming-Based Posture Manipulation and Thrust-vectoring for Agile Dynamic Walking on Narrow Pathways", "comment": null, "summary": "There has been significant advancement in legged robot's agility where they\ncan show impressive acrobatic maneuvers, such as parkour. These maneuvers rely\nheavily on posture manipulation. To expand the stability and locomotion\nplasticity, we use the multi-modal ability in our legged-aerial platform, the\nHusky Beta, to perform thruster-assisted walking. This robot has thrusters on\neach of its sagittal knee joints which can be used to stabilize its frontal\ndynamic as it walks. In this work, we perform a simulation study of quadruped\nnarrow-path walking with Husky $\\beta$, where the robot will utilize its\nthrusters to stably walk on a narrow path. The controller is designed based on\na centroidal dynamics model with thruster and foot ground contact forces as\ninputs. These inputs are regulated using a QP solver to be used in a model\npredictive control framework. In addition to narrow-path walking, we also\nperform a lateral push-recovery simulation to study how the thrusters can be\nused to stabilize the frontal dynamics.", "AI": {"tldr": "本文研究了利用推进器辅助四足机器人在狭窄路径上稳定行走的方法，通过仿真验证了其有效性。", "motivation": "提升四足机器人的稳定性和运动可塑性，特别是在狭窄路径上的行走能力。", "method": "基于质心动力学模型设计控制器，利用QP求解器调节推进器和足部接触力，结合模型预测控制框架。", "result": "仿真验证了推进器辅助的狭窄路径行走和侧向推力恢复的稳定性。", "conclusion": "推进器辅助显著提升了四足机器人在复杂环境中的动态稳定性。"}}
{"id": "2507.23273", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23273", "abs": "https://arxiv.org/abs/2507.23273", "authors": ["Jaeseok Park", "Chanoh Park", "Minsu Kim", "Soohwan Kim"], "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting", "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,\nconventional approaches based on camera sensor, even RGB-D, suffer from\nfundamental limitations such as high computational load, failure in\nenvironments with poor texture or illumination, and short operational ranges.\nLiDAR emerges as a robust alternative, but its integration with 3DGS introduces\nnew challenges, such as the need for exceptional global alignment for\nphotorealistic quality and prolonged optimization times caused by sparse data.\nTo address these challenges, we propose GSFusion, an online\nLiDAR-Inertial-Visual mapping system that ensures high-precision map\nconsistency through a surfel-to-surfel constraint in the global pose-graph\noptimization. To handle sparse data, our system employs a pixel-aware Gaussian\ninitialization strategy for efficient representation and a bounded sigmoid\nconstraint to prevent uncontrolled Gaussian growth. Experiments on public and\nour datasets demonstrate our system outperforms existing 3DGS SLAM systems in\nterms of rendering quality and map-building efficiency.", "AI": {"tldr": "GSFusion是一种结合LiDAR、惯性和视觉的在线映射系统，通过全局位姿图优化和高效高斯初始化策略，解决了3D高斯溅射在稀疏数据和长优化时间上的挑战。", "motivation": "传统基于相机的3D高斯溅射方法存在计算负载高、在低纹理或光照差环境中失效以及操作范围短的问题，而LiDAR虽然鲁棒，但集成3D高斯溅射时面临全局对齐和高优化时间的挑战。", "method": "GSFusion采用全局位姿图优化中的面元到面元约束确保高精度地图一致性，并通过像素感知的高斯初始化策略和边界Sigmoid约束处理稀疏数据。", "result": "实验表明，GSFusion在渲染质量和地图构建效率上优于现有3DGS SLAM系统。", "conclusion": "GSFusion通过创新方法解决了LiDAR与3D高斯溅射结合的挑战，显著提升了性能。"}}
{"id": "2507.23305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23305", "abs": "https://arxiv.org/abs/2507.23305", "authors": ["Yixuan Dang", "Qinyang Xu", "Yu Zhang", "Xiangtong Yao", "Liding Zhang", "Zhenshan Bing", "Florian Roehrbein", "Alois Knoll"], "title": "Whisker-based Active Tactile Perception for Contour Reconstruction", "comment": null, "summary": "Perception using whisker-inspired tactile sensors currently faces a major\nchallenge: the lack of active control in robots based on direct contact\ninformation from the whisker. To accurately reconstruct object contours, it is\ncrucial for the whisker sensor to continuously follow and maintain an\nappropriate relative touch pose on the surface. This is especially important\nfor localization based on tip contact, which has a low tolerance for sharp\nsurfaces and must avoid slipping into tangential contact. In this paper, we\nfirst construct a magnetically transduced whisker sensor featuring a compact\nand robust suspension system composed of three flexible spiral arms. We develop\na method that leverages a characterized whisker deflection profile to directly\nextract the tip contact position using gradient descent, with a Bayesian filter\napplied to reduce fluctuations. We then propose an active motion control policy\nto maintain the optimal relative pose of the whisker sensor against the object\nsurface. A B-Spline curve is employed to predict the local surface curvature\nand determine the sensor orientation. Results demonstrate that our algorithm\ncan effectively track objects and reconstruct contours with sub-millimeter\naccuracy. Finally, we validate the method in simulations and real-world\nexperiments where a robot arm drives the whisker sensor to follow the surfaces\nof three different objects.", "AI": {"tldr": "论文提出了一种基于磁性传导的触须传感器和主动运动控制策略，用于精确跟踪物体表面并重建轮廓。", "motivation": "当前基于触须的触觉传感器缺乏主动控制，难以准确重建物体轮廓，尤其是在尖锐表面上。", "method": "设计了一种磁性传导触须传感器，利用梯度下降提取接触点位置，并通过贝叶斯滤波减少波动；提出主动运动控制策略，使用B样条曲线预测局部曲率。", "result": "算法能有效跟踪物体并实现亚毫米级精度的轮廓重建。", "conclusion": "方法在仿真和实际实验中验证了其有效性，适用于机器人触须传感器的主动控制。"}}
{"id": "2507.23324", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23324", "abs": "https://arxiv.org/abs/2507.23324", "authors": ["Lucas Elbert Suryana", "Saeed Rahmani", "Simeon Craig Calvert", "Arkady Zgonnikov", "Bart van Arem"], "title": "Assessing the Alignment of Automated Vehicle Decisions with Human Reasons", "comment": "This version incorporates revisions based on peer-review feedback\n  from a prior submission. The work has not yet been accepted and is being\n  prepared for resubmission", "summary": "A key challenge in deploying automated vehicles (AVs) is ensuring they make\nappropriate decisions in ethically challenging everyday driving situations.\nWhile much attention has been paid to rare, high-stakes dilemmas such as\ntrolley problems, similar tensions also arise in routine scenarios, such as\nnavigating empty intersections, where multiple human considerations, including\nlegality and comfort, often conflict. Current AV planning systems typically\nrely on rigid rules, which struggle to balance these competing considerations\nand can lead to behaviour that misaligns with human expectations. This paper\nproposes a novel reasons-based trajectory evaluation framework that\noperationalises the tracking condition of Meaningful Human Control (MHC). The\nframework models the reasons of human agents, such as regulatory compliance, as\nquantifiable functions and evaluates how well candidate AV trajectories align\nwith these reasons. By assigning adjustable weights to agent priorities and\nintegrating a balance function to discourage the exclusion of any agent, the\nframework supports interpretable decision evaluation. Through a\nreal-world-inspired overtaking scenario, we show how this approach reveals\ntensions, for instance between regulatory compliance, efficiency, and comfort.\nThe framework functions as a modular evaluation layer over existing planning\nalgorithms. It offers a transparent tool for assessing ethical alignment in\neveryday scenarios and provides a practical step toward implementing MHC in\nreal-world AV deployment.", "AI": {"tldr": "论文提出了一种基于原因的轨迹评估框架，用于解决自动驾驶车辆在日常驾驶中的伦理决策问题，通过量化人类代理的原因（如法规遵从性）来评估候选轨迹的匹配度。", "motivation": "确保自动驾驶车辆在伦理挑战性驾驶情境中做出适当决策，解决当前基于刚性规则的规划系统难以平衡多重人类考虑（如合法性和舒适性）的问题。", "method": "提出了一种基于原因的轨迹评估框架，将人类代理的原因建模为可量化函数，并通过可调整权重和平衡函数评估候选轨迹的匹配度。", "result": "通过实际超车场景展示了该框架如何揭示法规遵从性、效率和舒适性之间的冲突，并作为现有规划算法的模块化评估层。", "conclusion": "该框架为评估日常场景中的伦理对齐提供了透明工具，是实现自动驾驶车辆有意义人类控制的实用步骤。"}}
{"id": "2507.23523", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23523", "abs": "https://arxiv.org/abs/2507.23523", "authors": ["Hongzhe Bi", "Lingxuan Wu", "Tianwei Lin", "Hengkai Tan", "Zhizhong Su", "Hang Su", "Jun Zhu"], "title": "H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation", "comment": null, "summary": "Imitation learning for robotic manipulation faces a fundamental challenge:\nthe scarcity of large-scale, high-quality robot demonstration data. Recent\nrobotic foundation models often pre-train on cross-embodiment robot datasets to\nincrease data scale, while they face significant limitations as the diverse\nmorphologies and action spaces across different robot embodiments make unified\ntraining challenging. In this paper, we present H-RDT (Human to Robotics\nDiffusion Transformer), a novel approach that leverages human manipulation data\nto enhance robot manipulation capabilities. Our key insight is that large-scale\negocentric human manipulation videos with paired 3D hand pose annotations\nprovide rich behavioral priors that capture natural manipulation strategies and\ncan benefit robotic policy learning. We introduce a two-stage training\nparadigm: (1) pre-training on large-scale egocentric human manipulation data,\nand (2) cross-embodiment fine-tuning on robot-specific data with modular action\nencoders and decoders. Built on a diffusion transformer architecture with 2B\nparameters, H-RDT uses flow matching to model complex action distributions.\nExtensive evaluations encompassing both simulation and real-world experiments,\nsingle-task and multitask scenarios, as well as few-shot learning and\nrobustness assessments, demonstrate that H-RDT outperforms training from\nscratch and existing state-of-the-art methods, including Pi0 and RDT, achieving\nsignificant improvements of 13.9% and 40.5% over training from scratch in\nsimulation and real-world experiments, respectively. The results validate our\ncore hypothesis that human manipulation data can serve as a powerful foundation\nfor learning bimanual robotic manipulation policies.", "AI": {"tldr": "H-RDT利用人类操作数据提升机器人操作能力，通过两阶段训练（预训练和跨具身微调），在仿真和现实实验中显著优于现有方法。", "motivation": "解决机器人模仿学习中高质量演示数据稀缺的问题，利用人类操作数据的丰富行为先验。", "method": "两阶段训练：预训练于大规模人类操作数据，跨具身微调于机器人数据；基于扩散变换器架构，使用流匹配建模复杂动作分布。", "result": "在仿真和现实实验中分别比从头训练提升13.9%和40.5%，优于Pi0和RDT等方法。", "conclusion": "人类操作数据可作为学习双手机器人操作策略的强大基础。"}}
{"id": "2507.23540", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23540", "abs": "https://arxiv.org/abs/2507.23540", "authors": ["Yi Zhang", "Erik Leo Haß", "Kuo-Yi Chao", "Nenad Petrovic", "Yinglei Song", "Chengdong Wu", "Alois Knoll"], "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving", "comment": null, "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.", "AI": {"tldr": "提出了一种统一的感知-语言-动作（PLA）框架，结合多传感器融合和大型语言模型（LLM），提升自动驾驶的适应性和可解释性。", "motivation": "解决自动驾驶系统在复杂开放环境中适应性、鲁棒性和可解释性不足的问题。", "method": "采用多传感器融合（摄像头、LiDAR、雷达）与GPT-4.1驱动的语言模型结合，构建感知-语言-动作框架。", "result": "在城市交叉路口场景中表现出色，轨迹跟踪、速度预测和自适应规划性能优越。", "conclusion": "语言增强的认知框架有望提升自动驾驶的安全性、可解释性和可扩展性。"}}
{"id": "2507.23544", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.23544", "abs": "https://arxiv.org/abs/2507.23544", "authors": ["Ryo Miyoshi", "Yuki Okafuji", "Takuya Iwamoto", "Junya Nakanishi", "Jun Baba"], "title": "User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals", "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "summary": "In recent years, the demand for social robots has grown, requiring them to\nadapt their behaviors based on users' states. Accurately assessing user\nexperience (UX) in human-robot interaction (HRI) is crucial for achieving this\nadaptability. UX is a multi-faceted measure encompassing aspects such as\nsentiment and engagement, yet existing methods often focus on these\nindividually. This study proposes a UX estimation method for HRI by leveraging\nmultimodal social signals. We construct a UX dataset and develop a\nTransformer-based model that utilizes facial expressions and voice for\nestimation. Unlike conventional models that rely on momentary observations, our\napproach captures both short- and long-term interaction patterns using a\nmulti-instance learning framework. This enables the model to capture temporal\ndynamics in UX, providing a more holistic representation. Experimental results\ndemonstrate that our method outperforms third-party human evaluators in UX\nestimation.", "AI": {"tldr": "提出了一种基于多模态社交信号（面部表情和声音）的Transformer模型，用于估计人机交互中的用户体验（UX），并通过多实例学习框架捕捉短期和长期交互模式。", "motivation": "社会机器人需要根据用户状态调整行为，准确评估用户体验（UX）是实现这一目标的关键。现有方法多关注单一维度，而UX是多方面的。", "method": "构建UX数据集，开发基于Transformer的模型，利用面部表情和声音信号，结合多实例学习框架捕捉交互的时空动态。", "result": "实验表明，该方法在UX估计上优于第三方人工评估。", "conclusion": "提出的方法能更全面地表示UX的时空动态，为人机交互的适应性提供了有效工具。"}}
{"id": "2507.23589", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23589", "abs": "https://arxiv.org/abs/2507.23589", "authors": ["Kai Goebel", "Patrik Zips"], "title": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study", "comment": null, "summary": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics.", "AI": {"tldr": "评估大型语言模型在机器人任务规划中的表现，发现其在简单任务中表现良好，但在复杂场景中仍有挑战。", "motivation": "探索大型语言模型在生成结构化、可执行机器人任务计划中的潜力。", "method": "系统评估多种先进语言模型，直接使用PDDL文件提示，并与Fast Downward规划器对比。", "result": "模型在简单任务中表现良好，但在需要精确资源管理、状态跟踪和约束遵守的复杂任务中表现不佳。", "conclusion": "未来研究应结合语言模型与传统规划器，以提高机器人规划的可靠性和扩展性。"}}
{"id": "2507.23629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23629", "abs": "https://arxiv.org/abs/2507.23629", "authors": ["Yewei Huang", "John McConnell", "Xi Lin", "Brendan Englot"], "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "comment": null, "summary": "We present DRACo-SLAM2, a distributed SLAM framework for underwater robot\nteams equipped with multibeam imaging sonar. This framework improves upon the\noriginal DRACo-SLAM by introducing a novel representation of sonar maps as\nobject graphs and utilizing object graph matching to achieve time-efficient\ninter-robot loop closure detection without relying on prior geometric\ninformation. To better-accommodate the needs and characteristics of underwater\nscan matching, we propose incremental Group-wise Consistent Measurement Set\nMaximization (GCM), a modification of Pairwise Consistent Measurement Set\nMaximization (PCM), which effectively handles scenarios where nearby\ninter-robot loop closures share similar registration errors. The proposed\napproach is validated through extensive comparative analyses on simulated and\nreal-world datasets.", "AI": {"tldr": "DRACo-SLAM2是一个用于水下机器人团队的多波束成像声纳分布式SLAM框架，通过对象图表示和匹配实现高效闭环检测，并改进GCM方法以适应水下扫描匹配需求。", "motivation": "改进原有DRACo-SLAM框架，解决水下机器人团队在缺乏几何先验信息时的闭环检测问题，并优化扫描匹配性能。", "method": "引入对象图表示声纳地图，利用对象图匹配实现高效闭环检测；提出增量GCM方法改进PCM，处理共享相似配准误差的闭环场景。", "result": "在模拟和真实数据集上验证了方法的有效性。", "conclusion": "DRACo-SLAM2通过对象图和增量GCM显著提升了水下SLAM的性能和效率。"}}
{"id": "2507.23660", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23660", "abs": "https://arxiv.org/abs/2507.23660", "authors": ["Haoxuan Jiang", "Peicong Qian", "Yusen Xie", "Xiaocong Li", "Ming Liu", "Jun Ma"], "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "comment": null, "summary": "LiDAR-based localization serves as a critical component in autonomous\nsystems, yet existing approaches face persistent challenges in balancing\nrepeatability, accuracy, and environmental adaptability. Traditional point\ncloud registration methods relying solely on offline maps often exhibit limited\nrobustness against long-term environmental changes, leading to localization\ndrift and reliability degradation in dynamic real-world scenarios. To address\nthese challenges, this paper proposes DuLoc, a robust and accurate localization\nmethod that tightly couples LiDAR-inertial odometry with offline map-based\nlocalization, incorporating a constant-velocity motion model to mitigate\noutlier noise in real-world scenarios. Specifically, we develop a LiDAR-based\nlocalization framework that seamlessly integrates a prior global map with\ndynamic real-time local maps, enabling robust localization in unbounded and\nchanging environments. Extensive real-world experiments in ultra unbounded port\nthat involve 2,856 hours of operational data across 32 Intelligent Guided\nVehicles (IGVs) are conducted and reported in this study. The results attained\ndemonstrate that our system outperforms other state-of-the-art LiDAR\nlocalization systems in large-scale changing outdoor environments.", "AI": {"tldr": "本文提出了一种名为DuLoc的LiDAR定位方法，结合LiDAR-惯性里程计与离线地图定位，通过恒定速度运动模型提升动态环境中的定位鲁棒性和准确性。", "motivation": "现有LiDAR定位方法在重复性、准确性和环境适应性方面存在不足，尤其在长期环境变化下表现不佳。", "method": "DuLoc方法通过紧密耦合LiDAR-惯性里程计与离线地图定位，并引入恒定速度运动模型，减少噪声干扰。", "result": "在超大型港口环境中进行了2856小时的实验，结果显示DuLoc优于其他先进LiDAR定位系统。", "conclusion": "DuLoc在动态和大规模室外环境中表现出卓越的定位性能。"}}
{"id": "2507.23677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23677", "abs": "https://arxiv.org/abs/2507.23677", "authors": ["Xiaohan Li", "Ziren Gong", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia", "Dong Liu", "Jun Wu"], "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM\napplications due to its fast rendering and high-fidelity representation.\nHowever, existing 3DGS-SLAM systems have predominantly focused on indoor\nenvironments and relied on active depth sensors, leaving a gap for large-scale\noutdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian\nSplatting SLAM system designed for outdoor scenarios. Our approach uses only\nRGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages\ndepth estimates from pre-trained deep stereo networks to guide 3D Gaussian\noptimization with a multi-loss strategy enhancing both geometric consistency\nand visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM\nachieves superior tracking accuracy and mapping performance compared to other\n3DGS-based solutions in complex outdoor environments.", "AI": {"tldr": "BGS-SLAM是首个基于双目RGB的3D高斯泼溅SLAM系统，专为户外场景设计，无需深度传感器，通过预训练立体网络和多损失策略优化几何一致性和视觉质量。", "motivation": "现有3DGS-SLAM系统主要针对室内环境且依赖主动深度传感器，缺乏适用于大规模户外场景的解决方案。", "method": "使用双目RGB图像，结合预训练立体网络的深度估计，通过多损失策略优化3D高斯泼溅。", "result": "在多个数据集上验证，BGS-SLAM在复杂户外环境中表现出更高的跟踪精度和建图性能。", "conclusion": "BGS-SLAM填补了户外3DGS-SLAM的空白，展示了无需深度传感器的可行性和优越性。"}}
{"id": "2507.23682", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23682", "abs": "https://arxiv.org/abs/2507.23682", "authors": ["Xiaoyu Chen", "Hangxing Wei", "Pushi Zhang", "Chuheng Zhang", "Kaixin Wang", "Yanjiang Guo", "Rushuai Yang", "Yucen Wang", "Xinquan Xiao", "Li Zhao", "Jianyu Chen", "Jiang Bian"], "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "comment": "Project page: https://aka.ms/villa-x", "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.", "AI": {"tldr": "论文提出ViLLA-X框架，通过改进潜在动作的学习和整合方式，提升视觉-语言-动作模型的泛化能力。", "motivation": "探索如何将潜在动作更好地融入视觉-语言-动作模型，以提升机器人操作策略的泛化能力。", "method": "提出ViLLA-X框架，改进潜在动作的学习方法及其在预训练中的整合方式。", "result": "在SIMPLER、LIBERO等模拟环境及真实机器人实验中表现优异。", "conclusion": "ViLLA-X为未来研究提供了坚实基础，展示了潜在动作模型的潜力。"}}
{"id": "2507.23698", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23698", "abs": "https://arxiv.org/abs/2507.23698", "authors": ["Shaofei Cai", "Zhancun Mu", "Haiwen Xia", "Bowei Zhang", "Anji Liu", "Yitao Liang"], "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "comment": null, "summary": "While Reinforcement Learning (RL) has achieved remarkable success in language\nmodeling, its triumph hasn't yet fully translated to visuomotor agents. A\nprimary challenge in RL models is their tendency to overfit specific tasks or\nenvironments, thereby hindering the acquisition of generalizable behaviors\nacross diverse settings. This paper provides a preliminary answer to this\nchallenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can\nachieve zero-shot generalization to unseen worlds. Specifically, we explore\nRL's potential to enhance generalizable spatial reasoning and interaction\ncapabilities in 3D worlds. To address challenges in multi-task RL\nrepresentation, we analyze and establish cross-view goal specification as a\nunified multi-task goal space for visuomotor policies. Furthermore, to overcome\nthe significant bottleneck of manual task design, we propose automated task\nsynthesis within the highly customizable Minecraft environment for large-scale\nmulti-task RL training, and we construct an efficient distributed RL framework\nto support this. Experimental results show RL significantly boosts interaction\nsuccess rates by $4\\times$ and enables zero-shot generalization of spatial\nreasoning across diverse environments, including real-world settings. Our\nfindings underscore the immense potential of RL training in 3D simulated\nenvironments, especially those amenable to large-scale task generation, for\nsignificantly advancing visuomotor agents' spatial reasoning.", "AI": {"tldr": "RL在视觉运动代理中实现零样本泛化，通过Minecraft环境的多任务训练和自动任务合成提升空间推理能力。", "motivation": "解决RL模型在视觉运动代理中过拟合特定任务或环境的问题，探索其在3D世界中增强泛化能力的潜力。", "method": "提出跨视图目标规范作为统一多任务目标空间，并设计自动任务合成与分布式RL框架。", "result": "RL将交互成功率提升4倍，并在多样环境中实现零样本空间推理泛化。", "conclusion": "RL在可大规模任务生成的3D模拟环境中具有巨大潜力，显著提升视觉运动代理的空间推理能力。"}}
{"id": "2507.23719", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23719", "abs": "https://arxiv.org/abs/2507.23719", "authors": ["Parker McDonnell", "Lingsheng Meng", "Hari Krishna Hariprasad", "Alexander Hedrick", "Eduardo Miscles", "Samuel Gilinsky", "Jean-Michel Mongeau", "Kaushik Jayaram"], "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "comment": null, "summary": "The American cockroach (Periplaneta americana) uses its soft antennae to\nguide decision making by extracting rich tactile information from tens of\nthousands of distributed mechanosensors. Although tactile sensors enable\nrobust, autonomous perception and navigation in natural systems, replicating\nthese capabilities in insect-scale robots remains challenging due to stringent\nsize, weight, and power constraints that limit existing sensor technologies. To\novercome these limitations, we introduce CITRAS (Cockroach Inspired Tactile\nRobotic Antenna Sensor), a bioinspired, multi-segmented, compliant laminate\nsensor with embedded capacitive angle sensors. CITRAS is compact (73.7x15.6x2.1\nmm), lightweight (491 mg), and low-power (32 mW), enabling seamless integration\nwith miniature robotic platforms. The segmented compliant structure passively\nbends in response to environmental stimuli, achieving accurate hinge angle\nmeasurements with maximum errors of just 0.79 degree (quasistatic bending) and\n3.58 degree (dynamic bending). Experimental evaluations demonstrate CITRAS'\nmultifunctional tactile perception capabilities: predicting base-to-tip\ndistances with 7.75 % error, estimating environmental gap widths with 6.73 %\nerror, and distinguishing surface textures through differential sensor\nresponse. The future integration of this bioinspired tactile antenna in\ninsect-scale robots addresses critical sensing gaps, promising enhanced\nautonomous exploration, obstacle avoidance, and environmental mapping in\ncomplex, confined environments.", "AI": {"tldr": "研究人员受美洲蟑螂触角的启发，开发了一种名为CITRAS的轻量、低功耗触觉传感器，用于昆虫级机器人，实现了高精度的环境感知和导航功能。", "motivation": "昆虫级机器人在复杂环境中需要高效的触觉感知能力，但现有传感器受限于尺寸、重量和功耗。", "method": "设计了一种多段式柔性层压传感器（CITRAS），内置电容角度传感器，通过被动弯曲响应环境刺激。", "result": "CITRAS在静态和动态弯曲中的角度测量误差分别仅为0.79度和3.58度，并能预测距离、估计间隙宽度和区分表面纹理。", "conclusion": "CITRAS为昆虫级机器人填补了关键感知空白，有望提升其在复杂环境中的自主探索和避障能力。"}}
{"id": "2507.23735", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23735", "abs": "https://arxiv.org/abs/2507.23735", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Yvan R. Petillot"], "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "comment": null, "summary": "Achieving robust cognitive autonomy in robots navigating complex,\nunpredictable environments remains a fundamental challenge in robotics. This\npaper presents Underwater Robot Self-Organizing Autonomy (UROSA), a\ngroundbreaking architecture leveraging distributed Large Language Model AI\nagents integrated within the Robot Operating System 2 (ROS 2) framework to\nenable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA\ndecentralises cognition into specialised AI agents responsible for multimodal\nperception, adaptive reasoning, dynamic mission planning, and real-time\ndecision-making. Central innovations include flexible agents dynamically\nadapting their roles, retrieval-augmented generation utilising vector databases\nfor efficient knowledge management, reinforcement learning-driven behavioural\noptimisation, and autonomous on-the-fly ROS 2 node generation for runtime\nfunctional extensibility. Extensive empirical validation demonstrates UROSA's\npromising adaptability and reliability through realistic underwater missions in\nsimulation and real-world deployments, showing significant advantages over\ntraditional rule-based architectures in handling unforeseen scenarios,\nenvironmental uncertainties, and novel mission objectives. This work not only\nadvances underwater autonomy but also establishes a scalable, safe, and\nversatile cognitive robotics framework capable of generalising to a diverse\narray of real-world applications.", "AI": {"tldr": "论文提出了一种名为UROSA的新型架构，通过分布式大型语言模型AI代理和ROS 2框架，为水下自主机器人提供高级认知能力，显著提升了复杂环境中的适应性和可靠性。", "motivation": "解决机器人在复杂、不可预测环境中实现稳健认知自主性的挑战，特别是在水下自主车辆领域。", "method": "采用分布式AI代理，结合多模态感知、自适应推理、动态任务规划和实时决策，利用检索增强生成、强化学习和动态ROS 2节点生成等技术。", "result": "通过仿真和实际部署验证，UROSA在处理意外场景、环境不确定性和新任务目标方面优于传统规则架构。", "conclusion": "UROSA不仅推动了水下自主性发展，还提供了一个可扩展、安全且通用的认知机器人框架，适用于多种现实应用。"}}
