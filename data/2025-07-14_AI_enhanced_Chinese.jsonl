{"id": "2507.08100", "categories": ["cs.RO", "cond-mat.soft", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08100", "abs": "https://arxiv.org/abs/2507.08100", "authors": ["Lucy Liu", "Justin Werfel", "Federico Toschi", "L. Mahadevan"], "title": "Noise-Enabled Goal Attainment in Crowded Collectives", "comment": null, "summary": "In crowded environments, individuals must navigate around other occupants to\nreach their destinations. Understanding and controlling traffic flows in these\nspaces is relevant to coordinating robot swarms and designing infrastructure\nfor dense populations. Here, we combine simulations, theory, and robotic\nexperiments to study how noisy motion can disrupt traffic jams and enable flow\nas agents travel to individual goals. Above a critical noise level, large jams\ndo not persist. From this observation, we analytically approximate the goal\nattainment rate as a function of the noise level, then solve for the optimal\nagent density and noise level that maximize the swarm's goal attainment rate.\nWe perform robotic experiments to corroborate our simulated and theoretical\nresults. Finally, we compare simple, local navigation approaches with a\nsophisticated but computationally costly central planner. A simple reactive\nscheme performs well up to moderate densities and is far more computationally\nefficient than a planner, suggesting lessons for real-world problems.", "AI": {"tldr": "研究通过模拟、理论和机器人实验，探讨噪声运动如何破坏交通堵塞并促进流动，发现噪声水平超过临界值时堵塞消失，提出了优化目标达成率的方法。", "motivation": "研究拥挤环境中个体导航和交通流控制，为机器人群体协调和基础设施设计提供理论支持。", "method": "结合模拟、理论和机器人实验，分析噪声对交通流的影响，并比较局部导航与中央规划器的性能。", "result": "噪声水平超过临界值时交通堵塞消失，简单反应性导航在中等密度下表现优异且计算高效。", "conclusion": "简单反应性导航方案在现实问题中具有高效性和实用性，适用于中等密度环境。"}}
{"id": "2507.08112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08112", "abs": "https://arxiv.org/abs/2507.08112", "authors": ["Lamiaa H. Zain", "Hossam H. Ammar", "Raafat E. Shalaby"], "title": "Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion", "comment": null, "summary": "Obstacle avoidance is crucial for mobile robots' navigation in both known and\nunknown environments. This research designs, trains, and tests two custom\nConvolutional Neural Networks (CNNs), using color and depth images from a depth\ncamera as inputs. Both networks adopt sensor fusion to produce an output: the\nmobile robot's angular velocity, which serves as the robot's steering command.\nA newly obtained visual dataset for navigation was collected in diverse\nenvironments with varying lighting conditions and dynamic obstacles. During\ndata collection, a communication link was established over Wi-Fi between a\nremote server and the robot, using Robot Operating System (ROS) topics.\nVelocity commands were transmitted from the server to the robot, enabling\nsynchronized recording of visual data and the corresponding steering commands.\nVarious evaluation metrics, such as Mean Squared Error, Variance Score, and\nFeed-Forward time, provided a clear comparison between the two networks and\nclarified which one to use for the application.", "AI": {"tldr": "研究设计并测试了两种自定义CNN，用于移动机器人的避障导航，通过传感器融合输出角速度作为转向指令。", "motivation": "移动机器人在已知和未知环境中的避障导航至关重要。", "method": "使用深度相机的彩色和深度图像作为输入，设计并训练两种CNN，通过传感器融合生成角速度输出。数据收集通过Wi-Fi和ROS实现同步记录。", "result": "通过多种评估指标（如均方误差、方差分数和前馈时间）比较了两种网络的性能。", "conclusion": "研究明确了哪种网络更适合实际应用。"}}
{"id": "2507.08224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08224", "abs": "https://arxiv.org/abs/2507.08224", "authors": ["Chan Young Park", "Jillian Fisher", "Marius Memmel", "Dipika Khullar", "Andy Yun", "Abhishek Gupta", "Yejin Choi"], "title": "Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning", "comment": "Code Available: https://github.com/chan0park/SelfReVision", "summary": "Large language models (LLMs) have shown promise in robotic procedural\nplanning, yet their human-centric reasoning often omits the low-level, grounded\ndetails needed for robotic execution. Vision-language models (VLMs) offer a\npath toward more perceptually grounded plans, but current methods either rely\non expensive, large-scale models or are constrained to narrow simulation\nsettings. We introduce SelfReVision, a lightweight and scalable\nself-improvement framework for vision-language procedural planning.\nSelfReVision enables small VLMs to iteratively critique, revise, and verify\ntheir own plans-without external supervision or teacher models-drawing\ninspiration from chain-of-thought prompting and self-instruct paradigms.\nThrough this self-distillation loop, models generate higher-quality,\nexecution-ready plans that can be used both at inference and for continued\nfine-tuning. Using models varying from 3B to 72B, our results show that\nSelfReVision not only boosts performance over weak base VLMs but also\noutperforms models 100X the size, yielding improved control in downstream\nembodied tasks.", "AI": {"tldr": "SelfReVision是一个轻量级、可扩展的自改进框架，用于提升视觉语言模型在机器人程序规划中的表现，无需外部监督。", "motivation": "大型语言模型（LLMs）在机器人程序规划中表现良好，但缺乏低层次细节；视觉语言模型（VLMs）虽有潜力，但现有方法依赖昂贵的大模型或局限于模拟环境。", "method": "SelfReVision通过自蒸馏循环（自我批评、修订和验证）提升小型VLMs的规划能力，无需外部监督。", "result": "SelfReVision显著提升了小型VLMs的性能，甚至优于规模大100倍的模型，并改善了下游具体任务的控制能力。", "conclusion": "SelfReVision为轻量级视觉语言模型提供了一种高效的自改进方法，适用于实际机器人任务。"}}
{"id": "2507.08262", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08262", "abs": "https://arxiv.org/abs/2507.08262", "authors": ["Wenbo Cui", "Chengyang Zhao", "Yuhui Chen", "Haoran Li", "Zhizheng Zhang", "Dongbin Zhao", "He Wang"], "title": "CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations", "comment": null, "summary": "Building a robust perception module is crucial for visuomotor policy\nlearning. While recent methods incorporate pre-trained 2D foundation models\ninto robotic perception modules to leverage their strong semantic\nunderstanding, they struggle to capture 3D spatial information and generalize\nacross diverse camera viewpoints. These limitations hinder the policy's\neffectiveness, especially in fine-grained robotic manipulation scenarios. To\naddress these challenges, we propose CL3R, a novel 3D pre-training framework\ndesigned to enhance robotic manipulation policies. Our method integrates both\nspatial awareness and semantic understanding by employing a point cloud Masked\nAutoencoder to learn rich 3D representations while leveraging pre-trained 2D\nfoundation models through contrastive learning for efficient semantic knowledge\ntransfer. Additionally, we propose a 3D visual representation pre-training\nframework for robotic tasks. By unifying coordinate systems across datasets and\nintroducing random fusion of multi-view point clouds, we mitigate camera view\nambiguity and improve generalization, enabling robust perception from novel\nviewpoints at test time. Extensive experiments in both simulation and the real\nworld demonstrate the superiority of our method, highlighting its effectiveness\nin visuomotor policy learning for robotic manipulation.", "AI": {"tldr": "CL3R是一种新型3D预训练框架，通过结合点云掩码自编码器和对比学习，提升机器人操作策略的空间感知和语义理解能力。", "motivation": "现有方法依赖预训练2D基础模型，但缺乏3D空间信息和对多样化相机视角的泛化能力，限制了机器人精细操作的效果。", "method": "提出CL3R框架，利用点云掩码自编码器学习3D表征，并通过对比学习结合预训练2D模型的语义知识。统一数据集坐标系并融合多视角点云，提升泛化能力。", "result": "在仿真和真实环境中的实验表明，CL3R在机器人操作任务中表现优越，尤其在处理新视角时具有鲁棒性。", "conclusion": "CL3R通过增强3D感知能力，显著提升了机器人操作策略的效果，为视觉运动策略学习提供了新方向。"}}
{"id": "2507.08303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08303", "abs": "https://arxiv.org/abs/2507.08303", "authors": ["Yang Zhang", "Zhanxiang Cao", "Buqing Nie", "Haoyang Li", "Yue Gao"], "title": "Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots", "comment": "10 pages, 9 figures", "summary": "Humanoid robots show significant potential in daily tasks. However,\nreinforcement learning-based motion policies often suffer from robustness\ndegradation due to the sim-to-real dynamics gap, thereby affecting the agility\nof real robots. In this work, we propose a novel robust adversarial training\nparadigm designed to enhance the robustness of humanoid motion policies in real\nworlds. The paradigm introduces a learnable adversarial attack network that\nprecisely identifies vulnerabilities in motion policies and applies targeted\nperturbations, forcing the motion policy to enhance its robustness against\nperturbations through dynamic adversarial training. We conduct experiments on\nthe Unitree G1 humanoid robot for both perceptive locomotion and whole-body\ncontrol tasks. The results demonstrate that our proposed method significantly\nenhances the robot's motion robustness in real world environments, enabling\nsuccessful traversal of challenging terrains and highly agile whole-body\ntrajectory tracking.", "AI": {"tldr": "提出了一种新的对抗训练范式，通过可学习的对抗攻击网络增强人形机器人运动策略的鲁棒性，实验证明其显著提升了机器人在真实环境中的运动能力。", "motivation": "基于强化学习的运动策略因仿真与现实的动态差异导致鲁棒性下降，影响机器人的敏捷性。", "method": "引入可学习的对抗攻击网络，识别运动策略的脆弱点并施加针对性扰动，通过动态对抗训练提升鲁棒性。", "result": "在Unitree G1人形机器人上的实验表明，该方法显著提升了机器人在真实环境中的运动鲁棒性，成功应对复杂地形和敏捷全身轨迹跟踪。", "conclusion": "提出的对抗训练范式有效解决了仿真与现实的动态差异问题，显著提升了人形机器人在真实环境中的运动能力。"}}
{"id": "2507.08349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08349", "abs": "https://arxiv.org/abs/2507.08349", "authors": ["Junhui Wang", "Yan Qiao", "Chao Gao", "Naiqi Wu"], "title": "Joint Optimization-based Targetless Extrinsic Calibration for Multiple LiDARs and GNSS-Aided INS of Ground Vehicles", "comment": null, "summary": "Accurate extrinsic calibration between multiple LiDAR sensors and a\nGNSS-aided inertial navigation system (GINS) is essential for achieving\nreliable sensor fusion in intelligent mining environments. Such calibration\nenables vehicle-road collaboration by aligning perception data from\nvehicle-mounted sensors to a unified global reference frame. However, existing\nmethods often depend on artificial targets, overlapping fields of view, or\nprecise trajectory estimation, which are assumptions that may not hold in\npractice. Moreover, the planar motion of mining vehicles leads to observability\nissues that degrade calibration performance. This paper presents a targetless\nextrinsic calibration method that aligns multiple onboard LiDAR sensors to the\nGINS coordinate system without requiring overlapping sensor views or external\ntargets. The proposed approach introduces an observation model based on the\nknown installation height of the GINS unit to constrain unobservable\ncalibration parameters under planar motion. A joint optimization framework is\ndeveloped to refine both the extrinsic parameters and GINS trajectory by\nintegrating multiple constraints derived from geometric correspondences and\nmotion consistency. The proposed method is applicable to heterogeneous LiDAR\nconfigurations, including both mechanical and solid-state sensors. Extensive\nexperiments on simulated and real-world datasets demonstrate the accuracy,\nrobustness, and practical applicability of the approach under diverse sensor\nsetups.", "AI": {"tldr": "提出了一种无需外部目标或重叠视野的多LiDAR与GINS系统的外参标定方法，适用于智能采矿环境。", "motivation": "现有标定方法依赖人工目标、重叠视野或精确轨迹估计，实际中难以满足；采矿车辆的平面运动导致标定参数不可观测。", "method": "基于GINS安装高度的观测模型约束不可观测参数，结合几何对应和运动一致性的联合优化框架。", "result": "在仿真和真实数据集上验证了方法的准确性、鲁棒性和适用性。", "conclusion": "该方法解决了采矿环境中多LiDAR与GINS系统的标定问题，适用于异构传感器配置。"}}
{"id": "2507.08364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08364", "abs": "https://arxiv.org/abs/2507.08364", "authors": ["Deteng Zhang", "Junjie Zhang", "Yan Sun", "Tao Li", "Hao Yin", "Hongzhao Xie", "Jie Yin"], "title": "Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework", "comment": "This paper has already been accepted to IROS2025. 8 pages", "summary": "Considerable advancements have been achieved in SLAM methods tailored for\nstructured environments, yet their robustness under challenging corner cases\nremains a critical limitation. Although multi-sensor fusion approaches\nintegrating diverse sensors have shown promising performance improvements, the\nresearch community faces two key barriers: On one hand, the lack of\nstandardized and configurable benchmarks that systematically evaluate SLAM\nalgorithms under diverse degradation scenarios hinders comprehensive\nperformance assessment. While on the other hand, existing SLAM frameworks\nprimarily focus on fusing a limited set of sensor types, without effectively\naddressing adaptive sensor selection strategies for varying environmental\nconditions.\n  To bridge these gaps, we make three key contributions: First, we introduce\nM3DGR dataset: a sensor-rich benchmark with systematically induced degradation\npatterns including visual challenge, LiDAR degeneracy, wheel slippage and GNSS\ndenial. Second, we conduct a comprehensive evaluation of forty SLAM systems on\nM3DGR, providing critical insights into their robustness and limitations under\nchallenging real-world conditions. Third, we develop a resilient modular\nmulti-sensor fusion framework named Ground-Fusion++, which demonstrates robust\nperformance by coupling GNSS, RGB-D, LiDAR, IMU (Inertial Measurement Unit) and\nwheel odometry. Codes and datasets are publicly available.", "AI": {"tldr": "论文提出了M3DGR数据集和Ground-Fusion++框架，填补了SLAM算法在复杂环境下评估和自适应传感器融合的空白。", "motivation": "当前SLAM方法在结构化环境中表现良好，但在极端情况下鲁棒性不足，且缺乏标准化评估和多传感器自适应融合策略。", "method": "1. 提出M3DGR数据集，包含多种传感器退化场景；2. 评估40种SLAM系统；3. 开发Ground-Fusion++框架，融合多种传感器。", "result": "M3DGR数据集为SLAM评估提供了标准化基准，Ground-Fusion++在复杂条件下表现鲁棒。", "conclusion": "论文通过数据集和框架填补了SLAM领域的评估和自适应融合空白，推动了该领域的发展。"}}
{"id": "2507.08366", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08366", "abs": "https://arxiv.org/abs/2507.08366", "authors": ["Ghaith El-Dalahmeh", "Mohammad Reza Jabbarpour", "Bao Quoc Vo", "Ryszard Kowalczyk"], "title": "Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning", "comment": null, "summary": "Reliable satellite attitude control is essential for the success of space\nmissions, particularly as satellites increasingly operate autonomously in\ndynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role\nin attitude control, and maintaining control resilience during RW faults is\ncritical to preserving mission objectives and system stability. However,\ntraditional Proportional Derivative (PD) controllers and existing deep\nreinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall\nshort in providing the real time adaptability and fault tolerance required for\nautonomous satellite operations. This study introduces a DRL-based control\nstrategy designed to improve satellite resilience and adaptability under fault\nconditions. Specifically, the proposed method integrates Twin Delayed Deep\nDeterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and\nDimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in\nsparse reward environments and maintain satellite stability during RW failures.\nThe proposed approach is benchmarked against PD control and leading DRL\nalgorithms. Experimental results show that TD3-HD achieves significantly lower\nattitude error, improved angular velocity regulation, and enhanced stability\nunder fault conditions. These findings underscore the proposed method potential\nas a powerful, fault tolerant, onboard AI solution for autonomous satellite\nattitude control.", "AI": {"tldr": "论文提出了一种基于深度强化学习（DRL）的卫星姿态控制策略TD3-HD，结合TD3、HER和DWC技术，显著提升了卫星在故障条件下的适应性和稳定性。", "motivation": "卫星在动态和不确定环境中自主运行时，传统PD控制器和现有DRL算法（如TD3、PPO、A2C）在实时适应性和容错性方面表现不足，亟需改进。", "method": "提出TD3-HD方法，整合Twin Delayed Deep Deterministic Policy Gradient（TD3）、Hindsight Experience Replay（HER）和Dimension Wise Clipping（DWC），以增强稀疏奖励环境下的学习能力。", "result": "实验表明，TD3-HD在姿态误差、角速度调节和故障条件下的稳定性方面显著优于PD控制器和其他DRL算法。", "conclusion": "TD3-HD是一种强大的、具有容错能力的机载AI解决方案，适用于自主卫星姿态控制。"}}
{"id": "2507.08420", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08420", "abs": "https://arxiv.org/abs/2507.08420", "authors": ["Haitian Wang", "Hezam Albaqami", "Xinyu Wang", "Muhammad Ibrahim", "Zainy M. Malakan", "Abdullah M. Algamdi", "Mohammed H. Alghamdi", "Ajmal Mian"], "title": "LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps", "comment": "Preparing to submit to International Journal of Applied Earth\n  Observation and Geoinformation", "summary": "LiDAR-based 3D mapping suffers from cumulative drift causing global\nmisalignment, particularly in GNSS-constrained environments. To address this,\nwe propose a unified framework that fuses LiDAR, GNSS, and IMU data for\nhigh-resolution city-scale mapping. The method performs velocity-based temporal\nalignment using Dynamic Time Warping and refines GNSS and IMU signals via\nextended Kalman filtering. Local maps are built using Normal Distributions\nTransform-based registration and pose graph optimization with loop closure\ndetection, while global consistency is enforced using GNSS-constrained anchors\nfollowed by fine registration of overlapping segments. We also introduce a\nlarge-scale multimodal dataset captured in Perth, Western Australia to\nfacilitate future research in this direction. Our dataset comprises 144{,}000\nframes acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS\ntrajectories, and MEMS-IMU measurements across 21 urban loops. To assess\ngeometric consistency, we evaluated our method using alignment metrics based on\nroad centerlines and intersections to capture both global and local accuracy.\nOur method reduces the average global alignment error from 3.32\\,m to 1.24\\,m,\nachieving a 61.4\\% improvement. The constructed high-fidelity map supports a\nwide range of applications, including smart city planning, geospatial data\nintegration, infrastructure monitoring, and GPS-free navigation. Our method,\nand dataset together establish a new benchmark for evaluating 3D city mapping\nin GNSS-constrained environments. The dataset and code will be released\npublicly.", "AI": {"tldr": "提出了一种融合LiDAR、GNSS和IMU数据的统一框架，用于解决GNSS受限环境下的3D地图全局漂移问题，并引入了一个大规模多模态数据集。", "motivation": "LiDAR-based 3D mapping在GNSS受限环境下存在累积漂移问题，导致全局对齐误差。", "method": "结合Dynamic Time Warping进行时间对齐，使用扩展卡尔曼滤波优化GNSS和IMU信号，并通过NDT注册和位姿图优化构建局部地图，最后利用GNSS约束锚点实现全局一致性。", "result": "全局对齐误差从3.32米降至1.24米，提升61.4%。", "conclusion": "该方法在GNSS受限环境下显著提升了3D城市地图的精度，并为未来研究提供了新基准。"}}
{"id": "2507.08572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08572", "abs": "https://arxiv.org/abs/2507.08572", "authors": ["Juraj Gavura", "Michal Vavrecka", "Igor Farkas", "Connor Gade"], "title": "Robotic Calibration Based on Haptic Feedback Improves Sim-to-Real Transfer", "comment": "ICANN 2025", "summary": "When inverse kinematics (IK) is adopted to control robotic arms in\nmanipulation tasks, there is often a discrepancy between the end effector (EE)\nposition of the robot model in the simulator and the physical EE in reality. In\nmost robotic scenarios with sim-to-real transfer, we have information about\njoint positions in both simulation and reality, but the EE position is only\navailable in simulation. We developed a novel method to overcome this\ndifficulty based on haptic feedback calibration, using a touchscreen in front\nof the robot that provides information on the EE position in the real\nenvironment. During the calibration procedure, the robot touches specific\npoints on the screen, and the information is stored. In the next stage, we\nbuild a transformation function from the data based on linear transformation\nand neural networks that is capable of outputting all missing variables from\nany partial input (simulated/real joint/EE position). Our results demonstrate\nthat a fully nonlinear neural network model performs best, significantly\nreducing positioning errors.", "AI": {"tldr": "提出了一种基于触觉反馈校准的新方法，通过触摸屏获取真实环境中末端执行器位置信息，利用线性变换和神经网络构建转换函数，显著减少定位误差。", "motivation": "解决机器人模拟到现实转移中末端执行器位置不一致的问题。", "method": "使用触摸屏校准末端执行器位置，构建基于线性变换和神经网络的转换函数。", "result": "非线性神经网络模型表现最佳，显著减少定位误差。", "conclusion": "该方法有效解决了模拟与现实末端执行器位置不一致的问题，提升了机器人控制的准确性。"}}
{"id": "2507.08656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08656", "abs": "https://arxiv.org/abs/2507.08656", "authors": ["Aravind Elanjimattathil Vijayan", "Andrei Cramariuc", "Mattia Risiglione", "Christian Gehring", "Marco Hutter"], "title": "Multi-critic Learning for Whole-body End-effector Twist Tracking", "comment": null, "summary": "Learning whole-body control for locomotion and arm motions in a single policy\nhas challenges, as the two tasks have conflicting goals. For instance,\nefficient locomotion typically favors a horizontal base orientation, while\nend-effector tracking may benefit from base tilting to extend reachability.\nAdditionally, current Reinforcement Learning (RL) approaches using a pose-based\ntask specification lack the ability to directly control the end-effector\nvelocity, making smoothly executing trajectories very challenging. To address\nthese limitations, we propose an RL-based framework that allows for dynamic,\nvelocity-aware whole-body end-effector control. Our method introduces a\nmulti-critic actor architecture that decouples the reward signals for\nlocomotion and manipulation, simplifying reward tuning and allowing the policy\nto resolve task conflicts more effectively. Furthermore, we design a\ntwist-based end-effector task formulation that can track both discrete poses\nand motion trajectories. We validate our approach through a set of simulation\nand hardware experiments using a quadruped robot equipped with a robotic arm.\nThe resulting controller can simultaneously walk and move its end-effector and\nshows emergent whole-body behaviors, where the base assists the arm in\nextending the workspace, despite a lack of explicit formulations.", "AI": {"tldr": "论文提出了一种基于强化学习的框架，用于动态、速度感知的全身末端执行器控制，解决了运动与手臂动作的冲突问题。", "motivation": "运动与手臂动作的目标冲突（如水平基座与末端执行器跟踪的需求矛盾），以及现有RL方法无法直接控制末端执行器速度的问题。", "method": "采用多批评者-行动者架构，解耦运动与操作的奖励信号，并设计了基于扭转的末端执行器任务公式。", "result": "在四足机器人上的仿真和硬件实验中，控制器能同时行走和移动末端执行器，并表现出全身协作行为。", "conclusion": "提出的方法有效解决了任务冲突，实现了动态、速度感知的全身控制，并展示了无需显式公式的全身协作行为。"}}
{"id": "2507.08726", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08726", "abs": "https://arxiv.org/abs/2507.08726", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Learning human-to-robot handovers through 3D scene reconstruction", "comment": "8 pages, 6 figures, 2 table", "summary": "Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.", "AI": {"tldr": "提出了一种仅从RGB图像学习机器人交接任务的方法，无需真实机器人训练或数据收集，利用稀疏视角高斯泼溅重建生成机器人演示。", "motivation": "解决从仿真到真实环境的视觉域差距问题，减少对真实机器人训练的需求。", "method": "使用稀疏视角高斯泼溅重建人类-机器人交接场景，生成图像-动作对，训练机器人策略。", "result": "在16种家庭物品上验证，策略可直接部署到真实环境，效果显著。", "conclusion": "H2RH-SGS为人类-机器人交接任务提供了新的有效表示方法。"}}
