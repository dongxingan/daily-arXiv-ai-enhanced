{"id": "2507.12489", "categories": ["cs.RO", "cs.CV", "cs.GR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12489", "abs": "https://arxiv.org/abs/2507.12489", "authors": ["Richard Marcus", "Marc Stamminger"], "title": "Physically Based Neural LiDAR Resimulation", "comment": "Accepted at ITSC 2025, Gold Coast Australia", "summary": "Methods for Novel View Synthesis (NVS) have recently found traction in the\nfield of LiDAR simulation and large-scale 3D scene reconstruction. While\nsolutions for faster rendering or handling dynamic scenes have been proposed,\nLiDAR specific effects remain insufficiently addressed. By explicitly modeling\nsensor characteristics such as rolling shutter, laser power variations, and\nintensity falloff, our method achieves more accurate LiDAR simulation compared\nto existing techniques. We demonstrate the effectiveness of our approach\nthrough quantitative and qualitative comparisons with state-of-the-art methods,\nas well as ablation studies that highlight the importance of each sensor model\ncomponent. Beyond that, we show that our approach exhibits advanced\nresimulation capabilities, such as generating high resolution LiDAR scans in\nthe camera perspective.\n  Our code and the resulting dataset are available at\nhttps://github.com/richardmarcus/PBNLiDAR.", "AI": {"tldr": "本文提出了一种针对LiDAR仿真的新视角合成方法，通过显式建模传感器特性（如滚动快门、激光功率变化和强度衰减）来提高LiDAR仿真的准确性，并展示了在相机视角下生成高分辨率LiDAR扫描的先进重仿真能力。", "motivation": "现有的新视角合成方法在LiDAR仿真和大规模3D场景重建中虽然有所应用，但对LiDAR特定效应的处理仍然不足。虽然已有更快渲染或处理动态场景的解决方案，但LiDAR传感器的特殊特性仍未得到充分解决。", "method": "通过显式建模LiDAR传感器的关键特性，包括滚动快门效应、激光功率变化和强度衰减等，来实现更准确的LiDAR仿真。该方法针对LiDAR传感器的物理特性进行专门优化。", "result": "与现有最先进方法相比，该方法在定量和定性比较中都表现出更好的LiDAR仿真效果。消融研究证明了每个传感器模型组件的重要性。此外，该方法还展示了先进的重仿真能力，能够在相机视角下生成高分辨率LiDAR扫描。", "conclusion": "通过显式建模LiDAR传感器特性，该方法成功提高了LiDAR仿真的准确性，超越了现有技术。该方法不仅在传统LiDAR仿真任务上表现优异，还具备了在不同视角（如相机视角）生成高质量LiDAR数据的能力，为LiDAR仿真领域提供了新的解决方案。"}}
{"id": "2507.12496", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12496", "abs": "https://arxiv.org/abs/2507.12496", "authors": ["Yucen Wang", "Rui Yu", "Shenghua Wan", "Le Gan", "De-Chuan Zhan"], "title": "FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making", "comment": "Accepted by Forty-Second International Conference on Machine Learning\n  (ICML 2025)", "summary": "Foundation Models (FMs) and World Models (WMs) offer complementary strengths\nin task generalization at different levels. In this work, we propose FOUNDER, a\nframework that integrates the generalizable knowledge embedded in FMs with the\ndynamic modeling capabilities of WMs to enable open-ended task solving in\nembodied environments in a reward-free manner. We learn a mapping function that\ngrounds FM representations in the WM state space, effectively inferring the\nagent's physical states in the world simulator from external observations. This\nmapping enables the learning of a goal-conditioned policy through imagination\nduring behavior learning, with the mapped task serving as the goal state. Our\nmethod leverages the predicted temporal distance to the goal state as an\ninformative reward signal. FOUNDER demonstrates superior performance on various\nmulti-task offline visual control benchmarks, excelling in capturing the\ndeep-level semantics of tasks specified by text or videos, particularly in\nscenarios involving complex observations or domain gaps where prior methods\nstruggle. The consistency of our learned reward function with the ground-truth\nreward is also empirically validated. Our project website is\nhttps://sites.google.com/view/founder-rl.", "AI": {"tldr": "FOUNDER框架结合基础模型（FMs）和世界模型（WMs）的优势，通过无奖励方式在具身环境中实现开放任务解决。", "motivation": "利用FMs的通用知识和WMs的动态建模能力，解决复杂观测或领域差距场景中的任务泛化问题。", "method": "学习映射函数将FM表示嵌入WM状态空间，通过想象学习目标条件策略，并利用预测的时间距离作为奖励信号。", "result": "FOUNDER在多任务离线视觉控制基准测试中表现优异，尤其在复杂观测或领域差距场景中。", "conclusion": "FOUNDER通过结合FMs和WMs的优势，实现了高效的任务解决和奖励一致性验证。"}}
{"id": "2507.12499", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12499", "abs": "https://arxiv.org/abs/2507.12499", "authors": ["Yuhang Lu", "Jiadong Tu", "Yuexin Ma", "Xinge Zhu"], "title": "ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving", "comment": "Accepted by ICCV2025", "summary": "End-to-end autonomous driving has emerged as a promising approach to unify\nperception, prediction, and planning within a single framework, reducing\ninformation loss and improving adaptability. However, existing methods often\nrely on fixed and sparse trajectory supervision, limiting their ability to\ncapture the hierarchical reasoning process that human drivers naturally employ.\nTo bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning\nframework that structures decision-making in autonomous driving based on the\nthree-tier human cognitive model: Driving Strategy, Driving Decision, and\nDriving Operation, where Vision-Language Models (VLMs) are incorporated to\nenhance situational awareness and structured reasoning across these levels.\nSpecifically, we introduce: (1) the Strategic Reasoning Injector, which\nformulates high-level driving strategies by interpreting complex traffic\ncontexts from VLM-generated insights; (2) the Tactical Reasoning Integrator,\nwhich refines strategic intent into interpretable tactical choices such as lane\nchanges, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory\nDecoder, which progressively translates tactical decisions into precise control\nactions for smooth and human-like trajectory execution. Extensive evaluations\nshow that integrating our framework improves planning accuracy and safety by\nover 30%, making end-to-end autonomous driving more interpretable and aligned\nwith human-like hierarchical reasoning. The project page can be found at:\n\\href{https://4dvlab.github.io/project_page/realad}{\\texttt{4dvlab.github.io/project\\_page/realad}}", "AI": {"tldr": "ReAL-AD框架通过结合人类认知模型和视觉语言模型，提升自动驾驶的决策层次性和可解释性，显著提高规划准确性和安全性。", "motivation": "现有端到端自动驾驶方法因依赖固定稀疏轨迹监督，难以模拟人类驾驶的层次推理过程，需改进。", "method": "提出ReAL-AD框架，包含战略推理注入器、战术推理整合器和分层轨迹解码器，结合视觉语言模型增强推理能力。", "result": "实验表明，该框架将规划准确性和安全性提升30%以上。", "conclusion": "ReAL-AD使自动驾驶更接近人类层次推理，更具可解释性和适应性。"}}
{"id": "2507.12644", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12644", "abs": "https://arxiv.org/abs/2507.12644", "authors": ["George Jiayuan Gao", "Tianyu Li", "Junyao Shi", "Yihan Li", "Zizhe Zhang", "Nadia Figueroa", "Dinesh Jayaraman"], "title": "VLMgineer: Vision Language Models as Robotic Toolsmiths", "comment": "Project Website: https://vlmgineer.github.io/release", "summary": "Tool design and use reflect the ability to understand and manipulate the\nphysical world through creativity, planning, and foresight. As such, these\ncapabilities are often regarded as measurable indicators of intelligence across\nbiological species. While much of today's research on robotic intelligence\nfocuses on generating better controllers, inventing smarter tools offers a\ncomplementary form of physical intelligence: shifting the onus of\nproblem-solving onto the tool's design. Given the vast and impressive\ncommon-sense, reasoning, and creative capabilities of today's foundation\nmodels, we investigate whether these models can provide useful priors to\nautomatically design and effectively wield such tools? We present VLMgineer, a\nframework that harnesses the code generation abilities of vision language\nmodels (VLMs) together with evolutionary search to iteratively co-design\nphysical tools and the action plans that operate them to perform a task. We\nevaluate VLMgineer on a diverse new benchmark of everyday manipulation\nscenarios that demand creative tool design and use. Across this suite,\nVLMgineer consistently discovers tools and policies that solve tasks more\neffectively and innovatively, transforming challenging robotics problems into\nstraightforward executions. It also outperforms VLM-generated designs from\nhuman specifications and existing human-crafted tools for everyday tasks. To\nfacilitate future research on automated tool invention, we will release our\nbenchmark and code.", "AI": {"tldr": "VLMgineer利用视觉语言模型和进化搜索共同设计工具和操作策略，显著提升任务解决效率和创新性。", "motivation": "探索基础模型是否能提供有用先验，自动设计和使用工具，以解决机器人智能中的物理问题。", "method": "结合视觉语言模型的代码生成能力和进化搜索，迭代设计工具及操作策略。", "result": "在多样化任务中，VLMgineer设计的工具和策略更高效、创新，优于人工设计的工具。", "conclusion": "VLMgineer为自动化工具发明提供了新方向，并释放了基准和代码以促进未来研究。"}}
{"id": "2507.12855", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12855", "abs": "https://arxiv.org/abs/2507.12855", "authors": ["Rahel Rickenbach", "Bruce Lee", "René Zurbrügg", "Carmen Amo Alonso", "Melanie N. Zeilinger"], "title": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning", "comment": null, "summary": "The integration of large language models (LLMs) with control systems has\ndemonstrated significant potential in various settings, such as task completion\nwith a robotic manipulator. A main reason for this success is the ability of\nLLMs to perform in-context learning, which, however, strongly relies on the\ndesign of task examples, closely related to the target tasks. Consequently,\nemploying LLMs to formulate optimal control problems often requires task\nexamples that contain explicit mathematical expressions, designed by trained\nengineers. Furthermore, there is often no principled way to evaluate for\nhallucination before task execution. To address these challenges, we propose\nDEMONSTRATE, a novel methodology that avoids the use of LLMs for complex\noptimization problem generations, and instead only relies on the embedding\nrepresentations of task descriptions. To do this, we leverage tools from\ninverse optimal control to replace in-context prompt examples with task\ndemonstrations, as well as the concept of multitask learning, which ensures\ntarget and example task similarity by construction. Given the fact that\nhardware demonstrations can easily be collected using teleoperation or guidance\nof the robot, our approach significantly reduces the reliance on engineering\nexpertise for designing in-context examples. Furthermore, the enforced\nmultitask structure enables learning from few demonstrations and assessment of\nhallucinations prior to task execution. We demonstrate the effectiveness of our\nmethod through simulation and hardware experiments involving a robotic arm\ntasked with tabletop manipulation.", "AI": {"tldr": "论文提出了一种名为DEMONSTRATE的新方法，通过利用任务描述的嵌入表示和逆最优控制工具，减少了对LLMs生成复杂优化问题的依赖，并降低了工程设计示例的需求。", "motivation": "当前LLMs在控制系统中依赖上下文学习，但需要精心设计的任务示例，且缺乏评估幻觉的原则性方法。", "method": "采用逆最优控制工具和多任务学习概念，用任务演示替代上下文提示示例，确保任务相似性。", "result": "通过仿真和硬件实验验证了方法的有效性，减少了工程设计需求并支持少量演示学习。", "conclusion": "DEMONSTRATE方法显著降低了对LLMs和工程设计的依赖，提升了任务执行的可靠性和效率。"}}
{"id": "2507.12716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12716", "abs": "https://arxiv.org/abs/2507.12716", "authors": ["Nathaniel Rose", "Hannah Chuang", "Manuel A Andrade-Rodriguez", "Rishi Parashar", "Dani Or", "Parikshit Maini"], "title": "MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil Moisture Mapping at Scale", "comment": "Accepted by 2025 IEEE 21st International Conference on Automation\n  Science and Engineering. 8 pages, 10 figures, 2 tables", "summary": "Soil moisture is a quantity of interest in many application areas including\nagriculture and climate modeling. Existing methods are not suitable for scale\napplications due to large deployment costs in high-resolution sensing\napplications such as for variable irrigation. In this work, we design, build\nand field deploy an autonomous mobile robot, MoistureMapper, for soil moisture\nsensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and\na direct push drill mechanism for deploying the sensor to measure volumetric\nwater content in the soil. Additionally, we implement and evaluate multiple\nadaptive sampling strategies based on a Gaussian Process based modeling to\nbuild a spatial mapping of moisture distribution in the soil. We present\nresults from large scale computational simulations and proof-of-concept\ndeployment on the field. The adaptive sampling approach outperforms a greedy\nbenchmark approach and results in up to 30\\% reduction in travel distance and\n5\\% reduction in variance in the reconstructed moisture maps. Link to video\nshowing field experiments: https://youtu.be/S4bJ4tRzObg", "AI": {"tldr": "本文提出了一种名为MoistureMapper的自主移动机器人，用于土壤湿度传感，结合自适应采样策略，显著减少了旅行距离和湿度地图的方差。", "motivation": "现有土壤湿度传感方法在大规模应用中成本高昂，无法满足高分辨率需求，如变量灌溉。", "method": "设计并部署了配备TDR传感器和直接推进钻探机制的机器人，采用高斯过程建模的自适应采样策略。", "result": "自适应采样策略优于贪婪基准方法，减少了30%的旅行距离和5%的湿度地图方差。", "conclusion": "MoistureMapper及其自适应采样策略为大规模土壤湿度传感提供了一种高效解决方案。"}}
{"id": "2507.13088", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13088", "abs": "https://arxiv.org/abs/2507.13088", "authors": ["Rahel Rickenbach", "Alan A. Lahoud", "Erik Schaffernicht", "Melanie N. Zeilinger", "Johannes A. Stork"], "title": "ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning", "comment": null, "summary": "The computational burden of model predictive control (MPC) limits its\napplication on real-time systems, such as robots, and often requires the use of\nshort prediction horizons. This not only affects the control performance, but\nalso increases the difficulty of designing MPC cost functions that reflect the\ndesired long-term objective. This paper proposes ZipMPC, a method that imitates\na long-horizon MPC behaviour by learning a compressed and context-dependent\ncost function for a short-horizon MPC. It improves performance over alternative\nmethods, such as approximate explicit MPC and automatic cost parameter tuning,\nin particular in terms of i) optimizing the long term objective; ii)\nmaintaining computational costs comparable to a short-horizon MPC; iii)\nensuring constraint satisfaction; and iv) generalizing control behaviour to\nenvironments not observed during training. For this purpose, ZipMPC leverages\nthe concept of differentiable MPC with neural networks to propagate gradients\nof the imitation loss through the MPC optimization. We validate our proposed\nmethod in simulation and real-world experiments on autonomous racing. ZipMPC\nconsistently completes laps faster than selected baselines, achieving lap times\nclose to the long-horizon MPC baseline. In challenging scenarios where the\nshort-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In\nparticular, these performance gains are also observed on tracks unseen during\ntraining.", "AI": {"tldr": "ZipMPC是一种通过学习压缩且上下文相关的成本函数来模拟长时域MPC行为的方法，适用于短时域MPC，提升性能并保持计算效率。", "motivation": "解决模型预测控制（MPC）在实时系统（如机器人）中因计算负担而难以应用长时域预测的问题，同时优化长期目标。", "method": "利用可微分MPC和神经网络传播模仿损失的梯度，学习压缩且上下文相关的成本函数。", "result": "在仿真和真实自动驾驶赛车实验中，ZipMPC表现优于基线方法，尤其在长期目标优化、计算效率、约束满足和泛化能力方面。", "conclusion": "ZipMPC能有效模拟长时域MPC行为，适用于实时系统，并在未见过的环境中保持高性能。"}}
{"id": "2507.12731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12731", "abs": "https://arxiv.org/abs/2507.12731", "authors": ["Nathaniel Rose", "Arif Ahmed", "Emanuel Gutierrez-Cornejo", "Parikshit Maini"], "title": "Learning to Predict Mobile Robot Stability in Off-Road Environments", "comment": "Nathaniel Rose and Arif Ahmed contributed equally to this work.\n  Accepted poster for RSS 2025 Workshop on Resilient Off-road Autonomous\n  Robotics. 8 pages, 8 figures, 1 table", "summary": "Navigating in off-road environments for wheeled mobile robots is challenging\ndue to dynamic and rugged terrain. Traditional physics-based stability metrics,\nsuch as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require\nknowledge of contact forces, terrain geometry, and the robot's precise\ncenter-of-mass that are difficult to measure accurately in real-world field\nconditions. In this work, we propose a learning-based approach to estimate\nrobot platform stability directly from proprioceptive data using a lightweight\nneural network, IMUnet. Our method enables data-driven inference of robot\nstability without requiring an explicit terrain model or force sensing.\n  We also develop a novel vision-based ArUco tracking method to compute a\nscalar score to quantify robot platform stability called C3 score. The score\ncaptures image-space perturbations over time as a proxy for physical\ninstability and is used as a training signal for the neural network based\nmodel. As a pilot study, we evaluate our approach on data collected across\nmultiple terrain types and speeds and demonstrate generalization to previously\nunseen conditions. These initial results highlight the potential of using IMU\nand robot velocity as inputs to estimate platform stability. The proposed\nmethod finds application in gating robot tasks such as precision actuation and\nsensing, especially for mobile manipulation tasks in agricultural and space\napplications. Our learning method also provides a supervision mechanism for\nperception based traversability estimation and planning.", "AI": {"tldr": "提出了一种基于学习的轻量级神经网络IMUnet，直接从本体感受数据估计机器人平台稳定性，无需地形模型或力传感。同时开发了一种基于视觉的ArUco跟踪方法，量化稳定性（C3分数）。", "motivation": "传统物理稳定性指标（如SSM或ZMP）需要难以实时准确测量的参数，如接触力、地形几何和机器人质心。", "method": "使用IMUnet从IMU和机器人速度数据中推断稳定性，并通过ArUco跟踪生成C3分数作为训练信号。", "result": "在多地形和速度数据上验证了方法的泛化能力，初步结果表明IMU和速度数据可用于稳定性估计。", "conclusion": "该方法适用于农业和太空应用中的移动操作任务，并为感知驱动的可通行性估计和规划提供了监督机制。"}}
{"id": "2507.12744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12744", "abs": "https://arxiv.org/abs/2507.12744", "authors": ["Cheng Liu", "Fan Zhu", "Yaoyu Zhuang Zhinan Chen Jiefeng Tang"], "title": "ASC-SW: Atrous strip convolution network with sliding windows for visual-assisted map navigation", "comment": null, "summary": "With the rapid development of lightweight visual neural network\narchitectures, traditional high-performance vision models have undergone\nsignificant compression, greatly improving their computational efficiency and\nenergy consumption ratio. This makes them feasible for deployment on\nresource-constrained edge computing devices. We propose a visual-assisted\nnavigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW),\nwhich leverages a depth camera and a lightweight visual neural network to\nassist map-based mobile robot navigation. This framework compensates for the\ninability of traditional light detection and range (LiDAR) sensors to detect\nground-level obstacles such as ground-level wires. We introduce a lightweight\nand efficient segmentation model, Atrous Strip Convolution Network (ASCnet),\nfor detecting deformable linear objects (DLOs). MobileNetV2 is used as the\nbackbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP)\nis designed to extract DLO features more effectively. Atrous Strip Convolution\nis integrated into ASCSPP to accurately identify the linear structure of DLOs\nwith low computational cost. Additionally, a Sliding Window (SW)\npost-processing module is proposed to denoise the output in complex\nenvironments, improving recognition accuracy. Our method strikes a balance\nbetween inference speed and segmentation performance. It achieves a mean\nIntersection over Union (Miou) score of 75.3% on a self-built dataset and\nreaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall,\nour approach outperforms existing DLO detection models and has been\nsuccessfully validated on a physical robotic platform.", "AI": {"tldr": "提出了一种轻量级视觉辅助导航框架ASC-SW，结合深度相机和轻量级视觉神经网络，用于移动机器人导航，解决了传统LiDAR无法检测地面障碍物的问题。", "motivation": "传统LiDAR传感器无法检测地面障碍物（如电线），限制了移动机器人的导航能力。通过轻量级视觉神经网络，提升计算效率和能耗比，使其适用于资源受限的边缘设备。", "method": "设计了轻量高效的ASCnet分割模型，采用MobileNetV2作为主干网络，结合ASCSPP模块提取DLO特征，并引入滑动窗口（SW）后处理模块提升复杂环境下的识别精度。", "result": "在自建数据集上达到75.3%的Miou分数，在Jetson Orin Nano设备上实现9.3 FPS的推理速度，性能优于现有DLO检测模型。", "conclusion": "ASC-SW框架在推理速度和分割性能间取得平衡，成功应用于物理机器人平台，验证了其有效性。"}}
{"id": "2507.12751", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12751", "abs": "https://arxiv.org/abs/2507.12751", "authors": ["Yasser G. Alqaham", "Jing Cheng", "Zhenyu Gan"], "title": "Refining Motion for Peak Performance: Identifying Optimal Gait Parameters for Energy-Efficient Quadrupedal Bounding", "comment": "Published in the ACC 2025 Conference proceedings", "summary": "Energy efficiency is a critical factor in the performance and autonomy of\nquadrupedal robots. While previous research has focused on mechanical design\nand actuation improvements, the impact of gait parameters on energetics has\nbeen less explored. In this paper, we hypothesize that gait parameters,\nspecifically duty factor, phase shift, and stride duration, are key\ndeterminants of energy consumption in quadrupedal locomotion. To test this\nhypothesis, we modeled the Unitree A1 quadrupedal robot and developed a\nlocomotion controller capable of independently adjusting these gait parameters.\nSimulations of bounding gaits were conducted in Gazebo across a range of gait\nparameters at three different speeds: low, medium, and high. Experimental tests\nwere also performed to validate the simulation results. The findings\ndemonstrate that optimizing gait parameters can lead to significant reductions\nin energy consumption, enhancing the overall efficiency of quadrupedal\nlocomotion. This work contributes to the advancement of energy-efficient\ncontrol strategies for legged robots, offering insights directly applicable to\ncommercially available platforms.", "AI": {"tldr": "研究探讨了步态参数对四足机器人能量效率的影响，发现优化这些参数可显著降低能耗。", "motivation": "尽管机械设计和驱动改进已有研究，但步态参数对能量效率的影响尚未充分探索。", "method": "通过建模Unitree A1机器人并开发控制器独立调整步态参数，在Gazebo中进行仿真和实验验证。", "result": "优化步态参数可显著减少能量消耗，提高四足运动的整体效率。", "conclusion": "该研究为四足机器人提供了节能控制策略，可直接应用于商业平台。"}}
{"id": "2507.12753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12753", "abs": "https://arxiv.org/abs/2507.12753", "authors": ["Fujing Xie", "Sören Schwertfeger", "Hermann Blum"], "title": "osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning", "comment": null, "summary": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features, achieving a high level of detail and\nguiding robots to find objects specified by open-vocabulary language queries.\nWhile the issue of scalability for such approaches has received some attention,\nanother fundamental problem is that high-detail object mapping quickly becomes\noutdated, as objects get moved around a lot. In this work, we develop a mapping\nand navigation system for object-goal navigation that, from the ground up,\nconsiders the possibilities that a queried object can have moved, or may not be\nmapped at all. Instead of striving for high-fidelity mapping detail, we\nconsider that the main purpose of a map is to provide environment grounding and\ncontext, which we combine with the semantic priors of LLMs to reason about\nobject locations and deploy an active, online approach to navigate to the\nobjects. Through simulated and real-world experiments we find that our approach\ntends to have higher retrieval success at shorter path lengths for static\nobjects and by far outperforms prior approaches in cases of dynamic or unmapped\nobject queries. We provide our code and dataset at:\nhttps://anonymous.4open.science/r/osmAG-LLM.", "AI": {"tldr": "提出了一种基于LLM语义先验的主动在线导航系统，用于解决动态或未映射对象的查询问题，性能优于现有方法。", "motivation": "传统高细节对象地图因对象移动而快速过时，需一种能适应动态环境的地图与导航系统。", "method": "结合环境地图的上下文信息与LLM语义先验，采用主动在线方法导航至目标对象。", "result": "在静态和动态对象查询中，新方法检索成功率更高且路径更短。", "conclusion": "新系统在动态或未映射对象导航中表现优异，优于现有方法。"}}
{"id": "2507.12800", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12800", "abs": "https://arxiv.org/abs/2507.12800", "authors": ["Jikai Wang", "Yunqi Cheng", "Zonghai Chen"], "title": "FFI-VTR: Lightweight and Robust Visual Teach and Repeat Navigation based on Feature Flow Indicator and Probabilistic Motion Planning", "comment": null, "summary": "Though visual and repeat navigation is a convenient solution for mobile robot\nself-navigation, achieving balance between efficiency and robustness in task\nenvironment still remains challenges. In this paper, we propose a novel visual\nand repeat robotic autonomous navigation method that requires no accurate\nlocalization and dense reconstruction modules, which makes our system featured\nby lightweight and robustness. Firstly, feature flow is introduced and we\ndevelop a qualitative mapping between feature flow and robot's motion, in which\nfeature flow is defined as pixel location bias between matched features. Based\non the mapping model, the map outputted by the teaching phase is represented as\na keyframe graph, in which the feature flow on the edge encodes the relative\nmotion between adjacent keyframes. Secondly, the visual repeating navigation is\nessentially modeled as a feature flow minimization problem between current\nobservation and the map keyframe. To drive the robot to consistently reduce the\nfeature flow between current frame and map keyframes without accurate\nlocalization, a probabilistic motion planning is developed based on our\nqualitative feature flow-motion mapping indicator. Extensive experiments using\nour mobile platform demonstrates that our proposed method is lightweight,\nrobust, and superior to baselines. The source code has been made public at\nhttps://github.com/wangjks/FFI-VTR to benefit the community.", "AI": {"tldr": "提出了一种轻量且鲁棒的视觉重复自主导航方法，无需精确定位和密集重建模块，通过特征流最小化实现高效导航。", "motivation": "解决移动机器人在视觉重复导航中效率与鲁棒性平衡的挑战。", "method": "引入特征流并建立其与机器人运动的映射模型，将地图表示为关键帧图，导航建模为特征流最小化问题，开发概率运动规划。", "result": "实验表明该方法轻量、鲁棒且优于基线。", "conclusion": "该方法在无需精确定位的情况下实现了高效导航，代码已开源。"}}
{"id": "2507.12846", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12846", "abs": "https://arxiv.org/abs/2507.12846", "authors": ["Muhammad Fadhil Ginting", "Dong-Ki Kim", "Xiangyun Meng", "Andrzej Reinke", "Bandi Jai Krishna", "Navid Kayhani", "Oriana Peltzer", "David D. Fan", "Amirreza Shaban", "Sung-Kyun Kim", "Mykel J. Kochenderfer", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering", "comment": null, "summary": "As robots become increasingly capable of operating over extended periods --\nspanning days, weeks, and even months -- they are expected to accumulate\nknowledge of their environments and leverage this experience to assist humans\nmore effectively. This paper studies the problem of Long-term Active Embodied\nQuestion Answering (LA-EQA), a new task in which a robot must both recall past\nexperiences and actively explore its environment to answer complex,\ntemporally-grounded questions. Unlike traditional EQA settings, which typically\nfocus either on understanding the present environment alone or on recalling a\nsingle past observation, LA-EQA challenges an agent to reason over past,\npresent, and possible future states, deciding when to explore, when to consult\nits memory, and when to stop gathering observations and provide a final answer.\nStandard EQA approaches based on large models struggle in this setting due to\nlimited context windows, absence of persistent memory, and an inability to\ncombine memory recall with active exploration. To address this, we propose a\nstructured memory system for robots, inspired by the mind palace method from\ncognitive science. Our method encodes episodic experiences as scene-graph-based\nworld instances, forming a reasoning and planning algorithm that enables\ntargeted memory retrieval and guided navigation. To balance the\nexploration-recall trade-off, we introduce value-of-information-based stopping\ncriteria that determines when the agent has gathered sufficient information. We\nevaluate our method on real-world experiments and introduce a new benchmark\nthat spans popular simulation environments and actual industrial sites. Our\napproach significantly outperforms state-of-the-art baselines, yielding\nsubstantial gains in both answer accuracy and exploration efficiency.", "AI": {"tldr": "论文提出了一种长期主动具身问答（LA-EQA）任务，通过结构化记忆系统和基于信息价值的停止标准，显著提升了机器人在复杂环境中的问答表现。", "motivation": "随着机器人操作时间的延长，如何有效利用积累的环境知识辅助人类成为关键问题。传统EQA方法因上下文窗口有限、缺乏持久记忆等问题难以胜任。", "method": "提出了一种基于场景图的世界实例结构化记忆系统，结合推理与规划算法，并引入基于信息价值的停止标准以平衡探索与记忆检索。", "result": "在真实世界实验和新基准测试中，该方法显著优于现有基线，提高了答案准确性和探索效率。", "conclusion": "结构化记忆系统和信息价值标准为长期具身问答提供了有效解决方案，显著提升了机器人的表现。"}}
{"id": "2507.12911", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12911", "abs": "https://arxiv.org/abs/2507.12911", "authors": ["Hayeon Oh"], "title": "LaViPlan : Language-Guided Visual Path Planning with RLVR", "comment": "11 pages, 6 figures", "summary": "Out-of-distribution (OOD) scenarios in autonomous driving refer to situations\nthat deviate from the training domain, often leading to unexpected and\npotentially hazardous behavior from planners that lack prior exposure to such\ncases. Recently, Vision-Language Models (VLMs) have been introduced into\nautonomous driving research for their promising generalization capabilities in\nOOD settings. Early studies demonstrated that VLMs could recognize OOD\nscenarios and generate user-level decisions such as \"go straight\" or \"turn\nright.\" However, a new challenge has emerged due to the misalignment between\nthe VLM's high-level decisions or visual reasoning expressed in language, and\nthe low-level predicted trajectories interpreted as actions. In this paper, we\npropose LaViPlan, a framework that leverages Reinforcement Learning with\nVerifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.\nThis approach addresses the vision-language-action misalignment observed in\nexisting VLMs fine-tuned via supervised learning, which can recognize driving\nscenarios but often produce context-unaware decisions. Experimental results\ndemonstrate that our method improves situational awareness and decision-making\nunder OOD conditions, highlighting its potential to mitigate the misalignment\nissue. This work introduces a promising post-training paradigm for VLM agents\nin the context of autonomous driving.", "AI": {"tldr": "论文提出LaViPlan框架，利用强化学习优化视觉语言模型（VLM）在自动驾驶中的决策，解决现有VLM在OOD场景下的视觉-语言-动作不对齐问题。", "motivation": "自动驾驶中的分布外（OOD）场景可能导致危险行为，现有VLM虽能识别场景但决策缺乏上下文感知。", "method": "提出LaViPlan框架，结合强化学习与可验证奖励（RLVR）优化VLM，以规划导向指标为目标。", "result": "实验表明，该方法提升了OOD条件下的情境感知和决策能力，缓解了对齐问题。", "conclusion": "LaViPlan为自动驾驶中的VLM提供了一种有前景的后训练范式。"}}
{"id": "2507.12920", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12920", "abs": "https://arxiv.org/abs/2507.12920", "authors": ["Zichao Shu", "Shitao Bei", "Jicheng Dai", "Lijun Li", "Zetao Chen"], "title": "MoCap2GT: A High-Precision Ground Truth Estimator for SLAM Benchmarking Based on Motion Capture and IMU Fusion", "comment": null, "summary": "Marker-based optical motion capture (MoCap) systems are widely used to\nprovide ground truth (GT) trajectories for benchmarking SLAM algorithms.\nHowever, the accuracy of MoCap-based GT trajectories is mainly affected by two\nfactors: spatiotemporal calibration errors between the MoCap system and the\ndevice under test (DUT), and inherent MoCap jitter. Consequently, existing\nbenchmarks focus primarily on absolute translation error, as accurate\nassessment of rotation and inter-frame errors remains challenging, hindering\nthorough SLAM evaluation. This paper proposes MoCap2GT, a joint optimization\napproach that integrates MoCap data and inertial measurement unit (IMU)\nmeasurements from the DUT for generating high-precision GT trajectories.\nMoCap2GT includes a robust state initializer to ensure global convergence,\nintroduces a higher-order B-spline pose parameterization on the SE(3) manifold\nwith variable time offset to effectively model MoCap factors, and employs a\ndegeneracy-aware measurement rejection strategy to enhance estimation accuracy.\nExperimental results demonstrate that MoCap2GT outperforms existing methods and\nsignificantly contributes to precise SLAM benchmarking. The source code is\navailable at https://anonymous.4open.science/r/mocap2gt (temporarily hosted\nanonymously for double-blind review).", "AI": {"tldr": "MoCap2GT是一种联合优化方法，结合MoCap数据和IMU测量生成高精度地面真实轨迹，用于SLAM算法的精确评估。", "motivation": "现有基于MoCap的地面真实轨迹存在时空校准误差和固有抖动问题，限制了SLAM算法的全面评估。", "method": "MoCap2GT采用鲁棒状态初始化、高阶B样条姿态参数化和退化感知测量拒绝策略。", "result": "实验表明，MoCap2GT优于现有方法，显著提高了SLAM评估的精确性。", "conclusion": "MoCap2GT为SLAM基准测试提供了高精度的地面真实轨迹，推动了SLAM算法的进一步发展。"}}
{"id": "2507.12977", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12977", "abs": "https://arxiv.org/abs/2507.12977", "authors": ["Giwon Lee", "Daehee Park", "Jaewoo Jeong", "Kuk-Jin Yoon"], "title": "Non-differentiable Reward Optimization for Diffusion-based Autonomous Motion Planning", "comment": "Accepted at IROS 2025", "summary": "Safe and effective motion planning is crucial for autonomous robots.\nDiffusion models excel at capturing complex agent interactions, a fundamental\naspect of decision-making in dynamic environments. Recent studies have\nsuccessfully applied diffusion models to motion planning, demonstrating their\ncompetence in handling complex scenarios and accurately predicting multi-modal\nfuture trajectories. Despite their effectiveness, diffusion models have\nlimitations in training objectives, as they approximate data distributions\nrather than explicitly capturing the underlying decision-making dynamics.\nHowever, the crux of motion planning lies in non-differentiable downstream\nobjectives, such as safety (collision avoidance) and effectiveness\n(goal-reaching), which conventional learning algorithms cannot directly\noptimize. In this paper, we propose a reinforcement learning-based training\nscheme for diffusion motion planning models, enabling them to effectively learn\nnon-differentiable objectives that explicitly measure safety and effectiveness.\nSpecifically, we introduce a reward-weighted dynamic thresholding algorithm to\nshape a dense reward signal, facilitating more effective training and\noutperforming models trained with differentiable objectives. State-of-the-art\nperformance on pedestrian datasets (CrowdNav, ETH-UCY) compared to various\nbaselines demonstrates the versatility of our approach for safe and effective\nmotion planning.", "AI": {"tldr": "论文提出了一种基于强化学习的扩散运动规划模型训练方法，以优化非可微目标（如安全性和有效性）。", "motivation": "扩散模型在运动规划中表现优异，但无法直接优化非可微目标（如碰撞避免和目标达成）。", "method": "采用强化学习训练扩散模型，引入奖励加权动态阈值算法以优化非可微目标。", "result": "在行人数据集（CrowdNav, ETH-UCY）上实现了最先进的性能。", "conclusion": "该方法为安全有效的运动规划提供了通用解决方案。"}}
{"id": "2507.12986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12986", "abs": "https://arxiv.org/abs/2507.12986", "authors": ["Sepeedeh Shahbeigi", "Nawshin Mannan Proma", "Victoria Hodge", "Richard Hawkins", "Boda Li", "Valentina Donzella"], "title": "Robustness Requirement Coverage using a Situation Coverage Approach for Vision-based AI Systems", "comment": "4 pages, 1 figure", "summary": "AI-based robots and vehicles are expected to operate safely in complex and\ndynamic environments, even in the presence of component degradation. In such\nsystems, perception relies on sensors such as cameras to capture environmental\ndata, which is then processed by AI models to support decision-making. However,\ndegradation in sensor performance directly impacts input data quality and can\nimpair AI inference. Specifying safety requirements for all possible sensor\ndegradation scenarios leads to unmanageable complexity and inevitable gaps. In\nthis position paper, we present a novel framework that integrates camera noise\nfactor identification with situation coverage analysis to systematically elicit\nrobustness-related safety requirements for AI-based perception systems. We\nfocus specifically on camera degradation in the automotive domain. Building on\nan existing framework for identifying degradation modes, we propose involving\ndomain, sensor, and safety experts, and incorporating Operational Design Domain\nspecifications to extend the degradation model by incorporating noise factors\nrelevant to AI performance. Situation coverage analysis is then applied to\nidentify representative operational contexts. This work marks an initial step\ntoward integrating noise factor analysis and situational coverage to support\nprincipled formulation and completeness assessment of robustness requirements\nfor camera-based AI perception.", "AI": {"tldr": "论文提出了一种新框架，结合相机噪声因素识别与情境覆盖分析，系统性地为基于AI的感知系统制定鲁棒性相关的安全要求。", "motivation": "AI机器人和车辆需在复杂动态环境中安全运行，但传感器性能退化会影响输入数据质量，进而损害AI推理能力。传统方法难以覆盖所有可能的退化场景。", "method": "整合相机噪声因素识别与情境覆盖分析，结合领域、传感器和安全专家意见，扩展退化模型以纳入影响AI性能的噪声因素。", "result": "提出了一种系统性方法，支持相机退化场景下鲁棒性要求的制定和完整性评估。", "conclusion": "该框架为相机退化场景下AI感知系统的鲁棒性要求提供了初步解决方案。"}}
{"id": "2507.13019", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13019", "abs": "https://arxiv.org/abs/2507.13019", "authors": ["Liuyi Wang", "Xinyuan Xia", "Hui Zhao", "Hanqing Wang", "Tai Wang", "Yilun Chen", "Chengju Liu", "Qijun Chen", "Jiangmiao Pang"], "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities", "comment": "Accepted by ICCV 2025", "summary": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.", "AI": {"tldr": "VLN-PE是一个物理真实的视觉与语言导航平台，支持多种机器人类型，揭示了当前模型在物理部署中的性能下降问题，并提供了扩展性强的评估工具。", "motivation": "解决现有视觉与语言导航（VLN）方法在物理机器人部署中的理想化假设问题，提出更真实的评估平台。", "method": "引入VLN-PE平台，评估多种技术方案，包括分类模型、扩散模型和基于大型语言模型的路径规划。", "result": "发现性能下降问题，如机器人观察空间有限、环境光照变化和物理碰撞等，同时揭示了腿式机器人在复杂环境中的运动限制。", "conclusion": "VLN-PE为改进跨具身适应性提供了新途径，并呼吁社区重新思考VLN的局限性。"}}
{"id": "2507.13041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13041", "abs": "https://arxiv.org/abs/2507.13041", "authors": ["Julien Wacquez", "Elisabetta Zibetti", "Joffrey Becker", "Lorenzo Aloe", "Fabio Amadio", "Salvatore Anzalone", "Lola Cañamero", "Serena Ivaldi"], "title": "What Can Robots Teach Us About Trust and Reliance? An interdisciplinary dialogue between Social Sciences and Social Robotics", "comment": null, "summary": "As robots find their way into more and more aspects of everyday life,\nquestions around trust are becoming increasingly important. What does it mean\nto trust a robot? And how should we think about trust in relationships that\ninvolve both humans and non-human agents? While the field of Human-Robot\nInteraction (HRI) has made trust a central topic, the concept is often\napproached in fragmented ways. At the same time, established work in sociology,\nwhere trust has long been a key theme, is rarely brought into conversation with\ndevelopments in robotics. This article argues that we need a more\ninterdisciplinary approach. By drawing on insights from both social sciences\nand social robotics, we explore how trust is shaped, tested and made visible.\nOur goal is to open up a dialogue between disciplines and help build a more\ngrounded and adaptable framework for understanding trust in the evolving world\nof human-robot interaction.", "AI": {"tldr": "本文探讨了人机交互中的信任问题，提出需要跨学科方法结合社会学与机器人学，以建立更全面的信任框架。", "motivation": "随着机器人融入日常生活，信任问题日益重要，但目前人机交互领域对信任的研究较为分散，且与社会学的相关研究缺乏对话。", "method": "通过结合社会学与社交机器人学的观点，分析信任的形成、测试与表现。", "result": "提出了一种跨学科的对话方法，为理解人机交互中的信任提供了更扎实和适应性强的框架。", "conclusion": "需要跨学科合作以建立更全面的信任理论，促进人机交互领域的发展。"}}
{"id": "2507.13053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13053", "abs": "https://arxiv.org/abs/2507.13053", "authors": ["Sanjeev Ramkumar Sudha", "Joel Jose", "Erlend M. Coates"], "title": "Efficient Online Learning and Adaptive Planning for Robotic Information Gathering Based on Streaming Data", "comment": null, "summary": "Robotic information gathering (RIG) techniques refer to methods where mobile\nrobots are used to acquire data about the physical environment with a suite of\nsensors. Informative planning is an important part of RIG where the goal is to\nfind sequences of actions or paths that maximize efficiency or the quality of\ninformation collected. Many existing solutions solve this problem by assuming\nthat the environment is known in advance. However, real environments could be\nunknown or time-varying, and adaptive informative planning remains an active\narea of research. Adaptive planning and incremental online mapping are required\nfor mapping initially unknown or varying spatial fields. Gaussian process (GP)\nregression is a widely used technique in RIG for mapping continuous spatial\nfields. However, it falls short in many applications as its real-time\nperformance does not scale well to large datasets. To address these challenges,\nthis paper proposes an efficient adaptive informative planning approach for\nmapping continuous scalar fields with GPs with streaming sparse GPs. Simulation\nexperiments are performed with a synthetic dataset and compared against\nexisting benchmarks. Finally, it is also verified with a real-world dataset to\nfurther validate the efficacy of the proposed method. Results show that our\nmethod achieves similar mapping accuracy to the baselines while reducing\ncomputational complexity for longer missions.", "AI": {"tldr": "本文提出了一种基于高斯过程（GP）的高效自适应信息规划方法，用于映射连续标量场，解决了现有方法在实时性和大数据集上的不足。", "motivation": "现有机器人信息采集（RIG）方法假设环境已知，但实际环境可能未知或动态变化，且高斯过程在大数据集上实时性能不足。", "method": "采用流式稀疏高斯过程（streaming sparse GPs）进行自适应规划，结合增量在线映射技术。", "result": "仿真和真实数据集实验表明，该方法在保持精度的同时显著降低了计算复杂度。", "conclusion": "该方法为动态环境中的高效信息采集提供了可行解决方案。"}}
{"id": "2507.13097", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13097", "abs": "https://arxiv.org/abs/2507.13097", "authors": ["Adithyavairavan Murali", "Balakumar Sundaralingam", "Yu-Wei Chao", "Wentao Yuan", "Jun Yamada", "Mark Carlson", "Fabio Ramos", "Stan Birchfield", "Dieter Fox", "Clemens Eppner"], "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training", "comment": null, "summary": "Grasping is a fundamental robot skill, yet despite significant research\nadvancements, learning-based 6-DOF grasping approaches are still not turnkey\nand struggle to generalize across different embodiments and in-the-wild\nsettings. We build upon the recent success on modeling the object-centric grasp\ngeneration process as an iterative diffusion process. Our proposed framework,\nGraspGen, consists of a DiffusionTransformer architecture that enhances grasp\ngeneration, paired with an efficient discriminator to score and filter sampled\ngrasps. We introduce a novel and performant on-generator training recipe for\nthe discriminator. To scale GraspGen to both objects and grippers, we release a\nnew simulated dataset consisting of over 53 million grasps. We demonstrate that\nGraspGen outperforms prior methods in simulations with singulated objects\nacross different grippers, achieves state-of-the-art performance on the\nFetchBench grasping benchmark, and performs well on a real robot with noisy\nvisual observations.", "AI": {"tldr": "GraspGen是一个基于扩散变换器架构的6-DOF抓取生成框架，通过高效判别器评分和过滤抓取样本，在仿真和真实机器人任务中表现优异。", "motivation": "尽管抓取是机器人的基本技能，但基于学习的6-DOF抓取方法仍难以泛化到不同机械臂和实际场景。", "method": "提出GraspGen框架，结合扩散变换器架构和高效判别器，并引入新的训练方法。", "result": "在仿真和FetchBench基准测试中表现优于现有方法，并在真实机器人任务中表现良好。", "conclusion": "GraspGen通过扩散变换器和判别器的结合，显著提升了抓取生成的性能。"}}
{"id": "2507.13171", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13171", "abs": "https://arxiv.org/abs/2507.13171", "authors": ["Suzie Kim", "Hye-Bin Shin", "Seong-Whan Lee"], "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback", "comment": null, "summary": "Conventional reinforcement learning (RL) ap proaches often struggle to learn\neffective policies under sparse reward conditions, necessitating the manual\ndesign of complex, task-specific reward functions. To address this limitation,\nrein forcement learning from human feedback (RLHF) has emerged as a promising\nstrategy that complements hand-crafted rewards with human-derived evaluation\nsignals. However, most existing RLHF methods depend on explicit feedback\nmechanisms such as button presses or preference labels, which disrupt the\nnatural interaction process and impose a substantial cognitive load on the\nuser. We propose a novel reinforcement learning from implicit human feedback\n(RLIHF) framework that utilizes non-invasive electroencephalography (EEG)\nsignals, specifically error-related potentials (ErrPs), to provide continuous,\nimplicit feedback without requiring explicit user intervention. The proposed\nmethod adopts a pre-trained decoder to transform raw EEG signals into\nprobabilistic reward components, en abling effective policy learning even in\nthe presence of sparse external rewards. We evaluate our approach in a\nsimulation environment built on the MuJoCo physics engine, using a Kinova Gen2\nrobotic arm to perform a complex pick-and-place task that requires avoiding\nobstacles while manipulating target objects. The results show that agents\ntrained with decoded EEG feedback achieve performance comparable to those\ntrained with dense, manually designed rewards. These findings validate the\npotential of using implicit neural feedback for scalable and human-aligned\nreinforcement learning in interactive robotics.", "AI": {"tldr": "提出了一种基于隐式人类反馈（RLIHF）的强化学习框架，利用EEG信号（ErrPs）提供连续反馈，无需用户显式干预，在稀疏奖励条件下仍能有效学习策略。", "motivation": "传统RL方法在稀疏奖励条件下效果不佳，而现有RLHF方法依赖显式反馈，干扰自然交互且增加用户认知负担。", "method": "采用预训练解码器将EEG信号转化为概率奖励，结合稀疏外部奖励进行策略学习，并在MuJoCo环境中验证。", "result": "实验表明，基于EEG反馈的智能体性能接近手动设计密集奖励的智能体。", "conclusion": "隐式神经反馈为交互式机器人提供了一种可扩展且与人类对齐的强化学习方法。"}}
{"id": "2507.13200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13200", "abs": "https://arxiv.org/abs/2507.13200", "authors": ["Marina Y. Aoyama", "Sethu Vijayakumar", "Tetsuya Narita"], "title": "Few-shot transfer of tool-use skills using human demonstrations with proximity and tactile sensing", "comment": "8 pages, 9 figures, IEEE Robotics and Automation Letters", "summary": "Tools extend the manipulation abilities of robots, much like they do for\nhumans. Despite human expertise in tool manipulation, teaching robots these\nskills faces challenges. The complexity arises from the interplay of two\nsimultaneous points of contact: one between the robot and the tool, and another\nbetween the tool and the environment. Tactile and proximity sensors play a\ncrucial role in identifying these complex contacts. However, learning tool\nmanipulation using these sensors remains challenging due to limited real-world\ndata and the large sim-to-real gap. To address this, we propose a few-shot\ntool-use skill transfer framework using multimodal sensing. The framework\ninvolves pre-training the base policy to capture contact states common in\ntool-use skills in simulation and fine-tuning it with human demonstrations\ncollected in the real-world target domain to bridge the domain gap. We validate\nthat this framework enables teaching surface-following tasks using tools with\ndiverse physical and geometric properties with a small number of demonstrations\non the Franka Emika robot arm. Our analysis suggests that the robot acquires\nnew tool-use skills by transferring the ability to recognise tool-environment\ncontact relationships from pre-trained to fine-tuned policies. Additionally,\ncombining proximity and tactile sensors enhances the identification of contact\nstates and environmental geometry.", "AI": {"tldr": "提出了一种基于多模态感知的少样本工具使用技能迁移框架，通过仿真预训练和真实世界微调，解决了机器人工具操作中的复杂接触问题。", "motivation": "人类擅长工具操作，但机器人学习这些技能面临复杂接触和传感器数据不足的挑战。", "method": "框架包括仿真预训练基础策略和真实世界人类演示微调，结合接近和触觉传感器。", "result": "验证了框架在Franka Emika机械臂上通过少量演示成功教授表面跟随任务。", "conclusion": "机器人通过迁移识别工具-环境接触关系的能力学习新技能，多模态传感器提升了接触状态和环境几何的识别。"}}
{"id": "2507.13225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13225", "abs": "https://arxiv.org/abs/2507.13225", "authors": ["Manas Sashank Juvvi", "Tushar Dilip Kurne", "Vaishnavi J", "Shishir Kolathaya", "Pushpak Jagtap"], "title": "Signal Temporal Logic Compliant Co-design of Planning and Control", "comment": null, "summary": "This work presents a novel co-design strategy that integrates trajectory\nplanning and control to handle STL-based tasks in autonomous robots. The method\nconsists of two phases: $(i)$ learning spatio-temporal motion primitives to\nencapsulate the inherent robot-specific constraints and $(ii)$ constructing an\nSTL-compliant motion plan from these primitives. Initially, we employ\nreinforcement learning to construct a library of control policies that perform\ntrajectories described by the motion primitives. Then, we map motion primitives\nto spatio-temporal characteristics. Subsequently, we present a sampling-based\nSTL-compliant motion planning strategy tailored to meet the STL specification.\nThe proposed model-free approach, which generates feasible STL-compliant motion\nplans across various environments, is validated on differential-drive and\nquadruped robots across various STL specifications. Demonstration videos are\navailable at https://tinyurl.com/m6zp7rsm.", "AI": {"tldr": "提出了一种结合轨迹规划与控制的新策略，用于处理自主机器人中基于STL的任务。", "motivation": "解决自主机器人在复杂环境中执行STL任务时的轨迹规划与控制问题。", "method": "分两阶段：学习时空运动基元以封装机器人约束，并构建符合STL的运动计划。使用强化学习生成控制策略库，再通过采样方法实现STL合规运动规划。", "result": "验证了该无模型方法在不同环境和机器人（差速驱动和四足机器人）上的可行性。", "conclusion": "该方法能有效生成符合STL的运动计划，适用于多种机器人任务。"}}
{"id": "2507.13277", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13277", "abs": "https://arxiv.org/abs/2507.13277", "authors": ["Emma M. A. Harrison"], "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour", "comment": null, "summary": "Robots are increasingly integrated across industries, particularly in\nhealthcare. However, many valuable applications for quadrupedal robots remain\noverlooked. This research explores the effectiveness of three reinforcement\nlearning algorithms in training a simulated quadruped robot for autonomous\nnavigation and obstacle avoidance. The goal is to develop a robotic guide dog\nsimulation capable of path following and obstacle avoidance, with long-term\npotential for real-world assistance to guide dogs and visually impaired\nindividuals. It also seeks to expand research into medical 'pets', including\nrobotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key\nevaluation criteria, including collision detection, pathfinding algorithms,\nsensor usage, robot type, and simulation platforms. The study focuses on sensor\ninputs, collision frequency, reward signals, and learning progression to\ndetermine which algorithm best supports robotic navigation in complex\nenvironments.\n  Custom-made environments were used to ensure fair evaluation of all three\nalgorithms under controlled conditions, allowing consistent data collection.\nResults show that Proximal Policy Optimization (PPO) outperformed Deep\nQ-Network (DQN) and Q-learning across all metrics, particularly in average and\nmedian steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI\nand medical robotics, offering insights into the feasibility of AI-driven\nquadruped mobility and its role in assistive robotics.", "AI": {"tldr": "研究比较了三种强化学习算法在四足机器人导航和避障中的表现，发现PPO算法表现最佳，为辅助机器人和医疗宠物研究提供了新方向。", "motivation": "探索四足机器人在医疗和辅助领域的潜力，特别是作为导盲犬的模拟，以帮助视障人士。", "method": "通过比较PPO、DQN和Q-learning三种算法，在定制环境中测试其导航和避障能力，评估指标包括碰撞检测、路径规划等。", "result": "PPO算法在所有指标上优于DQN和Q-learning，尤其在平均和每集步数上表现突出。", "conclusion": "PPO算法在四足机器人导航中表现最佳，为AI驱动的辅助机器人研究提供了可行性支持。"}}
{"id": "2507.13340", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13340", "abs": "https://arxiv.org/abs/2507.13340", "authors": ["Yiqi Wang", "Mrinal Verghese", "Jeff Schneider"], "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models", "comment": null, "summary": "Learning visuomotor policies via imitation has proven effective across a wide\nrange of robotic domains. However, the performance of these policies is heavily\ndependent on the number of training demonstrations, which requires expensive\ndata collection in the real world. In this work, we aim to reduce data\ncollection efforts when learning visuomotor robot policies by leveraging\nexisting or cost-effective data from a wide range of embodiments, such as\npublic robot datasets and the datasets of humans playing with objects (human\ndata from play). Our approach leverages two key insights. First, we use optic\nflow as an embodiment-agnostic action representation to train a World Model\n(WM) across multi-embodiment datasets, and finetune it on a small amount of\nrobot data from the target embodiment. Second, we develop a method, Latent\nPolicy Steering (LPS), to improve the output of a behavior-cloned policy by\nsearching in the latent space of the WM for better action sequences. In real\nworld experiments, we observe significant improvements in the performance of\npolicies trained with a small amount of data (over 50% relative improvement\nwith 30 demonstrations and over 20% relative improvement with 50\ndemonstrations) by combining the policy with a WM pretrained on two thousand\nepisodes sampled from the existing Open X-embodiment dataset across different\nrobots or a cost-effective human dataset from play.", "AI": {"tldr": "通过模仿学习视觉运动策略效果显著，但依赖大量训练数据。本文提出利用多实施例数据（如公开机器人数据集或人类玩耍数据）减少数据收集需求，采用光流作为动作表示训练世界模型，并通过潜在策略搜索优化策略性能。实验显示，小数据量下策略性能显著提升。", "motivation": "减少学习视觉运动机器人策略时的数据收集成本，利用现有或低成本的多实施例数据（如公开机器人数据集或人类玩耍数据）。", "method": "1. 使用光流作为动作表示训练跨实施例的世界模型（WM），并在目标实施例的小数据上微调。2. 提出潜在策略搜索（LPS）方法，通过在世界模型的潜在空间中搜索优化动作序列。", "result": "实验表明，结合预训练世界模型和小数据量（30或50次演示）的策略性能显著提升（相对改进超过50%和20%）。", "conclusion": "通过利用多实施例数据和潜在策略搜索，显著减少了数据需求并提升了小数据量下的策略性能。"}}
