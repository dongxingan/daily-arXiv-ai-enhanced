<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes](https://arxiv.org/abs/2508.10973)
*Hongchen Wang,Sima Zeinali Danalou,Jiahao Zhu,Kenneth Sulimro,Chaewon Lim,Smita Basak,Aimee Tai,Usan Siriwardana,Jason Hattrick-Simpers,Jay Werber*

Main category: cs.RO

TL;DR: 开发了一种全自动平台，用于通过非溶剂诱导相分离（NIPS）制备和表征多孔聚合物膜，提高了实验效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统多孔聚合物膜的开发过程耗时且依赖试错，需要一种自动化方法来优化制备参数。

Method: 集成自动化溶液制备、刀片涂布、控制浸没和压缩测试，通过NIPS制备膜，并分析应力-应变曲线。

Result: 系统成功验证了聚合物浓度和环境湿度对膜性能的影响，如刚性和均匀性。

Conclusion: 该自动化平台支持高通量实验，适用于数据驱动的膜优化，为自驱动实验室提供了可扩展的基础。

Abstract: The development of porous polymeric membranes remains a labor-intensive
process, often requiring extensive trial and error to identify optimal
fabrication parameters. In this study, we present a fully automated platform
for membrane fabrication and characterization via nonsolvent-induced phase
separation (NIPS). The system integrates automated solution preparation, blade
casting, controlled immersion, and compression testing, allowing precise
control over fabrication parameters such as polymer concentration and ambient
humidity. The modular design allows parallel processing and reproducible
handling of samples, reducing experimental time and increasing consistency.
Compression testing is introduced as a sensitive mechanical characterization
method for estimating membrane stiffness and as a proxy to infer porosity and
intra-sample uniformity through automated analysis of stress-strain curves. As
a proof of concept to demonstrate the effectiveness of the system, NIPS was
carried out with polysulfone, the green solvent PolarClean, and water as the
polymer, solvent, and nonsolvent, respectively. Experiments conducted with the
automated system reproduced expected effects of polymer concentration and
ambient humidity on membrane properties, namely increased stiffness and
uniformity with increasing polymer concentration and humidity variations in
pore morphology and mechanical response. The developed automated platform
supports high-throughput experimentation and is well-suited for integration
into self-driving laboratory workflows, offering a scalable and reproducible
foundation for data-driven optimization of porous polymeric membranes through
NIPS.

</details>


### [2] [Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction](https://arxiv.org/abs/2508.10999)
*Yizhi Zhou,Jie Xu,Jiawei Xia,Zechen Hu,Weizi Li,Xuan Wang*

Main category: cs.RO

TL;DR: 提出了一种新型的鲁棒在线校准框架，用于超宽带（UWB）锚点在UWB辅助的视觉惯性导航系统（VINS）中的校准，解决了现有方法对机器人定位误差和初始猜测敏感的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有UWB锚点校准方法假设机器人定位准确且对初始猜测敏感，限制了其在实际应用中的鲁棒性。本文旨在解决这些问题。

Method: 通过将机器人定位不确定性纳入校准过程，并提出基于Schmidt Kalman Filter（SKF）的紧密耦合在线细化方法，增强校准的鲁棒性。

Result: 仿真和实际实验验证了该方法在准确性和鲁棒性上的改进。

Conclusion: 该方法显著提高了UWB锚点校准的鲁棒性和实用性，适用于实际应用场景。

Abstract: This paper presents a novel robust online calibration framework for
Ultra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems
(VINS). Accurate anchor positioning, a process known as calibration, is crucial
for integrating UWB ranging measurements into state estimation. While several
prior works have demonstrated satisfactory results by using robot-aided systems
to autonomously calibrate UWB systems, there are still some limitations: 1)
these approaches assume accurate robot localization during the initialization
step, ignoring localization errors that can compromise calibration robustness,
and 2) the calibration results are highly sensitive to the initial guess of the
UWB anchors' positions, reducing the practical applicability of these methods
in real-world scenarios. Our approach addresses these challenges by explicitly
incorporating the impact of robot localization uncertainties into the
calibration process, ensuring robust initialization. To further enhance the
robustness of the calibration results against initialization errors, we propose
a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,
making the system suitable for practical applications. Simulations and
real-world experiments validate the improved accuracy and robustness of our
approach.

</details>


### [3] [3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation](https://arxiv.org/abs/2508.11002)
*Nikolaos Gkanatsios,Jiahe Xu,Matthew Bronars,Arsalan Mousavian,Tsung-Wei Ke,Katerina Fragkiadaki*

Main category: cs.RO

TL;DR: 3D FlowMatch Actor (3DFA) 是一种结合流匹配和3D预训练视觉表示的机器人操作策略，显著提升了训练和推理速度，并在多个任务中达到最新技术水平。


<details>
  <summary>Details</summary>
Motivation: 通过结合流匹配和3D视觉表示，解决现有3D扩散策略训练和推理速度慢的问题，同时提升性能。

Method: 利用3D相对注意力机制和流匹配技术，结合系统级和架构优化，实现高效的动作去噪和轨迹预测。

Result: 在训练和推理速度上提升30倍以上，在PerAct2基准测试中领先41.4%，并在真实世界和RLBench任务中表现优异。

Conclusion: 3DFA通过设计优化显著提升了策略的效率和性能，为机器人操作提供了新的解决方案。

Abstract: We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot
manipulation that combines flow matching for trajectory prediction with 3D
pretrained visual scene representations for learning from demonstration. 3DFA
leverages 3D relative attention between action and visual tokens during action
denoising, building on prior work in 3D diffusion-based single-arm policy
learning. Through a combination of flow matching and targeted system-level and
architectural optimizations, 3DFA achieves over 30x faster training and
inference than previous 3D diffusion-based policies, without sacrificing
performance. On the bimanual PerAct2 benchmark, it establishes a new state of
the art, outperforming the next-best method by an absolute margin of 41.4%. In
extensive real-world evaluations, it surpasses strong baselines with up to
1000x more parameters and significantly more pretraining. In unimanual
settings, it sets a new state of the art on 74 RLBench tasks by directly
predicting dense end-effector trajectories, eliminating the need for motion
planning. Comprehensive ablation studies underscore the importance of our
design choices for both policy effectiveness and efficiency.

</details>


### [4] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL通过从多样化的跨体现数据集中提取生成的流来塑造奖励，从而学习通用且鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型依赖生成数据质量且缺乏环境反馈，难以处理精细操作，同时视频强化学习受限于视频生成的不确定性和大规模数据集收集的挑战。

Method: GenFlowRL从多样化的跨体现数据集中提取生成的流，利用低维、以对象为中心的特征学习策略。

Result: 在10个操作任务的仿真和真实跨体现评估中，GenFlowRL表现优异。

Conclusion: GenFlowRL能有效利用生成的以对象为中心的流特征，在多样化场景中表现卓越。

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [5] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: 论文提出了一种结合视觉语言模型（VLM）和纯文本语言模型（LLM）的框架GUIDER，用于增强人机协作中的意图推断和目标选择能力。


<details>
  <summary>Details</summary>
Motivation: 提升机器人在人机协作中快速推断用户意图、提供透明推理并协助用户实现目标的能力。

Method: 通过VLM和LLM形成语义先验，结合视觉管道（YOLO和Segment Anything Model）筛选目标对象和位置，加权导航和操作层以选择相关目标。

Result: 系统能够根据操作提示选择上下文相关目标，并在置信度超过阈值时自主执行导航和抓取任务。

Conclusion: 未来工作将在Isaac Sim中评估系统，重点关注实时辅助能力。

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [6] [Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC](https://arxiv.org/abs/2508.11129)
*Ryan M. Bena,Gilbert Bahati,Blake Werner,Ryan K. Cosner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种基于控制屏障函数（CBFs）的非线性模型预测控制（MPC）算法，用于动态环境中腿式机器人的安全轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化和动态环境中腿式机器人安全导航的挑战，尤其是考虑其不对称几何形状。

Method: 利用泊松安全函数从感知数据中数值合成CBF约束，扩展理论框架以处理动态边界问题，并使用Minkowski集合操作考虑机器人几何形状。

Result: 在多种安全关键场景中实现了实时预测安全过滤器，验证了泊松安全函数的通用性和CBF约束MPC控制器的优势。

Conclusion: 该方法为动态环境中腿式机器人的安全导航提供了有效解决方案。

Abstract: Autonomous navigation through unstructured and dynamically-changing
environments is a complex task that continues to present many challenges for
modern roboticists. In particular, legged robots typically possess manipulable
asymmetric geometries which must be considered during safety-critical
trajectory planning. This work proposes a predictive safety filter: a nonlinear
model predictive control (MPC) algorithm for online trajectory generation with
geometry-aware safety constraints based on control barrier functions (CBFs).
Critically, our method leverages Poisson safety functions to numerically
synthesize CBF constraints directly from perception data. We extend the
theoretical framework for Poisson safety functions to incorporate temporal
changes in the domain by reformulating the static Dirichlet problem for
Poisson's equation as a parameterized moving boundary value problem.
Furthermore, we employ Minkowski set operations to lift the domain into a
configuration space that accounts for robot geometry. Finally, we implement our
real-time predictive safety filter on humanoid and quadruped robots in various
safety-critical scenarios. The results highlight the versatility of Poisson
safety functions, as well as the benefit of CBF constrained model predictive
safety-critical controllers.

</details>


### [7] [Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective](https://arxiv.org/abs/2508.11117)
*Xuning Yang,Clemens Eppner,Jonathan Tremblay,Dieter Fox,Stan Birchfield,Fabio Ramos*

Main category: cs.RO

TL;DR: 论文探讨了设计通用机器人操作策略基准的挑战与需求，提出了高视觉保真度仿真、任务复杂性评估和性能对齐量化方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的机器人仿真基准在真实世界应用中的评估滞后，需改进通用策略的仿真到现实迁移。

Method: 1) 使用高视觉保真度仿真；2) 通过任务复杂性和扰动评估策略鲁棒性；3) 量化仿真与现实性能对齐。

Result: 提出了改进仿真到现实迁移的基准设计方法。

Conclusion: 设计的基准有助于提升通用机器人操作策略的仿真到现实迁移效果。

Abstract: Current vision-based robotics simulation benchmarks have significantly
advanced robotic manipulation research. However, robotics is fundamentally a
real-world problem, and evaluation for real-world applications has lagged
behind in evaluating generalist policies. In this paper, we discuss challenges
and desiderata in designing benchmarks for generalist robotic manipulation
policies for the goal of sim-to-real policy transfer. We propose 1) utilizing
high visual-fidelity simulation for improved sim-to-real transfer, 2)
evaluating policies by systematically increasing task complexity and scenario
perturbation to assess robustness, and 3) quantifying performance alignment
between real-world performance and its simulation counterparts.

</details>


### [8] [A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning](https://arxiv.org/abs/2508.11520)
*Evangelos Tsiatsianas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文比较了不同浮基空间参数化方法在足式机器人敏捷运动轨迹优化中的性能，并提出了一种基于SE(3)切空间的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决足式和人形机器人生成敏捷全身运动的挑战，尤其是浮基空间参数化选择对性能的影响缺乏明确指导。

Method: 通过直接转录法轨迹优化，系统比较多种常见参数化方法，并提出基于SE(3)切空间的新参数化方法。

Result: 新方法避免了复杂的流形优化技术，可直接使用成熟数值求解器。

Conclusion: 研究为选择适合的浮基表示提供了实用见解，有助于提升敏捷全身运动生成的效率。

Abstract: Automatically generating agile whole-body motions for legged and humanoid
robots remains a fundamental challenge in robotics. While numerous trajectory
optimization approaches have been proposed, there is no clear guideline on how
the choice of floating-base space parameterization affects performance,
especially for agile behaviors involving complex contact dynamics. In this
paper, we present a comparative study of different parameterizations for direct
transcription-based trajectory optimization of agile motions in legged systems.
We systematically evaluate several common choices under identical optimization
settings to ensure a fair comparison. Furthermore, we introduce a novel
formulation based on the tangent space of SE(3) for representing the robot's
floating-base pose, which, to our knowledge, has not received attention from
the literature. This approach enables the use of mature off-the-shelf numerical
solvers without requiring specialized manifold optimization techniques. We hope
that our experiments and analysis will provide meaningful insights for
selecting the appropriate floating-based representation for agile whole-body
motion generation.

</details>


### [9] [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](https://arxiv.org/abs/2508.11547)
*Martin Jiroušek,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种仅使用无人机标准传感器（RTK-GNSS和IMU）的框架，用于估计和控制悬挂负载的位置，性能接近真实测量，仅轻微下降（<6%）。


<details>
  <summary>Details</summary>
Motivation: 解决无人机悬挂负载位置跟踪问题，减少对额外硬件（如运动捕捉系统）的依赖，提升实际部署的可行性。

Method: 结合线性卡尔曼滤波、模型预测轮廓控制规划和增量模型预测控制器，建模无人机与负载的耦合动力学。

Result: 仿真显示性能接近真实测量（仅下降<6%），对负载参数变化表现出强鲁棒性；户外实验验证了实用性。

Conclusion: 该框架在硬件要求低的情况下实现了可靠的负载位置跟踪，适用于实际户外环境。

Abstract: This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.

</details>


### [10] [Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward](https://arxiv.org/abs/2508.11143)
*Jiarui Yang,Bin Zhu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: AC3是一种新型强化学习框架，通过稳定机制高效学习连续动作序列，解决了稀疏奖励下长时程机器人操作任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在稀疏奖励的长时程机器人操作任务中表现不佳，AC3旨在通过稳定且数据高效的方式学习连续动作块。

Method: AC3结合了非对称更新规则训练actor（仅从成功轨迹学习）和稳定critic更新的机制（使用n步回报和自监督模块提供内在奖励）。

Result: 在BiGym和RLBench的25个任务中，AC3仅需少量演示和简单模型架构即取得较高成功率。

Conclusion: AC3通过创新的稳定机制，显著提升了稀疏奖励下长时程机器人操作任务的性能。

Abstract: Existing reinforcement learning (RL) methods struggle with long-horizon
robotic manipulation tasks, particularly those involving sparse rewards. While
action chunking is a promising paradigm for robotic manipulation, using RL to
directly learn continuous action chunks in a stable and data-efficient manner
remains a critical challenge. This paper introduces AC3 (Actor-Critic for
Continuous Chunks), a novel RL framework that learns to generate
high-dimensional, continuous action sequences. To make this learning process
stable and data-efficient, AC3 incorporates targeted stabilization mechanisms
for both the actor and the critic. First, to ensure reliable policy
improvement, the actor is trained with an asymmetric update rule, learning
exclusively from successful trajectories. Second, to enable effective value
learning despite sparse rewards, the critic's update is stabilized using
intra-chunk $n$-step returns and further enriched by a self-supervised module
providing intrinsic rewards at anchor points aligned with each action chunk. We
conducted extensive experiments on 25 tasks from the BiGym and RLBench
benchmarks. Results show that by using only a few demonstrations and a simple
model architecture, AC3 achieves superior success rates on most tasks,
validating its effective design.

</details>


### [11] [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](https://arxiv.org/abs/2508.11573)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 论文比较了自动多段控制方法与简化的一或两段预测喷雾切换方法，提出了适合手动驾驶的低成本传感器替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在复杂农业喷雾条件下，是否存在比传统自动分段控制更简单的替代方法。

Method: 比较了三种分段设置（48段、2段和单段控制）以及两种路径规划和喷雾切换逻辑，评估了10个实际农田案例。

Result: 结果表明，简化方法在路径长度、重叠率和成本方面表现良好，适合手动驾驶。

Conclusion: 结论是简化方法可实现低成本传感器替代，同时满足喷雾需求。

Abstract: Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.

</details>


### [12] [Visuomotor Grasping with World Models for Surgical Robots](https://arxiv.org/abs/2508.11200)
*Hongbin Lin,Bin Li,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: GASv2框架通过视觉运动学习实现手术抓取，解决了模拟到现实的迁移、单摄像头输入和对象无关性等挑战，成功率达到65%。


<details>
  <summary>Details</summary>
Motivation: 自动化手术抓取可减轻外科医生负担，提高效率、安全性和一致性，但现有方法在泛化性、鲁棒性和处理变形对象方面存在局限。

Method: GASv2结合基于世界模型的架构和手术感知管道，使用域随机化训练策略，并通过混合控制系统实现安全执行。

Result: 在模拟和现实手术场景中，策略成功率达65%，能泛化到未见对象和夹持器，并适应多种干扰。

Conclusion: GASv2展示了高性能、泛化性和鲁棒性，为手术自动化提供了可行解决方案。

Abstract: Grasping is a fundamental task in robot-assisted surgery (RAS), and
automating it can reduce surgeon workload while enhancing efficiency, safety,
and consistency beyond teleoperated systems. Most prior approaches rely on
explicit object pose tracking or handcrafted visual features, limiting their
generalization to novel objects, robustness to visual disturbances, and the
ability to handle deformable objects. Visuomotor learning offers a promising
alternative, but deploying it in RAS presents unique challenges, such as low
signal-to-noise ratio in visual observations, demands for high safety and
millimeter-level precision, as well as the complex surgical environment. This
paper addresses three key challenges: (i) sim-to-real transfer of visuomotor
policies to ex vivo surgical scenes, (ii) visuomotor learning using only a
single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic
grasping with a single policy that generalizes to diverse, unseen surgical
objects without retraining or task-specific models. We introduce Grasp Anything
for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.
GASv2 leverages a world-model-based architecture and a surgical perception
pipeline for visual observations, combined with a hybrid control system for
safe execution. We train the policy in simulation using domain randomization
for sim-to-real transfer and deploy it on a real robot in both phantom-based
and ex vivo surgical settings, using only a single pair of endoscopic cameras.
Extensive experiments show our policy achieves a 65% success rate in both
settings, generalizes to unseen objects and grippers, and adapts to diverse
disturbances, demonstrating strong performance, generality, and robustness.

</details>


### [13] [Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation](https://arxiv.org/abs/2508.11204)
*Hongbin Lin,Juan Rojas,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 论文提出了一种针对非等距对称性的数据增强方法（MEA），结合离线强化学习，提高了视觉运动学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 采样效率对真实机器人操作中的视觉运动学习至关重要。现有研究多局限于等距对称性，限制了效率提升。

Method: 提出了一种非等距对称性的POMDP模型，并设计了MEA数据增强方法，结合离线强化学习和体素视觉表示。

Result: 在仿真和真实机器人实验中验证了方法的有效性。

Conclusion: 非等距对称性方法显著提升了采样效率，适用于多种操作任务。

Abstract: Sampling efficiency is critical for deploying visuomotor learning in
real-world robotic manipulation. While task symmetry has emerged as a promising
inductive bias to improve efficiency, most prior work is limited to isometric
symmetries -- applying the same group transformation to all task objects across
all timesteps. In this work, we explore non-isometric symmetries, applying
multiple independent group transformations across spatial and temporal
dimensions to relax these constraints. We introduce a novel formulation of the
partially observable Markov decision process (POMDP) that incorporates the
non-isometric symmetry structures, and propose a simple yet effective data
augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate
MEA with offline reinforcement learning to enhance sampling efficiency, and
introduce a voxel-based visual representation that preserves translational
equivariance. Extensive simulation and real-robot experiments across two
manipulation domains demonstrate the effectiveness of our approach.

</details>


### [14] [Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification](https://arxiv.org/abs/2508.11232)
*Guoliang Li,Xibin Jin,Yujie Wan,Chenxuan Liu,Tong Zhang,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 论文提出了一种结合边缘智能与近场通信的新范式（NEEI），以解决大型模型在实时推理中的计算需求，同时优化通信效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决大型模型在实时推理中的高计算需求，同时提升通信效率、安全性和减少干扰。

Method: 提出NEEI范式，结合边缘智能（EEI）和近场通信（NFC），并设计了联合优化方法，包括无线电友好的规划与波束聚焦技术。

Result: 实验结果表明，提出的技术优于多种基准方法。

Conclusion: NEEI是一种有前景的解决方案，通过联合优化EEI和NFC，能够高效实现资源利用和性能提升。

Abstract: Realizing embodied artificial intelligence is challenging due to the huge
computation demands of large models (LMs). To support LMs while ensuring
real-time inference, embodied edge intelligence (EEI) is a promising paradigm,
which leverages an LM edge to provide computing powers in close proximity to
embodied robots. Due to embodied data exchange, EEI requires higher spectral
efficiency, enhanced communication security, and reduced inter-user
interference. To meet these requirements, near-field communication (NFC), which
leverages extremely large antenna arrays as its hardware foundation, is an
ideal solution. Therefore, this paper advocates the integration of EEI and NFC,
resulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces
new challenges that cannot be adequately addressed by isolated EEI or NFC
designs, creating research opportunities for joint optimization of both
functionalities. To this end, we propose radio-friendly embodied planning for
EEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI
scenarios. We also elaborate how to realize resource-efficient NEEI through
opportunistic collaborative navigation. Experimental results are provided to
confirm the superiority of the proposed techniques compared with various
benchmarks.

</details>


### [15] [Tactile Robotics: An Outlook](https://arxiv.org/abs/2508.11261)
*Shan Luo,Nathan F. Lepora,Wenzhen Yuan,Kaspar Althoefer,Gordon Cheng,Ravinder Dahiya*

Main category: cs.RO

TL;DR: 本文探讨了机器人触觉感知技术的发展现状、挑战及未来潜力，强调了其在人机交互中的重要性。


<details>
  <summary>Details</summary>
Motivation: 赋予机器人类似生物系统的触觉感知能力，以支持其与人类的紧密互动。

Method: 综述了多种触觉传感技术（如压阻、压电、电容、磁性和光学传感器）及模拟工具的应用。

Result: 触觉感知技术已取得显著进展，但仍需解决多模态集成和实际应用中的挑战。

Conclusion: 需采取整体方法推动触觉机器人技术的变革性发展，并在多个领域探索创新解决方案。

Abstract: Robotics research has long sought to give robots the ability to perceive the
physical world through touch in an analogous manner to many biological systems.
Developing such tactile capabilities is important for numerous emerging
applications that require robots to co-exist and interact closely with humans.
Consequently, there has been growing interest in tactile sensing, leading to
the development of various technologies, including piezoresistive and
piezoelectric sensors, capacitive sensors, magnetic sensors, and optical
tactile sensors. These diverse approaches utilise different transduction
methods and materials to equip robots with distributed sensing capabilities,
enabling more effective physical interactions. These advances have been
supported in recent years by simulation tools that generate large-scale tactile
datasets to support sensor designs and algorithms to interpret and improve the
utility of tactile data. The integration of tactile sensing with other
modalities, such as vision, as well as with action strategies for active
tactile perception highlights the growing scope of this field. To further the
transformative progress in tactile robotics, a holistic approach is essential.
In this outlook article, we examine several challenges associated with the
current state of the art in tactile robotics and explore potential solutions to
inspire innovations across multiple domains, including manufacturing,
healthcare, recycling and agriculture.

</details>


### [16] [Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation](https://arxiv.org/abs/2508.11275)
*Masaki Murooka,Iori Kumagai,Mitsuharu Morisawa,Fumio Kanehiro*

Main category: cs.RO

TL;DR: 提出了一种可微分可达性地图的新方法，用于降低人形机器人运动生成的算力成本。


<details>
  <summary>Details</summary>
Motivation: 减少人形机器人运动生成的计算成本。

Method: 通过神经网络或支持向量机学习可微分可达性地图，并将其作为约束条件用于连续优化运动规划。

Result: 该方法高效解决了多种运动规划问题，包括步态规划、多接触运动规划和操作运动规划。

Conclusion: 可微分可达性地图为连续优化运动规划提供了有效工具。

Abstract: To reduce the computational cost of humanoid motion generation, we introduce
a new approach to representing robot kinematic reachability: the differentiable
reachability map. This map is a scalar-valued function defined in the task
space that takes positive values only in regions reachable by the robot's
end-effector. A key feature of this representation is that it is continuous and
differentiable with respect to task-space coordinates, enabling its direct use
as constraints in continuous optimization for humanoid motion planning. We
describe a method to learn such differentiable reachability maps from a set of
end-effector poses generated using a robot's kinematic model, using either a
neural network or a support vector machine as the learning model. By
incorporating the learned reachability map as a constraint, we formulate
humanoid motion generation as a continuous optimization problem. We demonstrate
that the proposed approach efficiently solves various motion planning problems,
including footstep planning, multi-contact motion planning, and
loco-manipulation planning for humanoid robots.

</details>


### [17] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: 论文提出了一种主动重新规划框架，通过比较当前场景图与参考图，在子任务边界检测并纠正潜在失败，提升机器人任务的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类能根据环境状态调整行为，而自主机器人常因缺乏适应性导致失败。现有方法多为被动响应，主动规划依赖人工规则和监督。

Method: 构建当前RGB-D观测的场景图与成功演示的参考图对比，在子任务边界检测不匹配时，轻量级推理模块调整计划。

Result: 在AI2-THOR模拟器中，该方法能提前检测语义和空间不匹配，显著提高任务成功率和鲁棒性。

Conclusion: 主动重新规划框架有效预防执行失败，为机器人适应性行为提供了新思路。

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [18] [A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation](https://arxiv.org/abs/2508.11289)
*Lin Li,Xueming Liu,Zhoujingzi Qiu,Tianjiang Hu,Qingrui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于递归总最小二乘法（RTLS）的在线目标定位与跟踪方法，解决了仅方位目标运动分析（TMA）的非线性和范围信息缺失问题，并通过环绕控制器提升系统可观测性和估计器收敛性。


<details>
  <summary>Details</summary>
Motivation: 仅方位TMA因测量模型非线性和缺乏范围信息导致可观测性和估计器收敛性差，需改进。

Method: 采用递归总最小二乘法（RTLS）和环绕控制器，结合移动观测器进行目标定位与跟踪。

Result: 仿真和实验验证了方法的有效性和鲁棒性，性能优于现有方法。

Conclusion: RTLS方法在精度和稳定性上优于伪线性卡尔曼滤波（PLKF）等现有方法。

Abstract: Bearing-only Target Motion Analysis (TMA) is a promising technique for
passive tracking in various applications as a bearing angle is easy to measure.
Despite its advantages, bearing-only TMA is challenging due to the nonlinearity
of the bearing measurement model and the lack of range information, which
impairs observability and estimator convergence. This paper addresses these
issues by proposing a Recursive Total Least Squares (RTLS) method for online
target localization and tracking using mobile observers. The RTLS approach,
inspired by previous results on Total Least Squares (TLS), mitigates biases in
position estimation and improves computational efficiency compared to
pseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a
circumnavigation controller to enhance system observability and estimator
convergence by guiding the mobile observer in orbit around the target.
Extensive simulations and experiments are performed to demonstrate the
effectiveness and robustness of the proposed method. The proposed algorithm is
also compared with the state-of-the-art approaches, which confirms its superior
performance in terms of both accuracy and stability.

</details>


### [19] [Pedestrian Dead Reckoning using Invariant Extended Kalman Filter](https://arxiv.org/abs/2508.11396)
*Jingran Zhang,Zhengzhang Yan,Yiming Chen,Zeqiang He,Jiahao Chen*

Main category: cs.RO

TL;DR: 提出了一种在GPS缺失环境下成本效益高的惯性行人航位推算方法，通过伪测量和不变扩展卡尔曼滤波（InEKF）提高精度。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失环境下，为双足机器人提供一种成本效益高的定位方法。

Method: 利用惯性测量单元（IMU）在支撑脚时的伪测量，结合不变扩展卡尔曼滤波（InEKF）进行预测和校正。

Result: 实验表明InEKF比标准EKF更易调参且在实际机器人系统中可行。

Conclusion: InEKF在双足机器人定位中表现优越，具有实际应用潜力。

Abstract: This paper presents a cost-effective inertial pedestrian dead reckoning
method for the bipedal robot in the GPS-denied environment. Each time when the
inertial measurement unit (IMU) is on the stance foot, a stationary
pseudo-measurement can be executed to provide innovation to the IMU measurement
based prediction. The matrix Lie group based theoretical development of the
adopted invariant extended Kalman filter (InEKF) is set forth for tutorial
purpose. Three experiments are conducted to compare between InEKF and standard
EKF, including motion capture benchmark experiment, large-scale multi-floor
walking experiment, and bipedal robot experiment, as an effort to show our
method's feasibility in real-world robot system. In addition, a sensitivity
analysis is included to show that InEKF is much easier to tune than EKF.

</details>


### [20] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: AI与机器人技术结合，用于核设施结构检查，提高安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查存在安全风险、认知负担和人为误差，需更高效、安全的替代方法。

Method: 研究采用AI辅助视觉裂纹检测，集成到移动Jackal机器人平台中，实现人机协作（HRC）。

Result: 实验表明，HRC提高了检查准确性并减少操作员负担，优于传统人工方法。

Conclusion: AI与机器人协作在核设施检查中具有潜力，可替代传统方法。

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>


### [21] [Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing](https://arxiv.org/abs/2508.11406)
*Benjamin Alt,Mareike Picklum,Sorin Arion,Franklin Kenghagho Kenfack,Michael Beetz*

Main category: cs.RO

TL;DR: 论文提出了一种语义执行追踪框架和AICOR VRB平台，旨在实现透明、可复现的机器人自主科学实验。


<details>
  <summary>Details</summary>
Motivation: 实现机器人科学实验的透明性、可重复性和可信任性。

Method: 开发语义执行追踪框架记录传感器数据和机器人信念状态，并构建AICOR VRB云平台用于共享和验证任务执行。

Result: 工具整合了确定性执行、语义记忆和开放知识表示，支持机器人参与科学发现。

Conclusion: 这些工具为自主系统参与科学发现奠定了基础。

Abstract: We envision a future in which autonomous robots conduct scientific
experiments in ways that are not only precise and repeatable, but also open,
trustworthy, and transparent. To realize this vision, we present two key
contributions: a semantic execution tracing framework that logs sensor data
together with semantically annotated robot belief states, ensuring that
automated experimentation is transparent and replicable; and the AICOR Virtual
Research Building (VRB), a cloud-based platform for sharing, replicating, and
validating robot task executions at scale. Together, these tools enable
reproducible, robot-driven science by integrating deterministic execution,
semantic memory, and open knowledge representation, laying the foundation for
autonomous systems to participate in scientific discovery.

</details>


### [22] [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](https://arxiv.org/abs/2508.11453)
*Jiayue Jin,Lang Qian,Jingyu Zhang,Chuanyu Ju,Liang Song*

Main category: cs.RO

TL;DR: 论文提出了一种名为EvoPSF的在线进化框架，通过利用规划状态反馈来提升自动驾驶系统在新环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统多为离线训练，缺乏在线适应新环境的机制，导致泛化能力不足。

Method: 利用规划不确定性作为触发信号，通过自监督损失对关键对象进行针对性模型更新。

Result: 实验表明，EvoPSF在nuScenes数据集上显著提升了规划性能。

Conclusion: EvoPSF通过在线进化机制增强了自动驾驶系统对动态环境的适应性和规划准确性。

Abstract: Recent years have witnessed remarkable progress in autonomous driving, with
systems evolving from modular pipelines to end-to-end architectures. However,
most existing methods are trained offline and lack mechanisms to adapt to new
environments during deployment. As a result, their generalization ability
diminishes when faced with unseen variations in real-world driving scenarios.
In this paper, we break away from the conventional "train once, deploy forever"
paradigm and propose EvoPSF, a novel online Evolution framework for autonomous
driving based on Planning-State Feedback. We argue that planning failures are
primarily caused by inaccurate object-level motion predictions, and such
failures are often reflected in the form of increased planner uncertainty. To
address this, we treat planner uncertainty as a trigger for online evolution,
using it as a diagnostic signal to initiate targeted model updates. Rather than
performing blind updates, we leverage the planner's agent-agent attention to
identify the specific objects that the ego vehicle attends to most, which are
primarily responsible for the planning failures. For these critical objects, we
compute a targeted self-supervised loss by comparing their predicted waypoints
from the prediction module with their actual future positions, selected from
the perception module's outputs with high confidence scores. This loss is then
backpropagated to adapt the model online. As a result, our method improves the
model's robustness to environmental changes, leads to more precise motion
predictions, and therefore enables more accurate and stable planning behaviors.
Experiments on both cross-region and corrupted variants of the nuScenes dataset
demonstrate that EvoPSF consistently improves planning performance under
challenging conditions.

</details>


### [23] [OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation](https://arxiv.org/abs/2508.11479)
*Tatiana Zemskova,Aleksei Staroverov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: OVSegDT是一种轻量级Transformer策略，通过语义分支和熵自适应损失调制，解决了开放词汇目标导航中的过拟合和碰撞问题，显著提升了泛化能力和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端策略在小规模模拟数据集上过拟合，泛化能力差且行为不安全（频繁碰撞）。

Method: OVSegDT包含语义分支（目标二进制掩码编码器和辅助分割损失函数）和熵自适应损失调制（动态平衡模仿与强化信号）。

Result: 训练样本复杂度降低33%，碰撞次数减少一半，在未见类别上性能与已见类别相当（40.1% SR，20.9% SPL）。

Conclusion: OVSegDT在不依赖深度、里程计或大型视觉语言模型的情况下，实现了开放词汇目标导航的最先进性能。

Abstract: Open-vocabulary Object Goal Navigation requires an embodied agent to reach
objects described by free-form language, including categories never seen during
training. Existing end-to-end policies overfit small simulator datasets,
achieving high success on training scenes but failing to generalize and
exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a
lightweight transformer policy that tackles these issues with two synergistic
components. The first component is the semantic branch, which includes an
encoder for the target binary mask and an auxiliary segmentation loss function,
grounding the textual goal and providing precise spatial cues. The second
component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample
scheduler that continuously balances imitation and reinforcement signals
according to the policy entropy, eliminating brittle manual phase switches.
These additions cut the sample complexity of training by 33%, and reduce
collision count in two times while keeping inference cost low (130M parameters,
RGB-only input). On HM3D-OVON, our model matches the performance on unseen
categories to that on seen ones and establishes state-of-the-art results (40.1%
SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language
models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.

</details>


### [24] [i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping](https://arxiv.org/abs/2508.11485)
*Hailiang Tang,Tisheng Zhang,Liqiang Wang,Xin Ding,Man Yuan,Zhiyu Xiang,Jujin Chen,Yuhan Bian,Shuangyan Liu,Yuqing Wang,Guan Wang,Xiaoji Niu*

Main category: cs.RO

TL;DR: i2Nav-Robot是一个大规模多传感器融合数据集，旨在解决UGV导航和地图绘制中的传感器配置、时间同步和场景多样性不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前UGV数据集在传感器配置、时间同步、地面真实性和场景多样性方面存在不足，限制了导航和地图绘制技术的发展。

Method: 集成多模态传感器（如固态LiDAR、4D雷达、立体相机等），通过硬件同步和离线校准获取精确时间戳，覆盖多样化的室内外场景。

Result: 数据集包含10个大规模序列，总长约17060米，厘米级精度地面真实数据，经多个开源系统验证具有高质量。

Conclusion: i2Nav-Robot数据集为多传感器融合导航和地图绘制提供了高质量资源，推动了UGV技术的发展。

Abstract: Accurate and reliable navigation is crucial for autonomous unmanned ground
vehicle (UGV). However, current UGV datasets fall short in meeting the demands
for advancing navigation and mapping techniques due to limitations in sensor
configuration, time synchronization, ground truth, and scenario diversity. To
address these challenges, we present i2Nav-Robot, a large-scale dataset
designed for multi-sensor fusion navigation and mapping in indoor-outdoor
environments. We integrate multi-modal sensors, including the newest front-view
and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,
odometer, global navigation satellite system (GNSS) receiver, and inertial
measurement units (IMU) on an omnidirectional wheeled robot. Accurate
timestamps are obtained through both online hardware synchronization and
offline calibration for all sensors. The dataset comprises ten larger-scale
sequences covering diverse UGV operating scenarios, such as outdoor streets,
and indoor parking lots, with a total length of about 17060 meters.
High-frequency ground truth, with centimeter-level accuracy for position, is
derived from post-processing integrated navigation methods using a
navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more
than ten open-sourced multi-sensor fusion systems, and it has proven to have
superior data quality.

</details>


### [25] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: 论文提出了一种基于极坐标的轨迹预测与规划方法Polaris，解决了传统笛卡尔坐标系在建模动态环境中车辆与周围元素关系时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在笛卡尔坐标系中建模车辆与周围元素的关系时，未能有效捕捉距离和方向的影响，导致预测和规划效果不佳。

Method: 采用极坐标系表示位置（半径和角度），设计了Polaris方法，通过专用编码和细化模块显式建模距离和方向变化。

Result: 在Argoverse 2和nuPlan基准测试中，Polaris实现了最先进的性能。

Conclusion: 极坐标表示能更直观地建模空间关系和动态变化，为自动驾驶中的轨迹预测与规划提供了更优解决方案。

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [26] [Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language](https://arxiv.org/abs/2508.11498)
*Agnes Bressan de Almeida,Joao Aires Correa Fernandes Marsicano*

Main category: cs.RO

TL;DR: Swarm in Blocks 2.0是一个基于块语言的无人机群编程工具，简化了群管理，适合初学者和教育用途。


<details>
  <summary>Details</summary>
Motivation: 随着无人机群在配送、农业和监控等领域的应用增加，管理复杂性也随之提高，Atena团队开发此工具以降低ROS和编程知识门槛。

Method: 基于Clover平台，通过组装代码块实现循环和条件结构等功能。

Result: 2023年推出的2.0版本进一步优化了用户友好性，提升了群管理效率。

Conclusion: 该工具不仅简化了群控制，还为编程教育提供了新机会。

Abstract: Swarm in Blocks, originally developed for CopterHack 2022, is a high-level
interface that simplifies drone swarm programming using a block-based language.
Building on the Clover platform, this tool enables users to create
functionalities like loops and conditional structures by assembling code
blocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the
platform to address the complexities of swarm management in a user-friendly
way. As drone swarm applications grow in areas like delivery, agriculture, and
surveillance, the challenge of managing them, especially for beginners, has
also increased. The Atena team developed this interface to make swarm handling
accessible without requiring extensive knowledge of ROS or programming. The
block-based approach not only simplifies swarm control but also expands
educational opportunities in programming.

</details>


### [27] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 论文提出了一种用于行星表面自主导航的仿真到现实框架，通过大规模并行仿真训练强化学习代理，并在真实环境中零样本部署。


<details>
  <summary>Details</summary>
Motivation: 解决行星表面非结构化地形导航中仿真到现实的差距问题，特别是轮式机器人与颗粒介质交互的复杂动力学。

Method: 利用大规模并行仿真训练强化学习代理，生成多样化的随机环境，并在真实月球模拟设施中零样本部署。

Result: 实验证明，具有程序多样性的代理在零样本性能上优于静态场景训练的代理，同时分析了高保真粒子物理微调的权衡。

Conclusion: 该框架为开发可靠的基于学习的导航系统提供了验证的工作流程，是自主机器人在太空探索中部署的关键一步。

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [28] [MultiPark: Multimodal Parking Transformer with Next-Segment Prediction](https://arxiv.org/abs/2508.11537)
*Han Zheng,Zikang Zhou,Guli Zhang,Zhepei Wang,Kaixuan Wang,Peiliang Li,Shaojie Shen,Ming Yang,Tong Qin*

Main category: cs.RO

TL;DR: MultiPark是一种基于自回归Transformer的多模态停车方法，通过分段预测和分解查询设计解决停车行为的多样性和因果混淆问题，并在实际场景中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 停车在高度受限空间中的准确性和安全性是一个关键挑战，现有模仿学习方法未能处理停车行为的多模态性和因果混淆问题。

Method: 提出MultiPark，采用自回归Transformer和分段预测范式，设计可学习的停车查询（档位、纵向和横向组件），并使用目标中心位姿和自中心碰撞损失。

Result: 在真实数据集上评估，MultiPark实现了最先进的性能，并在实际车辆部署中验证了其鲁棒性。

Conclusion: MultiPark通过多模态设计和因果混淆缓解，显著提升了复杂停车场景下的性能。

Abstract: Parking accurately and safely in highly constrained spaces remains a critical
challenge. Unlike structured driving environments, parking requires executing
complex maneuvers such as frequent gear shifts and steering saturation. Recent
attempts to employ imitation learning (IL) for parking have achieved promising
results. However, existing works ignore the multimodal nature of parking
behavior in lane-free open space, failing to derive multiple plausible
solutions under the same situation. Notably, IL-based methods encompass
inherent causal confusion, so enabling a neural network to generalize across
diverse parking scenarios is particularly difficult. To address these
challenges, we propose MultiPark, an autoregressive transformer for multimodal
parking. To handle paths filled with abrupt turning points, we introduce a
data-efficient next-segment prediction paradigm, enabling spatial
generalization and temporal extrapolation. Furthermore, we design learnable
parking queries factorized into gear, longitudinal, and lateral components,
parallelly decoding diverse parking behaviors. To mitigate causal confusion in
IL, our method employs target-centric pose and ego-centric collision as
outcome-oriented loss across all modalities beyond pure imitation loss.
Evaluations on real-world datasets demonstrate that MultiPark achieves
state-of-the-art performance across various scenarios. We deploy MultiPark on a
production vehicle, further confirming our approach's robustness in real-world
parking environments.

</details>


### [29] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: VPEngine是一个模块化框架，通过共享基础模型和并行任务特定模型头，优化GPU使用，实现视觉多任务处理，速度提升3倍。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的机器人平台上部署多个机器学习模型时出现的计算冗余、内存占用大和集成复杂的问题。

Method: 使用共享基础模型提取图像表示，并行运行多个任务特定模型头，避免GPU-CPU内存传输，支持动态任务优先级调整。

Result: 在DINOv2基础模型上实现深度、目标检测和语义分割任务，速度提升3倍，实时性能达50 Hz。

Conclusion: VPEngine通过高效GPU利用和动态任务管理，为机器人社区提供了一个开源、易用的视觉多任务处理框架。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [30] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: 论文研究了农业采摘中抓取状态的分类方法，通过集成多种传感器并使用随机森林和LSTM模型，发现随机森林在实验室和实际环境中表现优异，准确率达100%。


<details>
  <summary>Details</summary>
Motivation: 农业环境的复杂性和遮挡问题使得准确识别抓取状态成为挑战，需要选择合适的传感器和建模技术以提高采摘效率。

Method: 研究集成了IMU、红外反射、张力、触觉传感器和RGB摄像头，比较了随机森林和LSTM模型的性能。

Result: 随机森林在实验室和实际樱桃番茄采摘中实现了100%的准确率，并确定了IMU和张力传感器的最小可行组合。

Conclusion: 该分类器能基于实时反馈规划纠正动作，显著提升了采摘效率和可靠性。

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>
