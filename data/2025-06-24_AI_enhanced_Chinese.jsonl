{"id": "2506.17488", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17488", "abs": "https://arxiv.org/abs/2506.17488", "authors": ["Pei-An Hsieh", "Kong Yao Chee", "M. Ani Hsieh"], "title": "Online Adaptation for Flying Quadrotors in Tight Formations", "comment": "10 pages, 4 figures", "summary": "The task of flying in tight formations is challenging for teams of quadrotors\nbecause the complex aerodynamic wake interactions can destabilize individual\nteam members as well as the team. Furthermore, these aerodynamic effects are\nhighly nonlinear and fast-paced, making them difficult to model and predict. To\novercome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed\nexpert learning based control framework that allows individual quadrotors to\naccurately track trajectories while adapting to time-varying aerodynamic\ninteractions during formation flights. We evaluate L1 KNODE-DW MPC in two\ndifferent three-quadrotor formations and show that it outperforms several MPC\nbaselines. Our results show that the proposed framework is capable of enabling\nthe three-quadrotor team to remain vertically aligned in close proximity\nthroughout the flight. These findings show that the L1 adaptive module\ncompensates for unmodeled disturbances most effectively when paired with an\naccurate dynamics model. A video showcasing our framework and the physical\nexperiments is available here: https://youtu.be/9QX1Q5Ut9Rs", "AI": {"tldr": "L1 KNODE-DW MPC框架通过自适应学习和精确动力学模型，解决了四旋翼无人机编队飞行中的气动干扰问题。", "motivation": "四旋翼无人机在紧密编队飞行时，复杂的气动尾流干扰会导致不稳定，且这些干扰难以建模和预测。", "method": "提出L1 KNODE-DW MPC框架，结合自适应学习和模型预测控制，使无人机能够适应时变气动干扰。", "result": "在两种三机编队中，该框架优于其他MPC基线，实现了紧密编队飞行。", "conclusion": "L1自适应模块与精确动力学模型结合，能有效补偿未建模干扰，提升编队稳定性。"}}
{"id": "2506.17775", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17775", "abs": "https://arxiv.org/abs/2506.17775", "authors": ["Sebastian Sansoni", "Javier Gimenez", "Gastón Castro", "Santiago Tosetti", "Flavio Craparo"], "title": "Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems", "comment": null, "summary": "Accurate reconstruction of the environment is a central goal of Simultaneous\nLocalization and Mapping (SLAM) systems. However, the agent's trajectory can\nsignificantly affect estimation accuracy. This paper presents a new method to\nmodel map uncertainty in Active SLAM systems using an Uncertainty Map (UM). The\nUM uses probability distributions to capture where the map is uncertain,\nallowing Uncertainty Frontiers (UF) to be defined as key\nexploration-exploitation objectives and potential stopping criteria. In\naddition, the method introduces the Signed Relative Entropy (SiREn), based on\nthe Kullback-Leibler divergence, to measure both coverage and uncertainty\ntogether. This helps balance exploration and exploitation through an\neasy-to-understand parameter. Unlike methods that depend on particular SLAM\nsetups, the proposed approach is compatible with different types of sensors,\nsuch as cameras, LiDARs, and multi-sensor fusion. It also addresses common\nproblems in exploration planning and stopping conditions. Furthermore,\nintegrating this map modeling approach with a UF-based planning system enables\nthe agent to autonomously explore open spaces, a behavior not previously\nobserved in the Active SLAM literature. Code and implementation details are\navailable as a ROS node, and all generated data are openly available for public\nuse, facilitating broader adoption and validation of the proposed approach.", "AI": {"tldr": "提出了一种基于不确定性地图（UM）的新方法，用于主动SLAM系统中建模地图不确定性，通过定义不确定性边界（UF）和引入SiREn度量，平衡探索与利用。", "motivation": "解决SLAM系统中因轨迹导致的地图估计不准确问题，并改进探索规划和停止条件。", "method": "使用概率分布建模地图不确定性，定义UF作为目标，引入SiREn度量覆盖与不确定性。", "result": "方法兼容多种传感器，解决了探索规划和停止条件问题，并实现自主探索开放空间。", "conclusion": "提出的方法具有通用性和实用性，代码和数据公开，便于推广和验证。"}}
{"id": "2506.17811", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17811", "abs": "https://arxiv.org/abs/2506.17811", "authors": ["Jacky Kwok", "Christopher Agia", "Rohan Sinha", "Matt Foutter", "Shulu Li", "Ion Stoica", "Azalia Mirhoseini", "Marco Pavone"], "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities\nin visuomotor control, yet ensuring their robustness in unstructured real-world\nenvironments remains a persistent challenge. In this paper, we investigate\ntest-time scaling through the lens of sampling and verification as means to\nenhance the robustness and generalization of VLAs. We first demonstrate that\nthe relationship between action error and the number of generated samples\nfollows an exponentiated power law across a range of VLAs, indicating the\nexistence of inference-time scaling laws. Building on these insights, we\nintroduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,\nRoboMonkey samples a small set of actions from a VLA, applies Gaussian\nperturbation and majority voting to construct an action proposal distribution,\nand then uses a Vision Language Model (VLM)-based verifier to select the\noptimal action. We propose a synthetic data generation pipeline for training\nsuch VLM-based action verifiers, and demonstrate that scaling the synthetic\ndataset consistently improves verification and downstream accuracy. Through\nextensive simulated and hardware experiments, we show that pairing existing\nVLAs with RoboMonkey yields significant performance gains, achieving a 25%\nabsolute improvement on out-of-distribution tasks and 8% on in-distribution\ntasks. Additionally, when adapting to new robot setups, we show that\nfine-tuning both VLAs and action verifiers yields a 7% performance increase\ncompared to fine-tuning VLAs alone.", "AI": {"tldr": "论文提出RoboMonkey框架，通过采样和验证增强Vision-Language-Action模型的鲁棒性，显著提升任务性能。", "motivation": "解决Vision-Language-Action模型在非结构化真实环境中的鲁棒性问题。", "method": "提出RoboMonkey框架，结合高斯扰动、多数投票和基于VLM的验证器优化动作选择。", "result": "在分布外任务上提升25%，分布内任务提升8%；适配新机器人时性能提升7%。", "conclusion": "RoboMonkey有效提升VLA模型的鲁棒性和泛化能力。"}}
{"id": "2506.17832", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17832", "abs": "https://arxiv.org/abs/2506.17832", "authors": ["Pratik Kunapuli", "Jake Welde", "Dinesh Jayaraman", "Vijay Kumar"], "title": "Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking", "comment": "Accepted for publication to RSS 2025. 10 pages, 5 figures. Project\n  website: https://pratikkunapuli.github.io/rl-vs-gc/", "summary": "Learning-based control approaches like reinforcement learning (RL) have\nrecently produced a slew of impressive results for tasks like quadrotor\ntrajectory tracking and drone racing. Naturally, it is common to demonstrate\nthe advantages of these new controllers against established methods like\nanalytical controllers. We observe, however, that reliably comparing the\nperformance of such very different classes of controllers is more complicated\nthan might appear at first sight. As a case study, we take up the problem of\nagile tracking of an end-effector for a quadrotor with a fixed arm. We develop\na set of best practices for synthesizing the best-in-class RL and geometric\ncontrollers (GC) for benchmarking. In the process, we resolve widespread\nRL-favoring biases in prior studies that provide asymmetric access to: (1) the\ntask definition, in the form of an objective function, (2) representative\ndatasets, for parameter optimization, and (3) feedforward information,\ndescribing the desired future trajectory. The resulting findings are the\nfollowing: our improvements to the experimental protocol for comparing learned\nand classical controllers are critical, and each of the above asymmetries can\nyield misleading conclusions. Prior works have claimed that RL outperforms GC,\nbut we find the gaps between the two controller classes are much smaller than\npreviously published when accounting for symmetric comparisons. Geometric\ncontrol achieves lower steady-state error than RL, while RL has better\ntransient performance, resulting in GC performing better in relatively slow or\nless agile tasks, but RL performing better when greater agility is required.\nFinally, we open-source implementations of geometric and RL controllers for\nthese aerial vehicles, implementing best practices for future development.\nWebsite and code is available at https://pratikkunapuli.github.io/rl-vs-gc/", "AI": {"tldr": "论文探讨了基于学习的控制方法（如强化学习RL）与传统几何控制器（GC）在四旋翼无人机轨迹跟踪任务中的性能比较，指出现有研究中的不对称比较会导致误导性结论。", "motivation": "研究动机在于揭示RL与GC性能比较中的不对称性问题，提出公平比较的最佳实践。", "method": "通过改进实验协议，消除任务定义、数据集和前馈信息的不对称性，对RL和GC进行对称比较。", "result": "研究发现RL与GC的性能差距比之前报道的小，GC在稳态误差上表现更好，而RL在瞬态性能上更优。", "conclusion": "结论强调了对称比较的重要性，并开源了控制器实现以促进未来研究。"}}
{"id": "2506.17328", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17328", "abs": "https://arxiv.org/abs/2506.17328", "authors": ["Yufan Liu", "Yi Wu", "Gweneth Ge", "Haoliang Cheng", "Rui Liu"], "title": "Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation", "comment": null, "summary": "Desktop cleaning demands open-vocabulary recognition and precise manipulation\nfor heterogeneous debris. We propose a hierarchical framework integrating\nreflective Vision-Language Model (VLM) planning with dual-arm execution via\nstructured scene representation. Grounded-SAM2 facilitates open-vocabulary\ndetection, while a memory-augmented VLM generates, critiques, and revises\nmanipulation sequences. These sequences are converted into parametric\ntrajectories for five primitives executed by coordinated Franka arms. Evaluated\nin simulated scenarios, our system achieving 87.2% task completion, a 28.8%\nimprovement over static VLM and 36.2% over single-arm baselines. Structured\nmemory integration proves crucial for robust, generalizable manipulation while\nmaintaining real-time control performance.", "AI": {"tldr": "提出了一种结合视觉语言模型（VLM）规划和双臂执行的层次化框架，用于桌面清洁任务，显著提高了任务完成率。", "motivation": "解决桌面清洁中开放词汇识别和精确操作的需求，处理异构碎片。", "method": "使用Grounded-SAM2进行开放词汇检测，结合记忆增强的VLM生成、评估和修订操作序列，转换为参数化轨迹并由双Franka臂执行。", "result": "在模拟场景中任务完成率达到87.2%，比静态VLM提高28.8%，比单臂基线提高36.2%。", "conclusion": "结构化记忆集成对鲁棒、可泛化的操作至关重要，同时保持实时控制性能。"}}
{"id": "2506.17378", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17378", "abs": "https://arxiv.org/abs/2506.17378", "authors": ["Abhishek Phadke", "Shakib Mahmud Dipto", "Pratip Rana"], "title": "A workflow for generating synthetic LiDAR datasets in simulation environments", "comment": null, "summary": "This paper presents a simulation workflow for generating synthetic LiDAR\ndatasets to support autonomous vehicle perception, robotics research, and\nsensor security analysis. Leveraging the CoppeliaSim simulation environment and\nits Python API, we integrate time-of-flight LiDAR, image sensors, and two\ndimensional scanners onto a simulated vehicle platform operating within an\nurban scenario. The workflow automates data capture, storage, and annotation\nacross multiple formats (PCD, PLY, CSV), producing synchronized multimodal\ndatasets with ground truth pose information. We validate the pipeline by\ngenerating large-scale point clouds and corresponding RGB and depth imagery.\nThe study examines potential security vulnerabilities in LiDAR data, such as\nadversarial point injection and spoofing attacks, and demonstrates how\nsynthetic datasets can facilitate the evaluation of defense strategies.\nFinally, limitations related to environmental realism, sensor noise modeling,\nand computational scalability are discussed, and future research directions,\nsuch as incorporating weather effects, real-world terrain models, and advanced\nscanner configurations, are proposed. The workflow provides a versatile,\nreproducible framework for generating high-fidelity synthetic LiDAR datasets to\nadvance perception research and strengthen sensor security in autonomous\nsystems. Documentation and examples accompany this framework; samples of\nanimated cloud returns and image sensor data can be found at this Link.", "AI": {"tldr": "本文提出了一种用于生成合成LiDAR数据集的仿真工作流程，支持自动驾驶感知、机器人研究和传感器安全分析。", "motivation": "为自动驾驶感知、机器人研究和传感器安全分析提供高质量合成数据，并探索LiDAR数据的安全漏洞。", "method": "利用CoppeliaSim仿真环境和Python API，集成LiDAR、图像传感器和二维扫描仪，自动化数据捕获、存储和标注。", "result": "生成了大规模点云及同步多模态数据集，验证了LiDAR数据的安全漏洞（如对抗点注入和欺骗攻击）。", "conclusion": "该工作流程为生成高保真合成LiDAR数据提供了可重复框架，未来可扩展环境真实性和传感器噪声建模。"}}
{"id": "2506.17458", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17458", "abs": "https://arxiv.org/abs/2506.17458", "authors": ["Abhay Negi", "Omey M. Manyar", "Satyandra K. Gupta"], "title": "Kinematic Model Optimization via Differentiable Contact Manifold for In-Space Manipulation", "comment": "Accepted and presented in RSS 2025 Space Robotics Workshop\n  (https://albee.github.io/space-robotics-rss/). 3 pages with 1 figure", "summary": "Robotic manipulation in space is essential for emerging applications such as\ndebris removal and in-space servicing, assembly, and manufacturing (ISAM). A\nkey requirement for these tasks is the ability to perform precise, contact-rich\nmanipulation under significant uncertainty. In particular, thermal-induced\ndeformation of manipulator links and temperature-dependent encoder bias\nintroduce kinematic parameter errors that significantly degrade end-effector\naccuracy. Traditional calibration techniques rely on external sensors or\ndedicated calibration procedures, which can be infeasible or risky in dynamic,\nspace-based operational scenarios.\n  This paper proposes a novel method for kinematic parameter estimation that\nonly requires encoder measurements and binary contact detection. The approach\nfocuses on estimating link thermal deformation strain and joint encoder biases\nby leveraging information of the contact manifold - the set of relative SE(3)\nposes at which contact between the manipulator and environment occurs. We\npresent two core contributions: (1) a differentiable, learning-based model of\nthe contact manifold, and (2) an optimization-based algorithm for estimating\nkinematic parameters from encoder measurements at contact instances. By\nenabling parameter estimation using only encoder measurements and contact\ndetection, this method provides a robust, interpretable, and data-efficient\nsolution for safe and accurate manipulation in the challenging conditions of\nspace.", "AI": {"tldr": "提出了一种仅需编码器测量和二进制接触检测的机器人运动学参数估计新方法，适用于太空环境下的精确操作。", "motivation": "太空机器人操作（如碎片清除和太空制造）需要高精度接触操作，但热变形和编码器偏差导致运动学参数误差，传统校准方法不适用。", "method": "利用接触流形信息，提出基于学习的可微分接触流形模型和优化算法，仅通过编码器测量和接触检测估计参数。", "result": "该方法在太空环境下实现了鲁棒、可解释且数据高效的参数估计，提升了操作精度和安全性。", "conclusion": "新方法为太空机器人操作提供了一种无需外部传感器的可靠解决方案。"}}
{"id": "2506.17462", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17462", "abs": "https://arxiv.org/abs/2506.17462", "authors": ["Bernard Lange", "Anil Yildiz", "Mansur Arief", "Shehryar Khattak", "Mykel Kochenderfer", "Georgios Georgakis"], "title": "General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting", "comment": null, "summary": "Developing general-purpose navigation policies for unknown environments\nremains a core challenge in robotics. Most existing systems rely on\ntask-specific neural networks and fixed data flows, limiting generalizability.\nLarge Vision-Language Models (LVLMs) offer a promising alternative by embedding\nhuman-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot\nintegrations typically depend on pre-mapped spaces, hard-coded representations,\nand myopic exploration. We introduce the Agentic Robotic Navigation\nArchitecture (ARNA), a general-purpose navigation framework that equips an\nLVLM-based agent with a library of perception, reasoning, and navigation tools\navailable within modern robotic stacks. At runtime, the agent autonomously\ndefines and executes task-specific workflows that iteratively query the robotic\nmodules, reason over multimodal inputs, and select appropriate navigation\nactions. This approach enables robust navigation and reasoning in previously\nunmapped environments, providing a new perspective on robotic stack design.\nEvaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves\nstate-of-the-art performance, demonstrating effective exploration, navigation,\nand embodied question answering without relying on handcrafted plans, fixed\ninput representations, or pre-existing maps.", "AI": {"tldr": "ARNA是一个基于大型视觉语言模型（LVLM）的通用导航框架，通过自主定义任务特定工作流，实现在未知环境中的鲁棒导航和推理。", "motivation": "现有导航系统依赖任务特定神经网络和固定数据流，泛化能力有限。LVLM提供了类似人类的知识嵌入，适合推理和规划，但现有集成依赖预映射空间和硬编码表示。", "method": "ARNA框架为LVLM代理提供感知、推理和导航工具库，运行时自主定义和执行任务特定工作流，迭代查询机器人模块并选择导航动作。", "result": "在Habitat Lab的HM-EQA基准测试中，ARNA实现了最先进的性能，无需依赖手工计划或预建地图。", "conclusion": "ARNA为机器人堆栈设计提供了新视角，展示了在未知环境中的有效探索、导航和问答能力。"}}
{"id": "2506.17473", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17473", "abs": "https://arxiv.org/abs/2506.17473", "authors": ["Shuyuan Wang", "Philip D. Loewen", "Michael Forbes", "Bhushan Gopaluni", "Wei Pan"], "title": "DiLQR: Differentiable Iterative Linear Quadratic Regulator via Implicit Differentiation", "comment": "Accepted at ICML 2025. Official conference page:\n  https://icml.cc/virtual/2025/poster/44176. OpenReview page:\n  https://openreview.net/forum?id=m2EfTrbv4o", "summary": "While differentiable control has emerged as a powerful paradigm combining\nmodel-free flexibility with model-based efficiency, the iterative Linear\nQuadratic Regulator (iLQR) remains underexplored as a differentiable component.\nThe scalability of differentiating through extended iterations and horizons\nposes significant challenges, hindering iLQR from being an effective\ndifferentiable controller. This paper introduces DiLQR, a framework that\nfacilitates differentiation through iLQR, allowing it to serve as a trainable\nand differentiable module, either as or within a neural network. A novel aspect\nof this framework is the analytical solution that it provides for the gradient\nof an iLQR controller through implicit differentiation, which ensures a\nconstant backward cost regardless of iteration, while producing an accurate\ngradient. We evaluate our framework on imitation tasks on famous control\nbenchmarks. Our analytical method demonstrates superior computational\nperformance, achieving up to 128x speedup and a minimum of 21x speedup compared\nto automatic differentiation. Our method also demonstrates superior learning\nperformance ($10^6$x) compared to traditional neural network policies and\nbetter model loss with differentiable controllers that lack exact analytical\ngradients. Furthermore, we integrate our module into a larger network with\nvisual inputs to demonstrate the capacity of our method for high-dimensional,\nfully end-to-end tasks. Codes can be found on the project homepage\nhttps://sites.google.com/view/dilqr/.", "AI": {"tldr": "DiLQR框架通过隐式微分实现iLQR的可微分性，显著提升计算和学习性能。", "motivation": "探索iLQR作为可微分组件的潜力，解决其迭代和时域扩展带来的计算挑战。", "method": "提出DiLQR框架，利用隐式微分解析iLQR梯度，确保恒定反向成本。", "result": "在模仿任务中，DiLQR计算速度提升21-128倍，学习性能优于传统神经网络策略。", "conclusion": "DiLQR为高维端到端任务提供高效可微分的iLQR控制器。"}}
{"id": "2506.17486", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17486", "abs": "https://arxiv.org/abs/2506.17486", "authors": ["Zachary Ravichandran", "Ignacio Hounie", "Fernando Cladera", "Alejandro Ribeiro", "George J. Pappas", "Vijay Kumar"], "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention", "comment": null, "summary": "Large language models (LLMs) provide robots with powerful contextual\nreasoning abilities and a natural human interface. Yet, current LLM-enabled\nrobots typically depend on cloud-hosted models, limiting their usability in\nenvironments with unreliable communication infrastructure, such as outdoor or\nindustrial settings. We present PRISM, a framework for distilling small\nlanguage model (SLM)-enabled robot planners that run on-device with minimal\nhuman supervision. Starting from an existing LLM-enabled planner, PRISM\nautomatically synthesizes diverse tasks and environments, elicits plans from\nthe LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in\nreplacement of the source model. We apply PRISM to three LLM-enabled planners\nfor mapping and exploration, manipulation, and household assistance, and we\ndemonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of\nGPT-4o's performance to over 93% - using only synthetic data. We further\ndemonstrate that the distilled planners generalize across heterogeneous robotic\nplatforms (ground and aerial) and diverse environments (indoor and outdoor). We\nrelease all software, trained models, and datasets at\nhttps://zacravichandran.github.io/PRISM.", "AI": {"tldr": "PRISM框架通过合成任务和环境数据，将大型语言模型（LLM）的能力蒸馏到小型语言模型（SLM）中，实现机器人规划器的本地化运行，性能接近GPT-4o的93%。", "motivation": "当前依赖云端LLM的机器人在通信不可靠的环境中（如户外或工业场景）实用性受限，需要本地化解决方案。", "method": "PRISM自动合成多样化任务和环境数据，利用LLM生成规划，并通过这些数据蒸馏出紧凑的SLM作为替代。", "result": "PRISM将Llama-3.2-3B的性能从GPT-4o的10-20%提升至93%以上，且支持跨平台（地面和空中机器人）和多样化环境（室内外）。", "conclusion": "PRISM为机器人提供了一种高效、本地化的语言模型规划解决方案，显著提升了性能和泛化能力。"}}
{"id": "2506.17516", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17516", "abs": "https://arxiv.org/abs/2506.17516", "authors": ["Zhou Chen", "Sanjoy Kundu", "Harsimran S. Baweja", "Sathyanarayanan N. Aakur"], "title": "EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization", "comment": "Accepted to IEEE Robotics and Automation Letters, 2025", "summary": "Active event perception, the ability to dynamically detect, track, and\nsummarize events in real time, is essential for embodied intelligence in tasks\nsuch as human-AI collaboration, assistive robotics, and autonomous navigation.\nHowever, existing approaches often depend on predefined action spaces,\nannotated datasets, and extrinsic rewards, limiting their adaptability and\nscalability in dynamic, real-world scenarios. Inspired by cognitive theories of\nevent perception and predictive coding, we propose EASE, a self-supervised\nframework that unifies spatiotemporal representation learning and embodied\ncontrol through free energy minimization. EASE leverages prediction errors and\nentropy as intrinsic signals to segment events, summarize observations, and\nactively track salient actors, operating without explicit annotations or\nexternal rewards. By coupling a generative perception model with an\naction-driven control policy, EASE dynamically aligns predictions with\nobservations, enabling emergent behaviors such as implicit memory, target\ncontinuity, and adaptability to novel environments. Extensive evaluations in\nsimulation and real-world settings demonstrate EASE's ability to achieve\nprivacy-preserving and scalable event perception, providing a robust foundation\nfor embodied systems in unscripted, dynamic tasks.", "AI": {"tldr": "EASE是一个自监督框架，通过自由能最小化统一时空表示学习和具身控制，无需标注或外部奖励即可实现事件感知。", "motivation": "现有方法依赖预定义动作空间和标注数据，限制了在动态现实场景中的适应性和可扩展性。", "method": "EASE利用预测误差和熵作为内在信号，结合生成感知模型和动作驱动控制策略，动态对齐预测与观察。", "result": "EASE在仿真和现实场景中表现出隐私保护和可扩展的事件感知能力。", "conclusion": "EASE为动态任务中的具身系统提供了鲁棒基础。"}}
{"id": "2506.17601", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17601", "abs": "https://arxiv.org/abs/2506.17601", "authors": ["Rohan Thakker", "Adarsh Patnaik", "Vince Kurtz", "Jonas Frey", "Jonathan Becktor", "Sangwoo Moon", "Rob Royce", "Marcel Kaufmann", "Georgios Georgakis", "Pascal Roth", "Joel Burdick", "Marco Hutter", "Shehryar Khattak"], "title": "Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option", "comment": null, "summary": "Safe, reliable navigation in extreme, unfamiliar terrain is required for\nfuture robotic space exploration missions. Recent generative-AI methods learn\nsemantically aware navigation policies from large, cross-embodiment datasets,\nbut offer limited safety guarantees. Inspired by human cognitive science, we\npropose a risk-guided diffusion framework that fuses a fast, learned \"System-1\"\nwith a slow, physics-based \"System-2\", sharing computation at both training and\ninference to couple adaptability with formal safety. Hardware experiments\nconducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our\napproach reduces failure rates by up to $4\\times$ while matching the\ngoal-reaching performance of learning-based robotic models by leveraging\ninference-time compute without any additional training.", "AI": {"tldr": "提出了一种风险引导的扩散框架，结合快速学习的“系统1”和基于物理的“系统2”，在极端地形中实现安全导航。", "motivation": "未来太空探索任务需要机器人在极端陌生地形中安全可靠地导航，现有生成式AI方法缺乏足够的安全保障。", "method": "融合快速学习的“系统1”和慢速物理模拟的“系统2”，在训练和推理阶段共享计算，兼顾适应性与安全性。", "result": "在NASA JPL的Mars Yard实验中，失败率降低4倍，同时保持目标达成性能。", "conclusion": "该方法通过推理阶段计算优化，无需额外训练即可显著提升安全性。"}}
{"id": "2506.17624", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17624", "abs": "https://arxiv.org/abs/2506.17624", "authors": ["Koki Nakagawa", "Yoshiyuki Ohmura", "Yasuo Kuniyoshi"], "title": "Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View", "comment": "6 pages", "summary": "Most prior research in deep imitation learning has predominantly utilized\nfixed cameras for image input, which constrains task performance to the\npredefined field of view. However, enabling a robot to actively maneuver its\nneck can significantly expand the scope of imitation learning to encompass a\nwider variety of tasks and expressive actions such as neck gestures. To\nfacilitate imitation learning in robots capable of neck movement while\nsimultaneously performing object manipulation, we propose a teaching system\nthat systematically collects datasets incorporating neck movements while\nminimizing discomfort caused by dynamic viewpoints during teleoperation. In\naddition, we present a novel network model for learning manipulation tasks\nincluding active neck motion. Experimental results showed that our model can\nachieve a high success rate of around 90\\%, regardless of the distraction from\nthe viewpoint variations by active neck motion. Moreover, the proposed model\nproved particularly effective in challenging scenarios, such as when objects\nwere situated at the periphery or beyond the standard field of view, where\ntraditional models struggled. The proposed approach contributes to the\nefficiency of dataset collection and extends the applicability of imitation\nlearning to more complex and dynamic scenarios.", "AI": {"tldr": "论文提出了一种结合颈部运动的机器人模仿学习系统，通过动态视角数据集和新网络模型，显著提高了任务成功率（约90%）和复杂场景适应性。", "motivation": "固定摄像头的模仿学习限制了任务范围和表现力，而机器人颈部运动可以扩展任务多样性。", "method": "开发了结合颈部运动的教学系统和新网络模型，以减少动态视角带来的不适并学习复杂任务。", "result": "模型在动态视角下任务成功率达90%，尤其在传统模型难以处理的边缘或视野外物体场景中表现优异。", "conclusion": "该方法提升了数据集收集效率，并将模仿学习扩展到更复杂、动态的场景。"}}
{"id": "2506.17639", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17639", "abs": "https://arxiv.org/abs/2506.17639", "authors": ["Yuxuan Chen", "Xiao Li"], "title": "RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action models (VLA) have demonstrated remarkable capabilities\nand promising potential in solving complex robotic manipulation tasks. However,\ntheir substantial parameter sizes and high inference latency pose significant\nchallenges for real-world deployment, particularly on resource-constrained\nrobotic platforms. To address this issue, we begin by conducting an extensive\nempirical study to explore the effectiveness of model compression techniques\nwhen applied to VLAs. Building on the insights gained from these preliminary\nexperiments, we propose RLRC, a three-stage recovery method for compressed\nVLAs, including structured pruning, performance recovery based on SFT and RL,\nand further quantization. RLRC achieves up to an 8x reduction in memory usage\nand a 2.3x improvement in inference throughput, while maintaining or even\nsurpassing the original VLA's task success rate. Extensive experiments show\nthat RLRC consistently outperforms existing compression baselines,\ndemonstrating strong potential for on-device deployment of VLAs. Project\nwebsite: https://rlrc-vla.github.io", "AI": {"tldr": "论文提出了一种名为RLRC的三阶段恢复方法，用于压缩Vision-Language-Action模型（VLA），显著减少了内存使用并提高了推理速度，同时保持了任务成功率。", "motivation": "VLA模型在复杂机器人操作任务中表现出色，但其庞大的参数量和较高的推理延迟限制了在资源受限平台上的实际部署。", "method": "RLRC方法包括结构化剪枝、基于SFT和RL的性能恢复以及进一步量化三个阶段。", "result": "RLRC实现了内存使用减少8倍，推理吞吐量提升2.3倍，同时任务成功率保持或超过原始VLA。", "conclusion": "RLRC在实验中表现优于现有压缩基线，展示了VLA在设备端部署的潜力。"}}
{"id": "2506.17823", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17823", "abs": "https://arxiv.org/abs/2506.17823", "authors": ["Kevin Chang", "Rakesh Vivekanandan", "Noah Pragin", "Sean Bullock", "Geoffrey Hollinger"], "title": "Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking", "comment": "Advancing Quantitative and Qualitative Simulators for Marine\n  Applications Workshop Paper at International Conference on Robotics and\n  Automation 2025", "summary": "Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain\nenvironments is a critical challenge for underwater robotics. Reinforcement\nlearning is a promising method for developing robust controllers, but the\ndisparity between training simulations and the real world, or the sim2real gap,\noften leads to a significant deterioration in performance. In this work, we\nperform a simulation study on reducing the sim2real gap in autonomous docking\nthrough training various controllers and then evaluating them under realistic\ndisturbances. In particular, we focus on the real-world challenge of docking\nunder different payloads that are potentially outside the original training\ndistribution. We explore existing methods for improving robustness including\nrandomization techniques and history-conditioned controllers. Our findings\nprovide insights into mitigating the sim2real gap when training docking\ncontrollers. Furthermore, our work indicates areas of future research that may\nbe beneficial to the marine robotics community.", "AI": {"tldr": "研究通过强化学习减少AUV动态对接中的模拟与现实差距，探索随机化和历史条件控制器等方法，以提升不同负载下的鲁棒性。", "motivation": "解决AUV在动态和不确定环境中对接时，模拟训练与现实性能差距的问题。", "method": "通过训练多种控制器，并在模拟中评估其在真实扰动下的表现，重点研究不同负载下的对接挑战。", "result": "发现随机化和历史条件控制器等方法有助于缩小模拟与现实差距。", "conclusion": "研究为减少AUV对接控制器的模拟与现实差距提供了见解，并指出了未来研究方向。"}}
{"id": "2506.17831", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17831", "abs": "https://arxiv.org/abs/2506.17831", "authors": ["Mina Kian", "Mingyu Zong", "Katrin Fischer", "Anna-Maria Velentza", "Abhyuday Singh", "Kaleen Shrestha", "Pau Sang", "Shriya Upadhyay", "Wallace Browning", "Misha Arif Faruki", "Sébastien M. R. Arnold", "Bhaskar Krishnamachari", "Maja Matarić"], "title": "Engagement and Disclosures in LLM-Powered Cognitive Behavioral Therapy Exercises: A Factorial Design Comparing the Influence of a Robot vs. Chatbot Over Time", "comment": null, "summary": "Many researchers are working to address the worldwide mental health crisis by\ndeveloping therapeutic technologies that increase the accessibility of care,\nincluding leveraging large language model (LLM) capabilities in chatbots and\nsocially assistive robots (SARs) used for therapeutic applications. Yet, the\neffects of these technologies over time remain unexplored. In this study, we\nuse a factorial design to assess the impact of embodiment and time spent\nengaging in therapeutic exercises on participant disclosures. We assessed\ntranscripts gathered from a two-week study in which 26 university student\nparticipants completed daily interactive Cognitive Behavioral Therapy (CBT)\nexercises in their residences using either an LLM-powered SAR or a disembodied\nchatbot. We evaluated the levels of active engagement and high intimacy of\ntheir disclosures (opinions, judgments, and emotions) during each session and\nover time. Our findings show significant interactions between time and\nembodiment for both outcome measures: participant engagement and intimacy\nincreased over time in the physical robot condition, while both measures\ndecreased in the chatbot condition.", "AI": {"tldr": "研究探讨了具身化（SAR）与无具身化（聊天机器人）对心理治疗参与度和亲密感的影响，发现具身化随时间提升效果，而无具身化则相反。", "motivation": "全球心理健康危机需要可及性治疗技术，但LLM驱动的技术（如聊天机器人或SAR）的长期效果尚不明确。", "method": "采用因子设计，评估具身化和时间对参与者披露行为的影响，26名大学生在两周内使用SAR或聊天机器人完成CBT练习。", "result": "具身化条件下参与度和亲密感随时间增加，而无具身化条件下两者均下降。", "conclusion": "具身化技术在长期心理治疗中可能更有效。"}}
{"id": "2506.17842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17842", "abs": "https://arxiv.org/abs/2506.17842", "authors": ["Al-Harith Farhad", "Khalil Abuibaid", "Christiane Plociennik", "Achim Wagner", "Martin Ruskowski"], "title": "Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria", "comment": "RAAD 2025: 34th International Conference on Robotics in\n  Alpe-Adria-Danube Region", "summary": "Neural networks are often regarded as universal equations that can estimate\nany function. This flexibility, however, comes with the drawback of high\ncomplexity, rendering these networks into black box models, which is especially\nrelevant in safety-centric applications. To that end, we propose a pipeline for\na collaborative robot (Cobot) grasping algorithm that detects relevant tools\nand generates the optimal grasp. To increase the transparency and reliability\nof this approach, we integrate an explainable AI method that provides an\nexplanation for the underlying prediction of a model by extracting the learned\nfeatures and correlating them to corresponding classes from the input. These\nconcepts are then used as additional criteria to ensure the safe handling of\nwork tools. In this paper, we show the consistency of this approach and the\ncriterion for improving the handover position. This approach was tested in an\nindustrial environment, where a camera system was set up to enable a robot to\npick up certain tools and objects.", "AI": {"tldr": "提出了一种结合可解释AI的协作机器人抓取算法，以提高透明度和可靠性。", "motivation": "神经网络虽然灵活，但复杂且难以解释，尤其在安全关键应用中。", "method": "集成可解释AI方法，提取学习特征并与输入类别关联，作为安全标准。", "result": "在工业环境中测试，优化了抓取位置，提高了可靠性。", "conclusion": "该方法在透明性和安全性方面表现一致且有效。"}}
{"id": "2506.17868", "categories": ["cs.RO", "cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2506.17868", "abs": "https://arxiv.org/abs/2506.17868", "authors": ["Andrea Testa", "Søren Hauberg", "Tamim Asfour", "Leonel Rozo"], "title": "Geometric Contact Flows: Contactomorphisms for Dynamics and Control", "comment": "Accepted at ICML 2025", "summary": "Accurately modeling and predicting complex dynamical systems, particularly\nthose involving force exchange and dissipation, is crucial for applications\nranging from fluid dynamics to robotics, but presents significant challenges\ndue to the intricate interplay of geometric constraints and energy transfer.\nThis paper introduces Geometric Contact Flows (GFC), a novel framework\nleveraging Riemannian and Contact geometry as inductive biases to learn such\nsystems. GCF constructs a latent contact Hamiltonian model encoding desirable\nproperties like stability or energy conservation. An ensemble of\ncontactomorphisms then adapts this model to the target dynamics while\npreserving these properties. This ensemble allows for uncertainty-aware\ngeodesics that attract the system's behavior toward the data support, enabling\nrobust generalization and adaptation to unseen scenarios. Experiments on\nlearning dynamics for physical systems and for controlling robots on\ninteraction tasks demonstrate the effectiveness of our approach.", "AI": {"tldr": "论文提出了一种基于黎曼几何和接触几何的框架（GFC），用于建模和预测复杂动力系统，特别是在涉及力交换和耗散的场景中。", "motivation": "复杂动力系统的建模和预测在流体动力学和机器人学等领域至关重要，但由于几何约束和能量传递的复杂性，传统方法面临挑战。", "method": "GFC框架通过构建一个潜在接触哈密顿模型，利用接触同胚的集合来适应目标动力学，同时保持稳定性或能量守恒等性质。", "result": "实验表明，该方法在物理系统动力学学习和机器人交互任务控制中表现出色。", "conclusion": "GFC框架通过几何约束和不确定性感知的测地线，实现了对未见场景的鲁棒泛化和适应。"}}
{"id": "2506.17902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17902", "abs": "https://arxiv.org/abs/2506.17902", "authors": ["Peiyu Luo", "Shilong Yao", "Yuhan Chen", "Max Q. -H. Meng"], "title": "Embedded Flexible Circumferential Sensing for Real-Time Intraoperative Environmental Perception in Continuum Robots", "comment": null, "summary": "Continuum robots have been widely adopted in robot-assisted minimally\ninvasive surgery (RMIS) because of their compact size and high flexibility.\nHowever, their proprioceptive capabilities remain limited, particularly in\nnarrow lumens, where lack of environmental awareness can lead to unintended\ntissue contact and surgical risks. To address this challenge, this work\nproposes a flexible annular sensor structure integrated around the vertebral\ndisks of continuum robots. The proposed design enables real-time environmental\nmapping by estimating the distance between the robotic disks and the\nsurrounding tissue, thereby facilitating safer operation through advanced\ncontrol strategies. The experiment has proven that its accuracy in obstacle\ndetection can reach 0.19 mm. Fabricated using flexible printed circuit (FPC)\ntechnology, the sensor demonstrates a modular and cost-effective design with\ncompact dimensions and low noise interference. Its adaptable parameters allow\ncompatibility with various continuum robot architectures, offering a promising\nsolution for enhancing intraoperative perception and control in surgical\nrobotics.", "AI": {"tldr": "提出了一种用于连续体机器人的柔性环形传感器结构，以提高其在狭窄腔道中的环境感知能力，实验显示其障碍物检测精度达0.19毫米。", "motivation": "连续体机器人在微创手术中应用广泛，但其在狭窄腔道中的环境感知能力有限，可能导致意外组织接触和手术风险。", "method": "设计了一种集成于机器人椎间盘周围的柔性环形传感器结构，通过实时估计机器人与周围组织的距离实现环境映射。", "result": "实验证明传感器障碍物检测精度为0.19毫米，且具有模块化、成本低、尺寸紧凑和低噪声干扰的特点。", "conclusion": "该传感器设计为增强手术机器人术中感知和控制提供了有前景的解决方案。"}}
{"id": "2506.17960", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17960", "abs": "https://arxiv.org/abs/2506.17960", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Jiaxuan Da", "Nuowen Qian", "Tram Minh Man", "Harold Soh"], "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments", "comment": "8 pages, 5 figures. Jiaming Wang, Diwen Liu, and Jizhuo Chen\n  contributed equally", "summary": "Reliable navigation in unstructured, real-world environments remains a\nsignificant challenge for embodied agents, especially when operating across\ndiverse terrains, weather conditions, and sensor configurations. In this paper,\nwe introduce GeNIE (Generalizable Navigation System for In-the-Wild\nEnvironments), a robust navigation framework designed for global deployment.\nGeNIE integrates a generalizable traversability prediction model built on SAM2\nwith a novel path fusion strategy that enhances planning stability in noisy and\nambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at\nICRA 2025, where it was evaluated across six countries spanning three\ncontinents. GeNIE took first place and achieved 79% of the maximum possible\nscore, outperforming the second-best team by 17%, and completed the entire\ncompetition without a single human intervention. These results set a new\nbenchmark for robust, generalizable outdoor robot navigation. We will release\nthe codebase, pretrained model weights, and newly curated datasets to support\nfuture research in real-world navigation.", "AI": {"tldr": "GeNIE是一种通用导航系统，通过结合可泛化的可通行性预测模型和路径融合策略，在复杂环境中实现稳定导航，并在国际竞赛中表现优异。", "motivation": "解决在非结构化、多样化环境中可靠导航的挑战，尤其是在不同地形、天气和传感器配置下的应用。", "method": "整合基于SAM2的可泛化可通行性预测模型和新型路径融合策略，提升规划稳定性。", "result": "在ICRA 2025的Earth Rover Challenge中，GeNIE获得第一名，得分超过第二名17%，且全程无需人工干预。", "conclusion": "GeNIE为户外机器人导航设定了新标准，代码和数据集将开源以推动进一步研究。"}}
{"id": "2506.17994", "categories": ["cs.RO", "cs.LG", "I.2.9; I.2.6; I.6.4"], "pdf": "https://arxiv.org/pdf/2506.17994", "abs": "https://arxiv.org/abs/2506.17994", "authors": ["Minh Trinh", "Andreas René Geist", "Josefine Monnet", "Stefan Vilceanu", "Sebastian Trimpe", "Christian Brecher"], "title": "Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification", "comment": "Paper accepted for publication in 14th IFAC Symposium on Robotics", "summary": "Accurate inverse dynamics models are essential tools for controlling\nindustrial robots. Recent research combines neural network regression with\ninverse dynamics formulations of the Newton-Euler and the Euler-Lagrange\nequations of motion, resulting in so-called Newtonian neural networks and\nLagrangian neural networks, respectively. These physics-informed models seek to\nidentify unknowns in the analytical equations from data. Despite their\npotential, current literature lacks guidance on choosing between Lagrangian and\nNewtonian networks. In this study, we show that when motor torques are\nestimated instead of directly measuring joint torques, Lagrangian networks\nprove less effective compared to Newtonian networks as they do not explicitly\nmodel dissipative torques. The performance of these models is compared to\nneural network regression on data of a MABI MAX 100 industrial robot.", "AI": {"tldr": "研究比较了牛顿神经网络和拉格朗日神经网络在工业机器人逆动力学建模中的表现，发现牛顿神经网络在估计电机扭矩时更有效。", "motivation": "工业机器人需要精确的逆动力学模型，但目前缺乏关于选择牛顿神经网络还是拉格朗日神经网络的指导。", "method": "结合神经网络回归与牛顿-欧拉和欧拉-拉格朗日运动方程，构建牛顿神经网络和拉格朗日神经网络，并在MABI MAX 100机器人数据上进行比较。", "result": "当估计电机扭矩而非直接测量关节扭矩时，拉格朗日神经网络效果较差，因其未显式建模耗散扭矩。", "conclusion": "牛顿神经网络在电机扭矩估计任务中表现更优。"}}
{"id": "2506.18016", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18016", "abs": "https://arxiv.org/abs/2506.18016", "authors": ["Yongxin Shao", "Binrui Wang", "Aihong Tan"], "title": "ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM", "comment": null, "summary": "LiDAR SLAM has demonstrated significant application value in various fields,\nincluding mobile robot navigation and high-precision map construction. However,\nexisting methods often need to make a trade-off between positioning accuracy\nand system robustness when faced with dynamic object interference, point cloud\nnoise, and unstructured environments. To address this challenge, we propose an\nadaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference\nin both aspects. We design the Dynamic Segmentation Head to predict the\ncategory of feature points belonging to dynamic points, to eliminate dynamic\nfeature points; design the Global Importance Scoring Head to adaptively select\nfeature points with higher contribution and features while suppressing noise\ninterference; and construct the Cross Layer Intra-Graph Convolution Module\n(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the\ndiscriminative ability of overlapping features. Finally, to further validate\nthe effectiveness of our method, we tested it on several publicly available\ndatasets and achieved outstanding results.", "AI": {"tldr": "提出了一种自适应噪声过滤的SLAM策略ADA-DPM，通过动态分割头和全局重要性评分头优化特征点选择，结合GLI-GCN模块提升特征判别能力，在动态干扰和噪声环境下表现优异。", "motivation": "现有LiDAR SLAM方法在动态物体干扰、点云噪声和非结构化环境中需在定位精度和系统鲁棒性之间权衡，亟需一种更优解决方案。", "method": "设计动态分割头预测动态特征点类别并剔除；全局重要性评分头自适应选择高贡献特征点；构建GLI-GCN模块融合多尺度邻域结构。", "result": "在多个公开数据集上测试，取得了出色的结果。", "conclusion": "ADA-DPM策略在定位精度和鲁棒性上均表现优异，解决了现有方法的权衡问题。"}}
{"id": "2506.18040", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18040", "abs": "https://arxiv.org/abs/2506.18040", "authors": ["Chenghua Lu", "Kailuan Tang", "Xueming Hui", "Haoran Li", "Saekwang Nam", "Nathan F. Lepora"], "title": "StereoTacTip: Vision-based Tactile Sensing with Biomimetic Skin-Marker Arrangements", "comment": "11 pages, 13 figures", "summary": "Vision-Based Tactile Sensors (VBTSs) stand out for their superior performance\ndue to their high-information content output. Recently, marker-based VBTSs have\nbeen shown to give accurate geometry reconstruction when using stereo cameras.\n\\uhl{However, many marker-based VBTSs use complex biomimetic skin-marker\narrangements, which presents issues for the geometric reconstruction of the\nskin surface from the markers}. Here we investigate how the marker-based skin\nmorphology affects stereo vision-based tactile sensing, using a novel VBTS\ncalled the StereoTacTip. To achieve accurate geometry reconstruction, we\nintroduce: (i) stereo marker matching and tracking using a novel\nDelaunay-Triangulation-Ring-Coding algorithm; (ii) a refractive depth\ncorrection model that corrects the depth distortion caused by refraction in the\ninternal media; (iii) a skin surface correction model from the marker\npositions, relying on an inverse calculation of normals to the skin surface;\nand (iv)~methods for geometry reconstruction over multiple contacts. To\ndemonstrate these findings, we reconstruct topographic terrains on a large 3D\nmap. Even though contributions (i) and (ii) were developed for biomimetic\nmarkers, they should improve the performance of all marker-based VBTSs.\nOverall, this work illustrates that a thorough understanding and evaluation of\nthe morphologically-complex skin and marker-based tactile sensor principles are\ncrucial for obtaining accurate geometric information.", "AI": {"tldr": "研究探讨了基于视觉的触觉传感器（VBTS）中标记形态对几何重建的影响，提出了一种新方法StereoTacTip，通过改进标记匹配、深度校正和皮肤表面模型，实现高精度几何重建。", "motivation": "许多基于标记的VBTS使用复杂的仿生皮肤标记排列，导致皮肤表面几何重建困难，因此需要研究标记形态对触觉传感的影响。", "method": "提出StereoTacTip传感器，包括：(i) 使用Delaunay三角环编码算法进行立体标记匹配与跟踪；(ii) 折射深度校正模型；(iii) 基于标记位置的皮肤表面校正模型；(iv) 多接触点的几何重建方法。", "result": "通过实验成功重建了大型3D地形图，验证了方法的有效性。", "conclusion": "研究表明，深入理解皮肤和标记形态对触觉传感原理的影响是实现高精度几何重建的关键。"}}
{"id": "2506.18088", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.18088", "abs": "https://arxiv.org/abs/2506.18088", "authors": ["Tianxing Chen", "Zanxin Chen", "Baijun Chen", "Zijian Cai", "Yibin Liu", "Qiwei Liang", "Zixuan Li", "Xianliang Lin", "Yiheng Ge", "Zhenyu Gu", "Weiliang Deng", "Yubin Guo", "Tian Nian", "Xuanbing Xie", "Qiangyu Chen", "Kailun Su", "Tianling Xu", "Guodong Liu", "Mengkang Hu", "Huan-ang Gao", "Kaixuan Wang", "Zhixuan Liang", "Yusen Qin", "Xiaokang Yang", "Ping Luo", "Yao Mu"], "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "comment": "Project Page: https://robotwin-platform.github.io/", "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.", "AI": {"tldr": "RoboTwin 2.0是一个可扩展的仿真框架，用于自动生成多样化和真实的双机械臂操作数据，解决了现有合成数据集的不足。", "motivation": "现有合成数据集在双机械臂操作中表现不足，主要由于缺乏高效的数据生成方法和过于简化的仿真环境。", "method": "构建RoboTwin-OD对象库，结合多模态大语言模型（MLLMs）和仿真循环优化，自动生成任务级执行代码，并通过结构化域随机化提升数据多样性。", "result": "在50个双机械臂任务中生成10万条专家轨迹，代码生成成功率提升10.9%，真实场景泛化能力显著增强。", "conclusion": "RoboTwin 2.0显著提升了双机械臂操作的鲁棒性和泛化能力，支持无监督的真实场景应用。"}}
{"id": "2506.18123", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18123", "abs": "https://arxiv.org/abs/2506.18123", "authors": ["Pranav Atreya", "Karl Pertsch", "Tony Lee", "Moo Jin Kim", "Arhan Jain", "Artur Kuramshin", "Clemens Eppner", "Cyrus Neary", "Edward Hu", "Fabio Ramos", "Jonathan Tremblay", "Kanav Arora", "Kirsty Ellis", "Luca Macesanu", "Matthew Leonard", "Meedeum Cho", "Ozgur Aslan", "Shivin Dass", "Jie Wang", "Xingfang Yuan", "Xuning Yang", "Abhishek Gupta", "Dinesh Jayaraman", "Glen Berseth", "Kostas Daniilidis", "Roberto Martin-Martin", "Youngwoon Lee", "Percy Liang", "Chelsea Finn", "Sergey Levine"], "title": "RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies", "comment": "Website: https://robo-arena.github.io/", "summary": "Comprehensive, unbiased, and comparable evaluation of modern generalist\npolicies is uniquely challenging: existing approaches for robot benchmarking\ntypically rely on heavy standardization, either by specifying fixed evaluation\ntasks and environments, or by hosting centralized ''robot challenges'', and do\nnot readily scale to evaluating generalist policies across a broad range of\ntasks and environments. In this work, we propose RoboArena, a new approach for\nscalable evaluation of generalist robot policies in the real world. Instead of\nstandardizing evaluations around fixed tasks, environments, or locations, we\npropose to crowd-source evaluations across a distributed network of evaluators.\nImportantly, evaluators can freely choose the tasks and environments they\nevaluate on, enabling easy scaling of diversity, but they are required to\nperform double-blind evaluations over pairs of policies. Then, by aggregating\npreference feedback from pairwise comparisons across diverse tasks and\nenvironments, we can derive a ranking of policies. We instantiate our approach\nacross a network of evaluators at seven academic institutions using the DROID\nrobot platform. Through more than 600 pairwise real-robot evaluation episodes\nacross seven generalist policies, we demonstrate that our crowd-sourced\napproach can more accurately rank the performance of existing generalist\npolicies than conventional, centralized evaluation approaches, while being more\nscalable, resilient, and trustworthy. We open our evaluation network to the\ncommunity and hope that it can enable more accessible comparisons of generalist\nrobot policies.", "AI": {"tldr": "RoboArena提出了一种分布式众包评估方法，用于在真实世界中评估通用机器人策略，通过双盲对比和偏好反馈生成策略排名。", "motivation": "现有机器人评估方法依赖标准化任务或集中式挑战，难以扩展至通用策略的多样化评估。", "method": "通过众包网络让评估者自由选择任务和环境，进行双盲策略对比，并汇总偏好反馈生成排名。", "result": "在7个学术机构的600多次对比实验中，RoboArena比传统方法更准确、可扩展且可靠。", "conclusion": "RoboArena为通用机器人策略提供了更灵活、可信的评估框架，并开放网络以促进社区合作。"}}
{"id": "2506.18160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18160", "abs": "https://arxiv.org/abs/2506.18160", "authors": ["Rutvik Patel", "Alec Kanyuck", "Zachary McNulty", "Zeren Yu", "Lisa Carlson", "Vann Heng", "Brice Johnson", "Satyandra K. Gupta"], "title": "Automated Plan Refinement for Improving Efficiency of Robotic Layup of Composite Sheets", "comment": null, "summary": "The automation of composite sheet layup is essential to meet the increasing\ndemand for composite materials in various industries. However, draping plans\nfor the robotic layup of composite sheets are not robust. A plan that works\nwell under a certain condition does not work well in a different condition.\nChanges in operating conditions due to either changes in material properties or\nworking environment may lead a draping plan to exhibit suboptimal performance.\nIn this paper, we present a comprehensive framework aimed at refining plans\nbased on the observed execution performance. Our framework prioritizes the\nminimization of uncompacted regions while simultaneously improving time\nefficiency. To achieve this, we integrate human expertise with data-driven\ndecision-making to refine expert-crafted plans for diverse production\nenvironments. We conduct experiments to validate the effectiveness of our\napproach, revealing significant reductions in the number of corrective paths\nrequired compared to initial expert-crafted plans. Through a combination of\nempirical data analysis, action-effectiveness modeling, and search-based\nrefinement, our system achieves superior time efficiency in robotic layup.\nExperimental results demonstrate the efficacy of our approach in optimizing the\nlayup process, thereby advancing the state-of-the-art in composite\nmanufacturing automation.", "AI": {"tldr": "提出了一种优化复合材料铺层自动化框架，结合人类经验和数据驱动决策，显著减少校正路径并提高时间效率。", "motivation": "复合材料铺层自动化需求增加，但现有铺层计划在不同条件下表现不稳定，需优化以适应多变的生产环境。", "method": "整合人类专业知识与数据驱动决策，通过经验数据分析和基于搜索的优化，改进专家制定的铺层计划。", "result": "实验显示，该方法显著减少了校正路径需求，提高了时间效率，优化了铺层过程。", "conclusion": "该框架有效提升了复合材料铺层自动化的性能，推动了该领域的技术进步。"}}
{"id": "2506.18178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18178", "abs": "https://arxiv.org/abs/2506.18178", "authors": ["Min Deng", "Bo Fu", "Lingyao Li", "Xi Wang"], "title": "Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction", "comment": null, "summary": "Multi-robot systems are emerging as a promising solution to the growing\ndemand for productivity, safety, and adaptability across industrial sectors.\nHowever, effectively coordinating multiple robots in dynamic and uncertain\nenvironments, such as construction sites, remains a challenge, particularly due\nto unpredictable factors like material delays, unexpected site conditions, and\nweather-induced disruptions. To address these challenges, this study proposes\nan adaptive task allocation framework that strategically leverages the\nsynergistic potential of Digital Twins, Integer Programming (IP), and Large\nLanguage Models (LLMs). The multi-robot task allocation problem is formally\ndefined and solved using an IP model that accounts for task dependencies, robot\nheterogeneity, scheduling constraints, and re-planning requirements. A\nmechanism for narrative-driven schedule adaptation is introduced, in which\nunstructured natural language inputs are interpreted by an LLM, and\noptimization constraints are autonomously updated, enabling human-in-the-loop\nflexibility without manual coding. A digital twin-based system has been\ndeveloped to enable real-time synchronization between physical operations and\ntheir digital representations. This closed-loop feedback framework ensures that\nthe system remains dynamic and responsive to ongoing changes on site. A case\nstudy demonstrates both the computational efficiency of the optimization\nalgorithm and the reasoning performance of several LLMs, with top-performing\nmodels achieving over 97% accuracy in constraint and parameter extraction. The\nresults confirm the practicality, adaptability, and cross-domain applicability\nof the proposed methods.", "AI": {"tldr": "提出了一种结合数字孪生、整数规划和大型语言模型的自适应任务分配框架，用于解决多机器人系统在动态环境中的协调问题。", "motivation": "工业领域对多机器人系统的需求增长，但在动态和不确定环境中协调机器人仍具挑战性。", "method": "使用整数规划模型解决任务分配问题，结合数字孪生和大型语言模型实现实时调整和自然语言输入处理。", "result": "优化算法高效，大型语言模型在约束和参数提取中准确率超过97%。", "conclusion": "该方法具有实用性、适应性和跨领域适用性。"}}
{"id": "2506.18212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18212", "abs": "https://arxiv.org/abs/2506.18212", "authors": ["Pedro Miguel Uriguen Eljuri", "Hironobu Shibata", "Maeyama Katsuyoshi", "Yuanyuan Jia", "Tadahiro Taniguchi"], "title": "Haptic-ACT -- Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS2025) Project website\n  https://upedrou.github.io/haptic-act_IROS2025", "summary": "In this paper we introduce Haptic-ACT, an advanced robotic system for pseudo\noocyte manipulation, integrating multimodal information and Action Chunking\nwith Transformers (ACT). Traditional automation methods for oocyte transfer\nrely heavily on visual perception, often requiring human supervision due to\nbiological variability and environmental disturbances. Haptic-ACT enhances ACT\nby incorporating haptic feedback, enabling real-time grasp failure detection\nand adaptive correction. Additionally, we introduce a 3D-printed TPU soft\ngripper to facilitate delicate manipulations. Experimental results demonstrate\nthat Haptic-ACT improves the task success rate, robustness, and adaptability\ncompared to conventional ACT, particularly in dynamic environments. These\nfindings highlight the potential of multimodal learning in robotics for\nbiomedical automation.", "AI": {"tldr": "Haptic-ACT是一种结合多模态信息和动作分块Transformer（ACT）的先进机器人系统，用于伪卵母细胞操作，通过触觉反馈提升任务成功率。", "motivation": "传统卵母细胞转移自动化方法依赖视觉感知，常需人工监督，Haptic-ACT旨在通过触觉反馈解决这一问题。", "method": "结合触觉反馈改进ACT，并引入3D打印TPU软夹爪，实现实时抓取失败检测和自适应校正。", "result": "实验表明Haptic-ACT在动态环境中比传统ACT更具任务成功率、鲁棒性和适应性。", "conclusion": "多模态学习在生物医学自动化机器人中具有潜力。"}}
{"id": "2506.18256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18256", "abs": "https://arxiv.org/abs/2506.18256", "authors": ["Shuo Jiang", "Boce Hu", "Linfeng Zhao", "Lawson L. S. Wong"], "title": "Robot Tactile Gesture Recognition Based on Full-body Modular E-skin", "comment": null, "summary": "With the development of robot electronic skin technology, various tactile\nsensors, enhanced by AI, are unlocking a new dimension of perception for\nrobots. In this work, we explore how robots equipped with electronic skin can\nrecognize tactile gestures and interpret them as human commands. We developed a\nmodular robot E-skin, composed of multiple irregularly shaped skin patches,\nwhich can be assembled to cover the robot's body while capturing real-time\npressure and pose data from thousands of sensing points. To process this\ninformation, we propose an equivariant graph neural network-based recognizer\nthat efficiently and accurately classifies diverse tactile gestures, including\npoke, grab, stroke, and double-pat. By mapping the recognized gestures to\npredefined robot actions, we enable intuitive human-robot interaction purely\nthrough tactile input.", "AI": {"tldr": "研究开发了一种模块化机器人电子皮肤，通过图神经网络识别触觉手势，实现直观的人机交互。", "motivation": "探索机器人电子皮肤技术如何通过触觉手势识别实现人机交互。", "method": "开发模块化电子皮肤，使用等变图神经网络分类触觉手势。", "result": "成功识别多种触觉手势（如戳、抓、抚摸等），并将其映射为机器人动作。", "conclusion": "通过触觉输入实现直观的人机交互，展示了电子皮肤技术的潜力。"}}
{"id": "2506.18264", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18264", "abs": "https://arxiv.org/abs/2506.18264", "authors": ["Jagadeswara PKV Pothuri", "Aditya Bhatt", "Prajit KrisshnaKumar", "Manaswin Oddiraju", "Souma Chowdhury"], "title": "Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle", "comment": "AIAA Aviation 2025", "summary": "Autonomous tracking of flying aerial objects has important civilian and\ndefense applications, ranging from search and rescue to counter-unmanned aerial\nsystems (counter-UAS). Ground based tracking requires setting up\ninfrastructure, could be range limited, and may not be feasible in remote\nareas, crowded cities or in dense vegetation areas. Vision based active\ntracking of aerial objects from another airborne vehicle, e.g., a chaser\nunmanned aerial vehicle (UAV), promises to fill this important gap, along with\nserving aerial coordination use cases. Vision-based active tracking by a UAV\nentails solving two coupled problems: 1) compute-efficient and accurate\n(target) object detection and target state estimation; and 2) maneuver\ndecisions to ensure that the target remains in the field of view in the future\ntime-steps and favorably positioned for continued detection. As a solution to\nthe first problem, this paper presents a novel integration of standard deep\nlearning based architectures with Kernelized Correlation Filter (KCF) to\nachieve compute-efficient object detection without compromising accuracy,\nunlike standalone learning or filtering approaches. The proposed perception\nframework is validated using a lab-scale setup. For the second problem, to\nobviate the linearity assumptions and background variations limiting\neffectiveness of the traditional controllers, we present the use of\nreinforcement learning to train a neuro-controller for fast computation of\nvelocity maneuvers. New state space, action space and reward formulations are\ndeveloped for this purpose, and training is performed in simulation using\nAirSim. The trained model is also tested in AirSim with respect to complex\ntarget maneuvers, and is found to outperform a baseline PID control in terms of\ntracking up-time and average distance maintained (from the target) during\ntracking.", "AI": {"tldr": "论文提出了一种结合深度学习与KCF的高效目标检测方法，以及基于强化学习的神经控制器，用于无人机对空中目标的视觉跟踪。", "motivation": "解决地面基础设施跟踪的局限性，如范围限制和地理限制，提出无人机视觉跟踪方案。", "method": "1) 结合深度学习与KCF实现高效目标检测；2) 使用强化学习训练神经控制器进行机动决策。", "result": "实验验证了感知框架的有效性，神经控制器在复杂目标机动中优于PID控制。", "conclusion": "该方法为无人机视觉跟踪提供了高效解决方案，适用于多种应用场景。"}}
{"id": "2506.18294", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18294", "abs": "https://arxiv.org/abs/2506.18294", "authors": ["Zhongyuan Li", "Honggang Gou", "Ping Li", "Jiaotong Guo", "Mao Ye"], "title": "Improvement on LiDAR-Camera Calibration Using Square Targets", "comment": null, "summary": "Precise sensor calibration is critical for autonomous vehicles as a\nprerequisite for perception algorithms to function properly. Rotation error of\none degree can translate to position error of meters in target object detection\nat large distance, leading to improper reaction of the system or even safety\nrelated issues. Many methods for multi-sensor calibration have been proposed.\nHowever, there are very few work that comprehensively consider the challenges\nof the calibration procedure when applied to factory manufacturing pipeline or\nafter-sales service scenarios. In this work, we introduce a fully automatic\nLiDAR-camera extrinsic calibration algorithm based on targets that is fast,\neasy to deploy and robust to sensor noises such as missing data. The core of\nthe method include: (1) an automatic multi-stage LiDAR board detection pipeline\nusing only geometry information with no specific material requirement; (2) a\nfast coarse extrinsic parameter search mechanism that is robust to initial\nextrinsic errors; (3) a direct optimization algorithm that is robust to sensor\nnoises. We validate the effectiveness of our methods through experiments on\ndata captured in real world scenarios.", "AI": {"tldr": "提出了一种基于目标的自动LiDAR-相机外参标定算法，快速、易部署且对传感器噪声鲁棒。", "motivation": "精确的传感器标定对自动驾驶车辆至关重要，现有方法未充分考虑工厂制造或售后服务场景的挑战。", "method": "包括自动多阶段LiDAR板检测、快速粗外参搜索和直接优化算法。", "result": "在真实场景数据中验证了方法的有效性。", "conclusion": "该方法解决了标定过程中的挑战，具有实际应用价值。"}}
{"id": "2506.18343", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18343", "abs": "https://arxiv.org/abs/2506.18343", "authors": ["Kawser Ahmed", "Mir Shahriar Fardin", "Md Arif Faysal Nayem", "Fahim Hafiz", "Swakkhar Shatabda"], "title": "TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for Exploration and Rescue Operations", "comment": "6 pages, 5 figures", "summary": "The increasing demand for underwater exploration and rescue operations\nenforces the development of advanced wireless or semi-wireless underwater\nvessels equipped with manipulator arms. This paper presents the implementation\nof a semi-wireless underwater vehicle, \"TritonZ\" equipped with a manipulator\narm, tailored for effective underwater exploration and rescue operations. The\nvehicle's compact design enables deployment in different submarine\nsurroundings, addressing the need for wireless systems capable of navigating\nchallenging underwater terrains. The manipulator arm can interact with the\nenvironment, allowing the robot to perform sophisticated tasks during\nexploration and rescue missions in emergency situations. TritonZ is equipped\nwith various sensors such as Pi-Camera, Humidity, and Temperature sensors to\nsend real-time environmental data. Our underwater vehicle controlled using a\ncustomized remote controller can navigate efficiently in the water where\nPi-Camera enables live streaming of the surroundings. Motion control and video\ncapture are performed simultaneously using this camera. The manipulator arm is\ndesigned to perform various tasks, similar to grasping, manipulating, and\ncollecting underwater objects. Experimental results shows the efficacy of the\nproposed remotely operated vehicle in performing a variety of underwater\nexploration and rescue tasks. Additionally, the results show that TritonZ can\nmaintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.\nFurthermore, the vehicle can sustain waves underwater by maintaining its\nposition as well as average velocity. The full project details and source code\ncan be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ", "AI": {"tldr": "论文介绍了一种半无线水下车辆'TritonZ'，配备机械臂，用于水下探索和救援任务，展示了其设计和性能。", "motivation": "水下探索和救援需求增加，需要开发能够适应复杂水下环境的无线或半无线设备。", "method": "实现了一种紧凑设计的半无线水下车辆，配备机械臂和多种传感器（如Pi-Camera、温湿度传感器），通过定制遥控器控制。", "result": "实验表明，TritonZ能以平均13.5cm/s的速度运行，延迟仅2-3秒，并能稳定维持位置和速度。", "conclusion": "TritonZ在水下探索和救援任务中表现出色，具备实际应用潜力。"}}
{"id": "2506.18355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18355", "abs": "https://arxiv.org/abs/2506.18355", "authors": ["Qi Jing Chen", "Shilin Shan", "Quang-Cuong Pham"], "title": "Robotic Manipulation of a Rotating Chain with Bottom End Fixed", "comment": "6 pages, 5 figures", "summary": "This paper studies the problem of using a robot arm to manipulate a uniformly\nrotating chain with its bottom end fixed. Existing studies have investigated\nideal rotational shapes for practical applications, yet they do not discuss how\nthese shapes can be consistently achieved through manipulation planning. Our\nwork presents a manipulation strategy for stable and consistent shape\ntransitions. We find that the configuration space of such a chain is\nhomeomorphic to a three-dimensional cube. Using this property, we suggest a\nstrategy to manipulate the chain into different configurations, specifically\nfrom one rotation mode to another, while taking stability and feasibility into\nconsideration. We demonstrate the effectiveness of our strategy in physical\nexperiments by successfully transitioning from rest to the first two rotation\nmodes. The concepts explored in our work has critical applications in ensuring\nsafety and efficiency of drill string and yarn spinning operations.", "AI": {"tldr": "本文研究了机器人手臂如何操纵底部固定的均匀旋转链条，提出了稳定的形状转换策略，并验证了其有效性。", "motivation": "现有研究探讨了理想旋转形状，但未涉及如何通过操纵规划实现这些形状，本文旨在填补这一空白。", "method": "利用链条构型空间与三维立方体的同胚性质，提出了一种考虑稳定性和可行性的操纵策略。", "result": "物理实验成功实现了从静止到前两种旋转模式的转换。", "conclusion": "该策略对钻杆和纺纱操作的安全与效率具有重要应用价值。"}}
{"id": "2506.18365", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.18365", "abs": "https://arxiv.org/abs/2506.18365", "authors": ["Imene Tarakli", "Samuele Vinanzi", "Richard Moore", "Alessandro Di Nuovo"], "title": "Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots", "comment": null, "summary": "Despite growing interest in Learning-by-Teaching (LbT), few studies have\nexplored how this paradigm can be implemented with autonomous, peer-like social\nrobots in real classrooms. Most prior work has relied on scripted or\nWizard-of-Oz behaviors, limiting our understanding of how real-time,\ninteractive learning can be supported by artificial agents. This study\naddresses this gap by introducing Interactive Reinforcement Learning (RL) as a\ncognitive model for teachable social robots. We conducted two between-subject\nexperiments with 58 primary school children, who either taught a robot or\npracticed independently on a tablet while learning French vocabulary\n(memorization) and grammatical rules (inference). The robot, powered by\nInteractive RL, learned from the child's evaluative feedback. Children in the\nLbT condition achieved significantly higher retention gains compared to those\nin the self-practice condition, especially on the grammar task. Learners with\nlower prior knowledge benefited most from teaching the robot. Behavioural\nmetrics revealed that children adapted their teaching strategies over time and\nengaged more deeply during inference tasks. This work makes two contributions:\n(1) it introduces Interactive RL as a pedagogically effective and scalable\nmodel for peer-robot learning, and (2) it demonstrates, for the first time, the\nfeasibility of deploying multiple autonomous robots simultaneously in real\nclassrooms. These findings extend theoretical understanding of LbT by showing\nthat social robots can function not only as passive tutees but as adaptive\npartners that enhance meta-cognitive engagement and long-term learning\noutcomes.", "AI": {"tldr": "研究通过交互式强化学习（RL）模型，探索了社交机器人在真实课堂中作为同伴的教学效果，发现学生通过教学机器人显著提高了学习效果，尤其是语法任务。", "motivation": "尽管学习-教学（LbT）范式受到关注，但缺乏对自主社交机器人在真实课堂中应用的研究。现有研究多依赖脚本或Wizard-of-Oz方法，限制了实时交互学习的理解。", "method": "采用交互式RL作为可教学机器人的认知模型，进行两项实验，58名小学生分别通过教学机器人或平板自主学习法语词汇和语法。机器人通过学生的反馈学习。", "result": "LbT组学生在语法任务中表现出显著更高的记忆保持率，尤其是低先验知识学生。行为数据显示学生调整教学策略并更深入参与推理任务。", "conclusion": "研究贡献在于：（1）提出交互式RL作为可扩展的同伴机器人学习模型；（2）首次展示多自主机器人同时在真实课堂中的可行性，证明机器人可作为适应性伙伴提升元认知和长期学习。"}}
{"id": "2506.18410", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18410", "abs": "https://arxiv.org/abs/2506.18410", "authors": ["Zhe Zhang", "Peijia Xie", "Zhirui Sun", "Bingyi Xia", "Bi-Ke Zhu", "Jiankun Wang"], "title": "Integrating Maneuverable Planning and Adaptive Control for Robot Cart-Pushing under Disturbances", "comment": "11 pages, 11 figures", "summary": "Precise and flexible cart-pushing is a challenging task for mobile robots.\nThe motion constraints during cart-pushing and the robot's redundancy lead to\ncomplex motion planning problems, while variable payloads and disturbances\npresent complicated dynamics. In this work, we propose a novel planning and\ncontrol framework for flexible whole-body coordination and robust adaptive\ncontrol. Our motion planning method employs a local coordinate representation\nand a novel kinematic model to solve a nonlinear optimization problem, thereby\nenhancing motion maneuverability by generating feasible and flexible push\nposes. Furthermore, we present a disturbance rejection control method to resist\ndisturbances and reduce control errors for the complex control problem without\nrequiring an accurate dynamic model. We validate our method through extensive\nexperiments in simulation and real-world settings, demonstrating its\nsuperiority over existing approaches. To the best of our knowledge, this is the\nfirst work to systematically evaluate the flexibility and robustness of\ncart-pushing methods in experiments. The video supplement is available at\nhttps://sites.google.com/view/mpac-pushing/.", "AI": {"tldr": "提出了一种新的规划和控制框架，用于移动机器人灵活推车任务，通过局部坐标表示和新型运动学模型优化运动规划，并结合抗干扰控制方法提升鲁棒性。", "motivation": "推车任务中运动约束和机器人冗余导致复杂运动规划问题，同时可变负载和干扰增加了动力学复杂性。", "method": "采用局部坐标表示和新型运动学模型解决非线性优化问题，提出抗干扰控制方法以减少控制误差。", "result": "通过仿真和实际实验验证了方法的优越性，首次系统评估了推车方法的灵活性和鲁棒性。", "conclusion": "该框架显著提升了推车任务的灵活性和鲁棒性，为复杂动态环境下的机器人控制提供了有效解决方案。"}}
{"id": "2506.18443", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18443", "abs": "https://arxiv.org/abs/2506.18443", "authors": ["Yang Lyu", "Zhenghao Zou", "Yanfeng Li", "Chunhui Zhao", "Quan Pan"], "title": "Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation", "comment": null, "summary": "Achieving reliable ego motion estimation for agile robots, e.g., aerobatic\naircraft, remains challenging because most robot sensors fail to respond timely\nand clearly to highly dynamic robot motions, often resulting in measurement\nblurring, distortion, and delays. In this paper, we propose an IMU-free and\nfeature-association-free framework to achieve aggressive ego-motion velocity\nestimation of a robot platform in highly dynamic scenarios by combining two\ntypes of exteroceptive sensors, an event camera and a millimeter wave radar,\nFirst, we used instantaneous raw events and Doppler measurements to derive\nrotational and translational velocities directly. Without a sophisticated\nassociation process between measurement frames, the proposed method is more\nrobust in texture-less and structureless environments and is more\ncomputationally efficient for edge computing devices. Then, in the back-end, we\npropose a continuous-time state-space model to fuse the hybrid time-based and\nevent-based measurements to estimate the ego-motion velocity in a fixed-lagged\nsmoother fashion. In the end, we validate our velometer framework extensively\nin self-collected experiment datasets. The results indicate that our IMU-free\nand association-free ego motion estimation framework can achieve reliable and\nefficient velocity output in challenging environments. The source code,\nillustrative video and dataset are available at\nhttps://github.com/ZzhYgwh/TwistEstimator.", "AI": {"tldr": "提出了一种基于事件相机和毫米波雷达的无IMU和无特征关联的机器人运动估计框架，适用于高动态场景。", "motivation": "高动态机器人运动导致传感器测量模糊、失真和延迟，传统方法难以应对。", "method": "结合事件相机和毫米波雷达，直接利用原始事件和多普勒测量推导速度，后端采用连续时间状态空间模型进行融合。", "result": "在挑战性环境中实现了可靠且高效的速度估计。", "conclusion": "该框架在高动态场景中表现优异，适合边缘计算设备。"}}
{"id": "2506.18448", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18448", "abs": "https://arxiv.org/abs/2506.18448", "authors": ["Quang Nguyen", "Tri Le", "Huy Nguyen", "Thieu Vo", "Tung D. Ta", "Baoru Huang", "Minh N. Vu", "Anh Nguyen"], "title": "GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System", "comment": "8 pages, accepted to IROS 2025", "summary": "Language-driven grasp detection has the potential to revolutionize\nhuman-robot interaction by allowing robots to understand and execute grasping\ntasks based on natural language commands. However, existing approaches face two\nkey challenges. First, they often struggle to interpret complex text\ninstructions or operate ineffectively in densely cluttered environments.\nSecond, most methods require a training or finetuning step to adapt to new\ndomains, limiting their generation in real-world applications. In this paper,\nwe introduce GraspMAS, a new multi-agent system framework for language-driven\ngrasp detection. GraspMAS is designed to reason through ambiguities and improve\ndecision-making in real-world scenarios. Our framework consists of three\nspecialized agents: Planner, responsible for strategizing complex queries;\nCoder, which generates and executes source code; and Observer, which evaluates\nthe outcomes and provides feedback. Intensive experiments on two large-scale\ndatasets demonstrate that our GraspMAS significantly outperforms existing\nbaselines. Additionally, robot experiments conducted in both simulation and\nreal-world settings further validate the effectiveness of our approach.", "AI": {"tldr": "GraspMAS是一种多智能体系统框架，用于语言驱动的抓取检测，解决了现有方法在复杂指令解释和密集环境中的不足，无需额外训练即可适应新领域。", "motivation": "提升机器人通过自然语言指令理解和执行抓取任务的能力，克服现有方法在复杂指令和密集环境中的局限性。", "method": "提出GraspMAS框架，包含三个智能体：Planner（策略规划）、Coder（代码生成与执行）和Observer（结果评估与反馈）。", "result": "在两个大规模数据集上显著优于现有基线，仿真和真实环境中的机器人实验验证了其有效性。", "conclusion": "GraspMAS在语言驱动的抓取检测中表现出色，具有实际应用潜力。"}}
{"id": "2506.18454", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18454", "abs": "https://arxiv.org/abs/2506.18454", "authors": ["Alejandro Romero", "Gianluca Baldassarre", "Richard J. Duro", "Vieri Giuliano Santucci"], "title": "A Motivational Architecture for Open-Ended Learning Challenges in Robots", "comment": "Accepted to RLDM 2025", "summary": "Developing agents capable of autonomously interacting with complex and\ndynamic environments, where task structures may change over time and prior\nknowledge cannot be relied upon, is a key prerequisite for deploying artificial\nsystems in real-world settings. The open-ended learning framework identifies\nthe core challenges for creating such agents, including the ability to\nautonomously generate new goals, acquire the necessary skills (or curricula of\nskills) to achieve them, and adapt to non-stationary environments. While many\nexisting works tackles various aspects of these challenges in isolation, few\npropose integrated solutions that address them simultaneously. In this paper,\nwe introduce H-GRAIL, a hierarchical architecture that, through the use of\ndifferent typologies of intrinsic motivations and interconnected learning\nmechanisms, autonomously discovers new goals, learns the required skills for\ntheir achievement, generates skill sequences for tackling interdependent tasks,\nand adapts to non-stationary environments. We tested H-GRAIL in a real robotic\nscenario, demonstrating how the proposed solutions effectively address the\nvarious challenges of open-ended learning.", "AI": {"tldr": "H-GRAIL是一种分层架构，通过内在动机和互联学习机制，自主发现目标、学习技能并适应非静态环境，解决了开放学习中的核心挑战。", "motivation": "开发能够在复杂动态环境中自主交互的智能体，是部署人工智能系统到现实世界的关键前提。现有研究多孤立解决部分问题，缺乏综合方案。", "method": "提出H-GRAIL分层架构，结合多种内在动机和互联学习机制，实现目标发现、技能学习、任务序列生成和环境适应。", "result": "在真实机器人场景中测试H-GRAIL，验证其有效解决开放学习中的多类挑战。", "conclusion": "H-GRAIL为开放学习提供了一种综合解决方案，展示了在复杂环境中自主学习和适应的潜力。"}}
{"id": "2506.18466", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.18466", "abs": "https://arxiv.org/abs/2506.18466", "authors": ["Matti Krüger", "Daniel Tanneberg", "Chao Wang", "Stephan Hasler", "Michael Gienger"], "title": "Mirror Eyes: Explainable Human-Robot Interaction at a Glance", "comment": "Accepted to the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)", "summary": "The gaze of a person tends to reflect their interest. This work explores what\nhappens when this statement is taken literally and applied to robots. Here we\npresent a robot system that employs a moving robot head with a screen-based eye\nmodel that can direct the robot's gaze to points in physical space and present\na reflection-like mirror image of the attended region on top of each eye. We\nconducted a user study with 33 participants, who were asked to instruct the\nrobot to perform pick-and-place tasks, monitor the robot's task execution, and\ninterrupt it in case of erroneous actions. Despite a deliberate lack of\ninstructions about the role of the eyes and a very brief system exposure,\nparticipants felt more aware about the robot's information processing, detected\nerroneous actions earlier, and rated the user experience higher when eye-based\nmirroring was enabled compared to non-reflective eyes. These results suggest a\nbeneficial and intuitive utilization of the introduced method in cooperative\nhuman-robot interaction.", "AI": {"tldr": "研究探索了机器人通过模拟人类注视行为（在屏幕上显示注视区域的镜像）如何提升人机协作体验。实验表明，这种设计能增强用户对机器人信息处理的感知，提高任务执行效率。", "motivation": "探索机器人模拟人类注视行为是否能提升人机协作的直观性和效率。", "method": "开发了一个带屏幕眼睛模型的机器人头部系统，能注视物理空间并在眼睛上显示注视区域的镜像。通过用户研究（33人）评估其效果。", "result": "用户对机器人信息处理的感知增强，错误检测更早，用户体验评分更高。", "conclusion": "模拟注视行为的机器人设计在人机协作中具有直观且有益的应用潜力。"}}
{"id": "2506.18526", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2506.18526", "abs": "https://arxiv.org/abs/2506.18526", "authors": ["Dhruv Sorathiya", "Sarthak Sahoo", "Vivek Natarajan"], "title": "Design, fabrication and control of a cable-driven parallel robot", "comment": "4 pages, 8 fugures", "summary": "In cable driven parallel robots (CDPRs), the payload is suspended using a\nnetwork of cables whose length can be controlled to maneuver the payload within\nthe workspace. Compared to rigid link robots, CDPRs provide better\nmaneuverability due to the flexibility of the cables and consume lesser power\ndue to the high strength-to-weight ratio of the cables. However, amongst other\nthings, the flexibility of the cables and the fact that they can only pull (and\nnot push) render the dynamics of CDPRs complex. Hence advanced modelling\nparadigms and control algorithms must be developed to fully utilize the\npotential of CDPRs. Furthermore, given the complex dynamics of CDPRs, the\nmodels and control algorithms proposed for them must be validated on\nexperimental setups to ascertain their efficacy in practice. We have recently\ndeveloped an elaborate experimental setup for a CDPR with three cables and\nvalidated elementary open-loop motion planning algorithms on it. In this paper,\nwe describe several aspects of the design and fabrication of our setup,\nincluding component selection and assembly, and present our experimental\nresults. Our setup can reproduce complex phenomenon such as the transverse\nvibration of the cables seen in large CDPRs and will in the future be used to\nmodel and control such phenomenon and also to validate more sophisticated\nmotion planning algorithms.", "AI": {"tldr": "本文介绍了电缆驱动并联机器人（CDPRs）的实验装置设计与验证，重点讨论了其复杂动力学特性及控制算法的实验验证。", "motivation": "CDPRs因其电缆的灵活性和高强重比具有优势，但其复杂动力学特性需要先进的建模和控制算法。实验验证是确保这些算法实际有效的关键。", "method": "设计并制造了一个三电缆CDPR实验装置，验证了基本的开环运动规划算法，并展示了装置的设计细节和实验结果。", "result": "实验装置能够复现大型CDPR中的电缆横向振动等复杂现象，为未来建模和控制研究提供了平台。", "conclusion": "该实验装置为CDPR的复杂动力学建模和控制算法验证提供了有效工具，未来将用于更高级的运动规划算法验证。"}}
{"id": "2506.18580", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18580", "abs": "https://arxiv.org/abs/2506.18580", "authors": ["Jan Michalczyk", "Stephan Weiss", "Jan Steinbrener"], "title": "Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry", "comment": null, "summary": "Using 3D point clouds in odometry estimation in robotics often requires\nfinding a set of correspondences between points in subsequent scans. While\nthere are established methods for point clouds of sufficient quality,\nstate-of-the-art still struggles when this quality drops. Thus, this paper\npresents a novel learning-based framework for predicting robust point\ncorrespondences between pairs of noisy, sparse and unstructured 3D point clouds\nfrom a light-weight, low-power, inexpensive, consumer-grade System-on-Chip\n(SoC) Frequency Modulated Continuous Wave (FMCW) radar sensor. Our network is\nbased on the transformer architecture which allows leveraging the attention\nmechanism to discover pairs of points in consecutive scans with the greatest\nmutual affinity. The proposed network is trained in a self-supervised way using\nset-based multi-label classification cross-entropy loss, where the ground-truth\nset of matches is found by solving the Linear Sum Assignment (LSA) optimization\nproblem, which avoids tedious hand annotation of the training data.\nAdditionally, posing the loss calculation as multi-label classification permits\nsupervising on point correspondences directly instead of on odometry error,\nwhich is not feasible for sparse and noisy data from the SoC radar we use. We\nevaluate our method with an open-source state-of-the-art Radar-Inertial\nOdometry (RIO) framework in real-world Unmanned Aerial Vehicle (UAV) flights\nand with the widely used public Coloradar dataset. Evaluation shows that the\nproposed method improves the position estimation accuracy by over 14 % and 19 %\non average, respectively. The open source code and datasets can be found here:\nhttps://github.com/aau-cns/radar_transformer.", "AI": {"tldr": "提出了一种基于Transformer的学习框架，用于在低质量3D点云中预测鲁棒的点对应关系，显著提升了位置估计精度。", "motivation": "现有方法在点云质量下降时表现不佳，特别是在低成本雷达传感器生成的数据上。", "method": "采用Transformer架构和自监督学习，通过多标签分类交叉熵损失训练，避免手工标注。", "result": "在真实无人机飞行和公开数据集上，位置估计精度分别提升了14%和19%。", "conclusion": "该方法为低质量点云提供了一种有效的解决方案，显著提升了雷达惯性里程计的精度。"}}
{"id": "2506.18583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18583", "abs": "https://arxiv.org/abs/2506.18583", "authors": ["Nikhil Khedekar", "Kostas Alexis"], "title": "PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry", "comment": "8 pages, 6 figures", "summary": "LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation\nand mapping which is an essential requirement for autonomous robots.\nConventional LIO methods typically rely on formulating constraints from the\ngeometric structure sampled by the LiDAR. Hence, in the lack of geometric\nstructure, these tend to become ill-conditioned (degenerate) and fail.\nRobustness of LIO to such conditions is a necessity for its broader deployment.\nTo address this, we propose PG-LIO, a real-time LIO method that fuses\nphotometric and geometric information sampled by the LiDAR along with inertial\nconstraints from an Inertial Measurement Unit (IMU). This multi-modal\ninformation is integrated into a factor graph optimized over a sliding window\nfor real-time operation. We evaluate PG-LIO on multiple datasets that include\nboth geometrically well-conditioned as well as self-similar scenarios. Our\nmethod achieves accuracy on par with state-of-the-art LIO in geometrically\nwell-structured settings while significantly improving accuracy in degenerate\ncases including against methods that also fuse intensity. Notably, we\ndemonstrate only 1 m drift over a 1 km manually piloted aerial trajectory\nthrough a geometrically self-similar tunnel at an average speed of 7.5m/s (max\nspeed 10.8 m/s). For the benefit of the community, we shall also release our\nsource code https://github.com/ntnu-arl/mimosa", "AI": {"tldr": "PG-LIO是一种融合LiDAR的光度和几何信息与IMU数据的实时LIO方法，显著提高了在几何结构缺失情况下的鲁棒性和精度。", "motivation": "传统LIO方法在几何结构缺失时容易失效，限制了其广泛应用。PG-LIO旨在通过多模态信息融合解决这一问题。", "method": "PG-LIO将LiDAR的光度、几何信息与IMU数据结合，通过滑动窗口因子图优化实现实时操作。", "result": "在几何结构良好和自相似场景中，PG-LIO表现优异，尤其在退化情况下精度显著提升，1公里轨迹漂移仅1米。", "conclusion": "PG-LIO通过多模态融合提高了LIO的鲁棒性和精度，适用于更广泛的场景。"}}
{"id": "2506.18689", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18689", "abs": "https://arxiv.org/abs/2506.18689", "authors": ["Alessandro Saviolo", "Giuseppe Loianno"], "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments", "comment": null, "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.", "AI": {"tldr": "NOVA是一个完全机载的、以目标为中心的框架，仅使用立体相机和IMU实现鲁棒的目标跟踪和碰撞感知导航，适用于无结构和GPS缺失环境。", "motivation": "解决在无结构和GPS缺失环境中自主空中目标跟踪的挑战，避免依赖运动捕捉系统或预建地图。", "method": "结合轻量级目标检测器、立体深度补全和直方图滤波，通过视觉-惯性状态估计器和NMPC实现目标参考框架内的6-DoF姿态恢复和轨迹规划。", "result": "在复杂场景（如城市迷宫、森林小径）中实现超过50 km/h的敏捷目标跟踪，无需外部定位或环境假设。", "conclusion": "NOVA证明了仅依靠机载传感器即可在复杂环境中实现高速视觉目标跟踪的可行性。"}}
{"id": "2506.18697", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18697", "abs": "https://arxiv.org/abs/2506.18697", "authors": ["Marios-Nektarios Stamatopoulos", "Shridhar Velhal", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Safety-Aware Optimal Scheduling for Autonomous Masonry Construction using Collaborative Heterogeneous Aerial Robots", "comment": "This paper has been accepted for publication at the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "This paper presents a novel high-level task planning and optimal coordination\nframework for autonomous masonry construction, using a team of heterogeneous\naerial robotic workers, consisting of agents with separate skills for brick\nplacement and mortar application. This introduces new challenges in scheduling\nand coordination, particularly due to the mortar curing deadline required for\nstructural bonding and ensuring the safety constraints among UAVs operating in\nparallel. To address this, an automated pipeline generates the wall\nconstruction plan based on the available bricks while identifying static\nstructural dependencies and potential conflicts for safe operation. The\nproposed framework optimizes UAV task allocation and execution timing by\nincorporating dynamically coupled precedence deadline constraints that account\nfor the curing process and static structural dependency constraints, while\nenforcing spatio-temporal constraints to prevent collisions and ensure safety.\nThe primary objective of the scheduler is to minimize the overall construction\nmakespan while minimizing logistics, traveling time between tasks, and the\ncuring time to maintain both adhesion quality and safe workspace separation.\nThe effectiveness of the proposed method in achieving coordinated and\ntime-efficient aerial masonry construction is extensively validated through\nGazebo simulated missions. The results demonstrate the framework's capability\nto streamline UAV operations, ensuring both structural integrity and safety\nduring the construction process.", "AI": {"tldr": "提出了一种用于自主砌体施工的高层任务规划和优化协调框架，利用异构无人机团队解决砖块放置和砂浆应用问题。", "motivation": "解决无人机团队在砌体施工中的调度和协调挑战，特别是砂浆固化时间限制和并行操作的安全约束。", "method": "自动化管道生成施工计划，优化无人机任务分配和执行时间，结合动态耦合的优先级约束和时空约束。", "result": "通过Gazebo模拟验证，框架能高效协调无人机操作，确保施工过程的结构完整性和安全性。", "conclusion": "该框架成功实现了时间高效的无人机砌体施工，同时满足结构要求和安全标准。"}}
{"id": "2506.18725", "categories": ["cs.RO", "cs.CG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18725", "abs": "https://arxiv.org/abs/2506.18725", "authors": ["Anirban Ghosh", "Ian Dahlin", "Ayan Dutta"], "title": "TDACloud: Point Cloud Recognition Using Topological Data Analysis", "comment": null, "summary": "Point cloud-based object/place recognition remains a problem of interest in\napplications such as autonomous driving, scene reconstruction, and\nlocalization. Extracting meaningful local descriptors from a query point cloud\nthat can be matched with the descriptors of the collected point clouds is a\nchallenging problem. Furthermore, when the query point cloud is noisy or has\nbeen transformed (e.g., rotated), it adds to the complexity. To this end, we\npropose a novel methodology, named TDACloud, using Topological Data Analysis\n(TDA) for local descriptor extraction from a point cloud, which does not need\nresource-intensive GPU-based machine learning training. More specifically, we\nused the ATOL vectorization method to generate vectors for point clouds. Unlike\nvoxelization, our proposed technique can take raw point clouds as inputs and\noutputs a fixed-size TDA-descriptor vector. To test the quality of the proposed\nTDACloud technique, we have implemented it on multiple real-world (e.g., Oxford\nRobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for\nobject and place recognition. We have also tested TDACloud on noisy and\ntransformed test cases where the query point cloud has been scaled, translated,\nor rotated. Our results demonstrate high recognition accuracies in noisy\nconditions and large-scale real-world place recognition while outperforming the\nbaselines by up to approximately 14%.", "AI": {"tldr": "提出了一种名为TDACloud的新方法，利用拓扑数据分析（TDA）从点云中提取局部描述符，无需GPU密集型训练，并在噪声和变换条件下表现优异。", "motivation": "点云识别在自动驾驶等应用中具有重要意义，但提取可匹配的局部描述符具有挑战性，尤其是在噪声或变换条件下。", "method": "使用ATOL向量化方法生成点云的固定大小TDA描述符向量，直接处理原始点云。", "result": "在真实和模拟数据集上测试，TDACloud在噪声和变换条件下识别准确率高，优于基线方法约14%。", "conclusion": "TDACloud是一种高效的点云识别方法，适用于复杂环境，且无需GPU训练。"}}
{"id": "2506.18779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18779", "abs": "https://arxiv.org/abs/2506.18779", "authors": ["Bao Thach", "Siyeon Kim", "Britton Jordan", "Mohanraj Shanthi", "Tanner Watts", "Shing-Hei Ho", "James M. Ferguson", "Tucker Hermans", "Alan Kuntz"], "title": "DefFusionNet: Learning Multimodal Goal Shapes for Deformable Object Manipulation via a Diffusion-based Probabilistic Model", "comment": null, "summary": "Deformable object manipulation is critical to many real-world robotic\napplications, ranging from surgical robotics and soft material handling in\nmanufacturing to household tasks like laundry folding. At the core of this\nimportant robotic field is shape servoing, a task focused on controlling\ndeformable objects into desired shapes. The shape servoing formulation requires\nthe specification of a goal shape. However, most prior works in shape servoing\nrely on impractical goal shape acquisition methods, such as laborious\ndomain-knowledge engineering or manual manipulation. DefGoalNet previously\nposed the current state-of-the-art solution to this problem, which learns\ndeformable object goal shapes directly from a small number of human\ndemonstrations. However, it significantly struggles in multi-modal settings,\nwhere multiple distinct goal shapes can all lead to successful task completion.\nAs a deterministic model, DefGoalNet collapses these possibilities into a\nsingle averaged solution, often resulting in an unusable goal. In this paper,\nwe address this problem by developing DefFusionNet, a novel neural network that\nleverages the diffusion probabilistic model to learn a distribution over all\nvalid goal shapes rather than predicting a single deterministic outcome. This\nenables the generation of diverse goal shapes and avoids the averaging\nartifacts. We demonstrate our method's effectiveness on robotic tasks inspired\nby both manufacturing and surgical applications, both in simulation and on a\nphysical robot. Our work is the first generative model capable of producing a\ndiverse, multi-modal set of deformable object goals for real-world robotic\napplications.", "AI": {"tldr": "DefFusionNet提出了一种基于扩散概率模型的新方法，用于生成多样化的可变形物体目标形状，解决了DefGoalNet在多模态场景下的局限性。", "motivation": "现有方法（如DefGoalNet）在多模态任务中无法生成多样化的目标形状，导致实用性受限。", "method": "利用扩散概率模型学习所有有效目标形状的分布，而非单一确定性预测。", "result": "在仿真和实际机器人任务中验证了方法的有效性，能够生成多样化的目标形状。", "conclusion": "DefFusionNet是首个能够为实际机器人应用生成多样化目标形状的生成模型。"}}
{"id": "2506.18812", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18812", "abs": "https://arxiv.org/abs/2506.18812", "authors": ["Aristotelis Papatheodorou", "Pranav Vaidhyanathan", "Natalia Ares", "Ioannis Havoutis"], "title": "Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures", "comment": "Presented at Equivariant Systems: Theory and Applications in State\n  Estimation, Artificial Intelligence and Control, Robotics: Science and\n  Systems (RSS) 2025 Workshop, 6 Pages, 3 Figures", "summary": "Physics-informed deep learning has achieved remarkable progress by embedding\ngeometric priors, such as Hamiltonian symmetries and variational principles,\ninto neural networks, enabling structure-preserving models that extrapolate\nwith high accuracy. However, in systems with dissipation and holonomic\nconstraints, ubiquitous in legged locomotion and multibody robotics, the\ncanonical symplectic form becomes degenerate, undermining the very invariants\nthat guarantee stability and long-term prediction. In this work, we tackle this\nfoundational limitation by introducing Presymplectification Networks (PSNs),\nthe first framework to learn the symplectification lift via Dirac structures,\nrestoring a non-degenerate symplectic geometry by embedding constrained systems\ninto a higher-dimensional manifold. Our architecture combines a recurrent\nencoder with a flow-matching objective to learn the augmented phase-space\ndynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet)\nto forecast constrained trajectories while preserving energy, momentum, and\nconstraint satisfaction. We demonstrate our method on the dynamics of the\nANYmal quadruped robot, a challenging contact-rich, multibody system. To the\nbest of our knowledge, this is the first framework that effectively bridges the\ngap between constrained, dissipative mechanical systems and symplectic\nlearning, unlocking a whole new class of geometric machine learning models,\ngrounded in first principles yet adaptable from data.", "AI": {"tldr": "提出了Presymplectification Networks（PSNs），通过Dirac结构学习symplectification lift，解决带耗散和约束系统的几何学习问题。", "motivation": "在带耗散和约束的系统中，传统辛形式退化，影响稳定性和长期预测。", "method": "结合循环编码器和流匹配目标学习高维流形上的动力学，并附加轻量级SympNet预测约束轨迹。", "result": "在ANYmal四足机器人上验证，首次有效连接约束耗散系统与辛学习。", "conclusion": "PSNs为几何机器学习开辟了新方向，结合第一性原理与数据适应性。"}}
{"id": "2506.18825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18825", "abs": "https://arxiv.org/abs/2506.18825", "authors": ["Yizhou Chen", "Hang Xu", "Dongjie Yu", "Zeqing Zhang", "Yi Ren", "Jia Pan"], "title": "SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives", "comment": "Project website: https://sites.google.com/view/svip-bimanual", "summary": "Imitation learning (IL), particularly when leveraging high-dimensional visual\ninputs for policy training, has proven intuitive and effective in complex\nbimanual manipulation tasks. Nonetheless, the generalization capability of\nvisuomotor policies remains limited, especially when small demonstration\ndatasets are available. Accumulated errors in visuomotor policies significantly\nhinder their ability to complete long-horizon tasks. To address these\nlimitations, we propose SViP, a framework that seamlessly integrates visuomotor\npolicies into task and motion planning (TAMP). SViP partitions human\ndemonstrations into bimanual and unimanual operations using a semantic scene\ngraph monitor. Continuous decision variables from the key scene graph are\nemployed to train a switching condition generator. This generator produces\nparameterized scripted primitives that ensure reliable performance even when\nencountering out-of-the-distribution observations. Using only 20 real-world\ndemonstrations, we show that SViP enables visuomotor policies to generalize\nacross out-of-distribution initial conditions without requiring object pose\nestimators. For previously unseen tasks, SViP automatically discovers effective\nsolutions to achieve the goal, leveraging constraint modeling in TAMP\nformulism. In real-world experiments, SViP outperforms state-of-the-art\ngenerative IL methods, indicating wider applicability for more complex tasks.\nProject website: https://sites.google.com/view/svip-bimanual", "AI": {"tldr": "SViP框架通过将视觉运动策略与任务和运动规划（TAMP）结合，解决了小样本下视觉运动策略泛化能力不足的问题。", "motivation": "视觉运动策略在小样本下泛化能力有限，且累积误差影响长时任务完成。", "method": "SViP利用语义场景图监控分割演示，训练切换条件生成器，生成参数化脚本基元。", "result": "仅需20个真实演示，SViP即可在未见初始条件下泛化，且无需物体姿态估计器。", "conclusion": "SViP在真实实验中优于现有生成式模仿学习方法，适用于更复杂任务。"}}
{"id": "2506.18844", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18844", "abs": "https://arxiv.org/abs/2506.18844", "authors": ["Olivier Gamache", "Jean-Michel Fortin", "Matěj Boxan", "François Pomerleau", "Philippe Giguère"], "title": "Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned", "comment": "19 pages, 11 figures, pre-print version of the accepted paper for\n  IEEE Transactions on Field Robotics (T-FR)", "summary": "Standard datasets often present limitations, particularly due to the fixed\nnature of input data sensors, which makes it difficult to compare methods that\nactively adjust sensor parameters to suit environmental conditions. This is the\ncase with Automatic-Exposure (AE) methods, which rely on environmental factors\nto influence the image acquisition process. As a result, AE methods have\ntraditionally been benchmarked in an online manner, rendering experiments\nnon-reproducible. Building on our prior work, we propose a methodology that\nutilizes an emulator capable of generating images at any exposure time. This\napproach leverages BorealHDR, a unique multi-exposure stereo dataset, along\nwith its new extension, in which data was acquired along a repeated trajectory\nat different times of the day to assess the impact of changing illumination. In\ntotal, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting\nconditions. The dataset also includes lidar-inertial-odometry-based maps with\npose estimation for each image frame, as well as Global Navigation Satellite\nSystem (GNSS) data for comparison. We demonstrate that by using images acquired\nat various exposure times, we can emulate realistic images with a\nRoot-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.\nUsing this offline approach, we benchmarked eight AE methods, concluding that\nthe classical AE method remains the field's best performer. To further support\nreproducibility, we provide in-depth details on the development of our backpack\nacquisition platform, including hardware, electrical components, and\nperformance specifications. Additionally, we share valuable lessons learned\nfrom deploying the backpack over more than 25 km across various environments.\nOur code and dataset are available online at this link:\nhttps://github.com/norlab-ulaval/TFR24 BorealHDR", "AI": {"tldr": "提出了一种利用模拟器生成任意曝光时间图像的方法，解决了传统自动曝光方法在线评估不可复现的问题，并通过BorealHDR数据集验证了方法的有效性。", "motivation": "标准数据集因固定传感器输入难以评估动态调整传感器参数的方法（如自动曝光方法），导致实验不可复现。", "method": "利用BorealHDR多曝光立体数据集及其扩展，通过模拟器生成不同曝光时间的图像，离线评估自动曝光方法。", "result": "模拟图像的RMSE低于1.78%，验证了方法的准确性；评估了八种自动曝光方法，发现传统方法表现最佳。", "conclusion": "提出的离线方法解决了自动曝光评估的复现性问题，BorealHDR数据集和模拟器为相关研究提供了支持。"}}
{"id": "2506.18885", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18885", "abs": "https://arxiv.org/abs/2506.18885", "authors": ["Annika Thomas", "Aneesa Sonawalla", "Alex Rose", "Jonathan P. How"], "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM", "comment": null, "summary": "3D Gaussian splatting has emerged as an expressive scene representation for\nRGB-D visual SLAM, but its application to large-scale, multi-agent outdoor\nenvironments remains unexplored. Multi-agent Gaussian SLAM is a promising\napproach to rapid exploration and reconstruction of environments, offering\nscalable environment representations, but existing approaches are limited to\nsmall-scale, indoor environments. To that end, we propose Gaussian\nReconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative\nGaussian splatting SLAM method that integrates i) an implicit tracking module\nbased on local optimization over submaps and ii) an approach to inter- and\nintra-robot loop closure integrated into a pose-graph optimization framework.\nExperiments show that GRAND-SLAM provides state-of-the-art tracking performance\nand 28% higher PSNR than existing methods on the Replica indoor dataset, as\nwell as 91% lower multi-agent tracking error and improved rendering over\nexisting multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.", "AI": {"tldr": "GRAND-SLAM是一种多智能体协作的3D高斯泼溅SLAM方法，适用于大规模户外环境，性能优于现有方法。", "motivation": "现有高斯泼溅SLAM方法局限于小规模室内环境，无法满足多智能体户外场景的需求。", "method": "结合局部优化的隐式跟踪模块和多机器人闭环检测的位姿图优化框架。", "result": "在Replica数据集上PSNR提高28%，在Kimera-Multi数据集上多智能体跟踪误差降低91%。", "conclusion": "GRAND-SLAM在大规模户外多智能体场景中表现出色，性能显著优于现有方法。"}}
{"id": "2506.18897", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18897", "abs": "https://arxiv.org/abs/2506.18897", "authors": ["Xiaowei Chi", "Kuangzhi Ge", "Jiaming Liu", "Siyuan Zhou", "Peidong Jia", "Zichen He", "Yuzhen Liu", "Tingguang Li", "Lei Han", "Sirui Han", "Shanghang Zhang", "Yike Guo"], "title": "MinD: Unified Visual Imagination and Control via Hierarchical World Models", "comment": null, "summary": "Video generation models (VGMs) offer a promising pathway for unified world\nmodeling in robotics by integrating simulation, prediction, and manipulation.\nHowever, their practical application remains limited due to (1) slowgeneration\nspeed, which limits real-time interaction, and (2) poor consistency between\nimagined videos and executable actions. To address these challenges, we propose\nManipulate in Dream (MinD), a hierarchical diffusion-based world model\nframework that employs a dual-system design for vision-language manipulation.\nMinD executes VGM at low frequencies to extract video prediction features,\nwhile leveraging a high-frequency diffusion policy for real-time interaction.\nThis architecture enables low-latency, closed-loop control in manipulation with\ncoherent visual guidance. To better coordinate the two systems, we introduce a\nvideo-action diffusion matching module (DiffMatcher), with a novel co-training\nstrategy that uses separate schedulers for each diffusion model. Specifically,\nwe introduce a diffusion-forcing mechanism to DiffMatcher that aligns their\nintermediate representations during training, helping the fast action model\nbetter understand video-based predictions. Beyond manipulation, MinD also\nfunctions as a world simulator, reliably predicting task success or failure in\nlatent space before execution. Trustworthy analysis further shows that VGMs can\npreemptively evaluate task feasibility and mitigate risks. Extensive\nexperiments across multiple benchmarks demonstrate that MinD achieves\nstate-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of\nunified world modeling in robotics.", "AI": {"tldr": "MinD提出了一种分层扩散模型框架，通过双系统设计解决视频生成模型在机器人中的实时交互和一致性不足问题。", "motivation": "视频生成模型在机器人应用中存在生成速度慢和视频与动作一致性差的问题，限制了实时交互。", "method": "MinD采用双系统设计，低频执行视频生成模型提取特征，高频扩散策略实现实时交互，并通过DiffMatcher模块协调两者。", "result": "MinD在RL-Bench上实现了63%+的操控性能，展示了其在统一世界建模中的前沿能力。", "conclusion": "MinD不仅提升了机器人操控的实时性和一致性，还可作为世界模拟器预测任务可行性，降低风险。"}}
