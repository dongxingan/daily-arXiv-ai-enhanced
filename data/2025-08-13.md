<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Large Scale Robotic Material Handling: Learning, Planning, and Control](https://arxiv.org/abs/2508.09003)
*Filippo A. Spinelli,Yifan Zhai,Fang Nan,Pascal Egli,Julian Nubert,Thilo Bleumer,Lukas Miller,Ferdinand Hofmann,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种用于大规模物料搬运任务的自主执行框架，结合了环境感知、抓取点选择、路径规划和运动控制模块，并通过强化学习方法优化抓取和轨迹控制。


<details>
  <summary>Details</summary>
Motivation: 物料搬运是许多行业中的核心操作，但通常依赖人工操作，存在效率低和安全风险的问题。本文旨在通过自动化技术提升效率和安全性。

Method: 系统整合了环境感知、抓取点选择、路径规划和运动控制模块，并采用强化学习方法优化抓取位置选择和轨迹跟踪。

Result: 在真实场景中验证了系统的有效性，与人工操作相比，在精度、重复性和安全性方面表现更优。

Conclusion: 本文首次实现了全尺寸物料搬运任务的完整自动化，为行业提供了高效、安全的解决方案。

Abstract: Bulk material handling involves the efficient and precise moving of large
quantities of materials, a core operation in many industries, including cargo
ship unloading, waste sorting, construction, and demolition. These repetitive,
labor-intensive, and safety-critical operations are typically performed using
large hydraulic material handlers equipped with underactuated grippers. In this
work, we present a comprehensive framework for the autonomous execution of
large-scale material handling tasks. The system integrates specialized modules
for environment perception, pile attack point selection, path planning, and
motion control. The main contributions of this work are two reinforcement
learning-based modules: an attack point planner that selects optimal grasping
locations on the material pile to maximize removal efficiency and minimize the
number of scoops, and a robust trajectory following controller that addresses
the precision and safety challenges associated with underactuated grippers in
movement, while utilizing their free-swinging nature to release material
through dynamic throwing. We validate our framework through real-world
experiments on a 40 t material handler in a representative worksite, focusing
on two key tasks: high-throughput bulk pile management and high-precision truck
loading. Comparative evaluations against human operators demonstrate the
system's effectiveness in terms of precision, repeatability, and operational
safety. To the best of our knowledge, this is the first complete automation of
material handling tasks on a full scale.

</details>


### [2] [Humanoid Robot Acrobatics Utilizing Complete Articulated Rigid Body Dynamics](https://arxiv.org/abs/2508.08258)
*Gerald Brantner*

Main category: cs.RO

TL;DR: 提出了一种结合轨迹优化和全身控制的架构，通过模型抽象实现人形机器人高动态运动。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人高自由度带来的运动规划和控制难题，避免传统线性化和模型近似方法的性能下降问题。

Method: 采用轨迹优化和全身控制结合的架构，引入匹配的模型抽象，基于完整的刚体动力学方程。

Result: 在仿真中验证了系统执行高动态运动（如杂技动作）的有效性。

Conclusion: 该架构通过模型抽象和完整动力学方程，成功实现了人形机器人的高动态运动控制。

Abstract: Endowing humanoid robots with the ability to perform highly dynamic motions
akin to human-level acrobatics has been a long-standing challenge. Successfully
performing these maneuvers requires close consideration of the underlying
physics in both trajectory optimization for planning and control during
execution. This is particularly challenging due to humanoids' high
degree-of-freedom count and associated exponentially scaling complexities,
which makes planning on the explicit equations of motion intractable. Typical
workarounds include linearization methods and model approximations. However,
neither are sufficient because they produce degraded performance on the true
robotic system. This paper presents a control architecture comprising
trajectory optimization and whole-body control, intermediated by a matching
model abstraction, that enables the execution of acrobatic maneuvers, including
constraint and posture behaviors, conditioned on the unabbreviated equations of
motion of the articulated rigid body model. A review of underlying modeling and
control methods is given, followed by implementation details including model
abstraction, trajectory optimization and whole-body controller. The system's
effectiveness is analyzed in simulation.

</details>


### [3] [Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting](https://arxiv.org/abs/2508.08259)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 使用Koopman算子为四足机器人构建高维线性模型，结合LMPC实现高精度跟踪和干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 在线最优控制使四足机器人能实时适应变化输入和条件，但传统LMPC的线性化模型可能不准确。

Method: 利用Koopman算子在保留非线性运动方程的高维空间中构建线性模型，并应用LMPC。

Result: 实现了四足机器人的高精度跟踪和干扰抑制。

Conclusion: 首次将Koopman算子理论应用于四足机器人LMPC，提升了控制性能。

Abstract: Online optimal control of quadruped robots would enable them to adapt to
varying inputs and changing conditions in real time. A common way of achieving
this is linear model predictive control (LMPC), where a quadratic programming
(QP) problem is formulated over a finite horizon with a quadratic cost and
linear constraints obtained by linearizing the equations of motion and solved
on the fly. However, the model linearization may lead to model inaccuracies. In
this paper, we use the Koopman operator to create a linear model of the
quadrupedal system in high dimensional space which preserves the nonlinearity
of the equations of motion. Then using LMPC, we demonstrate high fidelity
tracking and disturbance rejection on a quadrupedal robot. This is the first
work that uses the Koopman operator theory for LMPC of quadrupedal locomotion.

</details>


### [4] [Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance](https://arxiv.org/abs/2508.08264)
*Hadush Hailu,Bruk Gebregziabher,Prudhvi Raj*

Main category: cs.RO

TL;DR: eIFP-MPC是IFP的优化版本，通过改进威胁优先级、路径稳定性和动态可行性，提升了在多机器人路径规划中的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: IFP在对称配置中表现不佳，容易导致碰撞和死锁，因此需要一种更稳健的解决方案。

Method: 结合时间碰撞启发式优化威胁优先级，通过成本选择稳定路径，并集成模型预测控制（MPC）确保动态可行性。

Result: 在对称和高密度场景中，eIFP-MPC显著减少振荡，确保无碰撞运动，并提高轨迹效率。

Conclusion: 几何规划器通过优化可以提升性能，适用于复杂多智能体环境。

Abstract: The Iterative Forecast Planner (IFP) is a geometric planning approach that
offers lightweight computations, scalable, and reactive solutions for
multi-robot path planning in decentralized, communication-free settings.
However, it struggles in symmetric configurations, where mirrored interactions
often lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and
extended version of IFP that improves robustness and path consistency in dense,
dynamic environments. The method refines threat prioritization using a
time-to-collision heuristic, stabilizes path generation through cost-based
via-point selection, and ensures dynamic feasibility by incorporating model
predictive control (MPC) into the planning process. These enhancements are
tightly integrated into the IFP to preserve its efficiency while improving its
adaptability and stability. Extensive simulations across symmetric and
high-density scenarios show that eIFP-MPC significantly reduces oscillations,
ensures collision-free motion, and improves trajectory efficiency. The results
demonstrate that geometric planners can be strengthened through optimization,
enabling robust performance at scale in complex multi-agent environments.

</details>


### [5] [emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands](https://arxiv.org/abs/2508.08269)
*Sagar Verma*

Main category: cs.RO

TL;DR: 论文提出了首个大规模EMG到肌腱控制数据集，用于肌腱驱动机器人手的控制学习，并提出了基于扩散的回归模型。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动机器人手的控制学习复杂且昂贵，现有视觉跟踪方法易受遮挡和不准确性影响，而sEMG传感器提供了一种廉价且鲁棒的替代方案。然而，将sEMG信号映射到肌腱控制仍具挑战性。

Method: 扩展了emg2pose数据集，创建了包含193名受试者、370小时数据的EMG-to-Tendon Control数据集，并提供了三种基线回归模型及一种新型扩散回归模型。

Result: 数据集和模型框架为肌腱驱动的灵巧机器人操作提供了重要进展，解决了现有方法的局限性。

Conclusion: 该研究为机器人手的可扩展和精确肌腱控制奠定了基础。

Abstract: Tendon-driven robotic hands offer unparalleled dexterity for manipulation
tasks, but learning control policies for such systems presents unique
challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a
direct one-to-one mapping between motion capture (mocap) data and tendon
controls, making the learning process complex and expensive. Additionally,
visual tracking methods for real-world applications are prone to occlusions and
inaccuracies, further complicating joint tracking. Wrist-wearable surface
electromyography (sEMG) sensors present an inexpensive, robust alternative to
capture hand motion. However, mapping sEMG signals to tendon control remains a
significant challenge despite the availability of EMG-to-pose data sets and
regression-based models in the existing literature.
  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic
hands, extending the emg2pose dataset, which includes recordings from 193
subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset
incorporates tendon control signals derived using the MyoSuite MyoHand model,
addressing limitations such as invalid poses in prior methods. We provide three
baseline regression models to demonstrate emg2tendon utility and propose a
novel diffusion-based regression model for predicting tendon control from sEMG
recordings. This dataset and modeling framework marks a significant step
forward for tendon-driven dexterous robotic manipulation, laying the groundwork
for scalable and accurate tendon control in robotic hands.
https://emg2tendon.github.io/

</details>


### [6] [Evaluation of an Autonomous Surface Robot Equipped with a Transformable Mobility Mechanism for Efficient Mobility Control](https://arxiv.org/abs/2508.08303)
*Yasuyuki Fujii,Dinh Tuan Tran,Joo-Ho Lee*

Main category: cs.RO

TL;DR: 该研究开发了一种可变形的移动机制，用于水面机器人，以提高能源效率和机动性。实验表明，旅行模式比驻留模式节省10%的能耗，并减少5%的旅行时间。


<details>
  <summary>Details</summary>
Motivation: 提高水面机器人在长期环境监测中的移动效率和能源效率。

Method: 开发并评估了一种具有两种控制模式（驻留和旅行）的可变形移动机制。

Result: 旅行模式比驻留模式节省10%的能耗，并减少5%的旅行时间。

Conclusion: 可变形的移动机制显著提高了水面机器人的操作效率。

Abstract: Efficient mobility and power consumption are critical for autonomous water
surface robots in long-term water environmental monitoring. This study develops
and evaluates a transformable mobility mechanism for a water surface robot with
two control modes: station-keeping and traveling to improve energy efficiency
and maneuverability. Field experiments show that, in a round-trip task between
two points, the traveling mode reduces power consumption by 10\% and decreases
the total time required for travel by 5\% compared to the station-keeping mode.
These results confirm the effectiveness of the transformable mobility mechanism
for enhancing operational efficiency in patrolling on water surface.

</details>


### [7] [Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators](https://arxiv.org/abs/2508.08328)
*Qiwei Liang,Boyang Cai,Rongyi He,Hui Li,Tao Teng,Haihan Duan,Changxin Huang,Runhao Zeng*

Main category: cs.RO

TL;DR: DQ-Bench是一个新基准，用于评估动态抓取任务，DQ-Net是一个紧凑的师生框架，通过有限感知线索推断抓取配置，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态物体抓取，忽视了动态目标的挑战，限制了在动态场景中的应用。

Method: 提出DQ-Bench基准，并设计DQ-Net框架，包括教师网络（利用特权信息建模静态和动态特性）和学生网络（仅使用目标掩码、深度图和本体状态进行闭环输出）。

Result: DQ-Net在多个任务设置中实现了稳健的动态物体抓取，成功率和响应性显著优于基线方法。

Conclusion: DQ-Net和DQ-Bench为动态抓取任务提供了有效的解决方案，扩展了四足机器人抓取的应用场景。

Abstract: Quadrupedal robots with manipulators offer strong mobility and adaptability
for grasping in unstructured, dynamic environments through coordinated
whole-body control. However, existing research has predominantly focused on
static-object grasping, neglecting the challenges posed by dynamic targets and
thus limiting applicability in dynamic scenarios such as logistics sorting and
human-robot collaboration. To address this, we introduce DQ-Bench, a new
benchmark that systematically evaluates dynamic grasping across varying object
motions, velocities, heights, object types, and terrain complexities, along
with comprehensive evaluation metrics. Building upon this benchmark, we propose
DQ-Net, a compact teacher-student framework designed to infer grasp
configurations from limited perceptual cues. During training, the teacher
network leverages privileged information to holistically model both the static
geometric properties and dynamic motion characteristics of the target, and
integrates a grasp fusion module to deliver robust guidance for motion
planning. Concurrently, we design a lightweight student network that performs
dual-viewpoint temporal modeling using only the target mask, depth map, and
proprioceptive state, enabling closed-loop action outputs without reliance on
privileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net
achieves robust dynamic objects grasping across multiple task settings,
substantially outperforming baseline methods in both success rate and
responsiveness.

</details>


### [8] [A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems](https://arxiv.org/abs/2508.08473)
*Hossein B. Jond*

Main category: cs.RO

TL;DR: 提出了一种新的模型，通过相对位置、速度和局部密度调控代理动态，实现灵活、无碰撞的群体行为，并扩展至认知自主系统。


<details>
  <summary>Details</summary>
Motivation: 现有模型（如Vicsek、Cucker-Smale和Olfati-Saber）在碰撞避免或刚性结构上存在不足，限制了其在群体机器人中的应用。

Method: 使用相对位置、速度和局部密度调控代理动态，引入空间偏移和动力学偏移两个可调参数。

Result: 模型实现了空间灵活且无碰撞的群体行为，并扩展至认知自主系统，支持能量感知的相变。

Conclusion: 该模型为多机器人系统（如自主空中群体）提供了鲁棒的基础。

Abstract: Collective behaviors such as swarming and flocking emerge from simple,
decentralized interactions in biological systems. Existing models, such as
Vicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber
model imposes rigid formations, limiting their applicability in swarm robotics.
To address these limitations, this paper proposes a minimal yet expressive
model that governs agent dynamics using relative positions, velocities, and
local density, modulated by two tunable parameters: the spatial offset and
kinetic offset. The model achieves spatially flexible, collision-free behaviors
that reflect naturalistic group dynamics. Furthermore, we extend the framework
to cognitive autonomous systems, enabling energy-aware phase transitions
between swarming and flocking through adaptive control parameter tuning. This
cognitively inspired approach offers a robust foundation for real-world
applications in multi-robot systems, particularly autonomous aerial swarms.

</details>


### [9] [AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality](https://arxiv.org/abs/2508.08507)
*Shaun Macdonald,Salma ElSayed,Mark McGill*

Main category: cs.RO

TL;DR: AZRA是一个增强现实框架，通过扩展情感交互能力提升仿生机器人与用户的互动，无需物理改造。


<details>
  <summary>Details</summary>
Motivation: 仿生机器人的情感交互通常简单且短暂，限制了其在家用场景中的潜力。

Method: 提出AZRA框架，通过AR技术为仿生机器人增加情感显示和交互方式，并引入情感计算模型。

Result: 展示了AZRA如何增强Petit Qoobo机器人的情感交互能力，并支持快速原型设计。

Conclusion: AZRA为未来仿生机器人开发提供了新的可能性，强调了情感交互的重要性。

Abstract: Zoomorphic robots could serve as accessible and practical alternatives for
users unable or unwilling to keep pets. However, their affective interactions
are often simplistic and short-lived, limiting their potential for domestic
adoption. In order to facilitate more dynamic and nuanced affective
interactions and relationships between users and zoomorphic robots we present
AZRA, a novel augmented reality (AR) framework that extends the affective
capabilities of these robots without physical modifications. To demonstrate
AZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays
(face, light, sound, thought bubbles) and interaction modalities (voice, touch,
proximity, gaze). Additionally, AZRA features a computational model of emotion
to calculate the robot's emotional responses, daily moods, evolving personality
and needs. We highlight how AZRA can be used for rapid participatory
prototyping and enhancing existing robots, then discuss implications on future
zoomorphic robot development.

</details>


### [10] [DeepFleet: Multi-Agent Foundation Models for Mobile Robots](https://arxiv.org/abs/2508.08574)
*Ameya Agaskar,Sriram Siva,William Pickering,Kyle O'Brien,Charles Kekeh,Ang Li,Brianna Gallo Sarker,Alicia Chua,Mayur Nemade,Charun Thattai,Jiaming Di,Isaac Iyengar,Ramya Dharoor,Dino Kirouani,Jimmy Erskine,Tamir Hegazy,Scott Niekum,Usman A. Khan,Federico Pecora,Joseph W. Durham*

Main category: cs.RO

TL;DR: DeepFleet是一套用于大规模移动机器人车队协调与规划的基础模型，包含四种架构，其中机器人中心模型和图-地面模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器人车队的协调与规划问题，利用仓库数据优化模型设计。

Method: 四种模型架构：机器人中心模型（RC）、机器人-地面模型（RF）、图像-地面模型（IF）和图-地面模型（GF），分别采用不同的归纳偏置和设计思路。

Result: RC和GF模型表现最优，能有效利用更大规模的数据集。

Conclusion: DeepFleet展示了多智能体基础模型的设计潜力，RC和GF模型因其异步更新和局部交互结构表现突出。

Abstract: We introduce DeepFleet, a suite of foundation models designed to support
coordination and planning for large-scale mobile robot fleets. These models are
trained on fleet movement data, including robot positions, goals, and
interactions, from hundreds of thousands of robots in Amazon warehouses
worldwide. DeepFleet consists of four architectures that each embody a distinct
inductive bias and collectively explore key points in the design space for
multi-agent foundation models: the robot-centric (RC) model is an
autoregressive decision transformer operating on neighborhoods of individual
robots; the robot-floor (RF) model uses a transformer with cross-attention
between robots and the warehouse floor; the image-floor (IF) model applies
convolutional encoding to a multi-channel image representation of the full
fleet; and the graph-floor (GF) model combines temporal attention with graph
neural networks for spatial relationships. In this paper, we describe these
models and present our evaluation of the impact of these design choices on
prediction task performance. We find that the robot-centric and graph-floor
models, which both use asynchronous robot state updates and incorporate the
localized structure of robot interactions, show the most promise. We also
present experiments that show that these two models can make effective use of
larger warehouses operation datasets as the models are scaled up.

</details>


### [11] [Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles](https://arxiv.org/abs/2508.08576)
*Deniz Karanfil,Daniel Lindmark,Martin Servin,David Torick,Bahram Ravani*

Main category: cs.RO

TL;DR: 本文开发了一个校准的轮式装载机数字孪生体，通过高保真数字模型实现自动化诊断、操作优化和预规划模拟。


<details>
  <summary>Details</summary>
Motivation: 提升建筑操作的自动化能力，通过高保真模拟实现更现实的规划。

Method: 使用物理基础的多体动力学模型（AGX Dynamics软件）构建数字孪生体，并通过传感器校准模型。

Result: 校准后的数字孪生体能高精度估计铲斗受力，提供高保真模拟。

Conclusion: 校准的数字孪生体为建筑操作的自动化提供了有效的工具。

Abstract: This paper presents the development of a calibrated digital twin of a wheel
loader. A calibrated digital twin integrates a construction vehicle with a
high-fidelity digital model allowing for automated diagnostics and optimization
of operations as well as pre-planning simulations enhancing automation
capabilities. The high-fidelity digital model is a virtual twin of the physical
wheel loader. It uses a physics-based multibody dynamic model of the wheel
loader in the software AGX Dynamics. Interactions of the wheel loader's bucket
while in use in construction can be simulated in the virtual model. Calibration
makes this simulation of high-fidelity which can enhance realistic planning for
automation of construction operations. In this work, a wheel loader was
instrumented with several sensors used to calibrate the digital model. The
calibrated digital twin was able to estimate the magnitude of the forces on the
bucket base with high accuracy, providing a high-fidelity simulation.

</details>


### [12] [Autonomous Mobile Plant Watering Robot : A Kinematic Approach](https://arxiv.org/abs/2508.08607)
*Justin London*

Main category: cs.RO

TL;DR: 介绍了一种新型自主移动植物浇水机器人，具备6自由度机械臂和4轮驱动底盘，能识别植物并精准浇水。


<details>
  <summary>Details</summary>
Motivation: 现有农业机器人昂贵且功能有限，需更灵活高效的解决方案。

Method: 使用Jetson Nano和Arduino微控制器，结合YOLOv5和Pl@ntNet-300K数据集进行植物识别，LIDAR避障，无需预设路径。

Result: 提供了DH表、运动学分析及仿真实验结果，验证了机器人功能。

Conclusion: 该机器人能高效自主完成植物浇水任务，具有实用性和推广潜力。

Abstract: Plants need regular and the appropriate amount of watering to thrive and
survive. While agricultural robots exist that can spray water on plants and
crops such as the , they are expensive and have limited mobility and/or
functionality. We introduce a novel autonomous mobile plant watering robot that
uses a 6 degree of freedom (DOF) manipulator, connected to a 4 wheel drive
alloy chassis, to be able to hold a garden hose, recognize and detect plants,
and to water them with the appropriate amount of water by being able to insert
a soil humidity/moisture sensor into the soil. The robot uses Jetson Nano and
Arduino microcontroller and real sense camera to perform computer vision to
detect plants using real-time YOLOv5 with the Pl@ntNet-300K dataset. The robot
uses LIDAR for object and collision avoideance and does not need to move on a
pre-defined path and can keep track of which plants it has watered. We provide
the Denavit-Hartenberg (DH) Table, forward kinematics, differential driving
kinematics, and inverse kinematics along with simulation and experiment results

</details>


### [13] [Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization](https://arxiv.org/abs/2508.08624)
*Chenxuan Liu,He Li,Zongze Li,Shuai Wang,Wei Xu,Kejiang Ye,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.RO

TL;DR: 论文提出了一种基于高斯泼溅（GS）的机器人混合现实（RoboMR）系统GSMR，通过优化内容切换和功率分配，显著降低了通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决机器人混合现实系统中因上传高分辨率图像导致的通信成本高的问题。

Method: 提出GSMR框架和GSCLO优化方法，采用APO算法降低计算复杂度。

Result: 实验表明，GSMR和GSCLO在多种场景下优于现有基准，首次实现超低通信成本的RoboMR。

Conclusion: GSMR和GSCLO方法有效降低了通信成本，动态场景中混合数据有助于提升GS性能。

Abstract: Realizing low-cost communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR
(GSMR), which enables the simulator to opportunistically render a
photo-realistic view from the robot's pose by calling ``memory'' from a GS
model, thus reducing the need for excessive image uploads. However, the GS
model may involve discrepancies compared to the actual environments. To this
end, a GS cross-layer optimization (GSCLO) framework is further proposed, which
jointly optimizes content switching (i.e., deciding whether to upload image or
not) and power allocation (i.e., adjusting to content profiles) across
different frames by minimizing a newly derived GSMR loss function. The GSCLO
problem is addressed by an accelerated penalty optimization (APO) algorithm
that reduces computational complexity by over $10$x compared to traditional
branch-and-bound and search algorithms. Moreover, variants of GSCLO are
presented to achieve robust, low-power, and multi-robot GSMR. Extensive
experiments demonstrate that the proposed GSMR paradigm and GSCLO method
achieve significant improvements over existing benchmarks on both wheeled and
legged robots in terms of diverse metrics in various scenarios. For the first
time, it is found that RoboMR can be achieved with ultra-low communication
costs, and mixture of data is useful for enhancing GS performance in dynamic
scenarios.

</details>


### [14] [ZS-Puffin: Design, Modeling and Implementation of an Unmanned Aerial-Aquatic Vehicle with Amphibious Wings](https://arxiv.org/abs/2508.08690)
*Zhenjiang Wang,Yunhua Jiang,Zikun Zhen,Yifan Jiang,Yubin Tan,Wubin Wang*

Main category: cs.RO

TL;DR: 提出一种基于海雀翅膀启发的两栖翼无人空中-水下飞行器（UAAV），通过单自由度俯仰设计实现空中和水下的高效推进，并引入人工中央模式生成器（CPG）优化运动平滑性。


<details>
  <summary>Details</summary>
Motivation: 解决传统UAAV在不同介质（空气和水）中推进系统面临的挑战，同时减少对海洋生物的干扰，实现环保设计。

Method: 重新设计固定翼结构为两栖翼，具有单自由度俯仰功能，无需额外组件；引入CPG优化水下扑翼运动的平滑性。

Result: 原型机展示了两栖翼在空中产生升力和水下扑翼推进的双重功能，验证了设计的可行性和环保性。

Conclusion: 两栖翼UAAV设计有效解决了介质差异带来的问题，兼具高效和环保特性，为未来应用提供了新思路。

Abstract: Unmanned aerial-aquatic vehicles (UAAVs) can operate both in the air and
underwater, giving them broad application prospects. Inspired by the
dual-function wings of puffins, we propose a UAAV with amphibious wings to
address the challenge posed by medium differences on the vehicle's propulsion
system. The amphibious wing, redesigned based on a fixed-wing structure,
features a single degree of freedom in pitch and requires no additional
components. It can generate lift in the air and function as a flapping wing for
propulsion underwater, reducing disturbance to marine life and making it
environmentally friendly. Additionally, an artificial central pattern generator
(CPG) is introduced to enhance the smoothness of the flapping motion. This
paper presents the prototype, design details, and practical implementation of
this concept.

</details>


### [15] [OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing](https://arxiv.org/abs/2508.08706)
*Zhengxue Cheng,Yiqian Zhang,Wenkang Zhang,Haoyu Li,Keyu Wang,Li Song,Hengdi Zhang*

Main category: cs.RO

TL;DR: OmniVTLA是一种新型视觉-语言-动作（VLA）模型，通过引入触觉感知解决了现有模型在接触密集型任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因触觉传感器异质性和数据获取困难而忽视触觉感知，导致在接触密集型任务中表现不佳。

Method: 提出双路径触觉编码器框架，结合预训练视觉变换器（ViT）和语义对齐触觉ViT（SA-ViT），并引入多模态触觉数据集ObjTac。

Result: 在真实实验中，OmniVTLA在抓取任务中成功率显著提升（夹爪96.9%，灵巧手100%），并减少任务完成时间和生成更平滑轨迹。

Conclusion: OmniVTLA通过触觉感知显著提升了VLA模型的性能，尤其在接触密集型任务中表现优异。

Abstract: Recent vision-language-action (VLA) models build upon vision-language
foundations, and have achieved promising results and exhibit the possibility of
task generalization in robot manipulation. However, due to the heterogeneity of
tactile sensors and the difficulty of acquiring tactile data, current VLA
models significantly overlook the importance of tactile perception and fail in
contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a
novel architecture involving tactile sensing. Specifically, our contributions
are threefold. First, our OmniVTLA features a dual-path tactile encoder
framework. This framework enhances tactile perception across diverse
vision-based and force-based tactile sensors by using a pretrained vision
transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we
introduce ObjTac, a comprehensive force-based tactile dataset capturing
textual, visual, and tactile information for 56 objects across 10 categories.
With 135K tri-modal samples, ObjTac supplements existing visuo-tactile
datasets. Third, leveraging this dataset, we train a semantically-aligned
tactile encoder to learn a unified tactile representation, serving as a better
initialization for OmniVTLA. Real-world experiments demonstrate substantial
improvements over state-of-the-art VLA baselines, achieving 96.9% success rates
with grippers, (21.9% higher over baseline) and 100% success rates with
dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,
OmniVTLA significantly reduces task completion time and generates smoother
trajectories through tactile sensing compared to existing VLA.

</details>


### [16] [Towards Safe Imitation Learning via Potential Field-Guided Flow Matching](https://arxiv.org/abs/2508.08707)
*Haoran Ding,Anqing Duan,Zezhou Sun,Leonel Rozo,Noémie Jaquier,Dezhen Song,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: PF2MP是一种结合势场引导和流匹配的深度生成模型，用于安全运动生成，显著减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在模仿学习中生成复杂策略时，安全性常被忽视，尤其是在复杂障碍环境中。

Method: 提出PF2MP方法，通过成功演示同时学习任务策略和提取障碍信息（势场），在推理时通过势场调节流匹配向量场。

Result: 在仿真和实际环境中验证，PF2MP显著减少碰撞，提升安全性，同时保持任务成功率。

Conclusion: PF2MP为复杂障碍环境中的安全运动生成提供了有效解决方案。

Abstract: Deep generative models, particularly diffusion and flow matching models, have
recently shown remarkable potential in learning complex policies through
imitation learning. However, the safety of generated motions remains
overlooked, particularly in complex environments with inherent obstacles. In
this work, we address this critical gap by proposing Potential Field-Guided
Flow Matching Policy (PF2MP), a novel approach that simultaneously learns task
policies and extracts obstacle-related information, represented as a potential
field, from the same set of successful demonstrations. During inference, PF2MP
modulates the flow matching vector field via the learned potential field,
enabling safe motion generation. By leveraging these complementary fields, our
approach achieves improved safety without compromising task success across
diverse environments, such as navigation tasks and robotic manipulation
scenarios. We evaluate PF2MP in both simulation and real-world settings,
demonstrating its effectiveness in task space and joint space control.
Experimental results demonstrate that PF2MP enhances safety, achieving a
significant reduction of collisions compared to baseline policies. This work
paves the way for safer motion generation in unstructured and obstaclerich
environments.

</details>


### [17] [CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.08709)
*Lukas Krupp,Maximilian Schöffel,Elias Biehl,Norbert Wehn*

Main category: cs.RO

TL;DR: CRADLE是一个基于LLM多智能体系统的对话框架，用于RTL设计空间探索，支持用户引导的流程，并具备自我验证、修正和优化功能。实验显示其在FPGA资源最小化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于僵化，CRADLE旨在提供更灵活、用户友好的设计探索框架。

Method: 采用生成器-批评家智能体系统，结合先进LLM技术，实现FPGA资源最小化。

Result: 在RTLLM基准测试中，CRADLE平均减少LUTs和FFs资源使用48%和40%。

Conclusion: CRADLE通过多智能体协作和LLM技术，显著提升了RTL设计空间探索的效率和效果。

Abstract: This paper presents CRADLE, a conversational framework for design space
exploration of RTL designs using LLM-based multi-agent systems. Unlike existing
rigid approaches, CRADLE enables user-guided flows with internal
self-verification, correction, and optimization. We demonstrate the framework
with a generator-critic agent system targeting FPGA resource minimization using
state-of-the-art LLMs. Experimental results on the RTLLM benchmark show that
CRADLE achieves significant reductions in resource usage with averages of 48%
and 40% in LUTs and FFs across all benchmark designs.

</details>


### [18] [Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos](https://arxiv.org/abs/2508.08743)
*Haoyu Zhang,Long Cheng*

Main category: cs.RO

TL;DR: 论文提出了一种新框架，通过最大化潜在动作与真实动作的互信息，直接从无标签视频演示中学习，提升控制性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于演示的学习（LfD）依赖大量带标签的专家轨迹，限制了训练数据的规模。直接从无标签视频中学习是一种有前景的替代方案，但现有方法提取的潜在动作与真实动作互信息不足，导致控制性能不佳。

Method: 引入了一种新框架，利用变分信息瓶颈技术，显式最大化潜在动作与真实动作的互信息，即使在没有动作标签的情况下。

Result: 理论分析表明该方法确实能最大化潜在动作与真实动作的互信息。实验验证表明，该方法显著提升了互信息和策略性能。

Conclusion: 该方法通过最大化互信息，有效提升了从无标签视频中学习的控制性能，适用于模拟和真实机器人环境。

Abstract: Learning from demonstrations (LfD) typically relies on large amounts of
action-labeled expert trajectories, which fundamentally constrains the scale of
available training data. A promising alternative is to learn directly from
unlabeled video demonstrations. However, we find that existing methods tend to
encode latent actions that share little mutual information with the true robot
actions, leading to suboptimal control performance. To address this limitation,
we introduce a novel framework that explicitly maximizes the mutual information
between latent actions and true actions, even in the absence of action labels.
Our method leverage the variational information-bottleneck to extract
action-relevant representations while discarding task-irrelevant information.
We provide a theoretical analysis showing that our objective indeed maximizes
the mutual information between latent and true actions. Finally, we validate
our approach through extensive experiments: first in simulated robotic
environments and then on real-world robotic platforms, the experimental results
demonstrate that our method significantly enhances mutual information and
consistently improves policy performance.

</details>


### [19] [Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT](https://arxiv.org/abs/2508.08748)
*Muhammad A. Muttaqien,Tomohiro Motoda,Ryo Hanai,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 论文提出了一种基于标注引导视觉提示的感知-动作管道，用于解决便利店中机器人抓取任务因物体密集、遮挡和多样性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 便利店中的机器人抓取任务因物体密集、遮挡和多样性（如颜色、形状、大小和纹理）而复杂化，传统方法难以应对。

Method: 采用标注引导的视觉提示（通过边界框标注识别可抓取物体和放置位置），并结合基于模仿学习的Action Chunking with Transformers (ACT)算法，预测分块动作序列。

Result: 实验表明，系统在抓取成功率和适应性方面表现优异，适用于零售环境。

Conclusion: 该方法通过结构化空间指导和数据驱动的动作预测，提升了机器人抓取任务的准确性和适应性。

Abstract: Robotic pick-and-place tasks in convenience stores pose challenges due to
dense object arrangements, occlusions, and variations in object properties such
as color, shape, size, and texture. These factors complicate trajectory
planning and grasping. This paper introduces a perception-action pipeline
leveraging annotation-guided visual prompting, where bounding box annotations
identify both pickable objects and placement locations, providing structured
spatial guidance. Instead of traditional step-by-step planning, we employ
Action Chunking with Transformers (ACT) as an imitation learning algorithm,
enabling the robotic arm to predict chunked action sequences from human
demonstrations. This facilitates smooth, adaptive, and data-driven
pick-and-place operations. We evaluate our system based on success rate and
visual analysis of grasping behavior, demonstrating improved grasp accuracy and
adaptability in retail environments.

</details>


### [20] [Robot can reduce superior's dominance in group discussions with human social hierarchy](https://arxiv.org/abs/2508.08767)
*Kazuki Komura,Kumi Ozaki,Seiji Yamada*

Main category: cs.RO

TL;DR: 研究探讨机器人能否通过社交层级干预减少上级主导，促进讨论平等参与。实验结果显示机器人行为可能影响发言时间，但未显著差异，且不影响上级满意度。


<details>
  <summary>Details</summary>
Motivation: 解决层级讨论中上级主导问题，促进平等参与。

Method: 30名医生和学生参与实验，机器人根据层级关系鼓励发言，对比无干预和均等干预策略。

Result: 机器人行为可能影响发言时间，但未显著差异，且不影响上级满意度。

Conclusion: 机器人可通过控制反馈行为抑制上级主导，促进讨论平等。

Abstract: This study investigated whether robotic agents that deal with social
hierarchical relationships can reduce the dominance of superiors and equalize
participation among participants in discussions with hierarchical structures.
Thirty doctors and students having hierarchical relationship were gathered as
participants, and an intervention experiment was conducted using a robot that
can encourage participants to speak depending on social hierarchy. These were
compared with strategies that intervened equally for all participants without
considering hierarchy and with a no-action. The robots performed follow
actions, showing backchanneling to speech, and encourage actions, prompting
speech from members with less speaking time, on the basis of the hierarchical
relationships among group members to equalize participation. The experimental
results revealed that the robot's actions could potentially influence the
speaking time among members, but it could not be conclusively stated that there
were significant differences between the robot's action conditions. However,
the results suggested that it might be possible to influence speaking time
without decreasing the satisfaction of superiors. This indicates that in
discussion scenarios where experienced superiors are likely to dominate,
controlling the robot's backchanneling behavior could potentially suppress
dominance and equalize participation among group members.

</details>


### [21] [Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors](https://arxiv.org/abs/2508.08896)
*Haoyu Zhao,Linghao Zhuang,Xingyue Zhao,Cheng Zeng,Haoran Xu,Yuming Jiang,Jun Cen,Kexiang Wang,Jiayan Guo,Siteng Huang,Xin Li,Deli Zhao,Hua Zou*

Main category: cs.RO

TL;DR: AffordDex是一个两阶段训练框架，通过学习运动先验和物体功能理解，实现通用灵巧抓取，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注低级抓取稳定性，忽视了功能感知定位和类人姿态，这对下游操作至关重要。

Method: AffordDex采用两阶段训练：第一阶段预训练轨迹模仿器学习人类手部运动先验；第二阶段通过残差模块和NAA模块调整运动以适应具体物体。

Result: AffordDex实现了通用灵巧抓取，姿态类人且接触位置功能合理，在已知物体、未知实例和新类别上均显著优于基线。

Conclusion: AffordDex通过结合运动先验和功能理解，显著提升了灵巧抓取的通用性和功能性。

Abstract: A dexterous hand capable of generalizable grasping objects is fundamental for
the development of general-purpose embodied AI. However, previous methods focus
narrowly on low-level grasp stability metrics, neglecting affordance-aware
positioning and human-like poses which are crucial for downstream manipulation.
To address these limitations, we propose AffordDex, a novel framework with
two-stage training that learns a universal grasping policy with an inherent
understanding of both motion priors and object affordances. In the first stage,
a trajectory imitator is pre-trained on a large corpus of human hand motions to
instill a strong prior for natural movement. In the second stage, a residual
module is trained to adapt these general human-like motions to specific object
instances. This refinement is critically guided by two components: our Negative
Affordance-aware Segmentation (NAA) module, which identifies functionally
inappropriate contact regions, and a privileged teacher-student distillation
process that ensures the final vision-based policy is highly successful.
Extensive experiments demonstrate that AffordDex not only achieves universal
dexterous grasping but also remains remarkably human-like in posture and
functionally appropriate in contact location. As a result, AffordDex
significantly outperforms state-of-the-art baselines across seen objects,
unseen instances, and even entirely novel categories.

</details>


### [22] [Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion](https://arxiv.org/abs/2508.08982)
*Seungeun Rho,Kartik Garg,Morgan Byrd,Sehoon Ha*

Main category: cs.RO

TL;DR: SDAX是一种新型学习框架，通过无监督技能发现减少人工工程需求，使四足机器人掌握多种敏捷行为。


<details>
  <summary>Details</summary>
Motivation: 探索对机器人学习敏捷运动行为至关重要，但传统方法依赖人工设计，限制了泛化能力。

Method: SDAX利用无监督技能发现和双层优化动态调节探索程度。

Result: SDAX使机器人学会爬行、攀爬、跳跃等复杂行为，并成功迁移到现实硬件。

Conclusion: SDAX显著减少人工干预，提升机器人行为的多样性和适应性。

Abstract: Exploration is crucial for enabling legged robots to learn agile locomotion
behaviors that can overcome diverse obstacles. However, such exploration is
inherently challenging, and we often rely on extensive reward engineering,
expert demonstrations, or curriculum learning - all of which limit
generalizability. In this work, we propose Skill Discovery as Exploration
(SDAX), a novel learning framework that significantly reduces human engineering
effort. SDAX leverages unsupervised skill discovery to autonomously acquire a
diverse repertoire of skills for overcoming obstacles. To dynamically regulate
the level of exploration during training, SDAX employs a bi-level optimization
process that autonomously adjusts the degree of exploration. We demonstrate
that SDAX enables quadrupedal robots to acquire highly agile behaviors
including crawling, climbing, leaping, and executing complex maneuvers such as
jumping off vertical walls. Finally, we deploy the learned policy on real
hardware, validating its successful transfer to the real world.

</details>


### [23] [Rational Inverse Reasoning](https://arxiv.org/abs/2508.08983)
*Ben Zandonati,Tomás Lozano-Pérez,Leslie Pack Kaelbling*

Main category: cs.RO

TL;DR: 论文提出Rational Inverse Reasoning (RIR)框架，通过分层生成模型推断潜在程序，实现机器人从少量演示中泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 人类能从单一不完美演示中泛化到不同问题，而机器人需要大量数据且泛化能力差。作者认为这是因为机器人无法恢复智能行为的潜在解释。

Method: RIR通过分层生成模型推断潜在程序，结合视觉语言模型和规划器进行贝叶斯程序归纳，生成可执行程序。

Result: RIR在连续操作任务中，仅需一次演示即可推断任务结构并泛化到新场景，优于现有视觉语言模型基线。

Conclusion: RIR框架通过推断潜在程序，显著提升了机器人从少量演示中泛化的能力。

Abstract: Humans can observe a single, imperfect demonstration and immediately
generalize to very different problem settings. Robots, in contrast, often
require hundreds of examples and still struggle to generalize beyond the
training conditions. We argue that this limitation arises from the inability to
recover the latent explanations that underpin intelligent behavior, and that
these explanations can take the form of structured programs consisting of
high-level goals, sub-task decomposition, and execution constraints. In this
work, we introduce Rational Inverse Reasoning (RIR), a framework for inferring
these latent programs through a hierarchical generative model of behavior. RIR
frames few-shot imitation as Bayesian program induction: a vision-language
model iteratively proposes structured symbolic task hypotheses, while a
planner-in-the-loop inference scheme scores each by the likelihood of the
observed demonstration under that hypothesis. This loop yields a posterior over
concise, executable programs. We evaluate RIR on a suite of continuous
manipulation tasks designed to test one-shot and few-shot generalization across
variations in object pose, count, geometry, and layout. With as little as one
demonstration, RIR infers the intended task structure and generalizes to novel
settings, outperforming state-of-the-art vision-language model baselines.

</details>


### [24] [Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality](https://arxiv.org/abs/2508.08999)
*Chao Wang,Michael Gienger,Fan Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于混合现实（MR）的框架，通过专家示范自主生成机器人情感表达。


<details>
  <summary>Details</summary>
Motivation: 机器人情感表达对与人类互动至关重要，需多样化且真实。

Method: 利用MR捕捉专家示范，通过流匹配生成模型实时生成多样化行为。

Result: 初步测试验证了方法的有效性。

Conclusion: 该框架能有效生成自主情感表达。

Abstract: Expressive behaviors in robots are critical for effectively conveying their
emotional states during interactions with humans. In this work, we present a
framework that autonomously generates realistic and diverse robotic emotional
expressions based on expert human demonstrations captured in Mixed Reality
(MR). Our system enables experts to teleoperate a virtual robot from a
first-person perspective, capturing their facial expressions, head movements,
and upper-body gestures, and mapping these behaviors onto corresponding robotic
components including eyes, ears, neck, and arms. Leveraging a
flow-matching-based generative process, our model learns to produce coherent
and varied behaviors in real-time in response to moving objects, conditioned
explicitly on given emotional states. A preliminary test validated the
effectiveness of our approach for generating autonomous expressions.

</details>


### [25] [GeoVLA: Empowering 3D Representations in Vision-Language-Action Models](https://arxiv.org/abs/2508.09071)
*Lin Sun,Bin Xie,Yingfei Liu,Hao Shi,Tiancai Wang,Jiale Cao*

Main category: cs.RO

TL;DR: GeoVLA是一个新型的视觉-语言-动作（VLA）框架，通过整合3D几何信息提升机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界的几何信息，限制了空间感知和适应性。

Method: 结合视觉语言模型处理图像和语言指令，同时使用点嵌入网络处理深度图生成3D几何嵌入，最后由3D增强动作专家生成动作序列。

Result: 在LIBERO和ManiSkill2仿真基准测试中表现优异，并在需要高度适应、尺度感知和视角不变性的真实任务中展现出鲁棒性。

Conclusion: GeoVLA通过整合3D信息显著提升了机器人操作的性能和适应性。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising approach for
enabling robots to follow language instructions and predict corresponding
actions.However, current VLA models mainly rely on 2D visual inputs, neglecting
the rich geometric information in the 3D physical world, which limits their
spatial awareness and adaptability. In this paper, we present GeoVLA, a novel
VLA framework that effectively integrates 3D information to advance robotic
manipulation. It uses a vision-language model (VLM) to process images and
language instructions,extracting fused vision-language embeddings. In parallel,
it converts depth maps into point clouds and employs a customized point
encoder, called Point Embedding Network, to generate 3D geometric embeddings
independently. These produced embeddings are then concatenated and processed by
our proposed spatial-aware action expert, called 3D-enhanced Action Expert,
which combines information from different sensor modalities to produce precise
action sequences. Through extensive experiments in both simulation and
real-world environments, GeoVLA demonstrates superior performance and
robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2
simulation benchmarks and shows remarkable robustness in real-world tasks
requiring height adaptability, scale awareness and viewpoint invariance.

</details>
