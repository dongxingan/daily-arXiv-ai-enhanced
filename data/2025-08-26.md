<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 37]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [COSMO-Bench: A Benchmark for Collaborative SLAM Optimization](https://arxiv.org/abs/2508.16731)
*Daniel McGann,Easton R. Potokar,Michael Kaess*

Main category: cs.RO

TL;DR: 这篇论文提出了COSMO-Bench标准测试数据集，用于多机器人协同SLAM算法的评测和对比


<details>
  <summary>Details</summary>
Motivation: 多机器人协同SLAM领域缺乏标准测试数据集，影响了算法研究的可比性和进展

Method: 设计并释放了COSMO-Bench数据集，包含24个数据集，基于现有的C-SLAM前端技术和实际LiDAR数据

Result: 提供了一个开源的多机器人SLAM标准测试数据集，包含24个不同场景的数据

Conclusion: COSMO-Bench数据集将有助于推动多机器人协同SLAM领域的研究进展，提供标准化的评测工具

Abstract: Recent years have seen a focus on research into distributed optimization
algorithms for multi-robot Collaborative Simultaneous Localization and Mapping
(C-SLAM). Research in this domain, however, is made difficult by a lack of
standard benchmark datasets. Such datasets have been used to great effect in
the field of single-robot SLAM, and researchers focused on multi-robot problems
would benefit greatly from dedicated benchmark datasets. To address this gap,
we design and release the Collaborative Open-Source Multi-robot Optimization
Benchmark (COSMO-Bench) -- a suite of 24 datasets derived from a
state-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:
https://doi.org/10.1184/R1/29652158

</details>


### [2] [A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition](https://arxiv.org/abs/2508.16749)
*Victor-Louis De Gusseme,Thomas Lips,Remko Proesmans,Julius Hietala,Giwan Lee,Jiyoung Choi,Jeongil Choi,Geon Kim,Phayuth Yonrith,Domen Tabernik,Andrej Gams,Peter Nimac,Matej Urbas,Jon Muhovič,Danijel Skočaj,Matija Mavsar,Hyojeong Yu,Minseo Kwon,Young J. Kim,Yang Cong,Ronghan Chen,Yu Ren,Supeng Diao,Jiawei Weng,Jiayue Liu,Haoran Sun,Linhan Yang,Zeqing Zhang,Ning Guo,Lei Yang,Fang Wan,Chaoyang Song,Jia Pan,Yixiang Jin,Yong A,Jun Shi,Dingzhe Li,Yong Yang,Kakeru Yamasaki,Takumi Kajiwara,Yuki Nakadera,Krati Saxena,Tomohiro Shibata,Chongkun Xia,Kai Mo,Yanzhao Yu,Qihao Lin,Binqiang Ma,Uihun Sagong,JungHyun Choi,JeongHyun Park,Dongwoo Lee,Yeongmin Kim,Myun Joong Hwang,Yusuke Kuribayashi,Naoki Hiratsuka,Daisuke Tanaka,Solvi Arnold,Kimitoshi Yamazaki,Carlos Mateo-Agullo,Andreas Verleysen,Francis Wyffels*

Main category: cs.RO

TL;DR: 创建了机器人布料操作基准测试和ICRA 2024布料竞赛，发布了包含679次展开演示的公开数据集，分析了抓取成功率与覆盖范围的权衡，发现手工设计方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 机器人布料操作缺乏标准化基准测试和共享数据集来评估和比较不同方法，需要建立统一的评估标准。

Method: 组织ICRA 2024布料竞赛，11个团队参与，使用公开数据集设计展开方法，并扩展数据集至679次演示。

Result: 竞赛结果揭示了抓取成功率与覆盖范围的权衡关系，手工设计方法表现强劲，竞赛表现与先前研究存在显著差异。

Conclusion: 该基准测试、数据集和竞赛结果为未来基准测试奠定了基础，强调了独立实验室外评估的重要性，推动了数据驱动机器人布料操作的进展。

Abstract: Robotic cloth manipulation suffers from a lack of standardized benchmarks and
shared datasets for evaluating and comparing different approaches. To address
this, we created a benchmark and organized the ICRA 2024 Cloth Competition, a
unique head-to-head evaluation focused on grasp pose selection for in-air
robotic cloth unfolding. Eleven diverse teams participated in the competition,
utilizing our publicly released dataset of real-world robotic cloth unfolding
attempts and a variety of methods to design their unfolding approaches.
Afterwards, we also expanded our dataset with 176 competition evaluation
trials, resulting in a dataset of 679 unfolding demonstrations across 34
garments. Analysis of the competition results revealed insights about the
trade-off between grasp success and coverage, the surprisingly strong
achievements of hand-engineered methods and a significant discrepancy between
competition performance and prior work, underscoring the importance of
independent, out-of-the-lab evaluation in robotic cloth manipulation. The
associated dataset is a valuable resource for developing and evaluating grasp
selection methods, particularly for learning-based approaches. We hope that our
benchmark, dataset and competition results can serve as a foundation for future
benchmarks and drive further progress in data-driven robotic cloth
manipulation. The dataset and benchmarking code are available at
https://airo.ugent.be/cloth_competition.

</details>


### [3] [Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach](https://arxiv.org/abs/2508.16807)
*Marco S. Tayar,Lucas K. de Oliveira,Juliano D. Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 比较PPO和SAC两种深度强化学习算法在GPS拒止环境下无人机管道导航的性能，发现PPO表现更稳定可靠


<details>
  <summary>Details</summary>
Motivation: 工业基础设施（如通风管道）的人工检测危险且低效，无人机是替代方案，但GPS拒止环境需要鲁棒的控制策略

Method: 使用Genesis仿真环境中的程序化生成管道环境进行训练，设计奖励函数引导无人机通过航点，对碰撞施加重罚

Result: PPO学会了稳定策略，所有评估测试均无碰撞完成，轨迹平滑；SAC则收敛到次优行为，仅能通过初始段后失败

Conclusion: 在危险密集导航任务中，on-policy方法（PPO）的训练稳定性优于off-policy算法（SAC）的名义样本效率；程序化生成的高保真仿真是开发和基准测试导航策略的有效平台

Abstract: Inspecting confined industrial infrastructure, such as ventilation shafts, is
a hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)
offer a promising alternative, but GPS-denied environments require robust
control policies to prevent collisions. Deep Reinforcement Learning (DRL) has
emerged as a powerful framework for developing such policies, and this paper
provides a comparative study of two leading DRL algorithms for this task: the
on-policy Proximal Policy Optimization (PPO) and the off-policy Soft
Actor-Critic (SAC). The training was conducted with procedurally generated duct
environments in Genesis simulation environment. A reward function was designed
to guide a drone through a series of waypoints while applying a significant
penalty for collisions. PPO learned a stable policy that completed all
evaluation episodes without collision, producing smooth trajectories. By
contrast, SAC consistently converged to a suboptimal behavior that traversed
only the initial segments before failure. These results suggest that, in
hazard-dense navigation, the training stability of on-policy methods can
outweigh the nominal sample efficiency of off-policy algorithms. More broadly,
the study provides evidence that procedurally generated, high-fidelity
simulations are effective testbeds for developing and benchmarking robust
navigation policies.

</details>


### [4] [A Workflow for Map Creation in Autonomous Vehicle Simulations](https://arxiv.org/abs/2508.16856)
*Zubair Islam,Ahmaad Ansari,George Daoud,Mohamed El-Darieby*

Main category: cs.RO

TL;DR: 这篇论文提出了一种自动驾驶车开发的流程化地图创建方法，通过实例生成了安大略省科技大学停车场的3D地图，解决了传统地图制作资源浪费和简化问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车研究需要大量模拟测试，而准确的地图是定位、路径规划和场景测试的基础。但现有地图制作方法通常需要大量计算资源或依赖特定模拟器，影响了开发者的灵活性。

Method: 设计了一种自定义流程来流程化地图创建过程，并通过实例生成了安大略省科技大学停车场的3D地图。

Result: 成功开发出了一种更灵活的地图创建方法，减少了对特定模拟器的依赖和计算资源的需求。

Conclusion: 该方法有效提高了自动驾驶车地图制作的效率和灵活性，未来工作将重点关注集成SLAM技术、优化模拟器兼容性和提高经纬度数据处理的灵活性。

Abstract: The fast development of technology and artificial intelligence has
significantly advanced Autonomous Vehicle (AV) research, emphasizing the need
for extensive simulation testing. Accurate and adaptable maps are critical in
AV development, serving as the foundation for localization, path planning, and
scenario testing. However, creating simulation-ready maps is often difficult
and resource-intensive, especially with simulators like CARLA (CAR Learning to
Act). Many existing workflows require significant computational resources or
rely on specific simulators, limiting flexibility for developers. This paper
presents a custom workflow to streamline map creation for AV development,
demonstrated through the generation of a 3D map of a parking lot at Ontario
Tech University. Future work will focus on incorporating SLAM technologies,
optimizing the workflow for broader simulator compatibility, and exploring more
flexible handling of latitude and longitude values to enhance map generation
accuracy.

</details>


### [5] [Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations](https://arxiv.org/abs/2508.16901)
*David Baxter,Aldo Terán Espinoza,Antonio Terán Espinoza,Amy Loutfi,John Folkesson,Peter Sigray,Stephanie Lowry,Jakob Kuttenkeuler*

Main category: cs.RO

TL;DR: 提出了一种基于李群切空间的广义恒定扭转运动先验，用于水下近距离操作中目标6-DoF运动估计，通过SE(3)中的三元因子耦合平移和旋转，提高轨迹估计精度。


<details>
  <summary>Details</summary>
Motivation: 水下近距离操作中，追踪器缺乏目标侧本体感知，可用相对观测稀疏、噪声大且部分（如USBL位置），无运动先验时因子图最大后验估计欠约束，导致连续目标状态弱连接和方向漂移。

Method: 提出广义恒定扭转运动先验，定义在李群切空间上，确保时间一致的轨迹；设计三元因子并推导闭式雅可比矩阵，支持任意李群轨迹；评估两种部署模式：SE(3)表示和边界因子切换表示模式。

Result: 在真实动态对接场景数据集上验证，通过USBL-only和光学相对测量段实现一致的自我-目标轨迹估计，相比噪声测量提高了相对跟踪精度。

Conclusion: 该方法基于标准李群原语构建，可跨状态流形和感知模态移植，为水下6-DoF运动估计提供了有效的运动先验解决方案。

Abstract: Estimating a target's 6-DoF motion in underwater proximity operations is
difficult because the chaser lacks target-side proprioception and the available
relative observations are sparse, noisy, and often partial (e.g., Ultra-Short
Baseline (USBL) positions). Without a motion prior, factor-graph maximum a
posteriori estimation is underconstrained: consecutive target states are weakly
linked and orientation can drift. We propose a generalized constant-twist
motion prior defined on the tangent space of Lie groups that enforces
temporally consistent trajectories across all degrees of freedom; in SE(3) it
couples translation and rotation in the body frame. We present a ternary factor
and derive its closed-form Jacobians based on standard Lie group operations,
enabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two
deployment modes: (A) an SE(3)-only representation that regularizes orientation
even when only position is measured, and (B) a mode with boundary factors that
switches the target representation between SE(3) and 3D position while applying
the same generalized constant-twist prior across representation changes.
Validation on a real-world dynamic docking scenario dataset shows consistent
ego-target trajectory estimation through USBL-only and optical relative
measurement segments with an improved relative tracking accuracy compared to
the noisy measurements to the target. Because the construction relies on
standard Lie group primitives, it is portable across state manifolds and
sensing modalities.

</details>


### [6] [HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement](https://arxiv.org/abs/2508.16943)
*Haozhuo Zhang,Jingkai Sun,Michele Caprio,Jian Tang,Shanghang Zhang,Qiang Zhang,Wei Pan*

Main category: cs.RO

TL;DR: HumanoidVerse是一个新颖的视觉语言引导人形机器人控制框架，能够在多样化场景中执行长时程、多物体重排任务，仅使用自然语言指令和第一视角RGB观测。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在固定设置下进行单物体交互，无法处理现实世界中复杂的多物体连续操作任务。需要开发能够执行长时程、多物体重排的通用人形机器人控制方法。

Method: 采用多阶段课程学习和双教师蒸馏管道进行训练，构建了包含350个多物体任务的大规模数据集，涵盖四种房间布局。在Isaac Gym模拟器中进行物理仿真。

Result: 在任务成功率和空间精度方面显著优于现有最先进方法，能够很好地泛化到未见过的环境和指令。

Conclusion: 这项工作为实现能够在真实世界感官约束下执行复杂顺序任务的鲁棒通用人形智能体迈出了关键一步。

Abstract: We introduce HumanoidVerse, a novel framework for vision-language guided
humanoid control that enables a single physically simulated robot to perform
long-horizon, multi-object rearrangement tasks across diverse scenes. Unlike
prior methods that operate in fixed settings with single-object interactions,
our approach supports consecutive manipulation of multiple objects, guided only
by natural language instructions and egocentric camera RGB observations.
HumanoidVerse is trained via a multi-stage curriculum using a dual-teacher
distillation pipeline, enabling fluid transitions between sub-tasks without
requiring environment resets. To support this, we construct a large-scale
dataset comprising 350 multi-object tasks spanning four room layouts. Extensive
experiments in the Isaac Gym simulator demonstrate that our method
significantly outperforms prior state-of-the-art in both task success rate and
spatial precision, and generalizes well to unseen environments and
instructions. Our work represents a key step toward robust, general-purpose
humanoid agents capable of executing complex, sequential tasks under real-world
sensory constraints. The video visualization results can be found on the
project page: https://haozhuo-zhang.github.io/HumanoidVerse-project-page/.

</details>


### [7] [A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness](https://arxiv.org/abs/2508.17038)
*Zhouheng Li,Lei Xie,Cheng Hu,Hongye Su*

Main category: cs.RO

TL;DR: 自动馨车停车轨迹规划方法，通过新的碰撞避免框架和终端平滑约束，解决了轨迹规划中的时间效率与精确性、控制可行性的矛盾


<details>
  <summary>Details</summary>
Motivation: 自动馨车停车轨迹规划遇到两大挑战：快速精确的无碰撞轨迹规划，以及在换档点保持轨迹的控制可行性

Method: 提出基于PVD的快速迭代轨迹规划方法(RITP)，采用新型碰撞避免框架平衡时间效率与精确性，通过车辆运动学模型和终端平滑约束提高控制可行性

Result: 模拟结果显示该方法在时间效率和跟踪误差方面都优于模型集成和其他迭代基础方法，并在ROS基础车辆上实际验证了其实际可用性

Conclusion: 该RITP方法有效解决了自动停车轨迹规划中的关键挑战，为实际应用提供了高效、高精度且控制可行的解决方案

Abstract: As autonomous driving continues to advance, automated parking is becoming
increasingly essential. However, significant challenges arise when implementing
path velocity decomposition (PVD) trajectory planning for automated parking.
The primary challenge is ensuring rapid and precise collision-free trajectory
planning, which is often in conflict. The secondary challenge involves
maintaining sufficient control feasibility of the planned trajectory,
particularly at gear shifting points (GSP). This paper proposes a PVD-based
rapid iterative trajectory planning (RITP) method to solve the above
challenges. The proposed method effectively balances the necessity for time
efficiency and precise collision avoidance through a novel collision avoidance
framework. Moreover, it enhances the overall control feasibility of the planned
trajectory by incorporating the vehicle kinematics model and including terminal
smoothing constraints (TSC) at GSP during path planning. Specifically, the
proposed method leverages differential flatness to ensure the planned path
adheres to the vehicle kinematic model. Additionally, it utilizes TSC to
maintain curvature continuity at GSP, thereby enhancing the control feasibility
of the overall trajectory. The simulation results demonstrate superior time
efficiency and tracking errors compared to model-integrated and other
iteration-based trajectory planning methods. In the real-world experiment, the
proposed method was implemented and validated on a ROS-based vehicle,
demonstrating the applicability of the RITP method for real vehicles.

</details>


### [8] [Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model](https://arxiv.org/abs/2508.16947)
*Fan Ding,Xuewen Luo,Hwa Hui Tew,Ruturaj Reddy,Xikun Wang,Junn Yong Loo*

Main category: cs.RO

TL;DR: 提出基于扩散模型的多头轨迹规划器(M-diffusion planner)，通过GRPO策略优化实现多样化驾驶行为，结合LLM进行动态指令感知规划，在nuPlan基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶运动规划模型在监督训练后策略固定，导致驾驶行为僵化，无法反映人类偏好或适应动态指令驱动的需求。

Method: 使用扩散模型构建多头轨迹规划器，早期训练共享权重生成高质量轨迹，后期应用GRPO进行策略特定行为微调，推理时结合LLM指导策略选择。

Result: 闭环仿真显示规划器保持强规划能力，在nuPlan val14基准测试中达到SOTA性能；开环结果显示生成轨迹具有明显多样性，有效满足多模态驾驶行为需求。

Conclusion: 该方法成功实现了在保持规划质量的同时提供多样化、可适应动态指令的自动驾驶行为，为自动驾驶规划系统提供了新的解决方案。

Abstract: Recent advances in motion planning for autonomous driving have led to models
capable of generating high-quality trajectories. However, most existing
planners tend to fix their policy after supervised training, leading to
consistent but rigid driving behaviors. This limits their ability to reflect
human preferences or adapt to dynamic, instruction-driven demands. In this
work, we propose a diffusion-based multi-head trajectory planner(M-diffusion
planner). During the early training stage, all output heads share weights to
learn to generate high-quality trajectories. Leveraging the probabilistic
nature of diffusion models, we then apply Group Relative Policy Optimization
(GRPO) to fine-tune the pre-trained model for diverse policy-specific
behaviors. At inference time, we incorporate a large language model (LLM) to
guide strategy selection, enabling dynamic, instruction-aware planning without
switching models. Closed-loop simulation demonstrates that our post-trained
planner retains strong planning capability while achieving state-of-the-art
(SOTA) performance on the nuPlan val14 benchmark. Open-loop results further
show that the generated trajectories exhibit clear diversity, effectively
satisfying multi-modal driving behavior requirements. The code and related
experiments will be released upon acceptance of the paper.

</details>


### [9] [Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation](https://arxiv.org/abs/2508.17466)
*Dilermando Almeida,Guilherme Lazzarini,Juliano Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 基于深度学习的模型，通过模拟到实际的方法，为四足机器人提供了高精度和适应性的抓取能力，完成了从自主导航到准确抓取的完整任务。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂地形中具有优势，但配备操作臂后的精确抓取仍面临挑战，需要大量实际校准和预编程配置，很难适应动态场景。

Method: 采用模拟到实际的方法，在Genesis模拟环境中生成合成数据集，包含上千次抓取尝试的像素级注释抓取质量地图。训练了一个基于U-Net结构的自定义CNN模型，处理来自RGB和深度摄像头的多模态输入，输出抓取质量热力图来确定最佳抓取点。

Result: 在四足机器人上验证了完整框架，系统成功执行了完整的移动操作任务：自主导航到目标物体、使用传感器感知、使用模型预测最佳抓取姿势并执行精确抓取。

Conclusion: 这项工作证明，利用模拟训练结合先进感知技术可以为物体处理提供可扩展和高效的解决方案。

Abstract: Quadruped robots have emerged as highly efficient and versatile platforms,
excelling in navigating complex and unstructured terrains where traditional
wheeled robots might fail. Equipping these robots with manipulator arms unlocks
the advanced capability of loco-manipulation to perform complex physical
interaction tasks in areas ranging from industrial automation to
search-and-rescue missions. However, achieving precise and adaptable grasping
in such dynamic scenarios remains a significant challenge, often hindered by
the need for extensive real-world calibration and pre-programmed grasp
configurations. This paper introduces a deep learning framework designed to
enhance the grasping capabilities of quadrupeds equipped with arms, focusing on
improved precision and adaptability. Our approach centers on a sim-to-real
methodology that minimizes reliance on physical data collection. We developed a
pipeline within the Genesis simulation environment to generate a synthetic
dataset of grasp attempts on common objects. By simulating thousands of
interactions from various perspectives, we created pixel-wise annotated
grasp-quality maps to serve as the ground truth for our model. This dataset was
used to train a custom CNN with a U-Net-like architecture that processes
multi-modal input from an onboard RGB and depth cameras, including RGB images,
depth maps, segmentation masks, and surface normal maps. The trained model
outputs a grasp-quality heatmap to identify the optimal grasp point. We
validated the complete framework on a four-legged robot. The system
successfully executed a full loco-manipulation task: autonomously navigating to
a target object, perceiving it with its sensors, predicting the optimal grasp
pose using our model, and performing a precise grasp. This work proves that
leveraging simulated training with advanced sensing offers a scalable and
effective solution for object handling.

</details>


### [10] [LLM-based Human-like Traffic Simulation for Self-driving Tests](https://arxiv.org/abs/2508.16962)
*Wendi Li,Hao Wu,Han Gao,Bing Mao,Fengyuan Xu,Sheng Zhong*

Main category: cs.RO

TL;DR: HDSim是一个结合认知理论和大型语言模型的交通生成框架，通过分层驾驶员模型和感知介导行为影响策略，在仿真平台中生成可扩展且真实的交通场景。


<details>
  <summary>Details</summary>
Motivation: 现有仿真平台依赖手工启发式或有限的数据驱动模型，只能捕捉真实驾驶行为的片段，驾驶风格多样性和可解释性有限。需要更真实的交通动态来评估自动驾驶系统的可靠性。

Method: 1. 引入分层驾驶员模型来表示多样化的驾驶风格特征；2. 开发感知介导行为影响策略，使用LLM指导感知来间接塑造驾驶员行为。

Result: 将HDSim嵌入仿真后，自动驾驶系统的安全关键故障检测率提高高达68%，并实现了真实性一致的事故可解释性。

Conclusion: HDSim框架通过结合认知理论和LLM辅助，显著提升了交通仿真的真实性和有效性，为自动驾驶系统评估提供了更可靠的测试环境。

Abstract: Ensuring realistic traffic dynamics is a prerequisite for simulation
platforms to evaluate the reliability of self-driving systems before deployment
in the real world. Because most road users are human drivers, reproducing their
diverse behaviors within simulators is vital. Existing solutions, however,
typically rely on either handcrafted heuristics or narrow data-driven models,
which capture only fragments of real driving behaviors and offer limited
driving style diversity and interpretability. To address this gap, we introduce
HDSim, an HD traffic generation framework that combines cognitive theory with
large language model (LLM) assistance to produce scalable and realistic traffic
scenarios within simulation platforms. The framework advances the state of the
art in two ways: (i) it introduces a hierarchical driver model that represents
diverse driving style traits, and (ii) it develops a Perception-Mediated
Behavior Influence strategy, where LLMs guide perception to indirectly shape
driver actions. Experiments reveal that embedding HDSim into simulation
improves detection of safety-critical failures in self-driving systems by up to
68% and yields realism-consistent accident interpretability.

</details>


### [11] [Modeling and Control Framework for Autonomous Space Manipulator Handover Operations](https://arxiv.org/abs/2508.18039)
*Diego Quevedo,Sarah Hudson,Donghoon Kim*

Main category: cs.RO

TL;DR: 本文开发了双臂空间机械臂系统的动态模型，并比较了不同的跟踪控制律，以支持自主机器人间任务关键对象交接。


<details>
  <summary>Details</summary>
Motivation: 自主空间机器人技术在未来的空间任务中至关重要，特别是在空间服务、装配和制造(ISAM)领域。机器人间(R2R)任务关键对象的交接是此类任务中的关键能力。

Method: 开发了协作机械臂动态模型，并对各种跟踪控制律进行了比较分析。

Result: 提出了双臂空间机械臂系统的动态模型，并比较了不同控制律的性能。

Conclusion: 这项工作为自主ISAM场景中的R2R交接提供了动态模型和控制律分析基础，支持未来空间任务中机器人协作能力的发展。

Abstract: Autonomous space robotics is poised to play a vital role in future space
missions, particularly for In-space Servicing, Assembly, and Manufacturing
(ISAM). A key capability in such missions is the Robot-to-Robot (R2R) handover
of mission-critical objects. This work presents a dynamic model of a dual-arm
space manipulator system and compares various tracking control laws. The key
contributions of this work are the development of a cooperative manipulator
dynamic model and the comparative analysis of control laws to support
autonomous R2R handovers in ISAM scenarios.

</details>


### [12] [DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration](https://arxiv.org/abs/2508.17034)
*Jiayi Li,Yuxin Yao,Qiuhang Lu,Juyong Zhang*

Main category: cs.RO

TL;DR: 提出了一种新的双空间配准范式，结合特征匹配和几何匹配的优势，通过高效过滤机制和几何代理实现快速准确的刚性配准


<details>
  <summary>Details</summary>
Motivation: 刚性配准在SLAM和3D重建中至关重要，但噪声数据、部分重叠和实时处理需求带来挑战。特征匹配能处理大变换但精度有限，几何匹配能精细对齐但依赖好的初始变换

Method: 采用双空间范式：1) 使用轻量级单点RANSAC算法和精炼模块过滤不可靠特征对应；2) 将过滤后的对应作为锚点，提取几何代理，构建目标函数求解变换

Result: 在KITTI数据集上实现高达32倍的CPU时间加速，同时保持可比精度

Conclusion: 提出的双空间方法有效结合了特征匹配和几何匹配的优势，实现了快速准确的刚性配准，适用于实时应用场景

Abstract: Rigid registration, aiming to estimate a rigid transformation to align source
and target data, play a crucial role in applications such as SLAM and 3D
reconstruction. However, noisy, partially overlapping data and the need for
real-time processing pose major challenges for rigid registration. Considering
that feature-based matching can handle large transformation differences but
suffers from limited accuracy, while local geometry-based matching can achieve
fine-grained local alignment but relies heavily on a good initial
transformation, we propose a novel dual-space paradigm to fully leverage the
strengths of both approaches. First, we introduce an efficient filtering
mechanism that incorporates a computationally lightweight single-point RANSAC
algorithm followed by a refinement module to eliminate unreliable feature-based
correspondences. Subsequently, we treat filtered correspondences as anchor
points, extract geometric proxies, and formulates an effective objective
function with a tailored solver to estimate the transformation. Experiments
verify our method's effectiveness, as shown by achieving up to a 32x CPU-time
speedup over MAC on KITTI with comparable accuracy.

</details>


### [13] [LaGarNet: Goal-Conditioned Recurrent State-Space Models for Pick-and-Place Garment Flattening](https://arxiv.org/abs/2508.17070)
*Halid Abdulrahim Kadi,Kasim Terzić*

Main category: cs.RO

TL;DR: 提出了GC-RSSM模型LaGarNet，在衣物抓取放置操作中实现了与基于网格方法相当的性能，是首个在复杂衣物上成功应用状态空间模型的方法


<details>
  <summary>Details</summary>
Motivation: 解决衣物操作任务中状态空间模型应用困难的问题，减少先前方法中的归纳偏差，实现更通用的衣物操作策略

Method: 使用目标条件循环状态空间模型(GC-RSSM)，在覆盖对齐奖励和混合数据集（随机策略+扩散策略收集）上训练

Result: 单一策略LaGarNet在四种不同类型衣物的平整化任务中，在真实世界和仿真环境中均表现出色，匹配了基于网格方法的性能

Conclusion: LaGarNet成功证明了状态空间模型在复杂衣物操作任务中的有效性，为减少归纳偏差和实现通用衣物操作策略提供了新途径

Abstract: We present a novel goal-conditioned recurrent state space (GC-RSSM) model
capable of learning latent dynamics of pick-and-place garment manipulation. Our
proposed method LaGarNet matches the state-of-the-art performance of mesh-based
methods, marking the first successful application of state-space models on
complex garments. LaGarNet trains on a coverage-alignment reward and a dataset
collected through a general procedure supported by a random policy and a
diffusion policy learned from few human demonstrations; it substantially
reduces the inductive biases introduced in the previous similar methods. We
demonstrate that a single-policy LaGarNet achieves flattening on four different
types of garments in both real-world and simulation settings.

</details>


### [14] [OVITA: Open-Vocabulary Interpretable Trajectory Adaptations](https://arxiv.org/abs/2508.17260)
*Anurag Maurya,Tashmoy Ghosh,Anh Nguyen,Ravi Prakash*

Main category: cs.RO

TL;DR: OVITA是一个基于大型语言模型的框架，通过自然语言指令来适应和调整机器人轨迹，支持多种机器人平台和动态环境。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，非专业用户需要通过自然语言直观地调整机器人轨迹以适应动态情况和用户偏好。

Method: 利用多个预训练大型语言模型，通过生成代码作为适应策略来调整轨迹路径点，并使用另一个LLM作为代码解释器实现直观交互。

Result: 在模拟和真实环境中进行了广泛测试，证明了框架在具有时空变化的异构机器人平台上的有效性和重要性。

Conclusion: OVITA提供了一个可解释、开放词汇的语言驱动框架，能够有效适应动态和新颖情况下的机器人轨迹调整。

Abstract: Adapting trajectories to dynamic situations and user preferences is crucial
for robot operation in unstructured environments with non-expert users. Natural
language enables users to express these adjustments in an interactive manner.
We introduce OVITA, an interpretable, open-vocabulary, language-driven
framework designed for adapting robot trajectories in dynamic and novel
situations based on human instructions. OVITA leverages multiple pre-trained
Large Language Models (LLMs) to integrate user commands into trajectories
generated by motion planners or those learned through demonstrations. OVITA
employs code as an adaptation policy generated by an LLM, enabling users to
adjust individual waypoints, thus providing flexible control. Another LLM,
which acts as a code explainer, removes the need for expert users, enabling
intuitive interactions. The efficacy and significance of the proposed OVITA
framework is demonstrated through extensive simulations and real-world
environments with diverse tasks involving spatiotemporal variations on
heterogeneous robotic platforms such as a KUKA IIWA robot manipulator,
Clearpath Jackal ground robot, and CrazyFlie drone.

</details>


### [15] [Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges](https://arxiv.org/abs/2508.17449)
*Zezeng Li,Alexandre Chapin,Enda Xiang,Rui Yang,Bruno Machado,Na Lei,Emmanuel Dellandrea,Di Huang,Liming Chen*

Main category: cs.RO

TL;DR: 这篇综述论文系统回顾了机器人操作领域中基于模仿学习的方法，分析了该领域最具影响力的研究，提供了结构化总结和技术发展时间线。


<details>
  <summary>Details</summary>
Motivation: 机器人操作是自主机器人发展的核心，模仿学习作为一种强大技术，能让机器人通过模仿人类演示来学习复杂操作技能。本文旨在为该领域的研究者和实践者提供全面的资源。

Method: 通过识别和分析基于社区影响力和内在质量筛选出的最具影响力研究，为每篇论文提供结构化总结，包括研究目的、技术实现、层次分类、输入格式、关键先验、优缺点和引用指标等。

Result: 提供了模仿学习技术在机器人操作策略中的时间发展线，报告了可用的基准测试结果，并对现有方法进行了定量评估比较。

Conclusion: 这篇综述综合了机器人操作通过模仿学习领域的最新进展，既展示了当前技术水平，也指出了未来面临的挑战，为该领域的研究提供了重要参考。

Abstract: Robotic Manipulation (RM) is central to the advancement of autonomous robots,
enabling them to interact with and manipulate objects in real-world
environments. This survey focuses on RM methodologies that leverage imitation
learning, a powerful technique that allows robots to learn complex manipulation
skills by mimicking human demonstrations. We identify and analyze the most
influential studies in this domain, selected based on community impact and
intrinsic quality. For each paper, we provide a structured summary, covering
the research purpose, technical implementation, hierarchical classification,
input formats, key priors, strengths and limitations, and citation metrics.
Additionally, we trace the chronological development of imitation learning
techniques within RM policy (RMP), offering a timeline of key technological
advancements. Where available, we report benchmark results and perform
quantitative evaluations to compare existing methods. By synthesizing these
insights, this review provides a comprehensive resource for researchers and
practitioners, highlighting both the state of the art and the challenges that
lie ahead in the field of robotic manipulation through imitation learning.

</details>


### [16] [Evolutionary Brain-Body Co-Optimization Consistently Fails to Select for Morphological Potential](https://arxiv.org/abs/2508.17464)
*Alican Mertan,Nick Cheney*

Main category: cs.RO

TL;DR: 该研究通过详尽映射包含130万种形态的设计空间，揭示了进化脑体协同优化算法面临的挑战，发现算法无法持续找到接近最优解，且经常低估新突变形态的适应度。


<details>
  <summary>Details</summary>
Motivation: 脑体协同优化是一个具有挑战性的问题，研究团队希望通过详尽映射形态-适应度景观来深入理解这一问题的本质和现有算法的局限性。

Method: 在包含1,305,840种不同形态的设计空间中，为每个可行形态训练控制器，并在此空间上详尽映射形态-适应度景观，分析进化脑体协同优化算法的工作原理。

Result: 实验表明现有算法无法持续找到接近最优解，搜索过程经常陷入局部最优，算法无法有效追踪形态-适应度景观中的适应度梯度，且经常低估新突变形态的适应度值。

Conclusion: 这项工作为进化脑体协同优化的挑战提供了最具体的实证，揭示了算法在形态空间中追踪适应度梯度的困难，为未来研究提供了重要见解。

Abstract: Brain-body co-optimization remains a challenging problem, despite increasing
interest from the community in recent years. To understand and overcome the
challenges, we propose exhaustively mapping a morphology-fitness landscape to
study it. To this end, we train controllers for each feasible morphology in a
design space of 1,305,840 distinct morphologies, constrained by a computational
budget. First, we show that this design space constitutes a good model for
studying the brain-body co-optimization problem, and our attempt to
exhaustively map it roughly captures the landscape. We then proceed to analyze
how evolutionary brain-body co-optimization algorithms work in this design
space. The complete knowledge of the morphology-fitness landscape facilitates a
better understanding of the results of evolutionary brain-body co-optimization
algorithms and how they unfold over evolutionary time in the morphology space.
This investigation shows that the experimented algorithms cannot consistently
find near-optimal solutions. The search, at times, gets stuck on morphologies
that are sometimes one mutation away from better morphologies, and the
algorithms cannot efficiently track the fitness gradient in the
morphology-fitness landscape. We provide evidence that experimented algorithms
regularly undervalue the fitness of individuals with newly mutated bodies and,
as a result, eliminate promising morphologies throughout evolution. Our work
provides the most concrete demonstration of the challenges of evolutionary
brain-body co-optimization. Our findings ground the trends in the literature
and provide valuable insights for future work.

</details>


### [17] [Morphological Cognition: Classifying MNIST Digits Through Morphological Computation Alone](https://arxiv.org/abs/2508.17469)
*Alican Mertan,Nick Cheney*

Main category: cs.RO

TL;DR: 该论文展示了如何通过模拟物理身体的固定行为部件实现形态认知，无需神经网络即可完成MNIST数字图像分类任务


<details>
  <summary>Details</summary>
Motivation: 当前深度学习主导AI研究，但自然界存在多种智能机制未被探索。论文关注体现性在智能行为中的作用，研究如何通过简单固定行为的物理部件组合产生认知行为

Method: 使用具有固定行为的模拟体素组合构建机器人，当呈现MNIST数字0时向左移动，数字1时向右移动，实现形态认知

Result: 成功演示了无需神经电路的高层次心智功能（图像分类），这是首次通过纯形态过程实现认知行为的证明

Conclusion: 这项工作作为概念验证，展示了形态认知的可能性，希望促进对不同智能模型的进一步研究

Abstract: With the rise of modern deep learning, neural networks have become an
essential part of virtually every artificial intelligence system, making it
difficult even to imagine different models for intelligent behavior. In
contrast, nature provides us with many different mechanisms for intelligent
behavior, most of which we have yet to replicate. One of such underinvestigated
aspects of intelligence is embodiment and the role it plays in intelligent
behavior. In this work, we focus on how the simple and fixed behavior of
constituent parts of a simulated physical body can result in an emergent
behavior that can be classified as cognitive by an outside observer.
Specifically, we show how simulated voxels with fixed behaviors can be combined
to create a robot such that, when presented with an image of an MNIST digit
zero, it moves towards the left; and when it is presented with an image of an
MNIST digit one, it moves towards the right. Such robots possess what we refer
to as ``morphological cognition'' -- the ability to perform cognitive behavior
as a result of morphological processes. To the best of our knowledge, this is
the first demonstration of a high-level mental faculty such as image
classification performed by a robot without any neural circuitry. We hope that
this work serves as a proof-of-concept and fosters further research into
different models of intelligence.

</details>


### [18] [Variational Shape Inference for Grasp Diffusion on SE(3)](https://arxiv.org/abs/2508.17482)
*S. Talha Bukhari,Kaivalya Agrawal,Zachary Kingston,Aniket Bera*

Main category: cs.RO

TL;DR: 通过变分形状推断与SE(3)流形模型结合，提出了一种多模态捌取合成方法，在形状噪声和点云稀疏情况下显著提升了性能和稳定性


<details>
  <summary>Details</summary>
Motivation: 多模态捌取合成需要生成多种稳定捌取方案，对对象几何特征的稳健学习至关重要。但现有方法在形状噪声和测量稀疏性时表现不佳

Method: 首先训练变分自动编码器进行隐式神经表示的形状推断，然后利用这些学到的几何特征来指导SE(3)流形模型进行捌取合成，并提出了测试时捌取优化技术

Result: 在ACRONYM数据集上超过最先进方法6.3%，对点云密度降低具有更好的稳健性。在实际家庭物品操作中实现零样本迁移，成功捌取率比基线提高34%，尽管存在测量噪声和点云检定错误

Conclusion: 该框架通过结合变分形状推断与SE(3)流形模型，有效提升了多模态捌取合成的稳健性和性能，并具有良好的实际应用演示效果

Abstract: Grasp synthesis is a fundamental task in robotic manipulation which usually
has multiple feasible solutions. Multimodal grasp synthesis seeks to generate
diverse sets of stable grasps conditioned on object geometry, making the robust
learning of geometric features crucial for success. To address this challenge,
we propose a framework for learning multimodal grasp distributions that
leverages variational shape inference to enhance robustness against shape noise
and measurement sparsity. Our approach first trains a variational autoencoder
for shape inference using implicit neural representations, and then uses these
learned geometric features to guide a diffusion model for grasp synthesis on
the SE(3) manifold. Additionally, we introduce a test-time grasp optimization
technique that can be integrated as a plugin to further enhance grasping
performance. Experimental results demonstrate that our shape inference for
grasp synthesis formulation outperforms state-of-the-art multimodal grasp
synthesis methods on the ACRONYM dataset by 6.3%, while demonstrating
robustness to deterioration in point cloud density compared to other
approaches. Furthermore, our trained model achieves zero-shot transfer to
real-world manipulation of household objects, generating 34% more successful
grasps than baselines despite measurement noise and point cloud calibration
errors.

</details>


### [19] [LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations](https://arxiv.org/abs/2508.17547)
*Weikang Wan,Jiawei Fu,Xiaodi Yuan,Yifeng Zhu,Hao Su*

Main category: cs.RO

TL;DR: LodeStar是一个机器人学习框架，利用基础模型自动分解任务演示为语义技能，并通过强化学习生成多样化合成演示数据集，显著提升长时程灵巧操作任务的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发能够稳健执行长时程灵巧操作任务的机器人系统具有挑战性，需要物理灵巧性和技能无缝衔接，而模仿学习需要大量数据集，获取成本高昂。

Method: 使用现成的基础模型自动将任务演示分解为语义上有意义的技能，通过强化学习从少量人类演示生成多样化合成演示数据集，采用Skill Routing Transformer策略将学习到的技能链接执行复杂任务。

Result: 在三个具有挑战性的真实世界长时程灵巧操作任务上的实验评估表明，该方法相比之前的基线方法显著提高了任务性能和鲁棒性。

Conclusion: LodeStar框架通过自动技能分解和合成数据增强，有效解决了长时程灵巧操作任务的学习挑战，为机器人系统提供了更高效的模仿学习方案。

Abstract: Developing robotic systems capable of robustly executing long-horizon
manipulation tasks with human-level dexterity is challenging, as such tasks
require both physical dexterity and seamless sequencing of manipulation skills
while robustly handling environment variations. While imitation learning offers
a promising approach, acquiring comprehensive datasets is resource-intensive.
In this work, we propose a learning framework and system LodeStar that
automatically decomposes task demonstrations into semantically meaningful
skills using off-the-shelf foundation models, and generates diverse synthetic
demonstration datasets from a few human demos through reinforcement learning.
These sim-augmented datasets enable robust skill training, with a Skill Routing
Transformer (SRT) policy effectively chaining the learned skills together to
execute complex long-horizon manipulation tasks. Experimental evaluations on
three challenging real-world long-horizon dexterous manipulation tasks
demonstrate that our approach significantly improves task performance and
robustness compared to previous baselines. Videos are available at
lodestar-robot.github.io.

</details>


### [20] [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](https://arxiv.org/abs/2508.17600)
*Guanxing Lu,Baoxiong Jia,Puhao Li,Yixin Chen,Ziwei Wang,Yansong Tang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出基于高斯原语传播的3D世界模型GWM，通过扩散变换器和3D变分自编码器实现精细场景重建，显著提升机器人操作策略性能


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的世界模型缺乏稳健的几何信息，无法提供一致的空间和物理理解，限制了在机器人操作任务中的效果

Method: 使用高斯原语推断机器人动作下的传播，结合潜在扩散变换器(DiT)和3D变分自编码器，通过高斯溅射实现细粒度场景级未来状态重建

Result: 在仿真和真实世界实验中，GWM能够精确预测不同机器人动作条件下的未来场景，训练出的策略性能显著超越现有最优方法

Conclusion: GWM展示了3D世界模型在数据扩展方面的潜力，既能增强模仿学习的视觉表示，又能作为支持基于模型强化学习的神经模拟器

Abstract: Training robot policies within a learned world model is trending due to the
inefficiency of real-world interactions. The established image-based world
models and policies have shown prior success, but lack robust geometric
information that requires consistent spatial and physical understanding of the
three-dimensional world, even pre-trained on internet-scale video sources. To
this end, we propose a novel branch of world model named Gaussian World Model
(GWM) for robotic manipulation, which reconstructs the future state by
inferring the propagation of Gaussian primitives under the effect of robot
actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D
variational autoencoder, enabling fine-grained scene-level future state
reconstruction with Gaussian Splatting. GWM can not only enhance the visual
representation for imitation learning agent by self-supervised future
prediction training, but can serve as a neural simulator that supports
model-based reinforcement learning. Both simulated and real-world experiments
depict that GWM can precisely predict future scenes conditioned on diverse
robot actions, and can be further utilized to train policies that outperform
the state-of-the-art by impressive margins, showcasing the initial data scaling
potential of 3D world model.

</details>


### [21] [SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation](https://arxiv.org/abs/2508.17643)
*Krishna Vinod,Prithvi Jai Ramesh,Pavan Kumar B N,Bharatesh Chakravarthi*

Main category: cs.RO

TL;DR: 开发了一个开源ROS包，用于从Gazebo模拟器的RGB相机生成事件流，并研究了基于事件的机器人策略在导航和操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级延迟、高动态范围和低功耗等优势，但在主流机器人模拟器中缺乏合成事件视觉的支持，这阻碍了事件驱动方法在机器人操作和导航任务中的评估。

Method: 提出了一个开源的、用户友好的v2e ROS包，能够从RGB相机源无缝生成事件流。使用基于Transformer的事件机器人策略，通过行为克隆进行训练，并在移动机器人目标跟随和机械臂目标检测抓取两个场景中与RGB基准进行比较。

Result: 实验结果表明，事件引导的策略在各种操作条件下始终提供竞争优势，突显了事件驱动感知在改善实时机器人导航和操作方面的潜力。

Conclusion: 这项工作为事件相机更广泛地集成到机器人策略学习奠定了基础，展示了事件驱动感知在机器人应用中的实际价值。

Abstract: Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/

</details>


### [22] [MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding](https://arxiv.org/abs/2508.17684)
*Kento Kawaharazuka,Shogo Sawaguchi,Ayumu Iwata,Keita Yoneda,Temma Suzuki,Kei Okada*

Main category: cs.RO

TL;DR: 开发了MEVITA开源双足机器人，使用电商平台可获取的零件和钣金焊接技术，简化组装过程，并通过强化学习实现了稳健的行走能力


<details>
  <summary>Details</summary>
Motivation: 现有开源双足机器人大多依赖3D打印，尺寸扩展性有限且结构脆弱；金属基机器人零件数量多、组装困难且不易获取。需要开发易于组装、零件易得的开源双足机器人平台

Method: 采用钣金焊接技术将复杂几何结构集成到单个零件中，显著减少组件数量；通过仿真中的强化学习和Sim-to-Real迁移技术实现行走控制

Result: 成功开发出MEVITA机器人，实现了在各种环境中的稳健行走行为，验证了方法的有效性

Conclusion: MEVITA提供了一个可行的开源双足机器人解决方案，硬件、软件和训练环境全部开源，促进了双足机器人技术的普及和发展

Abstract: Various bipedal robots have been developed to date, and in recent years,
there has been a growing trend toward releasing these robots as open-source
platforms. This shift is fostering an environment in which anyone can freely
develop bipedal robots and share their knowledge, rather than relying solely on
commercial products. However, most existing open-source bipedal robots are
designed to be fabricated using 3D printers, which limits their scalability in
size and often results in fragile structures. On the other hand, some
metal-based bipedal robots have been developed, but they typically involve a
large number of components, making assembly difficult, and in some cases, the
parts themselves are not readily available through e-commerce platforms. To
address these issues, we developed MEVITA, an open-source bipedal robot that
can be built entirely from components available via e-commerce. Aiming for the
minimal viable configuration for a bipedal robot, we utilized sheet metal
welding to integrate complex geometries into single parts, thereby
significantly reducing the number of components and enabling easy assembly for
anyone. Through reinforcement learning in simulation and Sim-to-Real transfer,
we demonstrated robust walking behaviors across various environments,
confirming the effectiveness of our approach. All hardware, software, and
training environments can be obtained from https://github.com/haraduka/mevita .

</details>


### [23] [Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications](https://arxiv.org/abs/2508.17753)
*Theresa Pekarek Rosin,Julia Gachot,Henri-Leon Kordt,Matthias Kerzel,Stefan Wermter*

Main category: cs.RO

TL;DR: 这篇论文评估了4种先进语音识别系统在公开数据集上的性能，发现它们在实际环境中存在显著性能差异、幻觉倾向和偏见问题，这些问题对人机交互的任务执行、用户信任和安全造成严重影响。


<details>
  <summary>Details</summary>
Motivation: 实际环境中的语音识别系统需要处理不完美音频和多样化用户群体，而在人机交互环境中，这些挑战更为严峻，因此需要系统性评估现有ASR系统的实际性能。

Method: 使用8个公开数据集来测试4种先进语音识别系统，这些数据集涵盖了六个难度维度：领域特定、口音、噪音、年龄变化、障碍性和自发性语音。

Result: 分析显示了这些ASR系统在性能、幻觉倾向和内在偏见方面存在显著差异，尽管在标准测试集上获得相似的分数。

Conclusion: 这些限制对人机交互产生了严重影响，因为识别错误可能干扰任务执行、损害用户信任和影响安全。研究结果强调了在实际应用场景中更全面评估ASR系统性能的重要性。

Abstract: Automatic Speech Recognition (ASR) systems in real-world settings need to
handle imperfect audio, often degraded by hardware limitations or environmental
noise, while accommodating diverse user groups. In human-robot interaction
(HRI), these challenges intersect to create a uniquely challenging recognition
environment. We evaluate four state-of-the-art ASR systems on eight publicly
available datasets that capture six dimensions of difficulty: domain-specific,
accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis
demonstrates significant variations in performance, hallucination tendencies,
and inherent biases, despite similar scores on standard benchmarks. These
limitations have serious implications for HRI, where recognition errors can
interfere with task performance, user trust, and safety.

</details>


### [24] [Adaptive Output Steps: FlexiSteps Network for Dynamic Trajectory Prediction](https://arxiv.org/abs/2508.17797)
*Yunxiang Liu,Hongkuo Niu,Jianlin Zhu*

Main category: cs.RO

TL;DR: FlexiSteps Network (FSN) 是一个动态调整预测时间步长的轨迹预测框架，通过自适应预测模块和动态解码器实现灵活输出，在保持精度的同时提高适应性。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹预测模型使用固定长度输出，难以适应动态现实场景的需求，需要开发能够根据上下文条件动态调整预测步长的解决方案。

Method: 提出FSN框架，包含预训练的自适应预测模块(APM)评估和动态调整输出步长，设计动态解码器(DD)确保即插即用，并引入结合Fréchet距离和预测步长长度的评分机制。

Result: 在Argoverse和INTERACTION等基准数据集上的大量实验证明了FSN框架的有效性和灵活性。

Conclusion: FSN框架通过动态调整预测时间步长，在轨迹预测任务中实现了更好的适应性和预测精度平衡，为自动驾驶和智能决策系统提供了更实用的解决方案。

Abstract: Accurate trajectory prediction is vital for autonomous driving, robotics, and
intelligent decision-making systems, yet traditional models typically rely on
fixed-length output predictions, limiting their adaptability to dynamic
real-world scenarios. In this paper, we introduce the FlexiSteps Network (FSN),
a novel framework that dynamically adjusts prediction output time steps based
on varying contextual conditions. Inspired by recent advancements addressing
observation length discrepancies and dynamic feature extraction, FSN
incorporates an pre-trained Adaptive Prediction Module (APM) to evaluate and
adjust the output steps dynamically, ensuring optimal prediction accuracy and
efficiency. To guarantee the plug-and-play of our FSN, we also design a Dynamic
Decoder(DD). Additionally, to balance the prediction time steps and prediction
accuracy, we design a scoring mechanism, which not only introduces the
Fr\'echet distance to evaluate the geometric similarity between the predicted
trajectories and the ground truth trajectories but the length of predicted
steps is also considered. Extensive experiments conducted on benchmark datasets
including Argoverse and INTERACTION demonstrate the effectiveness and
flexibility of our proposed FSN framework.

</details>


### [25] [Effect of Performance Feedback Timing on Motor Learning for a Surgical Training Task](https://arxiv.org/abs/2508.17830)
*Mary Kate Gale,Kailana Baker-Matsuoka,Ilana Nisky,Allison Okamura*

Main category: cs.RO

TL;DR: 实时多感官（触觉和视觉）错误反馈相比任务回放反馈或无反馈，能更好地提高机器人辅助微创手术虚拟训练中的学习效果，特别是在环状物定向和弯曲路径段的位置准确性方面。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术已成为多种外科手术的金标准，但最佳的训练方法尚不清楚。研究者假设实时错误反馈比任务后反馈能更好地提高学习速度和减少错误。

Method: 42名手术新手学习虚拟环线任务，分为三组：(1)实时错误反馈组（触觉和视觉提示）、(2)任务回放错误反馈组、(3)无错误反馈组，比较不同反馈时机对学习效果的影响。

Result: 实时反馈组在环状物定向方面表现最佳；回放反馈组在直线路径段的定向表现优于无反馈组；实时反馈组在弯曲路径段的位置准确性方面表现最优；各组在整体位置准确性方面无显著差异。

Conclusion: 实时多感官错误反馈相比回放反馈或无反馈能改善虚拟手术任务的学习效果，这种新型训练方法可能使外科培训生以更快速度和更高准确性发展技能。

Abstract: Objective: Robot-assisted minimally invasive surgery (RMIS) has become the
gold standard for a variety of surgical procedures, but the optimal method of
training surgeons for RMIS is unknown. We hypothesized that real-time, rather
than post-task, error feedback would better increase learning speed and reduce
errors. Methods: Forty-two surgical novices learned a virtual version of the
ring-on-wire task, a canonical task in RMIS training. We investigated the
impact of feedback timing with multi-sensory (haptic and visual) cues in three
groups: (1) real-time error feedback, (2) trial replay with error feedback, and
(3) no error feedback. Results: Participant performance was evaluated based on
the accuracy of ring position and orientation during the task. Participants who
received real-time feedback outperformed other groups in ring orientation.
Additionally, participants who received feedback in replay outperformed
participants who did not receive any error feedback on ring orientation during
long, straight path sections. There were no significant differences between
groups for ring position overall, but participants who received real-time
feedback outperformed the other groups in positional accuracy on tightly curved
path sections. Conclusion: The addition of real-time haptic and visual error
feedback improves learning outcomes in a virtual surgical task over error
feedback in replay or no error feedback at all. Significance: This work
demonstrates that multi-sensory error feedback delivered in real time leads to
better training outcomes as compared to the same feedback delivered after task
completion. This novel method of training may enable surgical trainees to
develop skills with greater speed and accuracy.

</details>


### [26] [CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes](https://arxiv.org/abs/2508.17831)
*Yuan Fang,Fangzhan Shi,Xijia Wei,Qingchao Chen,Kevin Chetty,Simon Julier*

Main category: cs.RO

TL;DR: CubeDN是一个专门用于无人机3D检测的单阶段端到端雷达物体检测网络，通过双雷达配置和深度学习管道解决毫米波雷达在高度测量方面的限制，实现了分米级跟踪精度和实时处理能力。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的广泛使用，需要确保安全和安保。传统光学传感器在恶劣光照和环境条件下性能下降，而现有毫米波雷达系统主要专注于2D道路用户检测，缺乏对无人机3D检测所需的高度测量能力。

Method: 提出CubeDN单阶段端到端雷达物体检测网络，采用双雷达配置来克服高度分辨率差的问题，使用新颖的深度学习管道同时实现检测、定位和分类功能。

Result: 系统能够同时检测和分类两种尺寸的无人机，在近距离实现分米级跟踪精度，总体平均精度(AP)达到95%，平均召回率(AR)达到85%，数据处理和推理速度为10Hz。

Conclusion: CubeDN通过创新的双雷达配置和深度学习架构，成功解决了毫米波雷达在无人机3D检测中的高度测量挑战，提供了高精度、实时的无人机检测解决方案，非常适合实际应用。

Abstract: As drone use has become more widespread, there is a critical need to ensure
safety and security. A key element of this is robust and accurate drone
detection and localization. While cameras and other optical sensors like LiDAR
are commonly used for object detection, their performance degrades under
adverse lighting and environmental conditions. Therefore, this has generated
interest in finding more reliable alternatives, such as millimeter-wave
(mmWave) radar. Recent research on mmWave radar object detection has
predominantly focused on 2D detection of road users. Although these systems
demonstrate excellent performance for 2D problems, they lack the sensing
capability to measure elevation, which is essential for 3D drone detection. To
address this gap, we propose CubeDN, a single-stage end-to-end radar object
detection network specifically designed for flying drones. CubeDN overcomes
challenges such as poor elevation resolution by utilizing a dual radar
configuration and a novel deep learning pipeline. It simultaneously detects,
localizes, and classifies drones of two sizes, achieving decimeter-level
tracking accuracy at closer ranges with overall $95\%$ average precision (AP)
and $85\%$ average recall (AR). Furthermore, CubeDN completes data processing
and inference at 10Hz, making it highly suitable for practical applications.

</details>


### [27] [Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model](https://arxiv.org/abs/2508.17922)
*Bokai Ji,Jie Gu,Xiaokang Ma,Chu Tang,Jingmin Chen,Guangxia Li*

Main category: cs.RO

TL;DR: 本文提出了任务/指令依赖的affordance概念，构建了包含1.5万个物体-指令-affordance三元组的数据集，并开发了基于大型多模态模型的迭代推理预测方法。


<details>
  <summary>Details</summary>
Motivation: 传统affordance研究往往忽略了任务/指令依赖性，即同一物体在不同指令下会产生不同的操作区域和方向，这一重要特性需要被重新审视和研究。

Method: 构建了从自我中心视角的物体-指令-affordance三元组数据集；提出了"search against verifiers"管道，让大型多模态模型通过迭代推理过程逐步预测affordance，每一步输出都由模型自身验证。

Result: 实验表明该方法不仅解锁了新的指令导向affordance预测能力，而且在广泛范围内取得了出色的性能表现。

Conclusion: 任务/指令依赖的affordance是一个重要研究方向，通过大型多模态模型的迭代推理方法能够有效预测不同指令下的物体操作affordance。

Abstract: Affordance is crucial for intelligent robots in the context of object
manipulation. In this paper, we argue that affordance should be
task-/instruction-dependent, which is overlooked by many previous works. That
is, different instructions can lead to different manipulation regions and
directions even for the same object. According to this observation, we present
a new dataset comprising fifteen thousand object-instruction-affordance
triplets. All scenes in the dataset are from an egocentric viewpoint, designed
to approximate the perspective of a human-like robot. Furthermore, we
investigate how to enable large multimodal models (LMMs) to serve as affordance
predictors by implementing a ``search against verifiers'' pipeline. An LMM is
asked to progressively predict affordances, with the output at each step being
verified by itself during the iterative process, imitating a reasoning process.
Experiments show that our method not only unlocks new instruction-oriented
affordance prediction capabilities, but also achieves outstanding performance
broadly.

</details>


### [28] [A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm](https://arxiv.org/abs/2508.17969)
*Alexandros Gkillas,Christos Anagnostopoulos,Nikos Piperigkos,Dimitris Tsiktsiris,Theofilos Christodoulou,Theofanis Siamatras,Dimitrios Triantafyllou,Christos Basdekis,Theoktisti Marinopoulou,Panagiotis Lepentsiotis,Elefterios Blitsis,Aggeliki Zacharaki,Nearchos Stylianidis,Leonidas Katelaris,Lamberto Salvan,Aris S. Lalos,Christos Laoudias,Antonios Lalas,Konstantinos Votis*

Main category: cs.RO

TL;DR: 本文提出了一个用于自动驾驶车辆内外监控的整体感知系统，包含基于多摄像头和LLM的内部监控以及基于LiDAR的外部语义分割，在真实电动车上验证了性能提升


<details>
  <summary>Details</summary>
Motivation: 开发一个AI驱动的自适应框架来优化自动驾驶车辆的感知和体验，实现内外环境的全面监控

Method: 内部监控：多摄像头面部识别驾驶员行为+LLM虚拟助手+AI传感器空气质量监测；外部监控：基于LiDAR的成本效益语义分割方法，对低质量3D点云进行超分辨率处理

Result: 在欧盟AutoTRUST项目框架下集成到真实电动车上，在意大利Ispra联合研究中心实验验证，显示各模块性能和效率显著提升

Conclusion: 提出的整体感知架构通过内外监控系统的协同工作，为自动驾驶车辆提供了高效可靠的感知解决方案，在实际部署中表现出色

Abstract: This paper introduces a holistic perception system for internal and external
monitoring of autonomous vehicles, with the aim of demonstrating a novel
AI-leveraged self-adaptive framework of advanced vehicle technologies and
solutions that optimize perception and experience on-board. Internal monitoring
system relies on a multi-camera setup designed for predicting and identifying
driver and occupant behavior through facial recognition, exploiting in addition
a large language model as virtual assistant. Moreover, the in-cabin monitoring
system includes AI-empowered smart sensors that measure air-quality and perform
thermal comfort analysis for efficient on and off-boarding. On the other hand,
external monitoring system perceives the surrounding environment of vehicle,
through a LiDAR-based cost-efficient semantic segmentation approach, that
performs highly accurate and efficient super-resolution on low-quality raw 3D
point clouds. The holistic perception framework is developed in the context of
EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on
a real electric vehicle provided by ALKE. Experimental validation and
evaluation at the integration site of Joint Research Centre at Ispra, Italy,
highlights increased performance and efficiency of the modular blocks of the
proposed perception architecture.

</details>


### [29] [Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE](https://arxiv.org/abs/2508.17985)
*Abu Shad Ahammed,Md Shahi Amran Hossain,Sayeri Mukherjee,Roman Obermaisser,Md. Ziaur Rahman*

Main category: cs.RO

TL;DR: 该论文提出了一个结合上下文感知计算机视觉模型和ADORE自适应控制框架的自动驾驶系统，在CARLA模拟器中验证了其在恶劣天气条件下的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在天气变化和未知物体等不确定条件下的安全问题，传统计算机视觉模型在这些场景下性能会下降。

Method: 使用CARLA模拟器与ADORE框架通过ROS桥接集成，结合上下文感知CV模型和自适应控制，在清晰和恶劣天气条件下进行测试。

Result: 感知模型在恶劣天气条件下仍保持鲁棒检测性能，ADORE成功适应车速限制和障碍物，响应延迟低。

Conclusion: 深度学习感知与基于规则的自适应决策相结合，能够显著提升汽车安全关键系统的性能。

Abstract: Ensuring safety in autonomous driving requires a seamless integration of
perception and decision making under uncertain conditions. Although computer
vision (CV) models such as YOLO achieve high accuracy in detecting traffic
signs and obstacles, their performance degrades in drift scenarios caused by
weather variations or unseen objects. This work presents a simulated autonomous
driving system that combines a context aware CV model with adaptive control
using the ADORE framework. The CARLA simulator was integrated with ADORE via
the ROS bridge, allowing real-time communication between perception, decision,
and control modules. A simulated test case was designed in both clear and drift
weather conditions to demonstrate the robust detection performance of the
perception model while ADORE successfully adapted vehicle behavior to speed
limits and obstacles with low response latency. The findings highlight the
potential of coupling deep learning-based perception with rule-based adaptive
decision making to improve automotive safety critical system.

</details>


### [30] [No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin](https://arxiv.org/abs/2508.17986)
*Karel Bartunek,Lukas Rustler,Matej Hoffmann*

Main category: cs.RO

TL;DR: 本文提出了一种在完全无视觉输入情况下，利用机器人全身触觉皮肤进行物体定位和抓取的方法，通过粗粒度工作空间探索和精确定位两阶段实现，相比仅使用末端执行器触觉反馈的方法快6倍。


<details>
  <summary>Details</summary>
Motivation: 传统机器人物体定位和抓取主要依赖视觉传感器，触觉反馈仅作为辅助。本研究探索在完全无视觉输入的情况下，仅依靠触觉反馈来搜索和抓取物体，适用于视觉感知受限的环境（如光照不佳、灰尘、烟雾、遮挡等场景）。

Method: 将搜索分为两个阶段：(1) 使用覆盖敏感皮肤的完整机器人表面进行粗粒度工作空间探索；(2) 使用配备力/力矩传感器的末端执行器进行精确定位。该方法不限于特定设置，可部署在任何具有全身接触感知能力的平台上。

Result: 在仿真和真实机器人上系统评估表明，该方法能够定位、抓取和放置多种物体。真实机器人单物体抓取总体成功率为85.7%，失败主要发生在抓取特定物体时。使用全身接触的方法比仅使用末端执行器触觉反馈的基线方法快6倍。

Conclusion: 这种基于全身触觉感知的方法在视觉感知受限的环境中具有广泛应用前景，如农业中需要在树叶内部定位和采摘水果蔬菜等场景。

Abstract: Locating and grasping of objects by robots is typically performed using
visual sensors. Haptic feedback from contacts with the environment is only
secondary if present at all. In this work, we explored an extreme case of
searching for and grasping objects in complete absence of visual input, relying
on haptic feedback only. The main novelty lies in the use of contacts over the
complete surface of a robot manipulator covered with sensitive skin. The search
is divided into two phases: (1) coarse workspace exploration with the complete
robot surface, followed by (2) precise localization using the end-effector
equipped with a force/torque sensor. We systematically evaluated this method in
simulation and on the real robot, demonstrating that diverse objects can be
located, grasped, and put in a basket. The overall success rate on the real
robot for one object was 85.7\% with failures mainly while grasping specific
objects. The method using whole-body contacts is six times faster compared to a
baseline that uses haptic feedback only on the end-effector. We also show
locating and grasping multiple objects on the table. This method is not
restricted to our specific setup and can be deployed on any platform with the
ability of sensing contacts over the entire body surface. This work holds
promise for diverse applications in areas with challenging visual perception
(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or
vegetables need to be located inside foliage and picked.

</details>


### [31] [Arnold: a generalist muscle transformer policy](https://arxiv.org/abs/2508.18066)
*Alberto Silvio Chiappa,Boshi An,Merkourios Simos,Chengkun Li,Alexander Mathis*

Main category: cs.RO

TL;DR: 本文提出了Arnold，一个通用策略模型，能够掌握多种任务和身体构型，在14个挑战性控制任务中达到专家或超专家水平。


<details>
  <summary>Details</summary>
Motivation: 解决高维非线性人体肌肉骨骼模型的控制难题，现有方法多为单一技能专家，需要开发能够处理多任务和多身体构型的通用策略。

Method: 结合行为克隆和PPO微调，使用创新的感觉运动词汇表（sensorimotor vocabulary）和transformer架构处理可变观测和动作空间。

Result: 在14个挑战性任务中达到专家或超专家性能，支持高效多任务多身体构型学习，并能快速适应新任务。

Conclusion: Arnold框架为生物运动控制提供了新见解，证实了肌肉协同作用在任务间有限的可迁移性，推动了通用运动控制策略的发展。

Abstract: Controlling high-dimensional and nonlinear musculoskeletal models of the
human body is a foundational scientific challenge. Recent machine learning
breakthroughs have heralded policies that master individual skills like
reaching, object manipulation and locomotion in musculoskeletal systems with
many degrees of freedom. However, these agents are merely "specialists",
achieving high performance for a single skill. In this work, we develop Arnold,
a generalist policy that masters multiple tasks and embodiments. Arnold
combines behavior cloning and fine-tuning with PPO to achieve expert or
super-expert performance in 14 challenging control tasks from dexterous object
manipulation to locomotion. A key innovation is Arnold's sensorimotor
vocabulary, a compositional representation of the semantics of heterogeneous
sensory modalities, objectives, and actuators. Arnold leverages this vocabulary
via a transformer architecture to deal with the variable observation and action
spaces of each task. This framework supports efficient multi-task,
multi-embodiment learning and facilitates rapid adaptation to novel tasks.
Finally, we analyze Arnold to provide insights into biological motor control,
corroborating recent findings on the limited transferability of muscle
synergies across tasks.

</details>


### [32] [The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation](https://arxiv.org/abs/2508.18074)
*Zhaokun Chen,Wenshuo Wang,Wenzhuo Liu,Yichen Liu,Junqiang Xi*

Main category: cs.RO

TL;DR: 本研究通过脑电和行为实验首次系统揭示了移动机器人遥操作中通信延迟对人类操作表现和神经认知的影响，发现了200-300ms的性能下降阈值和100-200ms的早期感知窗口。


<details>
  <summary>Details</summary>
Motivation: 通信延迟严重影响移动机器人遥操作中的人机协作，但此前缺乏对延迟如何影响人类操作表现和神经认知的系统研究，需要填补这一空白。

Method: 采用10名参与者的人机闭环实验，结合脑电图(EEG)和机器人行为数据，在0-500ms延迟范围内以100ms为增量进行系统研究。

Result: 行为分析显示200-300ms延迟导致性能显著下降；EEG分析发现前额θ/β频带和顶叶α频带功率具有显著延迟依赖性；识别出100-200ms的早期感知窗口；400ms以上所有特征趋于饱和。

Conclusion: 研究首次提供了人类在遥操作任务中感知和认知延迟阈值的证据，为延迟补偿策略的设计提供了关键的神经认知学见解。

Abstract: Communication delays in mobile robot teleoperation adversely affect
human-machine collaboration. Understanding delay effects on human operational
performance and neurocognition is essential for resolving this issue. However,
no previous research has explored this. To fill this gap, we conduct a
human-in-the-loop experiment involving 10 participants, integrating
electroencephalography (EEG) and robot behavior data under varying delays
(0-500 ms in 100 ms increments) to systematically investigate these effects.
Behavior analysis reveals significant performance degradation at 200-300 ms
delays, affecting both task efficiency and accuracy. EEG analysis discovers
features with significant delay dependence: frontal $\theta/\beta$-band and
parietal $\alpha$-band power. We also identify a threshold window (100-200 ms)
for early perception of delay in humans, during which these EEG features first
exhibit significant differences. When delay exceeds 400 ms, all features
plateau, indicating saturation of cognitive resource allocation at
physiological limits. These findings provide the first evidence of perceptual
and cognitive delay thresholds during teleoperation tasks in humans, offering
critical neurocognitive insights for the design of delay compensation
strategies.

</details>


### [33] [Analysis of Harpy's Constrained Trotting and Jumping Maneuver](https://arxiv.org/abs/2508.18139)
*Prathima Ananda Kumar*

Main category: cs.RO

TL;DR: 对Harpy混合腿-推进器双足机器人的实验数据分析，揭示了腿提供主要推进力而推进器实现空中相位控制的协同机制，实现了稳定的有界轨迹和精确足部放置


<details>
  <summary>Details</summary>
Motivation: 研究混合腿-推进器 locomotion 的基本原理，了解腿和推进器在双足机器人运动中的协同作用机制

Method: 分析Harpy机器人在小跑和跳跃实验中的数据集，通过多运动模式的数据分析来理解混合驱动原理

Result: 实现了有界轨迹稳定运动、精确足部放置、低扭矩关节控制、对称跟踪，推进器提供额外空中相位控制，腿提供主要推进力

Conclusion: 混合驱动方法具有鲁棒性，需要针对空中相位的特定控制策略来管理关键的体-腿耦合动力学

Abstract: This study presents an analysis of experimental data from Harpy, a
thruster-assisted bipedal robot developed at Northeastern University. The study
examines data sets from trotting and jumping experiments to understand the
fundamental principles governing hybrid leg-thruster locomotion. Through data
analysis across multiple locomotion modes, this research reveals that Harpy
achieves stable locomotion with bounded trajectories and consistent foot
placement through strategic leg-thruster synergy. The results demonstrate
controlled joint behavior with low torques and symmetric tracking, accurate
foot placement within kinematic constraints despite phase-transition
perturbations, and underactuated degree-of-freedom stability without
divergence. Energy level analysis reveals that legs provide primary propulsion,
while the thrusters enable additional aerial phase control. The analysis
identifies critical body-leg coupling dynamics during aerial phases that
require phase-specific control strategies. Consistent repeatability and
symmetry across experiments validate the robustness of the hybrid actuation
approach.

</details>


### [34] [DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation](https://arxiv.org/abs/2508.18153)
*Aalok Patwardhan,Andrew J. Davison*

Main category: cs.RO

TL;DR: DANCeRS是一个统一的分布式算法，使用高斯置信传播在离散和连续决策空间中实现群体共识，通过因子图表示群体，依赖纯对等消息传递确保可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将离散和连续决策空间的共识视为不同问题，需要一种统一的分布式算法来解决群体在动态环境中的共识问题。

Method: 使用高斯置信传播(GBP)和因子图表示群体，通过纯对等消息传递实现分布式共识，适用于形状形成和离散决策两种应用场景。

Result: 实验结果表明该方法在可扩展性和效率方面优于现有方法，能够有效处理路径规划、碰撞避免和离散决策共识问题。

Conclusion: DANCeRS为需要分布式共识的多机器人系统提供了一个有前景的解决方案，统一处理了离散和连续决策空间的共识问题。

Abstract: Robot swarms require cohesive collective behaviour to address diverse
challenges, including shape formation and decision-making. Existing approaches
often treat consensus in discrete and continuous decision spaces as distinct
problems. We present DANCeRS, a unified, distributed algorithm leveraging
Gaussian Belief Propagation (GBP) to achieve consensus in both domains. By
representing a swarm as a factor graph our method ensures scalability and
robustness in dynamic environments, relying on purely peer-to-peer message
passing. We demonstrate the effectiveness of our general framework through two
applications where agents in a swarm must achieve consensus on global behaviour
whilst relying on local communication. In the first, robots must perform path
planning and collision avoidance to create shape formations. In the second, we
show how the same framework can be used by a group of robots to form a
consensus over a set of discrete decisions. Experimental results highlight our
method's scalability and efficiency compared to recent approaches to these
problems making it a promising solution for multi-robot systems requiring
distributed consensus. We encourage the reader to see the supplementary video
demo.

</details>


### [35] [Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework](https://arxiv.org/abs/2508.18249)
*Zipeng Fang,Yanbo Wang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: 提出了一种多模态自监督框架，通过整合足印、LiDAR和相机数据生成可通行性标签，并训练双流网络进行多模态学习，在多种环境中实现了88%的IoU和1.6-3.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法往往无法有效捕捉不可通行区域特征，且大多只关注单一模态，忽略了多模态传感器融合的优势。

Method: 1) 使用视觉基础模型整合多模态数据生成可通行性标签；2) 训练解耦的双流网络进行多模态学习；3) 引入稀疏LiDAR监督减少伪标签噪声。

Result: 在城市场景、越野环境和校园环境中广泛实验，自动标注方法达到约88%的IoU，相比现有最优自监督方法IoU提升1.6-3.5%。

Conclusion: 提出的多模态自监督框架有效解决了可通行性估计中的模态单一和伪标签噪声问题，在各种环境中都表现出优越性能。

Abstract: Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.

</details>


### [36] [SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation](https://arxiv.org/abs/2508.18268)
*Haoyuan Deng,Wenkai Guo,Qianzhun Wang,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: SafeBimanual是一个测试时轨迹优化框架，为预训练的基于扩散模型的双臂操作策略添加安全约束，避免危险行为并提高成功率


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的双臂操作策略忽略了物理安全约束，导致可能损坏机器人和物体的危险行为

Method: 设计不同双臂协作模式的安全约束成本函数，通过扩散去噪过程的引导采样优化轨迹，并使用视觉语言模型动态调度成本函数

Result: 在8个模拟任务中成功率提高13.7%，不安全交互减少18.8%；在4个真实任务中成功率提高32.5%

Conclusion: SafeBimanual框架有效提升了双臂操作的安全性和成功率，具有实际应用价值

Abstract: Bimanual manipulation has been widely applied in household services and
manufacturing, which enables the complex task completion with coordination
requirements. Recent diffusion-based policy learning approaches have achieved
promising performance in modeling action distributions for bimanual
manipulation. However, they ignored the physical safety constraints of bimanual
manipulation, which leads to the dangerous behaviors with damage to robots and
objects. To this end, we propose a test-time trajectory optimization framework
named SafeBimanual for any pre-trained diffusion-based bimanual manipulation
policies, which imposes the safety constraints on bimanual actions to avoid
dangerous robot behaviors with improved success rate. Specifically, we design
diverse cost functions for safety constraints in different dual-arm cooperation
patterns including avoidance of tearing objects and collision between arms and
objects, which optimizes the manipulator trajectories with guided sampling of
diffusion denoising process. Moreover, we employ a vision-language model (VLM)
to schedule the cost functions by specifying keypoints and corresponding
pairwise relationship, so that the optimal safety constraint is dynamically
generated in the entire bimanual manipulation process. SafeBimanual
demonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase
in success rate and a 18.8% reduction in unsafe interactions over
state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world
tasks further verify its practical value by improving the success rate by
32.5%.

</details>


### [37] [FlowVLA: Thinking in Motion with a Visual Chain of Thought](https://arxiv.org/abs/2508.18269)
*Zhide Zhong,Haodong Yan,Junfeng Li,Xiangchen Liu,Xin Gong,Wenxuan Song,Jiayi Chen,Haoang Li*

Main category: cs.RO

TL;DR: FlowVLA通过引入视觉思维链（Visual CoT）框架，在预测未来帧之前先生成中间光流表示来解耦静态外观和动态运动，从而提升物理推理能力和策略学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型通过下一帧预测训练内部世界模型，但这种方法将静态外观与动态运动纠缠在一起，导致不合理的视觉预测和低效的策略学习。

Method: 提出Visual CoT框架，在FlowVLA中实现"当前帧→光流→未来帧"的推理过程，使用单一自回归Transformer生成中间光流表示来编码运动动态。

Result: 在机器人操作基准测试中实现了最先进的性能，显著提高了样本效率，产生了更连贯的视觉预测。

Conclusion: 该方法为世界建模提供了更原则性的基础，通过解耦动态学习实现了更好的物理推理和策略学习效果。

Abstract: Many Vision-Language-Action (VLA) models rely on an internal world model
trained via next-frame prediction. This approach, however, struggles with
physical reasoning as it entangles static appearance with dynamic motion, often
resulting in implausible visual forecasts and inefficient policy learning. To
address these limitations, we introduce the Visual Chain of Thought (Visual
CoT): a pre-training framework that encourages a model to reason about how a
scene evolves before predicting what it will look like. We instantiate this
principle in FlowVLA, which predicts a future frame ($v_{t+1}$) only after
generating an intermediate optical flow representation ($f_t$) that encodes
motion dynamics. This ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'' reasoning
process is implemented within a single autoregressive Transformer, guiding the
model to learn disentangled dynamics. As a result, FlowVLA produces coherent
visual predictions and facilitates more efficient policy learning. Experiments
on challenging robotics manipulation benchmarks demonstrate state-of-the-art
performance with substantially improved sample efficiency, pointing toward a
more principled foundation for world modeling. Project page:
https://irpn-lab.github.io/FlowVLA/

</details>
