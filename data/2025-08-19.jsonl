{"id": "2508.11759", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11759", "abs": "https://arxiv.org/abs/2508.11759", "authors": ["Peter Lindes", "Kaoutar Skiker"], "title": "Using Natural Language for Human-Robot Collaboration in the Real World", "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6", "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans."}
{"id": "2508.11802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11802", "abs": "https://arxiv.org/abs/2508.11802", "authors": ["Luigi Penco", "Beomyeong Park", "Stefan Fasano", "Nehar Poddar", "Stephen McCrory", "Nicholas Kitchel", "Tomasz Bialek", "Dexton Anderson", "Duncan Calvert", "Robert Griffin"], "title": "Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots\n  (Humanoids)", "summary": "Achieving seamless synchronization between user and robot motion in\nteleoperation, particularly during high-speed tasks, remains a significant\nchallenge. In this work, we propose a novel approach for transferring stepping\nmotions from the user to the robot in real-time. Instead of directly\nreplicating user foot poses, we retarget user steps to robot footstep\nlocations, allowing the robot to utilize its own dynamics for locomotion,\nensuring better balance and stability. Our method anticipates user footsteps to\nminimize delays between when the user initiates and completes a step and when\nthe robot does it. The step estimates are continuously adapted to converge with\nthe measured user references. Additionally, the system autonomously adjusts the\nrobot's steps to account for its surrounding terrain, overcoming challenges\nposed by environmental mismatches between the user's flat-ground setup and the\nrobot's uneven terrain. Experimental results on the humanoid robot Nadia\ndemonstrate the effectiveness of the proposed system."}
{"id": "2508.11849", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11849", "abs": "https://arxiv.org/abs/2508.11849", "authors": ["Allen Wang", "Gavin Tao"], "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba", "comment": null, "summary": "We introduce LocoMamba, a vision-driven cross-modal DRL framework built on\nselective state-space models, specifically leveraging Mamba, that achieves\nnear-linear-time sequence modeling, effectively captures long-range\ndependencies, and enables efficient training with longer sequences. First, we\nembed proprioceptive states with a multilayer perceptron and patchify depth\nimages with a lightweight convolutional neural network, producing compact\ntokens that improve state representation. Second, stacked Mamba layers fuse\nthese tokens via near-linear-time selective scanning, reducing latency and\nmemory footprint, remaining robust to token length and image resolution, and\nproviding an inductive bias that mitigates overfitting. Third, we train the\npolicy end-to-end with Proximal Policy Optimization under terrain and\nappearance randomization and an obstacle-density curriculum, using a compact\nstate-centric reward that balances progress, smoothness, and safety. We\nevaluate our method in challenging simulated environments with static and\nmoving obstacles as well as uneven terrain. Compared with state-of-the-art\nbaselines, our method achieves higher returns and success rates with fewer\ncollisions, exhibits stronger generalization to unseen terrains and obstacle\ndensities, and improves training efficiency by converging in fewer updates\nunder the same compute budget."}
{"id": "2508.11868", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11868", "abs": "https://arxiv.org/abs/2508.11868", "authors": ["Lida Xu"], "title": "Data Shift of Object Detection in Autonomous Driving", "comment": null, "summary": "With the widespread adoption of machine learning technologies in autonomous\ndriving systems, their role in addressing complex environmental perception\nchallenges has become increasingly crucial. However, existing machine learning\nmodels exhibit significant vulnerability, as their performance critically\ndepends on the fundamental assumption that training and testing data satisfy\nthe independent and identically distributed condition, which is difficult to\nguarantee in real-world applications. Dynamic variations in data distribution\ncaused by seasonal changes, weather fluctuations lead to data shift problems in\nautonomous driving systems. This study investigates the data shift problem in\nautonomous driving object detection tasks, systematically analyzing its\ncomplexity and diverse manifestations. We conduct a comprehensive review of\ndata shift detection methods and employ shift detection analysis techniques to\nperform dataset categorization and balancing. Building upon this foundation, we\nconstruct an object detection model. To validate our approach, we optimize the\nmodel by integrating CycleGAN-based data augmentation techniques with the\nYOLOv5 framework. Experimental results demonstrate that our method achieves\nsuperior performance compared to baseline models on the BDD100K dataset."}
{"id": "2508.11883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11883", "abs": "https://arxiv.org/abs/2508.11883", "authors": ["Lei Li", "Boyang Qin", "Wenzhuo Gao", "Yanyu Li", "Yiyuan Zhang", "Bo Wang", "Shihan Kong", "Jian Wang", "Dekui He", "Junzhi Yu"], "title": "Bioinspired underwater soft robots: from biology to robotics and back", "comment": null, "summary": "The ocean vast unexplored regions and diverse soft-bodied marine organisms\nhave spurred interest in bio-inspired underwater soft robotics. Recent advances\nhave enabled new capabilities in underwater movement, sensing, and interaction.\nHowever, these efforts are largely unidirectional, with biology guiding\nrobotics while insights from robotics rarely feed back into biology. Here we\npropose a holistic, bidirectional framework that integrates biological\nprinciples, robotic implementation, and biological validation. We show that\nsoft robots can serve as experimental tools to probe biological functions and\neven test evolutionary hypotheses. Their inherent compliance also allows them\nto outperform rigid systems in unstructured environments, supporting\napplications in marine exploration, manipulation, and medicine. Looking\nforward, we introduce bio-universal-inspired robotics, a paradigm that\ntranscends species-specific mimicry by identifying convergent principles across\nspecies to inspire more adaptable designs. Despite rapid progress, challenges\npersist in material robustness, actuation efficiency, autonomy, and\nintelligence. By uniting biology and engineering, soft robots can advance ocean\nexploration and deepen scientific discovery."}
{"id": "2508.11884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11884", "abs": "https://arxiv.org/abs/2508.11884", "authors": ["Havel Liu", "Mingzhang Zhu", "Arturo Moises Flores Alvarez", "Yuan Hung Lo", "Conrad Ku", "Federico Parres", "Justin Quan", "Colin Togashi", "Aditya Navghare", "Quanyou Wang", "Dennis W. Hong"], "title": "From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics", "comment": "8 pages, 14 figures, accepted by IEEE Humanoids 2025", "summary": "Humanoid robots represent the cutting edge of robotics research, yet their\npotential in entertainment remains largely unexplored. Entertainment as a field\nprioritizes visuals and form, a principle that contrasts with the purely\nfunctional designs of most contemporary humanoid robots. Designing\nentertainment humanoid robots capable of fluid movement presents a number of\nunique challenges. In this paper, we present Kid Cosmo, a research platform\ndesigned for robust locomotion and life-like motion generation while imitating\nthe look and mannerisms of its namesake character from Netflix's movie The\nElectric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall\nand weighing 25 kg. It contains 28 degrees of freedom and primarily uses\nproprioceptive actuators, enabling torque-control walking and lifelike motion\ngeneration. Following worldwide showcases as part of the movie's press tour, we\npresent the system architecture, challenges of a functional entertainment robot\nand unique solutions, and our initial findings on stability during simultaneous\nupper and lower body movement. We demonstrate the viability of\nperformance-oriented humanoid robots that prioritize both character embodiment\nand technical functionality."}
{"id": "2508.11885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11885", "abs": "https://arxiv.org/abs/2508.11885", "authors": ["Haixin Gong", "Chen Zhang", "Yanan Sui"], "title": "Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System", "comment": "IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids\n  2025)", "summary": "The human foot serves as the critical interface between the body and\nenvironment during locomotion. Existing musculoskeletal models typically\noversimplify foot-ground contact mechanics, limiting their ability to\naccurately simulate human gait dynamics. We developed a novel contact-rich and\ndeformable model of the human foot integrated within a complete musculoskeletal\nsystem that captures the complex biomechanical interactions during walking. To\novercome the control challenges inherent in modeling multi-point contacts and\ndeformable material, we developed a two-stage policy training strategy to learn\nnatural walking patterns for this interface-enhanced model. Comparative\nanalysis between our approach and conventional rigid musculoskeletal models\ndemonstrated improvements in kinematic, kinetic, and gait stability metrics.\nValidation against human subject data confirmed that our simulation closely\nreproduced real-world biomechanical measurements. This work advances\ncontact-rich interface modeling for human musculoskeletal systems and\nestablishes a robust framework that can be extended to humanoid robotics\napplications requiring precise foot-ground interaction control."}
{"id": "2508.12166", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12166", "abs": "https://arxiv.org/abs/2508.12166", "authors": ["Gokul Puthumanaillam", "Aditya Penumarti", "Manav Vora", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Jane Shin", "Melkior Ornik"], "title": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing", "comment": "Accepted to CoRL 2025 (Conference on Robot Learning)", "summary": "Robots equipped with rich sensor suites can localize reliably in\npartially-observable environments, but powering every sensor continuously is\nwasteful and often infeasible. Belief-space planners address this by\npropagating pose-belief covariance through analytic models and switching\nsensors heuristically--a brittle, runtime-expensive approach. Data-driven\napproaches--including diffusion models--learn multi-modal trajectories from\ndemonstrations, but presuppose an accurate, always-on state estimate. We\naddress the largely open problem: for a given task in a mapped environment,\nwhich \\textit{minimal sensor subset} must be active at each location to\nmaintain state uncertainty \\textit{just low enough} to complete the task? Our\nkey insight is that when a diffusion planner is explicitly conditioned on a\npose-belief raster and a sensor mask, the spread of its denoising trajectories\nyields a calibrated, differentiable proxy for the expected localisation error.\nBuilding on this insight, we present Belief-Conditioned One-Step Diffusion\n(B-COD), the first planner that, in a 10 ms forward pass, returns a\nshort-horizon trajectory, per-waypoint aleatoric variances, and a proxy for\nlocalisation error--eliminating external covariance rollouts. We show that this\nsingle proxy suffices for a soft-actor-critic to choose sensors online,\noptimising energy while bounding pose-covariance growth. We deploy B-COD in\nreal-time marine trials on an unmanned surface vehicle and show that it reduces\nsensing energy consumption while matching the goal-reach performance of an\nalways-on baseline."}
{"id": "2508.11887", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11887", "abs": "https://arxiv.org/abs/2508.11887", "authors": ["Yousra Shleibik", "Jordan Sinclair", "Kerstin Haring"], "title": "Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards", "comment": null, "summary": "The advent of autonomous driving systems promises to transform transportation\nby enhancing safety, efficiency, and comfort. As these technologies evolve\ntoward higher levels of autonomy, the need for integrated systems that\nseamlessly support human involvement in decision-making becomes increasingly\ncritical. Certain scenarios necessitate human involvement, including those\nwhere the vehicle is unable to identify an object or element in the scene, and\nas such cannot take independent action. Therefore, situational awareness is\nessential to mitigate potential risks during a takeover, where a driver must\nassume control and autonomy from the vehicle. The need for driver attention is\nimportant to avoid collisions with external agents and ensure a smooth\ntransition during takeover operations. This paper explores the integration of\nattention redirection techniques, such as gaze manipulation through targeted\nvisual and auditory cues, to help drivers maintain focus on emerging hazards\nand reduce target fixation in semi-autonomous driving scenarios. We propose a\nconceptual framework that combines real-time gaze tracking, context-aware\nsaliency analysis, and synchronized visual and auditory alerts to enhance\nsituational awareness, proactively address potential hazards, and foster\neffective collaboration between humans and autonomous systems."}
{"id": "2508.12335", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12335", "abs": "https://arxiv.org/abs/2508.12335", "authors": ["Yunfan Gao", "Florian Messerer", "Niels van Duijkeren", "Rashmi Dabir", "Moritz Diehl"], "title": "Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control", "comment": "21 pages, 15 figures", "summary": "This paper presents a novel approach for collision avoidance in optimal and\nmodel predictive control, in which the environment is represented by a large\nnumber of points and the robot as a union of padded polygons. The conditions\nthat none of the points shall collide with the robot can be written in terms of\nan infinite number of constraints per obstacle point. We show that the\nresulting semi-infinite programming (SIP) optimal control problem (OCP) can be\nefficiently tackled through a combination of two methods: local reduction and\nan external active-set method. Specifically, this involves iteratively\nidentifying the closest point obstacles, determining the lower-level distance\nminimizer among all feasible robot shape parameters, and solving the\nupper-level finitely-constrained subproblems.\n  In addition, this paper addresses robust collision avoidance in the presence\nof ellipsoidal state uncertainties. Enforcing constraint satisfaction over all\npossible uncertainty realizations extends the dimension of constraint\ninfiniteness. The infinitely many constraints arising from translational\nuncertainty are handled by local reduction together with the robot shape\nparameterization, while rotational uncertainty is addressed via a backoff\nreformulation.\n  A controller implemented based on the proposed method is demonstrated on a\nreal-world robot running at 20Hz, enabling fast and collision-free navigation\nin tight spaces. An application to 3D collision avoidance is also demonstrated\nin simulation."}
{"id": "2508.11890", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11890", "abs": "https://arxiv.org/abs/2508.11890", "authors": ["Sangwoo Jeon", "Juchul Shin", "YeonJe Cho", "Gyeong-Tae Kim", "Seongwoo Kim"], "title": "Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation", "comment": null, "summary": "Modern autonomous drone missions increasingly require software frameworks\ncapable of seamlessly integrating structured symbolic planning with adaptive\nreinforcement learning (RL). Although traditional rule-based architectures\noffer robust structured reasoning for drone autonomy, their capabilities fall\nshort in dynamically complex operational environments that require adaptive\nsymbolic planning. Symbolic RL (SRL), using the Planning Domain Definition\nLanguage (PDDL), explicitly integrates domain-specific knowledge and\noperational constraints, significantly improving the reliability and safety of\nunmanned aerial vehicle (UAV) decision making. In this study, we propose the\nAMAD-SRL framework, an extended and refined version of the Autonomous Mission\nAgents for Drones (AMAD) cognitive multi-agent architecture, enhanced with\nsymbolic reinforcement learning for dynamic mission planning and execution. We\nvalidated our framework in a Software-in-the-Loop (SIL) environment structured\nidentically to an intended Hardware-In-the-Loop Simulation (HILS) platform,\nensuring seamless transition to real hardware. Experimental results demonstrate\nstable integration and interoperability of modules, successful transitions\nbetween BDI-driven and symbolic RL-driven planning phases, and consistent\nmission performance. Specifically, we evaluate a target acquisition scenario in\nwhich the UAV plans a surveillance path followed by a dynamic reentry path to\nsecure the target while avoiding threat zones. In this SIL evaluation, mission\nefficiency improved by approximately 75% over a coverage-based baseline,\nmeasured by travel distance reduction. This study establishes a robust\nfoundation for handling complex UAV missions and discusses directions for\nfurther enhancement and validation."}
{"id": "2508.12395", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12395", "abs": "https://arxiv.org/abs/2508.12395", "authors": ["Zihan Wang"], "title": "PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting", "comment": null, "summary": "This study presents the design and control of a Plasma-propelled\nUltra-silence Blimp (PUB), a novel aerial robot employing plasma vector\npropulsion for ultra-quiet flight without mechanical propellers. The system\nutilizes a helium-lift platform for extended endurance and a four-layer ring\nasymmetric capacitor to generate ionic wind thrust. The modular propulsion\nunits allow flexible configuration to meet mission-specific requirements, while\na two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop\nslip control scheme is implemented for stable maneuvering. Flight experiments\ndemonstrate full-envelope capability, including take-off, climb, hover,\ndescent, and smooth landing, confirming the feasibility of plasma vector\npropulsion, the effectiveness of DOF vector control, and the stability of the\ncontrol system. Owing to its low acoustic signature, structural simplicity, and\nhigh maneuverability, PUB is well suited for noise-sensitive, enclosed, and\nnear-space applications."}
{"id": "2508.11898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11898", "abs": "https://arxiv.org/abs/2508.11898", "authors": ["Jilei Mao", "Jiarui Guan", "Yingjuan Tang", "Qirui Hu", "Zhihang Li", "Junjie Yu", "Yongjie Mao", "Yunzhe Sun", "Shuang Liu", "Xiaozhu Ju"], "title": "OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation", "comment": null, "summary": "The visuomotor policy can easily overfit to its training datasets, such as\nfixed camera positions and backgrounds. This overfitting makes the policy\nperform well in the in-distribution scenarios but underperform in the\nout-of-distribution generalization. Additionally, the existing methods also\nhave difficulty fusing multi-view information to generate an effective 3D\nrepresentation. To tackle these issues, we propose Omni-Vision Diffusion Policy\n(OmniD), a multi-view fusion framework that synthesizes image observations into\na unified bird's-eye view (BEV) representation. We introduce a deformable\nattention-based Omni-Feature Generator (OFG) to selectively abstract\ntask-relevant features while suppressing view-specific noise and background\ndistractions. OmniD achieves 11\\%, 17\\%, and 84\\% average improvement over the\nbest baseline model for in-distribution, out-of-distribution, and few-shot\nexperiments, respectively. Training code and simulation benchmark are\navailable: https://github.com/1mather/omnid.git"}
{"id": "2508.12729", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12729", "abs": "https://arxiv.org/abs/2508.12729", "authors": ["Junhao Ye", "Cheng Hu", "Yiqin Wang", "Weizhan Huang", "Nicolas Baumann", "Jie He", "Meixun Qu", "Lei Xie", "Hongye Su"], "title": "MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA", "comment": null, "summary": "In autonomous racing, reactive controllers eliminate the computational burden\nof the full See-Think-Act autonomy stack by directly mapping sensor inputs to\ncontrol actions. This bypasses the need for explicit localization and\ntrajectory planning. A widely adopted baseline in this category is the\nFollow-The-Gap method, which performs trajectory planning using LiDAR data.\nBuilding on FTG, the Delaunay Triangulation-based Racing algorithm introduces\nfurther enhancements. However, DTR's use of circumcircles for trajectory\ngeneration often results in insufficiently smooth paths, ultimately degrading\nperformance. Additionally, the commonly used F1TENTH-simulator for autonomous\nracing competitions lacks support for 3D LiDAR perception, limiting its\neffectiveness in realistic testing. To address these challenges, this work\nproposes the MCTR algorithm. MCTR improves trajectory smoothness through the\nuse of Curvature Corrected Moving Average and implements a digital twin system\nwithin the CARLA simulator to validate the algorithm's robustness under 3D\nLiDAR perception. The proposed algorithm has been thoroughly validated through\nboth simulation and real-world vehicle experiments."}
{"id": "2508.11917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11917", "abs": "https://arxiv.org/abs/2508.11917", "authors": ["Hossein Keshavarz", "Alejandro Ramirez-Serrano", "Majid Khadiv"], "title": "Control of Legged Robots using Model Predictive Optimized Path Integral", "comment": "8 pages, 13 figures, Humanoid conference", "summary": "Legged robots possess a unique ability to traverse rough terrains and\nnavigate cluttered environments, making them well-suited for complex,\nreal-world unstructured scenarios. However, such robots have not yet achieved\nthe same level as seen in natural systems. Recently, sampling-based predictive\ncontrollers have demonstrated particularly promising results. This paper\ninvestigates a sampling-based model predictive strategy combining model\npredictive path integral (MPPI) with cross-entropy (CE) and covariance matrix\nadaptation (CMA) methods to generate real-time whole-body motions for legged\nrobots across multiple scenarios. The results show that combining the benefits\nof MPPI, CE and CMA, namely using model predictive optimized path integral\n(MPOPI), demonstrates greater sample efficiency, enabling robots to attain\nsuperior locomotion results using fewer samples when compared to typical MPPI\nalgorithms. Extensive simulation experiments in multiple scenarios on a\nquadruped robot show that MPOPI can be used as an anytime control strategy,\nincreasing locomotion capabilities at each iteration."}
{"id": "2508.13151", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13151", "abs": "https://arxiv.org/abs/2508.13151", "authors": ["Yuying Zhang", "Joni Pajarinen"], "title": "Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors", "comment": null, "summary": "Mobile manipulation in dynamic environments is challenging due to movable\nobstacles blocking the robot's path. Traditional methods, which treat\nnavigation and manipulation as separate tasks, often fail in such\n'manipulate-to-navigate' scenarios, as obstacles must be removed before\nnavigation. In these cases, active interaction with the environment is required\nto clear obstacles while ensuring sufficient space for movement. To address the\nmanipulate-to-navigate problem, we propose a reinforcement learning-based\napproach for learning manipulation actions that facilitate subsequent\nnavigation. Our method combines manipulability priors to focus the robot on\nhigh manipulability body positions with affordance maps for selecting\nhigh-quality manipulation actions. By focusing on feasible and meaningful\nactions, our approach reduces unnecessary exploration and allows the robot to\nlearn manipulation strategies more effectively. We present two new\nmanipulate-to-navigate simulation tasks called Reach and Door with the Boston\nDynamics Spot robot. The first task tests whether the robot can select a good\nhand position in the target area such that the robot base can move effectively\nforward while keeping the end effector position fixed. The second task requires\nthe robot to move a door aside in order to clear the navigation path. Both of\nthese tasks need first manipulation and then navigating the base forward.\nResults show that our method allows a robot to effectively interact with and\ntraverse dynamic environments. Finally, we transfer the learned policy to a\nreal Boston Dynamics Spot robot, which successfully performs the Reach task."}
{"id": "2508.11918", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11918", "abs": "https://arxiv.org/abs/2508.11918", "authors": ["Zhichen Lou", "Kechun Xu", "Zhongxiang Zhou", "Rong Xiong"], "title": "ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models", "comment": null, "summary": "The advancement of embodied intelligence is accelerating the integration of\nrobots into daily life as human assistants. This evolution requires robots to\nnot only interpret high-level instructions and plan tasks but also perceive and\nadapt within dynamic environments. Vision-Language Models (VLMs) present a\npromising solution by combining visual understanding and language reasoning.\nHowever, existing VLM-based methods struggle with interactive exploration,\naccurate perception, and real-time plan adaptation. To address these\nchallenges, we propose ExploreVLM, a novel closed-loop task planning framework\npowered by Vision-Language Models (VLMs). The framework is built around a\nstep-wise feedback mechanism that enables real-time plan adjustment and\nsupports interactive exploration. At its core is a dual-stage task planner with\nself-reflection, enhanced by an object-centric spatial relation graph that\nprovides structured, language-grounded scene representations to guide\nperception and planning. An execution validator supports the closed loop by\nverifying each action and triggering re-planning. Extensive real-world\nexperiments demonstrate that ExploreVLM significantly outperforms\nstate-of-the-art baselines, particularly in exploration-centric tasks. Ablation\nstudies further validate the critical role of the reflective planner and\nstructured perception in achieving robust and efficient task execution."}
{"id": "2508.11929", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11929", "abs": "https://arxiv.org/abs/2508.11929", "authors": ["Mohitvishnu S. Gadde", "Pranay Dugar", "Ashish Malik", "Alan Fern"], "title": "No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain", "comment": null, "summary": "Effective bipedal locomotion in dynamic environments, such as cluttered\nindoor spaces or uneven terrain, requires agile and adaptive movement in all\ndirections. This necessitates omnidirectional terrain sensing and a controller\ncapable of processing such input. We present a learning framework for\nvision-based omnidirectional bipedal locomotion, enabling seamless movement\nusing depth images. A key challenge is the high computational cost of rendering\nomnidirectional depth images in simulation, making traditional sim-to-real\nreinforcement learning (RL) impractical. Our method combines a robust blind\ncontroller with a teacher policy that supervises a vision-based student policy,\ntrained on noise-augmented terrain data to avoid rendering costs during RL and\nensure robustness. We also introduce a data augmentation technique for\nsupervised student training, accelerating training by up to 10 times compared\nto conventional methods. Our framework is validated through simulation and\nreal-world tests, demonstrating effective omnidirectional locomotion with\nminimal reliance on expensive rendering. This is, to the best of our knowledge,\nthe first demonstration of vision-based omnidirectional bipedal locomotion,\nshowcasing its adaptability to diverse terrains."}
{"id": "2508.11960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11960", "abs": "https://arxiv.org/abs/2508.11960", "authors": ["Sandeep Kanta", "Mehrdad Tavassoli", "Varun Teja Chirkuri", "Venkata Akhil Kumar", "Santhi Bharath Punati", "Praveen Damacharla", "Sunny Katyara"], "title": "Toward General Physical Intelligence for Resilient Agile Manufacturing Automation", "comment": "Advanced Engineering Informatics", "summary": "Agile and human-centric manufacturing stipulates resilient robotic solutions\ncapable of contextual reasoning and safe interaction in unstructured\nenvironments. Foundation models particularly the Vision Language Action (VLA)\nmodels have emerged to fuse multimodal perception, reasoning and physically\ngrounded action across varied embodiments into unified representation, termed\nas General Physical Intelligence (GPI). While GPI has already been described in\nthe literature but its practical application and evolving role in contemporary\nagile manufacturing processes have yet to be duly explored. To bridge this gap,\nthis practical review systematically surveys recent advancements in VLA models\nwithin GPI context, performs comprehensive comparative analysis of leading\nimplementations and evaluates their readiness for industrial deployment through\nstructured ablation study. Our analysis has organized state-of-the-art into\nfive thematic pillars including multisensory representation learning, sim2real\ntransfer, planning and control, uncertainty and safety measures and\nbenchmarking. Finally, we articulate open research challenges and propose\ndirections to better integrate GPI into next-generation industrial ecosystems\nin line with Industry 5.0."}
{"id": "2508.12038", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12038", "abs": "https://arxiv.org/abs/2508.12038", "authors": ["Liwen Zhang", "Heng Deng", "Guanghui Sun"], "title": "Fully Spiking Actor-Critic Neural Network for Robotic Manipulation", "comment": null, "summary": "This study proposes a hybrid curriculum reinforcement learning (CRL)\nframework based on a fully spiking neural network (SNN) for 9-degree-of-freedom\nrobotic arms performing target reaching and grasping tasks. To reduce network\ncomplexity and inference latency, the SNN architecture is simplified to include\nonly an input and an output layer, which shows strong potential for\nresource-constrained environments. Building on the advantages of SNNs-high\ninference speed, low energy consumption, and spike-based biological\nplausibility, a temporal progress-partitioned curriculum strategy is integrated\nwith the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy\nconsumption modeling framework is introduced to quantitatively compare the\ntheoretical energy consumption between SNNs and conventional Artificial Neural\nNetworks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized\nobservation space further improve learning efficiency and policy accuracy.\nExperiments on the Isaac Gym simulation platform demonstrate that the proposed\nmethod achieves superior performance under realistic physical constraints.\nComparative evaluations with conventional PPO and ANN baselines validate the\nscalability and energy efficiency of the proposed approach in dynamic robotic\nmanipulation tasks."}
{"id": "2508.12043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12043", "abs": "https://arxiv.org/abs/2508.12043", "authors": ["Fei Lin", "Tengchao Zhang", "Qinghua Ni", "Jun Huang", "Siji Ma", "Yonglin Tian", "Yisheng Lv", "Naiqi Wu"], "title": "Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs", "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) in unmanned systems has\nsignificantly enhanced the semantic understanding and autonomous task execution\ncapabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited\ncommunication bandwidth and the need for high-frequency interactions pose\nsevere challenges to semantic information transmission within the swarm. This\npaper explores the feasibility of LLM-driven UAV swarms for autonomous semantic\ncompression communication, aiming to reduce communication load while preserving\ncritical task semantics. To this end, we construct four types of 2D simulation\nscenarios with different levels of environmental complexity and design a\ncommunication-execution pipeline that integrates system prompts with task\ninstruction prompts. On this basis, we systematically evaluate the semantic\ncompression performance of nine mainstream LLMs in different scenarios and\nanalyze their adaptability and stability through ablation studies on\nenvironmental complexity and swarm size. Experimental results demonstrate that\nLLM-based UAV swarms have the potential to achieve efficient collaborative\ncommunication under bandwidth-constrained and multi-hop link conditions."}
{"id": "2508.12071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12071", "abs": "https://arxiv.org/abs/2508.12071", "authors": ["Amy Phung", "Richard Camilli"], "title": "OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments", "comment": "This paper has been accepted for publication in IROS 2025. Copyright\n  IEEE", "summary": "High resolution underwater 3D scene reconstruction is crucial for various\napplications, including construction, infrastructure maintenance, monitoring,\nexploration, and scientific investigation. Prior work has leveraged the\ncomplementary sensing modalities of imaging sonars and optical cameras for\nopti-acoustic 3D scene reconstruction, demonstrating improved results over\nmethods which rely solely on either sensor. However, while most existing\napproaches focus on offline reconstruction, real-time spatial awareness is\nessential for both autonomous and piloted underwater vehicle operations. This\npaper presents OASIS, an opti-acoustic fusion method that integrates data from\noptical images with voxel carving techniques to achieve real-time 3D\nreconstruction unstructured underwater workspaces. Our approach utilizes an\n\"eye-in-hand\" configuration, which leverages the dexterity of robotic\nmanipulator arms to capture multiple workspace views across a short baseline.\nWe validate OASIS through tank-based experiments and present qualitative and\nquantitative results that highlight its utility for underwater manipulation\ntasks."}
{"id": "2508.12075", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12075", "abs": "https://arxiv.org/abs/2508.12075", "authors": ["Shaul Ashkenazi", "Gabriel Skantze", "Jane Stuart-Smith", "Mary Ellen Foster"], "title": "Into the Wild: When Robots Are Not Welcome", "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025 (paper PubRob-Fails/2025/4)", "summary": "Social robots are increasingly being deployed in public spaces, where they\nface not only technological difficulties and unexpected user utterances, but\nalso objections from stakeholders who may not be comfortable with introducing a\nrobot into those spaces. We describe our difficulties with deploying a social\nrobot in two different public settings: 1) Student services center; 2) Refugees\nand asylum seekers drop-in service. Although this is a failure report, in each\nuse case we eventually managed to earn the trust of the staff and form a\nrelationship with them, allowing us to deploy our robot and conduct our\nstudies."}
{"id": "2508.12166", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12166", "abs": "https://arxiv.org/abs/2508.12166", "authors": ["Gokul Puthumanaillam", "Aditya Penumarti", "Manav Vora", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Jane Shin", "Melkior Ornik"], "title": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing", "comment": "Accepted to CoRL 2025 (Conference on Robot Learning)", "summary": "Robots equipped with rich sensor suites can localize reliably in\npartially-observable environments, but powering every sensor continuously is\nwasteful and often infeasible. Belief-space planners address this by\npropagating pose-belief covariance through analytic models and switching\nsensors heuristically--a brittle, runtime-expensive approach. Data-driven\napproaches--including diffusion models--learn multi-modal trajectories from\ndemonstrations, but presuppose an accurate, always-on state estimate. We\naddress the largely open problem: for a given task in a mapped environment,\nwhich \\textit{minimal sensor subset} must be active at each location to\nmaintain state uncertainty \\textit{just low enough} to complete the task? Our\nkey insight is that when a diffusion planner is explicitly conditioned on a\npose-belief raster and a sensor mask, the spread of its denoising trajectories\nyields a calibrated, differentiable proxy for the expected localisation error.\nBuilding on this insight, we present Belief-Conditioned One-Step Diffusion\n(B-COD), the first planner that, in a 10 ms forward pass, returns a\nshort-horizon trajectory, per-waypoint aleatoric variances, and a proxy for\nlocalisation error--eliminating external covariance rollouts. We show that this\nsingle proxy suffices for a soft-actor-critic to choose sensors online,\noptimising energy while bounding pose-covariance growth. We deploy B-COD in\nreal-time marine trials on an unmanned surface vehicle and show that it reduces\nsensing energy consumption while matching the goal-reach performance of an\nalways-on baseline."}
{"id": "2508.12170", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12170", "abs": "https://arxiv.org/abs/2508.12170", "authors": ["Aryan Gupta"], "title": "Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)", "comment": null, "summary": "This study presents a systematic literature review of software-level\napproaches to energy efficiency in robotics published from 2020 through 2024,\nupdating and extending pre-2020 evidence. An automated-but-audited pipeline\ncombined Google Scholar seeding, backward/forward snowballing, and\nlarge-language-model (LLM) assistance for screening and data extraction, with\n~10% human audits at each automated step and consensus-with-tie-breaks for\nfull-text decisions. The final corpus comprises 79 peer-reviewed studies\nanalyzed across application domain, metrics, evaluation type, energy models,\nmajor energy consumers, software technique families, and energy-quality\ntrade-offs. Industrial settings dominate (31.6%) followed by exploration\n(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of\nstudies, with computing/controllers a distant second (13.9%). Simulation-only\nevaluations remain most common (51.9%), though hybrid evaluations are frequent\n(25.3%). Representational (physics-grounded) energy models predominate (87.3%).\nMotion and trajectory optimization is the leading technique family (69.6%),\noften paired with learning/prediction (40.5%) and computation\nallocation/scheduling (26.6%); power management/idle control (11.4%) and\ncommunication/data efficiency (3.8%) are comparatively underexplored. Reporting\nis heterogeneous: composite objectives that include energy are most common,\nwhile task-normalized and performance-per-energy metrics appear less often,\nlimiting cross-paper comparability. The review offers a minimal reporting\nchecklist (e.g., total energy and average power plus a task-normalized metric\nand clear baselines) and highlights opportunities in cross-layer designs and in\nquantifying non-performance trade-offs (accuracy, stability). A replication\npackage with code, prompts, and frozen datasets accompanies the review."}
{"id": "2508.12184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12184", "abs": "https://arxiv.org/abs/2508.12184", "authors": ["Rhea Malhotra", "William Chong", "Catie Cuan", "Oussama Khatib"], "title": "Humanoid Motion Scripting with Postural Synergies", "comment": null, "summary": "Generating sequences of human-like motions for humanoid robots presents\nchallenges in collecting and analyzing reference human motions, synthesizing\nnew motions based on these reference motions, and mapping the generated motion\nonto humanoid robots. To address these issues, we introduce SynSculptor, a\nhumanoid motion analysis and editing framework that leverages postural\nsynergies for training-free human-like motion scripting. To analyze human\nmotion, we collect 3+ hours of motion capture data across 20 individuals where\na real-time operational space controller mimics human motion on a simulated\nhumanoid robot. The major postural synergies are extracted using principal\ncomponent analysis (PCA) for velocity trajectories segmented by changes in\nrobot momentum, constructing a style-conditioned synergy library for free-space\nmotion generation. To evaluate generated motions using the synergy library, the\nfoot-sliding ratio and proposed metrics for motion smoothness involving total\nmomentum and kinetic energy deviations are computed for each generated motion,\nand compared with reference motions. Finally, we leverage the synergies with a\nmotion-language transformer, where the humanoid, during execution of motion\ntasks with its end-effectors, adapts its posture based on the chosen synergy.\nSupplementary material, code, and videos are available at\nhttps://rhea-mal.github.io/humanoidsynergies.io."}
{"id": "2508.12189", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12189", "abs": "https://arxiv.org/abs/2508.12189", "authors": ["Rhea Malhotra", "Yuejiang Liu", "Chelsea Finn"], "title": "Self-Guided Action Diffusion", "comment": null, "summary": "Recent works have shown the promise of inference-time search over action\nsamples for improving generative robot policies. In particular, optimizing\ncross-chunk coherence via bidirectional decoding has proven effective in\nboosting the consistency and reactivity of diffusion policies. However, this\napproach remains computationally expensive as the diversity of sampled actions\ngrows. In this paper, we introduce self-guided action diffusion, a more\nefficient variant of bidirectional decoding tailored for diffusion-based\npolicies. At the core of our method is to guide the proposal distribution at\neach diffusion step based on the prior decision. Experiments in simulation\ntasks show that the proposed self-guidance enables near-optimal performance at\nnegligible inference cost. Notably, under a tight sampling budget, our method\nachieves up to 70% higher success rates than existing counterparts on\nchallenging dynamic tasks. See project website at\nhttps://rhea-mal.github.io/selfgad.github.io."}
{"id": "2508.12211", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12211", "abs": "https://arxiv.org/abs/2508.12211", "authors": ["Cyrus Neary", "Omar G. Younis", "Artur Kuramshin", "Ozgur Aslan", "Glen Berseth"], "title": "Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search", "comment": null, "summary": "Pre-trained vision-language-action (VLA) models offer a promising foundation\nfor generalist robot policies, but often produce brittle behaviours or unsafe\nfailures when deployed zero-shot in out-of-distribution scenarios. We present\nVision-Language-Action Planning & Search (VLAPS) -- a novel framework and\naccompanying algorithms that embed model-based search into the inference\nprocedure of pre-trained VLA policies to improve their performance on robotic\ntasks. Specifically, our method biases a modified Monte Carlo Tree Search\n(MCTS) algorithm -- run using a model of the target environment -- using action\npriors defined by the VLA policy. By using VLA-derived abstractions and priors\nin model-based search, VLAPS efficiently explores language-conditioned robotics\ntasks whose search spaces would otherwise be intractably large. Conversely, by\nintegrating model-based search with the VLA policy's inference procedure, VLAPS\nyields behaviours that are more performant than those obtained by directly\nfollowing the VLA policy's action predictions. VLAPS offers a principled\nframework to: i) control test-time compute in VLA models, ii) leverage a priori\nknowledge of the robotic environment, and iii) integrate established planning\nand reinforcement learning techniques into the VLA inference process. Across\nall experiments, VLAPS significantly outperforms VLA-only baselines on\nlanguage-specified tasks that would otherwise be intractable for uninformed\nsearch algorithms, increasing success rates by as much as 67 percentage points."}
{"id": "2508.12252", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12252", "abs": "https://arxiv.org/abs/2508.12252", "authors": ["Kaizhe Hu", "Haochen Shi", "Yao He", "Weizhuo Wang", "C. Karen Liu", "Shuran Song"], "title": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids", "comment": "Accepted to The Conference on Robot Learning (CoRL) 2025", "summary": "Simulation-based reinforcement learning (RL) has significantly advanced\nhumanoid locomotion tasks, yet direct real-world RL from scratch or adapting\nfrom pretrained policies remains rare, limiting the full potential of humanoid\nrobots. Real-world learning, despite being crucial for overcoming the\nsim-to-real gap, faces substantial challenges related to safety, reward design,\nand learning efficiency. To address these limitations, we propose\nRobot-Trains-Robot (RTR), a novel framework where a robotic arm teacher\nactively supports and guides a humanoid robot student. The RTR system provides\nprotection, learning schedule, reward, perturbation, failure detection, and\nautomatic resets. It enables efficient long-term real-world humanoid training\nwith minimal human intervention. Furthermore, we propose a novel RL pipeline\nthat facilitates and stabilizes sim-to-real transfer by optimizing a single\ndynamics-encoded latent variable in the real world. We validate our method\nthrough two challenging real-world humanoid tasks: fine-tuning a walking policy\nfor precise speed tracking and learning a humanoid swing-up task from scratch,\nillustrating the promising capabilities of real-world humanoid learning\nrealized by RTR-style systems. See https://robot-trains-robot.github.io/ for\nmore info."}
{"id": "2508.12274", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12274", "abs": "https://arxiv.org/abs/2508.12274", "authors": ["Jian Zhao", "Yunlong Lian", "Andy M Tyrrell", "Michael Gienger", "Jihong Zhu"], "title": "Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments", "comment": "8 pages, 41 figures", "summary": "Robot-assisted dressing is a popular but challenging topic in the field of\nrobotic manipulation, offering significant potential to improve the quality of\nlife for individuals with mobility limitations. Currently, the majority of\nresearch on robot-assisted dressing focuses on how to put on loose-fitting\nclothing, with little attention paid to tight garments. For the former, since\nthe armscye is larger, a single robotic arm can usually complete the dressing\ntask successfully. However, for the latter, dressing with a single robotic arm\noften fails due to the narrower armscye and the property of diminishing\nrigidity in the armscye, which eventually causes the armscye to get stuck. This\npaper proposes a bimanual dressing strategy suitable for dressing tight-fitting\nclothing. To facilitate the encoding of dressing trajectories that adapt to\ndifferent human arm postures, a spherical coordinate system for dressing is\nestablished. We uses the azimuthal angle of the spherical coordinate system as\na task-relevant feature for bimanual manipulation. Based on this new\ncoordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture\nRegression (GMR) for imitation learning of bimanual dressing trajectories,\ngenerating dressing strategies that adapt to different human arm postures. The\neffectiveness of the proposed method is validated through various experiments."}
{"id": "2508.12296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12296", "abs": "https://arxiv.org/abs/2508.12296", "authors": ["Bin Wang", "Jiwen Zhang", "Song Wang", "Dan Wu"], "title": "A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts", "comment": null, "summary": "In some high-precision industrial applications, robots are deployed to\nperform precision assembly tasks on mass batches of manufactured pegs and\nholes. If the peg and hole are designed with transition fit, machining errors\nmay lead to either a clearance or an interference fit for a specific pair of\ncomponents, with uncertain fit amounts. This paper focuses on the robotic batch\nprecision assembly task involving components with uncertain fit types and fit\namounts, and proposes an efficient methodology to construct the robust and\ncompliant assembly control strategy. Specifically, the batch precision assembly\ntask is decomposed into multiple deterministic subtasks, and a force-vision\nfusion controller-driven reinforcement learning method and a multi-task\nreinforcement learning training method (FVFC-MTRL) are proposed to jointly\nlearn multiple compliance control strategies for these subtasks. Subsequently,\nthe multi-teacher policy distillation approach is designed to integrate\nmultiple trained strategies into a unified student network, thereby\nestablishing a robust control strategy. Real-world experiments demonstrate that\nthe proposed method successfully constructs the robust control strategy for\nhigh-precision assembly task with different fit types and fit amounts.\nMoreover, the MTRL framework significantly improves training efficiency, and\nthe final developed control strategy achieves superior force compliance and\nhigher success rate compared with many existing methods."}
{"id": "2508.12312", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12312", "abs": "https://arxiv.org/abs/2508.12312", "authors": ["Marco Leon Rapp"], "title": "Implementation and evaluation of a prediction algorithm for an autonomous vehicle", "comment": "7 pages, 7 figures", "summary": "This paper presents a prediction algorithm that estimates the vehicle\ntrajectory every five milliseconds for an autonomous vehicle. A kinematic and a\ndynamic bicycle model are compared, with the dynamic model exhibiting superior\naccuracy at higher speeds. Vehicle parameters such as mass, center of gravity,\nmoment of inertia, and cornering stiffness are determined experimentally. For\ncornering stiffness, a novel measurement procedure using optical position\ntracking is introduced. The model is incorporated into an extended Kalman\nfilter and implemented in a ROS node in C++. The algorithm achieves a\npositional deviation of only 1.25 cm per meter over the entire test drive and\nis up to 82.6% more precise than the kinematic model."}
{"id": "2508.12335", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12335", "abs": "https://arxiv.org/abs/2508.12335", "authors": ["Yunfan Gao", "Florian Messerer", "Niels van Duijkeren", "Rashmi Dabir", "Moritz Diehl"], "title": "Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control", "comment": "21 pages, 15 figures", "summary": "This paper presents a novel approach for collision avoidance in optimal and\nmodel predictive control, in which the environment is represented by a large\nnumber of points and the robot as a union of padded polygons. The conditions\nthat none of the points shall collide with the robot can be written in terms of\nan infinite number of constraints per obstacle point. We show that the\nresulting semi-infinite programming (SIP) optimal control problem (OCP) can be\nefficiently tackled through a combination of two methods: local reduction and\nan external active-set method. Specifically, this involves iteratively\nidentifying the closest point obstacles, determining the lower-level distance\nminimizer among all feasible robot shape parameters, and solving the\nupper-level finitely-constrained subproblems.\n  In addition, this paper addresses robust collision avoidance in the presence\nof ellipsoidal state uncertainties. Enforcing constraint satisfaction over all\npossible uncertainty realizations extends the dimension of constraint\ninfiniteness. The infinitely many constraints arising from translational\nuncertainty are handled by local reduction together with the robot shape\nparameterization, while rotational uncertainty is addressed via a backoff\nreformulation.\n  A controller implemented based on the proposed method is demonstrated on a\nreal-world robot running at 20Hz, enabling fast and collision-free navigation\nin tight spaces. An application to 3D collision avoidance is also demonstrated\nin simulation."}
{"id": "2508.12394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12394", "abs": "https://arxiv.org/abs/2508.12394", "authors": ["Zichen Yan", "Rui Huang", "Lei He", "Shao Guo", "Lin Zhao"], "title": "SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning", "comment": null, "summary": "Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an\nunknown environment and reaching a location that visually matches a given\ntarget image. While prior works primarily study ImageNav for ground robots,\nenabling this capability for autonomous drones is substantially more\nchallenging due to their need for high-frequency feedback control and global\nlocalization for stable flight. In this paper, we propose a novel sim-to-real\nframework that leverages visual reinforcement learning (RL) to achieve ImageNav\nfor drones. To enhance visual representation ability, our approach trains the\nvision backbone with auxiliary tasks, including image perturbations and future\ntransition prediction, which results in more effective policy training. The\nproposed algorithm enables end-to-end ImageNav with direct velocity control,\neliminating the need for external localization. Furthermore, we integrate a\ndepth-based safety module for real-time obstacle avoidance, allowing the drone\nto safely navigate in cluttered environments. Unlike most existing drone\nnavigation methods that focus solely on reference tracking or obstacle\navoidance, our framework supports comprehensive navigation\nbehaviors--autonomous exploration, obstacle avoidance, and image-goal\nseeking--without requiring explicit global mapping. Code and model checkpoints\nwill be released upon acceptance."}
{"id": "2508.12395", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12395", "abs": "https://arxiv.org/abs/2508.12395", "authors": ["Zihan Wang"], "title": "PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting", "comment": null, "summary": "This study presents the design and control of a Plasma-propelled\nUltra-silence Blimp (PUB), a novel aerial robot employing plasma vector\npropulsion for ultra-quiet flight without mechanical propellers. The system\nutilizes a helium-lift platform for extended endurance and a four-layer ring\nasymmetric capacitor to generate ionic wind thrust. The modular propulsion\nunits allow flexible configuration to meet mission-specific requirements, while\na two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop\nslip control scheme is implemented for stable maneuvering. Flight experiments\ndemonstrate full-envelope capability, including take-off, climb, hover,\ndescent, and smooth landing, confirming the feasibility of plasma vector\npropulsion, the effectiveness of DOF vector control, and the stability of the\ncontrol system. Owing to its low acoustic signature, structural simplicity, and\nhigh maneuverability, PUB is well suited for noise-sensitive, enclosed, and\nnear-space applications."}
{"id": "2508.12435", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12435", "abs": "https://arxiv.org/abs/2508.12435", "authors": ["Deqing Song", "Weimin Yang", "Maryam Rezayati", "Hans Wernher van de Venn"], "title": "Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots", "comment": null, "summary": "While gesture recognition using vision or robot skins is an active research\narea in Human-Robot Collaboration (HRC), this paper explores deep learning\nmethods relying solely on a robot's built-in joint sensors, eliminating the\nneed for external sensors. We evaluated various convolutional neural network\n(CNN) architectures and collected two datasets to study the impact of data\nrepresentation and model architecture on the recognition accuracy. Our results\nshow that spectrogram-based representations significantly improve accuracy,\nwhile model architecture plays a smaller role. We also tested generalization to\nnew robot poses, where spectrogram-based models performed better. Implemented\non a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,\nachieved over 95% accuracy in contact detection and gesture classification.\nThese findings demonstrate the feasibility of external-sensor-free tactile\nrecognition and promote further research toward cost-effective, scalable\nsolutions for HRC."}
{"id": "2508.12439", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12439", "abs": "https://arxiv.org/abs/2508.12439", "authors": ["Sunyu Wang", "Arjun S. Lakshmipathy", "Jean Oh", "Nancy S. Pollard"], "title": "Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation", "comment": null, "summary": "Reasoning about rolling and sliding contact, or roll-slide contact for short,\nis critical for dexterous manipulation tasks that involve intricate geometries.\nBut existing works on roll-slide contact mostly focus on continuous shapes with\ndifferentiable parametrizations. This work extends roll-slide contact modeling\nto manifold meshes. Specifically, we present an integration scheme based on\ngeodesic tracing to first-order time-integrate roll-slide contact directly on\nmeshes, enabling dexterous manipulation to reason over high-fidelity discrete\nrepresentations of an object's true geometry. Using our method, we planned\ndexterous motions of a multi-finger robotic hand manipulating five objects\nin-hand in simulation. The planning was achieved with a least-squares optimizer\nthat strives to maintain the most stable instantaneous grasp by minimizing\ncontact sliding and spinning. Then, we evaluated our method against a baseline\nusing collision detection and a baseline using primitive shapes. The results\nshow that our method performed the best in accuracy and precision, even for\ncoarse meshes. We conclude with a future work discussion on incorporating\nmultiple contacts and contact forces to achieve accurate and robust mesh-based\nsurface contact modeling."}
{"id": "2508.12456", "categories": ["cs.RO", "68T07, 93C85, 86A05", "I.2.6; I.2.9; J.2"], "pdf": "https://arxiv.org/pdf/2508.12456", "abs": "https://arxiv.org/abs/2508.12456", "authors": ["Hadas C. Kuzmenko", "David Ehevich", "Oren Gal"], "title": "Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics", "comment": "30 pages, 40 figures. Framework combining Liquid Time-Constant Neural\n  Networks with autonomous marine robotics for oil spill trajectory prediction\n  and response coordination", "summary": "Marine oil spills pose grave environmental and economic risks, threatening\nmarine ecosystems, coastlines, and dependent industries. Predicting and\nmanaging oil spill trajectories is highly complex, due to the interplay of\nphysical, chemical, and environmental factors such as wind, currents, and\ntemperature, which makes timely and effective response challenging. Accurate\nreal-time trajectory forecasting and coordinated mitigation are vital for\nminimizing the impact of these disasters. This study introduces an integrated\nframework combining a multi-agent swarm robotics system built on the MOOS-IvP\nplatform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system\nfuses adaptive machine learning with autonomous marine robotics, enabling\nreal-time prediction, dynamic tracking, and rapid response to evolving oil\nspills. By leveraging LTCNs--well-suited for modeling complex, time-dependent\nprocesses--the framework achieves real-time, high-accuracy forecasts of spill\nmovement. Swarm intelligence enables decentralized, scalable, and resilient\ndecision-making among robot agents, enhancing collective monitoring and\ncontainment efforts. Our approach was validated using data from the Deepwater\nHorizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,\nsurpassing LSTM approaches by 23%. The integration of advanced neural modeling\nwith autonomous, coordinated robotics demonstrates substantial improvements in\nprediction precision, flexibility, and operational scalability. Ultimately,\nthis research advances the state-of-the-art for sustainable, autonomous oil\nspill management and environmental protection by enhancing both trajectory\nprediction and response coordination."}
{"id": "2508.12469", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12469", "abs": "https://arxiv.org/abs/2508.12469", "authors": ["Abhinav Chalise", "Nimesh Gopal Pradhan", "Nishan Khanal", "Prashant Raj Bista", "Dinesh Baniya Kshatri"], "title": "Mechanical Automation with Vision: A Design for Rubik's Cube Solver", "comment": "Presented at the 15th IOE Graduate Conference, Tribhuvan University,\n  May 2024. Original paper available at\n  https://conference.ioe.edu.np/publications/ioegc15/IOEGC-15-023-C1-2-42.pdf", "summary": "The core mechanical system is built around three stepper motors for physical\nmanipulation, a microcontroller for hardware control, a camera and YOLO\ndetection model for real-time cube state detection. A significant software\ncomponent is the development of a user-friendly graphical user interface (GUI)\ndesigned in Unity. The initial state after detection from real-time YOLOv8\nmodel (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)\nis virtualized on GUI. To get the solution, the system employs the Kociemba's\nalgorithm while physical manipulation with a single degree of freedom is done\nby combination of stepper motors' interaction with the cube achieving the\naverage solving time of ~2.2 minutes."}
{"id": "2508.12554", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12554", "abs": "https://arxiv.org/abs/2508.12554", "authors": ["Hamza El-Kebir"], "title": "PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions", "comment": "Accepted for presentation at the 2025 IEEE Conference on Decision and\n  Control (CDC)", "summary": "We introduce PROD (Palpative Reconstruction of Deformables), a novel method\nfor reconstructing the shape and mechanical properties of deformable objects\nusing elastostatic signed distance functions (SDFs). Unlike traditional\napproaches that rely on purely geometric or visual data, PROD integrates\npalpative interaction -- measured through force-controlled surface probing --\nto estimate both the static and dynamic response of soft materials. We model\nthe deformation of an object as an elastostatic process and derive a governing\nPoisson equation for estimating its SDF from a sparse set of pose and force\nmeasurements. By incorporating steady-state elastodynamic assumptions, we show\nthat the undeformed SDF can be recovered from deformed observations with\nprovable convergence. Our approach also enables the estimation of material\nstiffness by analyzing displacement responses to varying force inputs. We\ndemonstrate the robustness of PROD in handling pose errors, non-normal force\napplication, and curvature errors in simulated soft body interactions. These\ncapabilities make PROD a powerful tool for reconstructing deformable objects in\napplications ranging from robotic manipulation to medical imaging and haptic\nfeedback systems."}
{"id": "2508.12564", "categories": ["cs.RO", "cs.CV", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.12564", "abs": "https://arxiv.org/abs/2508.12564", "authors": ["Jiayao Mai", "Xiuyuan Lu", "Kuan Dai", "Shaojie Shen", "Yi Zhou"], "title": "Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems", "comment": "8 pages, 5 figures", "summary": "Event cameras generate asynchronous signals in response to pixel-level\nbrightness changes, offering a sensing paradigm with theoretically\nmicrosecond-scale latency that can significantly enhance the performance of\nmulti-sensor systems. Extrinsic calibration is a critical prerequisite for\neffective sensor fusion; however, the configuration that involves event cameras\nremains an understudied topic. In this paper, we propose a motion-based\ntemporal and rotational calibration framework tailored for event-centric\nmulti-sensor systems, eliminating the need for dedicated calibration targets.\nOur method uses as input the rotational motion estimates obtained from event\ncameras and other heterogeneous sensors, respectively. Different from\nconventional approaches that rely on event-to-frame conversion, our method\nefficiently estimates angular velocity from normal flow observations, which are\nderived from the spatio-temporal profile of event data. The overall calibration\npipeline adopts a two-step approach: it first initializes the temporal offset\nand rotational extrinsics by exploiting kinematic correlations in the spirit of\nCanonical Correlation Analysis (CCA), and then refines both temporal and\nrotational parameters through a joint non-linear optimization using a\ncontinuous-time parametrization in SO(3). Extensive evaluations on both\npublicly available and self-collected datasets validate that the proposed\nmethod achieves calibration accuracy comparable to target-based methods, while\nexhibiting superior stability over purely CCA-based methods, and highlighting\nits precision, robustness and flexibility. To facilitate future research, our\nimplementation will be made open-source. Code:\nhttps://github.com/NAIL-HNU/EvMultiCalib."}
{"id": "2508.12681", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12681", "abs": "https://arxiv.org/abs/2508.12681", "authors": ["Johann Licher", "Max Bartholdt", "Henrik Krauss", "Tim-Lukas Habich", "Thomas Seel", "Moritz Schappler"], "title": "Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory", "comment": "20 pages, 15 figures", "summary": "Dynamic control of soft continuum robots (SCRs) holds great potential for\nexpanding their applications, but remains a challenging problem due to the high\ncomputational demands of accurate dynamic models. While data-driven approaches\nlike Koopman-operator-based methods have been proposed, they typically lack\nadaptability and cannot capture the full robot shape, limiting their\napplicability. This work introduces a real-time-capable nonlinear\nmodel-predictive control (MPC) framework for SCRs based on a domain-decoupled\nphysics-informed neural network (DD-PINN) with adaptable bending stiffness. The\nDD-PINN serves as a surrogate for the dynamic Cosserat rod model with a\nspeed-up factor of 44000. It is also used within an unscented Kalman filter for\nestimating the model states and bending compliance from end-effector position\nmeasurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the\nGPU. In simulation, it demonstrates accurate tracking of dynamic trajectories\nand setpoint control with end-effector position errors below 3 mm (2.3% of the\nactuator's length). In real-world experiments, the controller achieves similar\naccuracy and accelerations up to 3.55 m/s2."}
{"id": "2508.12729", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12729", "abs": "https://arxiv.org/abs/2508.12729", "authors": ["Junhao Ye", "Cheng Hu", "Yiqin Wang", "Weizhan Huang", "Nicolas Baumann", "Jie He", "Meixun Qu", "Lei Xie", "Hongye Su"], "title": "MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA", "comment": null, "summary": "In autonomous racing, reactive controllers eliminate the computational burden\nof the full See-Think-Act autonomy stack by directly mapping sensor inputs to\ncontrol actions. This bypasses the need for explicit localization and\ntrajectory planning. A widely adopted baseline in this category is the\nFollow-The-Gap method, which performs trajectory planning using LiDAR data.\nBuilding on FTG, the Delaunay Triangulation-based Racing algorithm introduces\nfurther enhancements. However, DTR's use of circumcircles for trajectory\ngeneration often results in insufficiently smooth paths, ultimately degrading\nperformance. Additionally, the commonly used F1TENTH-simulator for autonomous\nracing competitions lacks support for 3D LiDAR perception, limiting its\neffectiveness in realistic testing. To address these challenges, this work\nproposes the MCTR algorithm. MCTR improves trajectory smoothness through the\nuse of Curvature Corrected Moving Average and implements a digital twin system\nwithin the CARLA simulator to validate the algorithm's robustness under 3D\nLiDAR perception. The proposed algorithm has been thoroughly validated through\nboth simulation and real-world vehicle experiments."}
{"id": "2508.12916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12916", "abs": "https://arxiv.org/abs/2508.12916", "authors": ["Hecheng Wang", "Jiankun Ren", "Jia Yu", "Lizhe Qi", "Yunquan Sun"], "title": "RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph", "comment": null, "summary": "Humans effortlessly retrieve objects in cluttered, partially observable\nenvironments by combining visual reasoning, active viewpoint adjustment, and\nphysical interaction-with only a single pair of eyes. In contrast, most\nexisting robotic systems rely on carefully positioned fixed or multi-camera\nsetups with complete scene visibility, which limits adaptability and incurs\nhigh hardware costs. We present \\textbf{RoboRetriever}, a novel framework for\nreal-world object retrieval that operates using only a \\textbf{single}\nwrist-mounted RGB-D camera and free-form natural language instructions.\nRoboRetriever grounds visual observations to build and update a \\textbf{dynamic\nhierarchical scene graph} that encodes object semantics, geometry, and\ninter-object relations over time. The supervisor module reasons over this\nmemory and task instruction to infer the target object and coordinate an\nintegrated action module combining \\textbf{active perception},\n\\textbf{interactive perception}, and \\textbf{manipulation}. To enable\ntask-aware scene-grounded active perception, we introduce a novel visual\nprompting scheme that leverages large reasoning vision-language models to\ndetermine 6-DoF camera poses aligned with the semantic task goal and geometry\nscene context. We evaluate RoboRetriever on diverse real-world object retrieval\ntasks, including scenarios with human intervention, demonstrating strong\nadaptability and robustness in cluttered scenes with only one RGB-D camera."}
{"id": "2508.12925", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12925", "abs": "https://arxiv.org/abs/2508.12925", "authors": ["Eetu Laukka", "Evan G. Center", "Timo Ojala", "Steven M. LaValle", "Matti Pouke"], "title": "Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users", "comment": "2025 IEEE Conference on Telepresence", "summary": "Mobile telepresence robots allow users to feel present and explore remote\nenvironments using technology. Traditionally, these systems are implemented\nusing a camera onboard a mobile robot that can be controlled. Although\nhigh-immersion technologies, such as 360-degree cameras, can increase\nsituational awareness and presence, they also introduce significant challenges.\nAdditional processing and bandwidth requirements often result in latencies of\nup to seconds. The current delay with a 360-degree camera streaming over the\ninternet makes real-time control of these systems difficult. Working with\nhigh-latency systems requires some form of assistance to the users.\n  This study presents a novel way to utilize optical flow to create an illusion\nof self-motion to the user during the latency period between user sending\nmotion commands to the robot and seeing the actual motion through the\n360-camera stream. We find no significant benefit of using the self-motion\nillusion to performance or accuracy of controlling a telepresence robot with a\nlatency of 500 ms, as measured by the task completion time and collisions into\nobjects. Some evidence is shown that the method might increase virtual reality\n(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We\nconclude that further adjustments are necessary in order to render the method\nviable."}
{"id": "2508.12928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12928", "abs": "https://arxiv.org/abs/2508.12928", "authors": ["Victor Dhédin", "Haizhou Zhao", "Majid Khadiv"], "title": "Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion", "comment": null, "summary": "Legged robots have the potential to traverse highly constrained environments\nwith agile maneuvers. However, planning such motions requires solving a highly\nchallenging optimization problem with a mixture of continuous and discrete\ndecision variables. In this paper, we present a full pipeline based on\nMonte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to\nperform simultaneous contact sequence and patch selection on highly challenging\nenvironments. Through extensive simulation experiments, we show that our\nframework can quickly find a diverse set of dynamically consistent plans. We\nexperimentally show that these plans are transferable to a real quadruped\nrobot. We further show that the same framework can find highly complex acyclic\nhumanoid maneuvers. To the best of our knowledge, this is the first\ndemonstration of simultaneous contact sequence and patch selection for acyclic\nmulti-contact locomotion using the whole-body dynamics of a quadruped."}
{"id": "2508.12946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12946", "abs": "https://arxiv.org/abs/2508.12946", "authors": ["Ann-Sophie Schenk", "Stefan Schiffer", "Heqiu Song"], "title": "Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade", "comment": null, "summary": "In this paper we report on first insights from interviews with teachers and\nstudents on using social robots in computer science class in sixth grade. Our\nfocus is on learning about requirements and potential applications. We are\nparticularly interested in getting both perspectives, the teachers' and the\nlearners' view on how robots could be used and what features they should or\nshould not have. Results show that teachers as well as students are very open\nto robots in the classroom. However, requirements are partially quite\nheterogeneous among the groups. This leads to complex design challenges which\nwe discuss at the end of this paper."}
{"id": "2508.12980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12980", "abs": "https://arxiv.org/abs/2508.12980", "authors": ["Victor Levé", "João Moura", "Sachiya Fujita", "Tamon Miyake", "Steve Tonneau", "Sethu Vijayakumar"], "title": "Scaling Whole-body Multi-contact Manipulation with Contact Optimization", "comment": "This work has been accepted for publication in IEEE-RAS 24th\n  International Conference on Humanoid Robots (Humanoids 2025). Copyrights to\n  IEEE", "summary": "Daily tasks require us to use our whole body to manipulate objects, for\ninstance when our hands are unavailable. We consider the issue of providing\nhumanoid robots with the ability to autonomously perform similar whole-body\nmanipulation tasks. In this context, the infinite possibilities for where and\nhow contact can occur on the robot and object surfaces hinder the scalability\nof existing planning methods, which predominantly rely on discrete sampling.\nGiven the continuous nature of contact surfaces, gradient-based optimization\noffers a more suitable approach for finding solutions. However, a key remaining\nchallenge is the lack of an efficient representation of robot surfaces. In this\nwork, we propose (i) a representation of robot and object surfaces that enables\nclosed-form computation of proximity points, and (ii) a cost design that\neffectively guides whole-body manipulation planning. Our experiments\ndemonstrate that the proposed framework can solve problems unaddressed by\nexisting methods, and achieves a 77% improvement in planning time over the\nstate of the art. We also validate the suitability of our approach on real\nhardware through the whole-body manipulation of boxes by a humanoid robot."}
{"id": "2508.13052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13052", "abs": "https://arxiv.org/abs/2508.13052", "authors": ["Sourav Raxit", "Abdullah Al Redwan Newaz", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla"], "title": "BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments", "comment": null, "summary": "This paper introduces the BOW Planner, a scalable motion planning algorithm\ndesigned to navigate robots through complex environments using constrained\nBayesian optimization (CBO). Unlike traditional methods, which often struggle\nwith kinodynamic constraints such as velocity and acceleration limits, the BOW\nPlanner excels by concentrating on a planning window of reachable velocities\nand employing CBO to sample control inputs efficiently. This approach enables\nthe planner to manage high-dimensional objective functions and stringent safety\nconstraints with minimal sampling, ensuring rapid and secure trajectory\ngeneration. Theoretical analysis confirms the algorithm's asymptotic\nconvergence to near-optimal solutions, while extensive evaluations in cluttered\nand constrained settings reveal substantial improvements in computation times,\ntrajectory lengths, and solution times compared to existing techniques.\nSuccessfully deployed across various real-world robotic systems, the BOW\nPlanner demonstrates its practical significance through exceptional sample\nefficiency, safety-aware optimization, and rapid planning capabilities, making\nit a valuable tool for advancing robotic applications. The BOW Planner is\nreleased as an open-source package and videos of real-world and simulated\nexperiments are available at https://bow-web.github.io."}
{"id": "2508.13073", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13073", "abs": "https://arxiv.org/abs/2508.13073", "authors": ["Rui Shao", "Wei Li", "Lingsen Zhang", "Renshan Zhang", "Zhiyang Liu", "Ran Chen", "Liqiang Nie"], "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey", "comment": "Project Page:\n  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation", "summary": "Robotic manipulation, a key frontier in robotics and embodied AI, requires\nprecise motor control and multimodal understanding, yet traditional rule-based\nmethods fail to scale or generalize in unstructured, novel environments. In\nrecent years, Vision-Language-Action (VLA) models, built upon Large\nVision-Language Models (VLMs) pretrained on vast image-text datasets, have\nemerged as a transformative paradigm. This survey provides the first\nsystematic, taxonomy-oriented review of large VLM-based VLA models for robotic\nmanipulation. We begin by clearly defining large VLM-based VLA models and\ndelineating two principal architectural paradigms: (1) monolithic models,\nencompassing single-system and dual-system designs with differing levels of\nintegration; and (2) hierarchical models, which explicitly decouple planning\nfrom execution via interpretable intermediate representations. Building on this\nfoundation, we present an in-depth examination of large VLM-based VLA models:\n(1) integration with advanced domains, including reinforcement learning,\ntraining-free optimization, learning from human videos, and world model\nintegration; (2) synthesis of distinctive characteristics, consolidating\narchitectural traits, operational strengths, and the datasets and benchmarks\nthat support their development; (3) identification of promising directions,\nincluding memory mechanisms, 4D perception, efficient adaptation, multi-agent\ncooperation, and other emerging capabilities. This survey consolidates recent\nadvances to resolve inconsistencies in existing taxonomies, mitigate research\nfragmentation, and fill a critical gap through the systematic integration of\nstudies at the intersection of large VLMs and robotic manipulation. We provide\na regularly updated project page to document ongoing progress:\nhttps://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation."}
{"id": "2508.13103", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13103", "abs": "https://arxiv.org/abs/2508.13103", "authors": ["Tianyi Zhang", "Haonan Duan", "Haoran Hao", "Yu Qiao", "Jifeng Dai", "Zhi Hou"], "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy", "comment": null, "summary": "Vision-Language-Action (VLA) models frequently encounter challenges in\ngeneralizing to real-world environments due to inherent discrepancies between\nobservation and action spaces. Although training data are collected from\ndiverse camera perspectives, the models typically predict end-effector poses\nwithin the robot base coordinate frame, resulting in spatial inconsistencies.\nTo mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)\nframework, which grounds action predictions directly in the camera observation\nspace. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms\nend-effector poses from the robot base coordinate system into the camera\ncoordinate system, thereby unifying prediction targets across heterogeneous\nviewpoints. This lightweight, plug-and-play strategy ensures robust alignment\nbetween perception and action, substantially improving model resilience to\ncamera viewpoint variations. The proposed approach is readily compatible with\nexisting VLA architectures, requiring no substantial modifications.\nComprehensive evaluations on both simulated and real-world robotic manipulation\ntasks demonstrate that OC-VLA accelerates convergence, enhances task success\nrates, and improves cross-view generalization. The code will be publicly\navailable."}
{"id": "2508.13151", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13151", "abs": "https://arxiv.org/abs/2508.13151", "authors": ["Yuying Zhang", "Joni Pajarinen"], "title": "Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors", "comment": null, "summary": "Mobile manipulation in dynamic environments is challenging due to movable\nobstacles blocking the robot's path. Traditional methods, which treat\nnavigation and manipulation as separate tasks, often fail in such\n'manipulate-to-navigate' scenarios, as obstacles must be removed before\nnavigation. In these cases, active interaction with the environment is required\nto clear obstacles while ensuring sufficient space for movement. To address the\nmanipulate-to-navigate problem, we propose a reinforcement learning-based\napproach for learning manipulation actions that facilitate subsequent\nnavigation. Our method combines manipulability priors to focus the robot on\nhigh manipulability body positions with affordance maps for selecting\nhigh-quality manipulation actions. By focusing on feasible and meaningful\nactions, our approach reduces unnecessary exploration and allows the robot to\nlearn manipulation strategies more effectively. We present two new\nmanipulate-to-navigate simulation tasks called Reach and Door with the Boston\nDynamics Spot robot. The first task tests whether the robot can select a good\nhand position in the target area such that the robot base can move effectively\nforward while keeping the end effector position fixed. The second task requires\nthe robot to move a door aside in order to clear the navigation path. Both of\nthese tasks need first manipulation and then navigating the base forward.\nResults show that our method allows a robot to effectively interact with and\ntraverse dynamic environments. Finally, we transfer the learned policy to a\nreal Boston Dynamics Spot robot, which successfully performs the Reach task."}
