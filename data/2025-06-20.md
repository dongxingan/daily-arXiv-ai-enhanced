<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers](https://arxiv.org/abs/2506.14855)
*Tommaso Belvedere,Michael Ziegltrum,Giulio Turrisi,Valerio Modugno*

Main category: cs.RO

TL;DR: F-MPPI是一种改进的MPPI方法，通过引入局部线性反馈增益提升实时控制性能，适用于复杂机器人任务。


<details>
  <summary>Details</summary>
Motivation: 标准MPPI在实时高频控制中计算需求高，限制了其应用。

Method: F-MPPI结合灵敏度分析计算局部线性反馈增益，避免每一步的完全重新优化。

Result: 在四足机器人和四旋翼平台上验证，显著提升控制性能和稳定性。

Conclusion: F-MPPI通过局部反馈实现了复杂机器人系统的高频鲁棒控制。

Abstract: Model Predictive Path Integral control is a powerful sampling-based approach suitable for complex robotic tasks due to its flexibility in handling nonlinear dynamics and non-convex costs. However, its applicability in real-time, highfrequency robotic control scenarios is limited by computational demands. This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments standard MPPI by computing local linear feedback gains derived from sensitivity analysis inspired by Riccati-based feedback used in gradient-based MPC. These gains allow for rapid closed-loop corrections around the current state without requiring full re-optimization at each timestep. We demonstrate the effectiveness of F-MPPI through simulations and real-world experiments on two robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven terrain and a quadrotor executing aggressive maneuvers with onboard computation. Results illustrate that incorporating local feedback significantly improves control performance and stability, enabling robust, high-frequency operation suitable for complex robotic systems.

</details>


### [2] [Towards Perception-based Collision Avoidance for UAVs when Guiding the Visually Impaired](https://arxiv.org/abs/2506.14857)
*Suman Raj,Swapnil Padhi,Ruchi Bhoot,Prince Modi,Yogesh Simmhan*

Main category: cs.RO

TL;DR: 论文提出了一种基于感知的路径规划系统，结合无人机和机器学习技术，帮助视障人士在户外城市环境中导航。


<details>
  <summary>Details</summary>
Motivation: 利用无人机和机器学习技术解决视障人士在户外导航中的障碍物避让问题。

Method: 采用几何问题建模和多DNN框架，结合局部感知规划和全局GPS地图规划。

Result: 在校园环境中验证了算法在三种场景下的可行性：人行道行走、停车场附近和拥挤街道。

Conclusion: 提出的系统在视障人士导航中具有实际应用潜力。

Abstract: Autonomous navigation by drones using onboard sensors combined with machine learning and computer vision algorithms is impacting a number of domains, including agriculture, logistics, and disaster management. In this paper, we examine the use of drones for assisting visually impaired people (VIPs) in navigating through outdoor urban environments. Specifically, we present a perception-based path planning system for local planning around the neighborhood of the VIP, integrated with a global planner based on GPS and maps for coarse planning. We represent the problem using a geometric formulation and propose a multi DNN based framework for obstacle avoidance of the UAV as well as the VIP. Our evaluations conducted on a drone human system in a university campus environment verifies the feasibility of our algorithms in three scenarios; when the VIP walks on a footpath, near parked vehicles, and in a crowded street.

</details>


### [3] [Efficient and Real-Time Motion Planning for Robotics Using Projection-Based Optimization](https://arxiv.org/abs/2506.14865)
*Xuemin Chi,Hakan Girgin,Tobias Löw,Yangyang Xie,Teng Xue,Jihao Huang,Cheng Hu,Zhitao Liu,Sylvain Calinon*

Main category: cs.RO

TL;DR: 提出了一种高效的一阶方法ALSPG，通过几何投影优化机器人运动生成，显著提升实时性能。


<details>
  <summary>Details</summary>
Motivation: 机器人运动生成在处理不同形状物体时面临复杂挑战，现有方法常局限于特定问题或未充分利用几何约束。

Method: 采用ALSPG方法，结合欧几里得投影、Minkowski和基函数，利用几何约束而非完整约束和梯度。

Result: ALSPG在实时性能上显著优于现有方法，且在无约束情况下与二阶方法（如iLQR）竞争。

Conclusion: ALSPG在仿真和真实机器人实验中验证了其有效性，适用于多种机器人平台。

Abstract: Generating motions for robots interacting with objects of various shapes is a complex challenge, further complicated by the robot geometry and multiple desired behaviors. While current robot programming tools (such as inverse kinematics, collision avoidance, and manipulation planning) often treat these problems as constrained optimization, many existing solvers focus on specific problem domains or do not exploit geometric constraints effectively. We propose an efficient first-order method, Augmented Lagrangian Spectral Projected Gradient Descent (ALSPG), which leverages geometric projections via Euclidean projections, Minkowski sums, and basis functions. We show that by using geometric constraints rather than full constraints and gradients, ALSPG significantly improves real-time performance. Compared to second-order methods like iLQR, ALSPG remains competitive in the unconstrained case. We validate our method through toy examples and extensive simulations, and demonstrate its effectiveness on a 7-axis Franka robot, a 6-axis P-Rob robot and a 1:10 scale car in real-world experiments. Source codes, experimental data and videos are available on the project webpage: https://sites.google.com/view/alspg-oc

</details>


### [4] [FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization](https://arxiv.org/abs/2506.14968)
*Rajat Kumar Jenamani,Tom Silver,Ben Dodson,Shiqin Tong,Anthony Song,Yuting Yang,Ziang Liu,Benjamin Howe,Aimee Whitneck,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: FEAST是一个灵活的进餐辅助系统，旨在满足个性化需求，通过模块化硬件、多样化交互方式和可调行为树实现适应性、透明性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决家庭环境中进餐辅助机器人面临的多样性和个性化需求挑战。

Method: 采用模块化硬件、多种交互方式（如网页界面、头部手势、物理按钮）和基于大语言模型的可调行为树。

Result: FEAST在透明性和安全性上优于现有基线，并通过家庭用户研究和职业治疗师评估验证了其实用性。

Conclusion: FEAST成功满足了个性化需求，展示了在真实环境中的适用性。

Abstract: Physical caregiving robots hold promise for improving the quality of life of millions worldwide who require assistance with feeding. However, in-home meal assistance remains challenging due to the diversity of activities (e.g., eating, drinking, mouth wiping), contexts (e.g., socializing, watching TV), food items, and user preferences that arise during deployment. In this work, we propose FEAST, a flexible mealtime-assistance system that can be personalized in-the-wild to meet the unique needs of individual care recipients. Developed in collaboration with two community researchers and informed by a formative study with a diverse group of care recipients, our system is guided by three key tenets for in-the-wild personalization: adaptability, transparency, and safety. FEAST embodies these principles through: (i) modular hardware that enables switching between assisted feeding, drinking, and mouth-wiping, (ii) diverse interaction methods, including a web interface, head gestures, and physical buttons, to accommodate diverse functional abilities and preferences, and (iii) parameterized behavior trees that can be safely and transparently adapted using a large language model. We evaluate our system based on the personalization requirements identified in our formative study, demonstrating that FEAST offers a wide range of transparent and safe adaptations and outperforms a state-of-the-art baseline limited to fixed customizations. To demonstrate real-world applicability, we conduct an in-home user study with two care recipients (who are community researchers), feeding them three meals each across three diverse scenarios. We further assess FEAST's ecological validity by evaluating with an Occupational Therapist previously unfamiliar with the system. In all cases, users successfully personalize FEAST to meet their individual needs and preferences. Website: https://emprise.cs.cornell.edu/feast

</details>


### [5] [Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation](https://arxiv.org/abs/2506.15150)
*Yuanlong Ji,Xingbang Yang,Ruoqi Zhao,Qihan Ye,Quan Zheng,Yubo Fan*

Main category: cs.RO

TL;DR: 提出了一种基于IMU信号的步态相位估计神经网络，结合时间卷积和Transformer层，通过通道掩码重建预训练策略提升模型泛化能力，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决步态相位估计在高精度和鲁棒性方面的挑战，特别是在地形变化时。

Method: 开发了一种基于隐式建模的步态相位估计神经网络，结合时间卷积和Transformer层，并提出通道掩码重建预训练策略。

Result: 在稳定地形和地形变化条件下，步态相位RMSE和相位率MAE均优于基线方法，硬件验证证实了算法的可靠性。

Conclusion: 该方法为更智能和自适应的外骨骼系统铺平了道路，提升了人机交互的安全性和效率。

Abstract: Gait phase estimation based on inertial measurement unit (IMU) signals facilitates precise adaptation of exoskeletons to individual gait variations. However, challenges remain in achieving high accuracy and robustness, particularly during periods of terrain changes. To address this, we develop a gait phase estimation neural network based on implicit modeling of human locomotion, which combines temporal convolution for feature extraction with transformer layers for multi-channel information fusion. A channel-wise masked reconstruction pre-training strategy is proposed, which first treats gait phase state vectors and IMU signals as joint observations of human locomotion, thus enhancing model generalization. Experimental results demonstrate that the proposed method outperforms existing baseline approaches, achieving a gait phase RMSE of $2.729 \pm 1.071%$ and phase rate MAE of $0.037 \pm 0.016%$ under stable terrain conditions with a look-back window of 2 seconds, and a phase RMSE of $3.215 \pm 1.303%$ and rate MAE of $0.050 \pm 0.023%$ under terrain transitions. Hardware validation on a hip exoskeleton further confirms that the algorithm can reliably identify gait cycles and key events, adapting to various continuous motion scenarios. This research paves the way for more intelligent and adaptive exoskeleton systems, enabling safer and more efficient human-robot interaction across diverse real-world environments.

</details>


### [6] [Time-Optimized Safe Navigation in Unstructured Environments through Learning Based Depth Completion](https://arxiv.org/abs/2506.14975)
*Jeffrey Mao,Raghuram Cauligi Srinivas,Steven Nogar,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 提出了一种基于轻量级传感器的四旋翼无人机实时导航系统，通过融合立体和单目深度估计构建密集3D地图，并实现时间最优的全局轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 四旋翼无人机在复杂未知环境中实现自主导航面临传感器和计算资源的限制，需解决实时路径规划和轨迹优化问题。

Method: 结合立体和单目学习深度估计构建密集3D地图，开发快速计算时间最优全局轨迹的规划框架。

Result: 系统在计算效率和避障性能上优于现有方法，并在多样环境中验证了其有效性。

Conclusion: 该系统为轻量级无人机在未知环境中的安全自主导航提供了高效解决方案。

Abstract: Quadrotors hold significant promise for several applications such as agriculture, search and rescue, and infrastructure inspection. Achieving autonomous operation requires systems to navigate safely through complex and unfamiliar environments. This level of autonomy is particularly challenging due to the complexity of such environments and the need for real-time decision making especially for platforms constrained by size, weight, and power (SWaP), which limits flight time and precludes the use of bulky sensors like Light Detection and Ranging (LiDAR) for mapping. Furthermore, computing globally optimal, collision-free paths and translating them into time-optimized, safe trajectories in real time adds significant computational complexity. To address these challenges, we present a fully onboard, real-time navigation system that relies solely on lightweight onboard sensors. Our system constructs a dense 3D map of the environment using a novel visual depth estimation approach that fuses stereo and monocular learning-based depth, yielding longer-range, denser, and less noisy depth maps than conventional stereo methods. Building on this map, we introduce a novel planning and trajectory generation framework capable of rapidly computing time-optimal global trajectories. As the map is incrementally updated with new depth information, our system continuously refines the trajectory to maintain safety and optimality. Both our planner and trajectory generator outperforms state-of-the-art methods in terms of computational efficiency and guarantee obstacle-free trajectories. We validate our system through robust autonomous flight experiments in diverse indoor and outdoor environments, demonstrating its effectiveness for safe navigation in previously unknown settings.

</details>


### [7] [Comparison of Innovative Strategies for the Coverage Problem: Path Planning, Search Optimization, and Applications in Underwater Robotics](https://arxiv.org/abs/2506.15376)
*Ahmed Ibrahim,Francisco F. C. Rego,Éric Busvelle*

Main category: cs.RO

TL;DR: 本文研究了水下滑翔机的覆盖路径规划策略，比较了TSP、MST和OCP三种方法，发现OCP在时间受限时更优，但计算成本高，而MST速度快但效果较差。


<details>
  <summary>Details</summary>
Motivation: 提升水下滑翔机在探测放射性源时的覆盖效率，同时确保安全导航。

Method: 通过MATLAB模拟比较TSP、MST和OCP三种路径规划方法，评估处理时间、未覆盖区域、路径长度和遍历时间。

Result: OCP在时间受限时表现最佳但计算成本高，MST速度快但效果较差。

Conclusion: 根据任务优先级选择算法，平衡效率和计算可行性。

Abstract: In many applications, including underwater robotics, the coverage problem requires an autonomous vehicle to systematically explore a defined area while minimizing redundancy and avoiding obstacles. This paper investigates coverage path planning strategies to enhance the efficiency of underwater gliders, particularly in maximizing the probability of detecting a radioactive source while ensuring safe navigation.
  We evaluate three path-planning approaches: the Traveling Salesman Problem (TSP), Minimum Spanning Tree (MST), and Optimal Control Problem (OCP). Simulations were conducted in MATLAB, comparing processing time, uncovered areas, path length, and traversal time. Results indicate that OCP is preferable when traversal time is constrained, although it incurs significantly higher computational costs. Conversely, MST-based approaches provide faster but less optimal solutions. These findings offer insights into selecting appropriate algorithms based on mission priorities, balancing efficiency and computational feasibility.

</details>


### [8] [Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots](https://arxiv.org/abs/2506.15009)
*Jinjie Li,Jiaxuan Li,Kotaro Kaneko,Liming Shu,Moju Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种利用人类手部全方位运动能力的无人机遥操作系统，通过肩部和手部动作捕捉以及手势识别，设计了四种交互模式，提升了无人机在复杂环境中的操控能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多旋翼无人机遥操作未能充分利用全方位旋转的自由度，且未充分挖掘人类手指的灵活性。本文旨在通过结合人类手部动作和手势识别，提升无人机在复杂环境中的操控能力。

Method: 系统包括肩部和手部的动作捕捉标记以及数据手套，设计了四种交互模式（球形模式、笛卡尔模式、操作模式和锁定模式），并通过手势实现模式切换。

Result: 在真实世界的阀门旋转任务中验证了系统的有效性，展示了每种模式对无人机操控的贡献。

Conclusion: 该交互框架将人类灵活性与无人机技术结合，为复杂环境中的无人机遥操作提供了新思路。

Abstract: Omnidirectional aerial robots offer full 6-DoF independent control over position and orientation, making them popular for aerial manipulation. Although advancements in robotic autonomy, operating by human remains essential in complex aerial environments. Existing teleoperation approaches for multirotors fail to fully leverage the additional DoFs provided by omnidirectional rotation. Additionally, the dexterity of human fingers should be exploited for more engaged interaction. In this work, we propose an aerial teleoperation system that brings the omnidirectionality of human hands into the unbounded aerial workspace. Our system includes two motion-tracking marker sets -- one on the shoulder and one on the hand -- along with a data glove to capture hand gestures. Using these inputs, we design four interaction modes for different tasks, including Spherical Mode and Cartesian Mode for long-range moving as well as Operation Mode and Locking Mode for precise manipulation, where the hand gestures are utilized for seamless mode switching. We evaluate our system on a valve-turning task in real world, demonstrating how each mode contributes to effective aerial manipulation. This interaction framework bridges human dexterity with aerial robotics, paving the way for enhanced teleoperated aerial manipulation in unstructured environments.

</details>


### [9] [Context Matters: Learning Generalizable Rewards via Calibrated Features](https://arxiv.org/abs/2506.15012)
*Alexandra Forsey-Smerek,Julie Shah,Andreea Bobu*

Main category: cs.RO

TL;DR: 论文提出了一种显式建模上下文不变偏好与上下文相关特征显著性的方法，通过校准特征和专用配对比较查询，显著提高了奖励学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将每个新上下文视为独立任务，忽略了底层偏好的不变性，导致数据需求大。本文观察到上下文影响的是特征显著性而非偏好本身，因此提出显式分离建模。

Method: 引入校准特征捕捉上下文对特征显著性的影响，设计专用配对比较查询以高效学习。

Result: 实验显示，该方法比基线方法样本效率提高10倍，低数据量下性能提升15%。用户研究证实其能有效学习个性化上下文偏好。

Conclusion: 显式分离偏好与特征显著性的方法显著提升了奖励学习的适应性和效率。

Abstract: A key challenge in reward learning from human input is that desired agent behavior often changes based on context. Traditional methods typically treat each new context as a separate task with its own reward function. For example, if a previously ignored stove becomes too hot to be around, the robot must learn a new reward from scratch, even though the underlying preference for prioritizing safety over efficiency remains unchanged. We observe that context influences not the underlying preference itself, but rather the $\textit{saliency}$--or importance--of reward features. For instance, stove heat affects the importance of the robot's proximity, yet the human's safety preference stays the same. Existing multi-task and meta IRL methods learn context-dependent representations $\textit{implicitly}$--without distinguishing between preferences and feature importance--resulting in substantial data requirements. Instead, we propose $\textit{explicitly}$ modeling context-invariant preferences separately from context-dependent feature saliency, creating modular reward representations that adapt to new contexts. To achieve this, we introduce $\textit{calibrated features}$--representations that capture contextual effects on feature saliency--and present specialized paired comparison queries that isolate saliency from preference for efficient learning. Experiments with simulated users show our method significantly improves sample efficiency, requiring 10x fewer preference queries than baselines to achieve equivalent reward accuracy, with up to 15% better performance in low-data regimes (5-10 queries). An in-person user study (N=12) demonstrates that participants can effectively teach their unique personal contextual preferences using our method, enabling more adaptable and personalized reward learning.

</details>


### [10] [Assigning Multi-Robot Tasks to Multitasking Robots](https://arxiv.org/abs/2506.15032)
*Winston Smith,Andrew Boateng,Taha Shaheen,Yu Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种多任务机器人任务分配框架，考虑了多任务引入的物理约束，并通过加权MAX-SAT编译和贪心启发式方法求解。实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有任务分配方法假设机器人是单任务的，这在某些情况下效率低下或不可行。本文旨在解决多任务机器人的任务分配问题。

Method: 提出了一种任务分配框架，考虑了多任务的物理约束，并通过加权MAX-SAT编译和贪心启发式方法求解。

Result: 实验验证了多任务机器人在合成域和实际场景中的性能优势。

Conclusion: 本文提出的多任务机器人任务分配框架能有效提升任务效率，适用于复杂任务交互场景。

Abstract: One simplifying assumption in existing and well-performing task allocation methods is that the robots are single-tasking: each robot operates on a single task at any given time. While this assumption is harmless to make in some situations, it can be inefficient or even infeasible in others. In this paper, we consider assigning multi-robot tasks to multitasking robots. The key contribution is a novel task allocation framework that incorporates the consideration of physical constraints introduced by multitasking. This is in contrast to the existing work where such constraints are largely ignored. After formulating the problem, we propose a compilation to weighted MAX-SAT, which allows us to leverage existing solvers for a solution. A more efficient greedy heuristic is then introduced. For evaluation, we first compare our methods with a modern baseline that is efficient for single-tasking robots to validate the benefits of multitasking in synthetic domains. Then, using a site-clearing scenario in simulation, we further illustrate the complex task interaction considered by the multitasking robots in our approach to demonstrate its performance. Finally, we demonstrate a physical experiment to show how multitasking enabled by our approach can benefit task efficiency in a realistic setting.

</details>


### [11] [EmojiVoice: Towards long-term controllable expressivity in robot speech](https://arxiv.org/abs/2506.15085)
*Paige Tuttösí,Shivam Mehta,Zachary Syvenky,Bermet Burkanova,Gustav Eje Henter,Angelica Lim*

Main category: cs.RO

TL;DR: EmojiVoice是一个免费、可定制的TTS工具包，用于为社交机器人提供长期变化的表达性语音，通过表情符号提示实现细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 社交机器人通常使用单调的‘快乐’语音，缺乏人类语音的长期变化，而现有TTS系统难以离线部署。

Method: 采用轻量级Matcha-TTS框架，结合表情符号提示技术，实现实时语音生成。

Result: 在讲故事任务中，表情符号提示提升了语音表达性和感知效果，但在助手场景中表达性语音不受欢迎。

Conclusion: EmojiVoice为社交机器人提供了灵活的表达性语音解决方案，但需根据场景调整使用。

Abstract: Humans vary their expressivity when speaking for extended periods to maintain engagement with their listener. Although social robots tend to be deployed with ``expressive'' joyful voices, they lack this long-term variation found in human speech. Foundation model text-to-speech systems are beginning to mimic the expressivity in human speech, but they are difficult to deploy offline on robots. We present EmojiVoice, a free, customizable text-to-speech (TTS) toolkit that allows social roboticists to build temporally variable, expressive speech on social robots. We introduce emoji-prompting to allow fine-grained control of expressivity on a phase level and use the lightweight Matcha-TTS backbone to generate speech in real-time. We explore three case studies: (1) a scripted conversation with a robot assistant, (2) a storytelling robot, and (3) an autonomous speech-to-speech interactive agent. We found that using varied emoji prompting improved the perception and expressivity of speech over a long period in a storytelling task, but expressive voice was not preferred in the assistant use case.

</details>


### [12] [3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception](https://arxiv.org/abs/2506.15087)
*Yuankai Lin,Xiaofan Lu,Jiahui Chen,Hua Yang*

Main category: cs.RO

TL;DR: 论文提出了一种改进的视觉触觉传感器（VTS）设计，通过引入GelSplitter3D、光测立体神经网络和法线积分方法，解决了曲面触觉感知中的光照不足、重建模糊和边界条件复杂等问题。


<details>
  <summary>Details</summary>
Motivation: 为了实现仿人手的触觉感知，需要将传统的平面VTS扩展到具有连续表面梯度的仿生曲面结构，但现有技术存在光照不足、重建模糊和边界条件复杂等挑战。

Method: 1. 开发GelSplitter3D，通过棱镜和近红外相机扩展成像通道；2. 提出基于CAD的光测立体神经网络校准触觉几何；3. 设计带有深度先验信息的法线积分方法以修正积分累积误差。

Result: 实验表明，该方法显著提升了触觉感知性能，法线估计精度提高了40%，并在抓取和操作任务中验证了传感器形状的优势。

Conclusion: 该研究为仿生曲面触觉传感器设计提供了有效解决方案，显著提升了触觉感知的准确性和实用性。

Abstract: To achieve human-like haptic perception in anthropomorphic grippers, the compliant sensing surfaces of vision tactile sensor (VTS) must evolve from conventional planar configurations to biomimetically curved topographies with continuous surface gradients. However, planar VTSs have challenges when extended to curved surfaces, including insufficient lighting of surfaces, blurring in reconstruction, and complex spatial boundary conditions for surface structures. With an end goal of constructing a human-like fingertip, our research (i) develops GelSplitter3D by expanding imaging channels with a prism and a near-infrared (NIR) camera, (ii) proposes a photometric stereo neural network with a CAD-based normal ground truth generation method to calibrate tactile geometry, and (iii) devises a normal integration method with boundary constraints of depth prior information to correcting the cumulative error of surface integrals. We demonstrate better tactile sensing performance, a 40$\%$ improvement in normal estimation accuracy, and the benefits of sensor shapes in grasping and manipulation tasks.

</details>


### [13] [DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory](https://arxiv.org/abs/2506.15096)
*Zihe Ji,Huangxuan Lin,Yue Gao*

Main category: cs.RO

TL;DR: DyNaVLM是一种端到端的视觉语言导航框架，通过视觉语言模型（VLM）实现自由目标选择，无需任务特定训练。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法受限于固定角度或距离间隔的问题，实现更灵活的导航。

Method: 采用自优化图记忆存储对象位置，支持分布式图更新和检索增强决策。

Result: 在GOAT和ObjectNav基准测试中表现优异，实际测试验证了其鲁棒性和泛化能力。

Conclusion: DyNaVLM通过动态动作空间、协作图记忆和无训练部署，为可扩展的机器人导航设定了新范式。

Abstract: We present DyNaVLM, an end-to-end vision-language navigation framework using Vision-Language Models (VLM). In contrast to prior methods constrained by fixed angular or distance intervals, our system empowers agents to freely select navigation targets via visual-language reasoning. At its core lies a self-refining graph memory that 1) stores object locations as executable topological relations, 2) enables cross-robot memory sharing through distributed graph updates, and 3) enhances VLM's decision-making via retrieval augmentation. Operating without task-specific training or fine-tuning, DyNaVLM demonstrates high performance on GOAT and ObjectNav benchmarks. Real-world tests further validate its robustness and generalization. The system's three innovations: dynamic action space formulation, collaborative graph memory, and training-free deployment, establish a new paradigm for scalable embodied robot, bridging the gap between discrete VLN tasks and continuous real-world navigation.

</details>


### [14] [I Know You're Listening: Adaptive Voice for HRI](https://arxiv.org/abs/2506.15107)
*Paige Tuttösí*

Main category: cs.RO

TL;DR: 该论文针对语言教学机器人缺乏任务特定合成语音的问题，提出了一种轻量级、表达丰富的语音系统，并探索了语音适应环境的方法，同时改进了英语TTS系统以提升L2学习者的理解。


<details>
  <summary>Details</summary>
Motivation: 语言教学机器人缺乏针对任务的合成语音，可能影响教学效果。

Method: 1. 使用微调的Matcha-TTS和表情符号提示创建轻量级、表达丰富的语音；2. 探索语音如何适应物理和社交环境；3. 改进英语TTS系统，针对L2学习者优化元音清晰度。

Result: 1. 语音更具表现力且适合长时间表达；2. 环境适应性调整使语音更合适；3. L2清晰模式提高了理解度和尊重感。

Conclusion: 该研究为语言教学机器人提供了更有效的语音解决方案，提升了教学效果和用户体验。

Abstract: While the use of social robots for language teaching has been explored, there remains limited work on a task-specific synthesized voices for language teaching robots. Given that language is a verbal task, this gap may have severe consequences for the effectiveness of robots for language teaching tasks. We address this lack of L2 teaching robot voices through three contributions: 1. We address the need for a lightweight and expressive robot voice. Using a fine-tuned version of Matcha-TTS, we use emoji prompting to create an expressive voice that shows a range of expressivity over time. The voice can run in real time with limited compute resources. Through case studies, we found this voice more expressive, socially appropriate, and suitable for long periods of expressive speech, such as storytelling. 2. We explore how to adapt a robot's voice to physical and social ambient environments to deploy our voices in various locations. We found that increasing pitch and pitch rate in noisy and high-energy environments makes the robot's voice appear more appropriate and makes it seem more aware of its current environment. 3. We create an English TTS system with improved clarity for L2 listeners using known linguistic properties of vowels that are difficult for these listeners. We used a data-driven, perception-based approach to understand how L2 speakers use duration cues to interpret challenging words with minimal tense (long) and lax (short) vowels in English. We found that the duration of vowels strongly influences the perception for L2 listeners and created an "L2 clarity mode" for Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels unchanged. Our clarity mode was found to be more respectful, intelligible, and encouraging than base Matcha-TTS while reducing transcription errors in these challenging tense/lax minimal pairs.

</details>


### [15] [VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments](https://arxiv.org/abs/2506.15126)
*Bingbing Zhang,Huan Yin,Shuo Liu,Fumin Zhang,Wen Xu*

Main category: cs.RO

TL;DR: VIMS是一种新型水下SLAM系统，通过结合低成本单波束声纳和高采样率磁力计，解决了水下环境中尺度估计和闭环检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-惯性状态估计在水下环境中面临尺度估计和闭环检测的困难，VIMS旨在解决这些问题。

Method: VIMS利用单波束声纳改进尺度估计，并通过磁力计和磁场线圈实现基于磁特征的闭环检测。采用分层方案实现视觉-磁特征识别。

Result: 实验表明，VIMS在水下环境中显著提高了状态估计的鲁棒性和准确性。

Conclusion: VIMS通过创新的传感器融合和分层识别方案，有效解决了水下SLAM的挑战。

Abstract: In this study, we present a novel simultaneous localization and mapping (SLAM) system, VIMS, designed for underwater navigation. Conventional visual-inertial state estimators encounter significant practical challenges in perceptually degraded underwater environments, particularly in scale estimation and loop closing. To address these issues, we first propose leveraging a low-cost single-beam sonar to improve scale estimation. Then, VIMS integrates a high-sampling-rate magnetometer for place recognition by utilizing magnetic signatures generated by an economical magnetic field coil. Building on this, a hierarchical scheme is developed for visual-magnetic place recognition, enabling robust loop closure. Furthermore, VIMS achieves a balance between local feature tracking and descriptor-based loop closing, avoiding additional computational burden on the front end. Experimental results highlight the efficacy of the proposed VIMS, demonstrating significant improvements in both the robustness and accuracy of state estimation within underwater environments.

</details>


### [16] [Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion](https://arxiv.org/abs/2506.15132)
*Yushi Wang,Penghui Chen,Xinyu Han,Feng Wu,Mingguo Zhao*

Main category: cs.RO

TL;DR: 提出了一个全面的代码框架，用于简化强化学习策略从仿真到真实人形机器人的迁移，并在Booster T1机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在仿真中取得了进展，但将策略迁移到真实机器人仍面临挑战，因此需要一个覆盖全流程的解决方案。

Method: 开发了一个包含常见RL训练方法、域随机化、奖励函数设计和并行结构处理的代码框架。

Result: 在Booster T1机器人上验证了框架的有效性，实现了全向行走、抗干扰和地形适应能力。

Conclusion: 该框架为机器人社区提供了便利工具，有望加速人形机器人的发展。

Abstract: Recent advancements in reinforcement learning (RL) have led to significant progress in humanoid robot locomotion, simplifying the design and training of motion policies in simulation. However, the numerous implementation details make transferring these policies to real-world robots a challenging task. To address this, we have developed a comprehensive code framework that covers the entire process from training to deployment, incorporating common RL training methods, domain randomization, reward function design, and solutions for handling parallel structures. This library is made available as a community resource, with detailed descriptions of its design and experimental results. We validate the framework on the Booster T1 robot, demonstrating that the trained policies seamlessly transfer to the physical platform, enabling capabilities such as omnidirectional walking, disturbance resistance, and terrain adaptability. We hope this work provides a convenient tool for the robotics community, accelerating the development of humanoid robots. The code can be found in https://github.com/BoosterRobotics/booster_gym.

</details>


### [17] [TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality](https://arxiv.org/abs/2506.15146)
*Masaki Murooka,Takahiro Hoshi,Kensuke Fukumitsu,Shimpei Masuda,Marwan Hamze,Tomoya Sasaki,Mitsuharu Morisawa,Eiichi Yoshida*

Main category: cs.RO

TL;DR: 本文提出了一种名为TACT的控制系统，通过模仿学习人类远程操作数据，使人形机器人能够进行全身接触操作，并结合视觉和触觉模态提高操作的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人进行全身接触操作具有增强稳定性和减少负载的优势，但面临运动生成计算成本高和广泛接触测量困难等挑战。

Method: 开发了TACT策略，基于模仿学习人类远程操作数据，整合关节位置、视觉和触觉测量作为输入，并结合双足模型的重新定位和运动控制。

Result: 实验验证表明，输入视觉和触觉模态能显著提高涉及广泛和精细接触的操作鲁棒性。

Conclusion: TACT策略成功实现了人形机器人在保持平衡和行走的同时进行全身接触操作，展示了多模态输入的优越性。

Abstract: Manipulation with whole-body contact by humanoid robots offers distinct advantages, including enhanced stability and reduced load. On the other hand, we need to address challenges such as the increased computational cost of motion generation and the difficulty of measuring broad-area contact. We therefore have developed a humanoid control system that allows a humanoid robot equipped with tactile sensors on its upper body to learn a policy for whole-body manipulation through imitation learning based on human teleoperation data. This policy, named tactile-modality extended ACT (TACT), has a feature to take multiple sensor modalities as input, including joint position, vision, and tactile measurements. Furthermore, by integrating this policy with retargeting and locomotion control based on a biped model, we demonstrate that the life-size humanoid robot RHP7 Kaleido is capable of achieving whole-body contact manipulation while maintaining balance and walking. Through detailed experimental verification, we show that inputting both vision and tactile modalities into the policy contributes to improving the robustness of manipulation involving broad and delicate contact.

</details>


### [18] [Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation](https://arxiv.org/abs/2506.15157)
*Hanbit Oh,Andrea M. Salcedo-Vázquez,Ixchel G. Ramirez-Alpizar,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 论文提出了一种名为RIP的鲁棒上下文模仿学习算法，通过利用Student's t回归模型来减少LLM生成的幻觉轨迹，从而生成更可靠的机器人轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的即时策略在机器人领域存在幻觉问题，导致生成的轨迹偏离演示，影响可靠性。

Method: RIP通过生成多个候选轨迹，并使用Student's t分布聚合它们，以忽略异常值（幻觉），从而生成鲁棒轨迹。

Result: 实验表明，RIP在模拟和真实环境中显著优于现有方法，任务成功率至少提高26%，尤其在低数据场景下表现优异。

Conclusion: RIP通过鲁棒方法有效解决了LLM即时策略的幻觉问题，提升了机器人模仿学习的可靠性。

Abstract: Imitation learning (IL) aims to enable robots to perform tasks autonomously by observing a few human demonstrations. Recently, a variant of IL, called In-Context IL, utilized off-the-shelf large language models (LLMs) as instant policies that understand the context from a few given demonstrations to perform a new task, rather than explicitly updating network models with large-scale demonstrations. However, its reliability in the robotics domain is undermined by hallucination issues such as LLM-based instant policy, which occasionally generates poor trajectories that deviate from the given demonstrations. To alleviate this problem, we propose a new robust in-context imitation learning algorithm called the robust instant policy (RIP), which utilizes a Student's t-regression model to be robust against the hallucinated trajectories of instant policies to allow reliable trajectory generation. Specifically, RIP generates several candidate robot trajectories to complete a given task from an LLM and aggregates them using the Student's t-distribution, which is beneficial for ignoring outliers (i.e., hallucinations); thereby, a robust trajectory against hallucinations is generated. Our experiments, conducted in both simulated and real-world environments, show that RIP significantly outperforms state-of-the-art IL methods, with at least $26\%$ improvement in task success rates, particularly in low-data scenarios for everyday tasks. Video results available at https://sites.google.com/view/robustinstantpolicy.

</details>


### [19] [SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization](https://arxiv.org/abs/2506.15175)
*Hanjun Kim,Minwoo Jung,Wooseong Yang,Ayoung Kim*

Main category: cs.RO

TL;DR: SHeRLoc是一种针对异构雷达设计的深度网络，通过RCS极坐标匹配和多尺度特征聚合，显著提升了异构雷达地点识别的性能。


<details>
  <summary>Details</summary>
Motivation: 当前雷达研究多局限于同质传感器，忽视了异构雷达的集成和跨模态挑战，导致难以泛化到多样雷达数据类型。

Method: 提出SHeRLoc，利用RCS极坐标匹配对齐多模态雷达数据，采用分层最优传输特征聚合生成旋转鲁棒的多尺度描述符，并通过FFT相似性数据挖掘和自适应边距三元组损失实现FOV感知度量学习。

Result: 在公开数据集上，SHeRLoc将异构雷达地点识别的recall@1从低于0.1提升至0.9，性能优于现有方法。

Conclusion: SHeRLoc不仅适用于雷达，还可推广到LiDAR，为跨模态地点识别和异构传感器SLAM开辟了新途径。

Abstract: Despite the growing adoption of radar in robotics, the majority of research has been confined to homogeneous sensor types, overlooking the integration and cross-modality challenges inherent in heterogeneous radar technologies. This leads to significant difficulties in generalizing across diverse radar data types, with modality-aware approaches that could leverage the complementary strengths of heterogeneous radar remaining unexplored. To bridge these gaps, we propose SHeRLoc, the first deep network tailored for heterogeneous radar, which utilizes RCS polar matching to align multimodal radar data. Our hierarchical optimal transport-based feature aggregation method generates rotationally robust multi-scale descriptors. By employing FFT-similarity-based data mining and adaptive margin-based triplet loss, SHeRLoc enables FOV-aware metric learning. SHeRLoc achieves an order of magnitude improvement in heterogeneous radar place recognition, increasing recall@1 from below 0.1 to 0.9 on a public dataset and outperforming state of-the-art methods. Also applicable to LiDAR, SHeRLoc paves the way for cross-modal place recognition and heterogeneous sensor SLAM. The source code will be available upon acceptance.

</details>


### [20] [Context-Aware Deep Lagrangian Networks for Model Predictive Control](https://arxiv.org/abs/2506.15249)
*Lucas Schulze,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 论文提出了一种基于上下文感知的深度拉格朗日网络（DeLaN）方法，结合在线系统识别和模型预测控制（MPC），用于机器人自适应物理控制。实验表明，该方法显著减少了末端执行器的跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，全局物理模型难以应对大量不确定物体，因此需要上下文感知的局部模型来适应动态环境，同时保持物理合理性。

Method: 扩展DeLaN为上下文感知模型，结合循环网络进行在线系统识别，并集成MPC和残差动力学模型。

Result: 在7自由度机械臂的轨迹跟踪实验中，该方法将末端执行器跟踪误差降低了39%，优于基线方法的21%改进。

Conclusion: 上下文感知的DeLaN结合MPC和残差模型，显著提升了机器人控制的适应性和精度。

Abstract: Controlling a robot based on physics-informed dynamic models, such as deep Lagrangian networks (DeLaN), can improve the generalizability and interpretability of the resulting behavior. However, in complex environments, the number of objects to potentially interact with is vast, and their physical properties are often uncertain. This complexity makes it infeasible to employ a single global model. Therefore, we need to resort to online system identification of context-aware models that capture only the currently relevant aspects of the environment. While physical principles such as the conservation of energy may not hold across varying contexts, ensuring physical plausibility for any individual context-aware model can still be highly desirable, particularly when using it for receding horizon control methods such as Model Predictive Control (MPC). Hence, in this work, we extend DeLaN to make it context-aware, combine it with a recurrent network for online system identification, and integrate it with a MPC for adaptive, physics-informed control. We also combine DeLaN with a residual dynamics model to leverage the fact that a nominal model of the robot is typically available. We evaluate our method on a 7-DOF robot arm for trajectory tracking under varying loads. Our method reduces the end-effector tracking error by 39%, compared to a 21% improvement achieved by a baseline that uses an extended Kalman filter.

</details>


### [21] [Offensive Robot Cybersecurity](https://arxiv.org/abs/2506.15343)
*Víctor Mayoral-Vilches*

Main category: cs.RO

TL;DR: 论文提出了一种通过自动化支持的进攻性安全方法，强调通过理解攻击者策略和提前识别漏洞来改进机器人安全。结合机器学习和博弈论，论文揭示了机器人架构与网络安全的深层联系，并提出了一种新型网络安全认知引擎架构。


<details>
  <summary>Details</summary>
Motivation: 机器人安全面临日益复杂的网络威胁，传统防御方法不足以应对。论文旨在通过进攻性安全策略，提前发现漏洞并提升机器人系统的自主防御能力。

Method: 利用机器学习和博弈论优化漏洞识别与利用过程，开发网络安全认知引擎，并通过攻击测试验证其有效性。

Result: 提出了一种新型网络安全认知引擎架构，支持机器人自主实施进攻性安全策略，显著提升其防御能力。

Conclusion: 进攻性安全策略是提升机器人网络安全的关键，未来机器人将具备自主防御能力，标志着网络安全领域的重大进步。

Abstract: Offensive Robot Cybersecurity introduces a groundbreaking approach by advocating for offensive security methods empowered by means of automation. It emphasizes the necessity of understanding attackers' tactics and identifying vulnerabilities in advance to develop effective defenses, thereby improving robots' security posture. This thesis leverages a decade of robotics experience, employing Machine Learning and Game Theory to streamline the vulnerability identification and exploitation process. Intrinsically, the thesis uncovers a profound connection between robotic architecture and cybersecurity, highlighting that the design and creation aspect of robotics deeply intertwines with its protection against attacks. This duality -- whereby the architecture that shapes robot behavior and capabilities also necessitates a defense mechanism through offensive and defensive cybersecurity strategies -- creates a unique equilibrium. Approaching cybersecurity with a dual perspective of defense and attack, rooted in an understanding of systems architecture, has been pivotal. Through comprehensive analysis, including ethical considerations, the development of security tools, and executing cyber attacks on robot software, hardware, and industry deployments, this thesis proposes a novel architecture for cybersecurity cognitive engines. These engines, powered by advanced game theory and machine learning, pave the way for autonomous offensive cybersecurity strategies for robots, marking a significant shift towards self-defending robotic systems. This research not only underscores the importance of offensive measures in enhancing robot cybersecurity but also sets the stage for future advancements where robots are not just resilient to cyber threats but are equipped to autonomously safeguard themselves.

</details>


### [22] [Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning](https://arxiv.org/abs/2506.15380)
*Taegeun Yang,Jiwoo Hwang,Jeil Jeong,Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: 提出了一种分层强化学习框架，用于移动机械臂在可移动障碍物环境中的高效导航，结合了交互式障碍物属性估计和结构化推动策略。


<details>
  <summary>Details</summary>
Motivation: 解决在动态环境中导航时遇到的未预见障碍物问题，同时保持全局路径规划的稳定性。

Method: 采用分层强化学习框架，高层策略生成考虑环境约束和路径跟踪目标的推动命令，低层策略通过协调全身运动精确执行命令。

Result: 仿真实验显示，该方法在任务成功率、路径长度和到达时间上优于基线方法，并通过消融实验验证了各组件的重要性。

Conclusion: 该方法在动态环境中高效导航和障碍物处理方面表现出色，实时障碍物属性估计准确可靠。

Abstract: We propose a hierarchical reinforcement learning (HRL) framework for efficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator. Our approach combines interaction-based obstacle property estimation with structured pushing strategies, facilitating the dynamic manipulation of unforeseen obstacles while adhering to a pre-planned global path. The high-level policy generates pushing commands that consider environmental constraints and path-tracking objectives, while the low-level policy precisely and stably executes these commands through coordinated whole-body movements. Comprehensive simulation-based experiments demonstrate improvements in performing NAMO tasks, including higher success rates, shortened traversed path length, and reduced goal-reaching times, compared to baselines. Additionally, ablation studies assess the efficacy of each component, while a qualitative analysis further validates the accuracy and reliability of the real-time obstacle property estimation.

</details>


### [23] [MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System](https://arxiv.org/abs/2506.15402)
*Miaoxin Pan,Jinnan Li,Yaowen Zhang,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: MCOO-SLAM是一种多相机全景物体SLAM系统，利用环绕视角相机配置，在复杂户外场景中实现鲁棒、一致且语义丰富的建图。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法依赖RGB-D或单目相机，视野窄、易受遮挡且深度感知有限，导致物体建模不准确和数据关联不可靠。

Method: 整合点特征和物体级地标，引入语义-几何-时间融合策略，设计全景闭环模块，构建分层3D场景图。

Result: 实验表明MCOO-SLAM在真实场景中实现了精确定位和可扩展的物体级建图，对遮挡、姿态变化和环境复杂性具有更强鲁棒性。

Conclusion: MCOO-SLAM通过多相机配置和语义增强，显著提升了复杂户外场景中的SLAM性能。

Abstract: Object-level SLAM offers structured and semantically meaningful environment representations, making it more interpretable and suitable for high-level robotic tasks. However, most existing approaches rely on RGB-D sensors or monocular views, which suffer from narrow fields of view, occlusion sensitivity, and limited depth perception-especially in large-scale or outdoor environments. These limitations often restrict the system to observing only partial views of objects from limited perspectives, leading to inaccurate object modeling and unreliable data association. In this work, we propose MCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully leverages surround-view camera configurations to achieve robust, consistent, and semantically enriched mapping in complex outdoor scenarios. Our approach integrates point features and object-level landmarks enhanced with open-vocabulary semantics. A semantic-geometric-temporal fusion strategy is introduced for robust object association across multiple views, leading to improved consistency and accurate object modeling, and an omnidirectional loop closure module is designed to enable viewpoint-invariant place recognition using scene-level descriptors. Furthermore, the constructed map is abstracted into a hierarchical 3D scene graph to support downstream reasoning tasks. Extensive experiments in real-world demonstrate that MCOO-SLAM achieves accurate localization and scalable object-level mapping with improved robustness to occlusion, pose variation, and environmental complexity.

</details>


### [24] [SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle](https://arxiv.org/abs/2506.15450)
*Kun Liu,Junhao Xiao,Hao Lin,Yue Cao,Hui Peng,Kaihong Huang,Huimin Lu*

Main category: cs.RO

TL;DR: 提出了一种新型多模式水上-空中机器人SurfAAV，集成了水下、水面和空中运动能力，无需浮力调节系统即可高效运行。


<details>
  <summary>Details</summary>
Motivation: 现有水上-空中机器人难以同时高效完成水下、水面和空中运动，需要一种更灵活的解决方案。

Method: 设计了一种差分推力矢量水翼，支持水面滑行和水下导航，并通过滑翔起飞实现空中飞行。

Result: SurfAAV水面滑行速度达7.96 m/s，水下速度达3.1 m/s，性能优于现有机器人。

Conclusion: SurfAAV为水下、水面和空中运动提供了新解决方案，并验证了其高效性和灵活性。

Abstract: Despite significant advancements in the research of aquatic-aerial robots, existing configurations struggle to efficiently perform underwater, surface, and aerial movement simultaneously. In this paper, we propose a novel multimodal surfing aquatic-aerial vehicle, SurfAAV, which efficiently integrates underwater navigation, surface gliding, and aerial flying capabilities. Thanks to the design of the novel differential thrust vectoring hydrofoil, SurfAAV can achieve efficient surface gliding and underwater navigation without the need for a buoyancy adjustment system. This design provides flexible operational capabilities for both surface and underwater tasks, enabling the robot to quickly carry out underwater monitoring activities. Additionally, when it is necessary to reach another water body, SurfAAV can switch to aerial mode through a gliding takeoff, flying to the target water area to perform corresponding tasks. The main contribution of this letter lies in proposing a new solution for underwater, surface, and aerial movement, designing a novel hybrid prototype concept, developing the required control laws, and validating the robot's ability to successfully perform surface gliding and gliding takeoff. SurfAAV achieves a maximum surface gliding speed of 7.96 m/s and a maximum underwater speed of 3.1 m/s. The prototype's surface gliding maneuverability and underwater cruising maneuverability both exceed those of existing aquatic-aerial vehicles.

</details>


### [25] [Real-Time Initialization of Unknown Anchors for UWB-aided Navigation](https://arxiv.org/abs/2506.15518)
*Giulio Delama,Igor Borowski,Roland Jung,Stephan Weiss*

Main category: cs.RO

TL;DR: 本文提出了一种实时初始化未知超宽带（UWB）锚点的框架，用于UWB辅助导航系统，通过自动检测和校准锚点，减少人工干预，并提高了鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在UWB辅助导航系统中，传统方法需要手动设置锚点，限制了系统的灵活性和实用性。本文旨在解决这一问题，实现自动初始化未知锚点。

Method: 结合在线位置精度稀释（PDOP）估计、轻量级异常检测方法和自适应鲁棒核的非线性优化，提出了一种更保守的初始化决策指标。

Result: 实验在自主叉车和配备UWB辅助视觉惯性里程计（VIO）的四旋翼飞行器上进行，结果显示该方法具有鲁棒的初始化和低定位误差。

Conclusion: 该方法显著优于现有技术，适用于实际应用，并开源了C++库和ROS封装。

Abstract: This paper presents a framework for the real-time initialization of unknown Ultra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is designed for localization solutions where UWB modules act as supplementary sensors. Our approach enables the automatic detection and calibration of previously unknown anchors during operation, removing the need for manual setup. By combining an online Positional Dilution of Precision (PDOP) estimation, a lightweight outlier detection method, and an adaptive robust kernel for non-linear optimization, our approach significantly improves robustness and suitability for real-world applications compared to state-of-the-art. In particular, we show that our metric which triggers an initialization decision is more conservative than current ones commonly based on initial linear or non-linear initialization guesses. This allows for better initialization geometry and subsequently lower initialization errors. We demonstrate the proposed approach on two different mobile robots: an autonomous forklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry (VIO) framework. The results highlight the effectiveness of the proposed method with robust initialization and low positioning error. We open-source our code in a C++ library including a ROS wrapper.

</details>


### [26] [Aerial Grasping via Maximizing Delta-Arm Workspace Utilization](https://arxiv.org/abs/2506.15539)
*Haoran Chen,Weiliang Deng,Biyu Ye,Yifan Xiong,Ximin Lyu*

Main category: cs.RO

TL;DR: 提出了一种新型空中抓取规划框架，通过优化轨迹和利用MLP与RevNet技术，最大化机械臂工作空间利用率。


<details>
  <summary>Details</summary>
Motivation: 机械臂的工作空间限制了其操作能力和运动范围，最大化工作空间利用率可提升空中抓取任务的灵活性和效率。

Method: 通过优化问题规划轨迹，利用MLP映射非凸工作空间，RevNet近似正向运动学以消除约束。

Result: 仿真和实验验证了方法的有效性。

Conclusion: 该框架显著提升了空中抓取的工作空间利用率和操作效率。

Abstract: The workspace limits the operational capabilities and range of motion for the systems with robotic arms. Maximizing workspace utilization has the potential to provide more optimal solutions for aerial manipulation tasks, increasing the system's flexibility and operational efficiency. In this paper, we introduce a novel planning framework for aerial grasping that maximizes workspace utilization. We formulate an optimization problem to optimize the aerial manipulator's trajectory, incorporating task constraints to achieve efficient manipulation. To address the challenge of incorporating the delta arm's non-convex workspace into optimization constraints, we leverage a Multilayer Perceptron (MLP) to map position points to feasibility probabilities.Furthermore, we employ Reversible Residual Networks (RevNet) to approximate the complex forward kinematics of the delta arm, utilizing efficient model gradients to eliminate workspace constraints. We validate our methods in simulations and real-world experiments to demonstrate their effectiveness.

</details>


### [27] [GRIM: Task-Oriented Grasping with Conditioning on Generative Examples](https://arxiv.org/abs/2506.15607)
*Shailesh,Alok Raj,Nayan Kumar,Priya Shukla,Andrew Melnik,Micheal Beetz,Gora Chand Nandi*

Main category: cs.RO

TL;DR: GRIM是一个无需训练的任务导向抓取框架，通过几何线索和PCA降维的DINO特征进行粗对齐，再通过任务无关的几何稳定抓取进行细化。


<details>
  <summary>Details</summary>
Motivation: 任务导向抓取（TOG）需要理解任务语义、对象功能及抓取约束，现有方法泛化能力不足。

Method: GRIM采用粗对齐策略（几何线索+PCA降维DINO特征），再通过任务无关的几何稳定抓取细化。

Result: GRIM在少量示例下表现出强泛化能力，性能稳健。

Conclusion: GRIM为任务导向抓取提供了一种高效、无需训练的新方法。

Abstract: Task-Oriented Grasping (TOG) presents a significant challenge, requiring a nuanced understanding of task semantics, object affordances, and the functional constraints dictating how an object should be grasped for a specific task. To address these challenges, we introduce GRIM (Grasp Re-alignment via Iterative Matching), a novel training-free framework for task-oriented grasping. Initially, a coarse alignment strategy is developed using a combination of geometric cues and principal component analysis (PCA)-reduced DINO features for similarity scoring. Subsequently, the full grasp pose associated with the retrieved memory instance is transferred to the aligned scene object and further refined against a set of task-agnostic, geometrically stable grasps generated for the scene object, prioritizing task compatibility. In contrast to existing learning-based methods, GRIM demonstrates strong generalization capabilities, achieving robust performance with only a small number of conditioning examples.

</details>


### [28] [Vision in Action: Learning Active Perception from Human Demonstrations](https://arxiv.org/abs/2506.15666)
*Haoyu Xiong,Xiaomeng Xu,Jimmy Wu,Yifan Hou,Jeannette Bohg,Shuran Song*

Main category: cs.RO

TL;DR: ViA是一个用于双手机器人操作的活动感知系统，通过学习人类演示的任务相关感知策略，结合6自由度机器人颈部和VR远程操作接口，显著优于基线系统。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够学习人类活动感知策略的机器人系统，以解决复杂双手操作任务中的视觉遮挡问题。

Method: 使用6自由度机器人颈部实现灵活头部运动，设计VR远程操作接口捕捉人类策略，并通过3D场景表示减少VR运动眩晕。

Result: 在涉及视觉遮挡的复杂双手操作任务中，ViA显著优于基线系统。

Conclusion: ViA通过结合硬件和软件设计，成功学习了人类活动感知策略，提升了机器人操作的鲁棒性。

Abstract: We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems.

</details>


### [29] [Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos](https://arxiv.org/abs/2506.15680)
*Kaifeng Zhang,Baoyu Li,Kris Hauser,Yunzhu Li*

Main category: cs.RO

TL;DR: 该论文提出了一种结合粒子与空间网格的神经动力学框架，用于建模可变形物体的动态行为，并通过实验验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 可变形物体的动态建模因物理属性多样和视觉信息有限而具有挑战性，需一种高效且通用的方法。

Method: 采用粒子-网格混合表示，粒子捕捉物体形状，空间网格确保空间连续性，结合高斯渲染生成3D视频。

Result: 模型能从稀疏RGB-D数据学习多种物体的动态，并在类别级别泛化到未见实例，性能优于现有方法。

Conclusion: 该框架为可变形物体的数字孪生和任务规划提供了有效工具，适用于有限视角场景。

Abstract: Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects -- such as ropes, cloths, stuffed animals, and paper bags -- from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at https://kywind.github.io/pgnd .

</details>
