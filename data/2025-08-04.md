<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: XRoboToolkit是一个基于OpenXR标准的跨平台扩展现实机器人远程操作框架，解决了现有数据收集方法的可扩展性和数据质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器人演示数据收集方法存在可扩展性差、设置复杂和数据质量低的问题，需要一种更高效的解决方案。

Method: 开发了XRoboToolkit框架，支持低延迟立体视觉反馈、优化逆运动学和多种跟踪方式，模块化设计支持多种机器人平台和仿真环境。

Result: 通过精确操作任务验证了框架的有效性，并训练出具有鲁棒自主性能的VLA模型。

Conclusion: XRoboToolkit为大规模高质量机器人数据收集提供了一种高效、灵活的解决方案。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [2] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: CHILD是一种紧凑可重构的遥操作系统，支持人形机器人的关节级控制，适用于全身控制和移动操作。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少支持人形机器人的全身关节级遥操作，限制了任务的多样性。

Method: CHILD系统设计为紧凑且可重构，适配标准婴儿背带，支持直接关节映射和自适应力反馈。

Result: 验证了系统在人形机器人和双臂系统上的移动操作和全身控制能力。

Conclusion: CHILD系统提升了遥操作的灵活性和安全性，并开源硬件设计以促进可访问性和可重复性。

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [3] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于拓扑学的形态描述符，结合伪刚体模型和莫尔斯理论，用于软体连续机器人的形态定量表征与控制。


<details>
  <summary>Details</summary>
Motivation: 提升软体连续机器人在医疗应用（如微创手术和血管内介入）中的精确性和适应性。

Method: 结合伪刚体模型和莫尔斯理论，通过计数方向投影的临界点实现形态的离散表示和分类。

Result: 实现了软体机器人形态的定量描述、分类及控制，并通过优化问题求解目标形态的驱动参数。

Conclusion: 该框架为软体连续机器人提供了一种统一的形态描述与控制方法，具有广泛的应用潜力。

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [4] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: UAV-ON是一个用于无人机在开放环境中进行目标导航的基准测试，摆脱了对语言指令的依赖，强调语义目标驱动的自主导航。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖语言指令（如VLN），限制了无人机导航的扩展性和自主性。UAV-ON旨在填补这一空白，推动无人机在复杂环境中的语义目标导航研究。

Method: 提出UAV-ON基准，包含14个高保真虚拟环境和1270个标注目标对象，提供语义目标描述。开发了AOA等基线方法，结合语义目标和自身观测进行导航。

Result: 实验显示基线方法在UAV-ON中表现不佳，突显了无人机导航与语义目标结合的挑战。

Conclusion: UAV-ON为无人机在复杂环境中的语义目标导航研究提供了新方向，推动了自主导航技术的发展。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [5] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser是一种基于扩散的多模态轨迹预测框架，利用拓扑地图生成准确、多样且符合道路的未来运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在轨迹预测中难以同时保证准确性和道路几何一致性，TopoDiffuser旨在通过结合拓扑地图解决这一问题。

Method: 通过将拓扑地图的结构信息嵌入条件扩散模型的去噪过程，生成自然符合道路几何的轨迹。多模态编码器融合LiDAR观测、历史运动和路线信息为统一的BEV表示。

Result: 在KITTI基准测试中，TopoDiffuser优于现有方法，并保持几何一致性。消融研究验证了各输入模态和去噪步骤的贡献。

Conclusion: TopoDiffuser通过结合拓扑地图和扩散模型，实现了准确、多样且道路合规的轨迹预测，为未来研究提供了新思路。

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [6] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: Omni-Scan是一种利用双机械臂和静态相机生成高质量3D高斯溅射模型的流程，适用于物体缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 传统3D物体扫描方法受限于工作空间和设备，Omni-Scan通过双机械臂旋转物体解决遮挡问题。

Method: 使用DepthAnything、Segment Anything和RAFT光流模型识别物体并去除背景，改进3DGS训练流程以处理遮挡。

Result: 在12种工业和家用物体上检测缺陷，平均准确率达83%。

Conclusion: Omni-Scan提供了一种高效且灵活的3D物体建模方法，适用于多种应用场景。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [7] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的时间优化策略（TOP），用于训练人形机器人的站立操纵控制模型，同时确保平衡、精确和时间效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足高维上半身关节的精确控制和鲁棒性，尤其是在上半身动作快速时。

Method: 结合运动先验（VAE）、解耦控制（上半身PD控制器和下半身RL控制器）以及TOP方法，优化时间轨迹。

Result: 通过仿真和真实实验验证了方法的有效性，实现了稳定且精确的站立操纵任务。

Conclusion: TOP方法显著提升了人形机器人在快速上半身动作时的平衡和精确控制能力。

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [8] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种用于全尺寸人形机器人的全身运动模仿框架，结合接触感知的运动重定向和非线性质心模型预测控制，实现高精度运动模仿与实时平衡。


<details>
  <summary>Details</summary>
Motivation: 人形机器人模仿人类运动时，由于动力学和运动学的差异，难以同时保持平衡和运动准确性。

Method: 采用接触感知的全身运动重定向生成参考轨迹，结合非线性质心模型预测控制器实现实时平衡与运动精度。

Result: 实验验证了该方法在仿真和真实机器人上的准确性和适应性。

Conclusion: 所提框架有效解决了人形机器人运动模仿中的平衡与精度问题。

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [9] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: NIVA是一个基于分层贝叶斯模型的概率框架，用于多智能体交通模拟，通过自回归采样实现闭环、观察条件模拟。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆的快速迭代，需要构建真实且可扩展的多智能体交通模拟器以高效评估。

Method: NIVA采用分层贝叶斯模型，通过自回归采样从潜在的高斯分布混合中生成模拟数据，统一了轨迹预测模型和闭环模拟模型。

Result: 在Waymo Open Motion数据集上的实验表明，NIVA性能与现有方法相当，同时提供了对意图和驾驶风格的精细控制。

Conclusion: NIVA为多智能体交通模拟提供了一种高效且灵活的方法，具有潜在的实际应用价值。

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [10] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: 提出了一种基于子集的集体决策方法（SubCDM），通过动态构建子集减少资源消耗，同时保持决策准确性。


<details>
  <summary>Details</summary>
Motivation: 现有集体决策策略需要所有机器人参与，资源消耗大且限制了其他任务分配。

Method: 动态构建子集，仅依赖局部信息，自适应调整子集大小以适应不同决策难度。

Result: 仿真实验显示，SubCDM在减少机器人参与数量的同时，保持了与全群决策相当的准确性。

Conclusion: SubCDM是一种资源高效的集体决策方法，适用于机器人群体。

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [11] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 论文提出了一种基于模仿学习的HannesImitationPolicy方法，用于控制假肢手在非结构化环境中抓取物体，并展示了其优于传统分割视觉伺服控制器的性能。


<details>
  <summary>Details</summary>
Motivation: 通过模仿学习减少假肢手控制的认知负荷，提升灵活性和适应性，使其能在非结构化场景中学习任务。

Method: 采用模仿学习方法，利用HannesImitationDataset中的抓取演示数据训练扩散策略，预测手腕方向和手部闭合动作。

Result: 实验表明，该方法能成功抓取多种物体，并在非结构化场景中优于分割视觉伺服控制器。

Conclusion: 模仿学习为假肢手控制提供了新思路，增强了其在复杂环境中的适应性和灵活性。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [12] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: OmniUnet是一种基于Transformer的神经网络架构，用于RGB-D-T图像语义分割，在火星探测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人导航需要多模态感知系统以确保安全，而火星探测中热成像对地形安全评估尤为重要。

Method: 开发了OmniUnet网络和定制多模态传感器，并在模拟火星环境的半沙漠中收集数据集进行训练和评估。

Result: 模型像素准确率达80.37%，推理时间673毫秒，适合在资源受限设备上部署。

Conclusion: OmniUnet和公开数据集为行星机器人多模态地形感知研究提供了重要支持。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [13] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 提出了一种基于导纳控制和可变阻尼项的人机协作物体运输控制方案，结合屏障人工势能确保物体不脱落，并通过实验验证了其被动性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作运输中控制性和人类操作者负担的问题，同时确保物体在运输过程中不脱落。

Method: 采用导纳控制框架，引入可变阻尼项和基于屏障人工势能的额外控制信号。

Result: 实验证明该控制方案具有被动性，能有效减少人类操作者的负担并提高控制性。

Conclusion: 提出的控制方案在人机协作物体运输中表现出色，兼具控制性和安全性。

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [14] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: OpenScout v1.1是一款开源硬件移动机器人，升级了计算硬件、ROS2接口和Gazebo模拟，并报告了改进原因和方法。


<details>
  <summary>Details</summary>
Motivation: 为研究和工业提供更简化、更便宜且更强大的开源硬件移动机器人解决方案。

Method: 通过改进计算硬件、增加ROS2接口和Gazebo模拟功能，进行项目方法论和结果报告。

Result: 成功开发了OpenScout v1.1，提升了性能和功能。

Conclusion: OpenScout v1.1为研究和工业提供了更高效的开源机器人平台。

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [15] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: 该研究提出了一种基于数据驱动的方法，通过多任务时间卷积网络（TCN）为中风后行走者提供踝关节扭矩估计，以实现外骨骼的自适应控制。


<details>
  <summary>Details</summary>
Motivation: 尽管数据驱动方法在健康年轻人中表现出色，但应用于中风后步态缺陷人群时面临挑战，如高异质性和数据缺乏。研究旨在解决这些问题，推动外骨骼在非结构化社区环境中的应用。

Method: 使用四名中风后参与者的跑步机行走数据训练多任务TCN模型，并结合六名健康参与者的数据进行预训练。模型基于三个惯性测量单元（IMU）的数据。

Result: 模型在扭矩估计上表现出色（R²为0.74±0.13），并通过一名中风后参与者验证了实时传感、估计和驱动的可行性。

Conclusion: 该研究为中风后行走者的外骨骼自适应控制提供了初步解决方案，展示了数据驱动方法在此领域的潜力。

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [16] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP通过压缩去噪模块和减少采样步骤，优化了Diffusion Policies在移动设备上的实时部署，性能接近现有最优方法。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies在资源受限的移动平台上因计算效率低和内存占用大而难以应用。

Method: LightDP采用网络压缩和采样步骤减少策略，结合统一剪枝与再训练流程及一致性蒸馏技术。

Result: 实验表明LightDP在多个标准数据集上实现实时预测，性能接近最优Diffusion Policies。

Conclusion: LightDP为资源受限环境中扩散策略的实际部署迈出了重要一步。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [17] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: 论文提出Video Policy框架，通过视频生成作为机器人策略学习的代理，解决了感知和行为分布偏移的泛化问题，并减少对人类演示数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前视觉运动策略在泛化和数据效率上存在局限，需要一种更高效的方法来提升机器人策略学习。

Method: 提出Video Policy框架，结合视频和动作生成，端到端训练，利用视频生成模型提取策略。

Result: 方法在仿真和现实世界中均表现出强泛化能力，显著提升样本效率和鲁棒性。

Conclusion: 通过大规模视频生成模型，实现了比传统行为克隆更优的性能，为机器人策略学习提供了更高效和可扩展的途径。

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>
