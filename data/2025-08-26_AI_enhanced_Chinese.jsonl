{"id": "2508.16731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16731", "abs": "https://arxiv.org/abs/2508.16731", "authors": ["Daniel McGann", "Easton R. Potokar", "Michael Kaess"], "title": "COSMO-Bench: A Benchmark for Collaborative SLAM Optimization", "comment": null, "summary": "Recent years have seen a focus on research into distributed optimization\nalgorithms for multi-robot Collaborative Simultaneous Localization and Mapping\n(C-SLAM). Research in this domain, however, is made difficult by a lack of\nstandard benchmark datasets. Such datasets have been used to great effect in\nthe field of single-robot SLAM, and researchers focused on multi-robot problems\nwould benefit greatly from dedicated benchmark datasets. To address this gap,\nwe design and release the Collaborative Open-Source Multi-robot Optimization\nBenchmark (COSMO-Bench) -- a suite of 24 datasets derived from a\nstate-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:\nhttps://doi.org/10.1184/R1/29652158", "AI": {"tldr": "这篇论文提出了COSMO-Bench标准测试数据集，用于多机器人协同SLAM算法的评测和对比", "motivation": "多机器人协同SLAM领域缺乏标准测试数据集，影响了算法研究的可比性和进展", "method": "设计并释放了COSMO-Bench数据集，包含24个数据集，基于现有的C-SLAM前端技术和实际LiDAR数据", "result": "提供了一个开源的多机器人SLAM标准测试数据集，包含24个不同场景的数据", "conclusion": "COSMO-Bench数据集将有助于推动多机器人协同SLAM领域的研究进展，提供标准化的评测工具"}}
{"id": "2508.16749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16749", "abs": "https://arxiv.org/abs/2508.16749", "authors": ["Victor-Louis De Gusseme", "Thomas Lips", "Remko Proesmans", "Julius Hietala", "Giwan Lee", "Jiyoung Choi", "Jeongil Choi", "Geon Kim", "Phayuth Yonrith", "Domen Tabernik", "Andrej Gams", "Peter Nimac", "Matej Urbas", "Jon Muhovič", "Danijel Skočaj", "Matija Mavsar", "Hyojeong Yu", "Minseo Kwon", "Young J. Kim", "Yang Cong", "Ronghan Chen", "Yu Ren", "Supeng Diao", "Jiawei Weng", "Jiayue Liu", "Haoran Sun", "Linhan Yang", "Zeqing Zhang", "Ning Guo", "Lei Yang", "Fang Wan", "Chaoyang Song", "Jia Pan", "Yixiang Jin", "Yong A", "Jun Shi", "Dingzhe Li", "Yong Yang", "Kakeru Yamasaki", "Takumi Kajiwara", "Yuki Nakadera", "Krati Saxena", "Tomohiro Shibata", "Chongkun Xia", "Kai Mo", "Yanzhao Yu", "Qihao Lin", "Binqiang Ma", "Uihun Sagong", "JungHyun Choi", "JeongHyun Park", "Dongwoo Lee", "Yeongmin Kim", "Myun Joong Hwang", "Yusuke Kuribayashi", "Naoki Hiratsuka", "Daisuke Tanaka", "Solvi Arnold", "Kimitoshi Yamazaki", "Carlos Mateo-Agullo", "Andreas Verleysen", "Francis Wyffels"], "title": "A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition", "comment": "submitted to IJRR", "summary": "Robotic cloth manipulation suffers from a lack of standardized benchmarks and\nshared datasets for evaluating and comparing different approaches. To address\nthis, we created a benchmark and organized the ICRA 2024 Cloth Competition, a\nunique head-to-head evaluation focused on grasp pose selection for in-air\nrobotic cloth unfolding. Eleven diverse teams participated in the competition,\nutilizing our publicly released dataset of real-world robotic cloth unfolding\nattempts and a variety of methods to design their unfolding approaches.\nAfterwards, we also expanded our dataset with 176 competition evaluation\ntrials, resulting in a dataset of 679 unfolding demonstrations across 34\ngarments. Analysis of the competition results revealed insights about the\ntrade-off between grasp success and coverage, the surprisingly strong\nachievements of hand-engineered methods and a significant discrepancy between\ncompetition performance and prior work, underscoring the importance of\nindependent, out-of-the-lab evaluation in robotic cloth manipulation. The\nassociated dataset is a valuable resource for developing and evaluating grasp\nselection methods, particularly for learning-based approaches. We hope that our\nbenchmark, dataset and competition results can serve as a foundation for future\nbenchmarks and drive further progress in data-driven robotic cloth\nmanipulation. The dataset and benchmarking code are available at\nhttps://airo.ugent.be/cloth_competition.", "AI": {"tldr": "创建了机器人布料操作基准测试和ICRA 2024布料竞赛，发布了包含679次展开演示的公开数据集，分析了抓取成功率与覆盖范围的权衡，发现手工设计方法表现优异。", "motivation": "机器人布料操作缺乏标准化基准测试和共享数据集来评估和比较不同方法，需要建立统一的评估标准。", "method": "组织ICRA 2024布料竞赛，11个团队参与，使用公开数据集设计展开方法，并扩展数据集至679次演示。", "result": "竞赛结果揭示了抓取成功率与覆盖范围的权衡关系，手工设计方法表现强劲，竞赛表现与先前研究存在显著差异。", "conclusion": "该基准测试、数据集和竞赛结果为未来基准测试奠定了基础，强调了独立实验室外评估的重要性，推动了数据驱动机器人布料操作的进展。"}}
{"id": "2508.16807", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.16807", "abs": "https://arxiv.org/abs/2508.16807", "authors": ["Marco S. Tayar", "Lucas K. de Oliveira", "Juliano D. Negri", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach", "comment": null, "summary": "Inspecting confined industrial infrastructure, such as ventilation shafts, is\na hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)\noffer a promising alternative, but GPS-denied environments require robust\ncontrol policies to prevent collisions. Deep Reinforcement Learning (DRL) has\nemerged as a powerful framework for developing such policies, and this paper\nprovides a comparative study of two leading DRL algorithms for this task: the\non-policy Proximal Policy Optimization (PPO) and the off-policy Soft\nActor-Critic (SAC). The training was conducted with procedurally generated duct\nenvironments in Genesis simulation environment. A reward function was designed\nto guide a drone through a series of waypoints while applying a significant\npenalty for collisions. PPO learned a stable policy that completed all\nevaluation episodes without collision, producing smooth trajectories. By\ncontrast, SAC consistently converged to a suboptimal behavior that traversed\nonly the initial segments before failure. These results suggest that, in\nhazard-dense navigation, the training stability of on-policy methods can\noutweigh the nominal sample efficiency of off-policy algorithms. More broadly,\nthe study provides evidence that procedurally generated, high-fidelity\nsimulations are effective testbeds for developing and benchmarking robust\nnavigation policies.", "AI": {"tldr": "比较PPO和SAC两种深度强化学习算法在GPS拒止环境下无人机管道导航的性能，发现PPO表现更稳定可靠", "motivation": "工业基础设施（如通风管道）的人工检测危险且低效，无人机是替代方案，但GPS拒止环境需要鲁棒的控制策略", "method": "使用Genesis仿真环境中的程序化生成管道环境进行训练，设计奖励函数引导无人机通过航点，对碰撞施加重罚", "result": "PPO学会了稳定策略，所有评估测试均无碰撞完成，轨迹平滑；SAC则收敛到次优行为，仅能通过初始段后失败", "conclusion": "在危险密集导航任务中，on-policy方法（PPO）的训练稳定性优于off-policy算法（SAC）的名义样本效率；程序化生成的高保真仿真是开发和基准测试导航策略的有效平台"}}
{"id": "2508.16856", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.16856", "abs": "https://arxiv.org/abs/2508.16856", "authors": ["Zubair Islam", "Ahmaad Ansari", "George Daoud", "Mohamed El-Darieby"], "title": "A Workflow for Map Creation in Autonomous Vehicle Simulations", "comment": "6 pages, 12 figures. Published in the Proceedings of GEOProcessing\n  2025: The Seventeenth International Conference on Advanced Geographic\n  Information Systems, Applications, and Services (IARIA)", "summary": "The fast development of technology and artificial intelligence has\nsignificantly advanced Autonomous Vehicle (AV) research, emphasizing the need\nfor extensive simulation testing. Accurate and adaptable maps are critical in\nAV development, serving as the foundation for localization, path planning, and\nscenario testing. However, creating simulation-ready maps is often difficult\nand resource-intensive, especially with simulators like CARLA (CAR Learning to\nAct). Many existing workflows require significant computational resources or\nrely on specific simulators, limiting flexibility for developers. This paper\npresents a custom workflow to streamline map creation for AV development,\ndemonstrated through the generation of a 3D map of a parking lot at Ontario\nTech University. Future work will focus on incorporating SLAM technologies,\noptimizing the workflow for broader simulator compatibility, and exploring more\nflexible handling of latitude and longitude values to enhance map generation\naccuracy.", "AI": {"tldr": "这篇论文提出了一种自动驾驶车开发的流程化地图创建方法，通过实例生成了安大略省科技大学停车场的3D地图，解决了传统地图制作资源浪费和简化问题。", "motivation": "自动驾驶车研究需要大量模拟测试，而准确的地图是定位、路径规划和场景测试的基础。但现有地图制作方法通常需要大量计算资源或依赖特定模拟器，影响了开发者的灵活性。", "method": "设计了一种自定义流程来流程化地图创建过程，并通过实例生成了安大略省科技大学停车场的3D地图。", "result": "成功开发出了一种更灵活的地图创建方法，减少了对特定模拟器的依赖和计算资源的需求。", "conclusion": "该方法有效提高了自动驾驶车地图制作的效率和灵活性，未来工作将重点关注集成SLAM技术、优化模拟器兼容性和提高经纬度数据处理的灵活性。"}}
{"id": "2508.16901", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY", "I.2.9; I.2.8; F.2.2"], "pdf": "https://arxiv.org/pdf/2508.16901", "abs": "https://arxiv.org/abs/2508.16901", "authors": ["David Baxter", "Aldo Terán Espinoza", "Antonio Terán Espinoza", "Amy Loutfi", "John Folkesson", "Peter Sigray", "Stephanie Lowry", "Jakob Kuttenkeuler"], "title": "Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations", "comment": "10 pages, 7 figures. Equal contribution by David Baxter and Aldo\n  Ter\\'an Espinoza. Supported by SAAB, SMaRC, and WASP. Supported by SAAB and\n  the Swedish Maritime Robotics Centre (SMaRC), and by the Wallenberg AI,\n  Autonomous Systems and Software Program (WASP) funded by the Knut and Alice\n  Wallenberg Foundation", "summary": "Estimating a target's 6-DoF motion in underwater proximity operations is\ndifficult because the chaser lacks target-side proprioception and the available\nrelative observations are sparse, noisy, and often partial (e.g., Ultra-Short\nBaseline (USBL) positions). Without a motion prior, factor-graph maximum a\nposteriori estimation is underconstrained: consecutive target states are weakly\nlinked and orientation can drift. We propose a generalized constant-twist\nmotion prior defined on the tangent space of Lie groups that enforces\ntemporally consistent trajectories across all degrees of freedom; in SE(3) it\ncouples translation and rotation in the body frame. We present a ternary factor\nand derive its closed-form Jacobians based on standard Lie group operations,\nenabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two\ndeployment modes: (A) an SE(3)-only representation that regularizes orientation\neven when only position is measured, and (B) a mode with boundary factors that\nswitches the target representation between SE(3) and 3D position while applying\nthe same generalized constant-twist prior across representation changes.\nValidation on a real-world dynamic docking scenario dataset shows consistent\nego-target trajectory estimation through USBL-only and optical relative\nmeasurement segments with an improved relative tracking accuracy compared to\nthe noisy measurements to the target. Because the construction relies on\nstandard Lie group primitives, it is portable across state manifolds and\nsensing modalities.", "AI": {"tldr": "提出了一种基于李群切空间的广义恒定扭转运动先验，用于水下近距离操作中目标6-DoF运动估计，通过SE(3)中的三元因子耦合平移和旋转，提高轨迹估计精度。", "motivation": "水下近距离操作中，追踪器缺乏目标侧本体感知，可用相对观测稀疏、噪声大且部分（如USBL位置），无运动先验时因子图最大后验估计欠约束，导致连续目标状态弱连接和方向漂移。", "method": "提出广义恒定扭转运动先验，定义在李群切空间上，确保时间一致的轨迹；设计三元因子并推导闭式雅可比矩阵，支持任意李群轨迹；评估两种部署模式：SE(3)表示和边界因子切换表示模式。", "result": "在真实动态对接场景数据集上验证，通过USBL-only和光学相对测量段实现一致的自我-目标轨迹估计，相比噪声测量提高了相对跟踪精度。", "conclusion": "该方法基于标准李群原语构建，可跨状态流形和感知模态移植，为水下6-DoF运动估计提供了有效的运动先验解决方案。"}}
{"id": "2508.16943", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16943", "abs": "https://arxiv.org/abs/2508.16943", "authors": ["Haozhuo Zhang", "Jingkai Sun", "Michele Caprio", "Jian Tang", "Shanghang Zhang", "Qiang Zhang", "Wei Pan"], "title": "HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement", "comment": "Project Page:\n  https://haozhuo-zhang.github.io/HumanoidVerse-project-page/", "summary": "We introduce HumanoidVerse, a novel framework for vision-language guided\nhumanoid control that enables a single physically simulated robot to perform\nlong-horizon, multi-object rearrangement tasks across diverse scenes. Unlike\nprior methods that operate in fixed settings with single-object interactions,\nour approach supports consecutive manipulation of multiple objects, guided only\nby natural language instructions and egocentric camera RGB observations.\nHumanoidVerse is trained via a multi-stage curriculum using a dual-teacher\ndistillation pipeline, enabling fluid transitions between sub-tasks without\nrequiring environment resets. To support this, we construct a large-scale\ndataset comprising 350 multi-object tasks spanning four room layouts. Extensive\nexperiments in the Isaac Gym simulator demonstrate that our method\nsignificantly outperforms prior state-of-the-art in both task success rate and\nspatial precision, and generalizes well to unseen environments and\ninstructions. Our work represents a key step toward robust, general-purpose\nhumanoid agents capable of executing complex, sequential tasks under real-world\nsensory constraints. The video visualization results can be found on the\nproject page: https://haozhuo-zhang.github.io/HumanoidVerse-project-page/.", "AI": {"tldr": "HumanoidVerse是一个新颖的视觉语言引导人形机器人控制框架，能够在多样化场景中执行长时程、多物体重排任务，仅使用自然语言指令和第一视角RGB观测。", "motivation": "现有方法通常在固定设置下进行单物体交互，无法处理现实世界中复杂的多物体连续操作任务。需要开发能够执行长时程、多物体重排的通用人形机器人控制方法。", "method": "采用多阶段课程学习和双教师蒸馏管道进行训练，构建了包含350个多物体任务的大规模数据集，涵盖四种房间布局。在Isaac Gym模拟器中进行物理仿真。", "result": "在任务成功率和空间精度方面显著优于现有最先进方法，能够很好地泛化到未见过的环境和指令。", "conclusion": "这项工作为实现能够在真实世界感官约束下执行复杂顺序任务的鲁棒通用人形智能体迈出了关键一步。"}}
{"id": "2508.17038", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17038", "abs": "https://arxiv.org/abs/2508.17038", "authors": ["Zhouheng Li", "Lei Xie", "Cheng Hu", "Hongye Su"], "title": "A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness", "comment": "Published in the journal Robotics and Autonomous Systems", "summary": "As autonomous driving continues to advance, automated parking is becoming\nincreasingly essential. However, significant challenges arise when implementing\npath velocity decomposition (PVD) trajectory planning for automated parking.\nThe primary challenge is ensuring rapid and precise collision-free trajectory\nplanning, which is often in conflict. The secondary challenge involves\nmaintaining sufficient control feasibility of the planned trajectory,\nparticularly at gear shifting points (GSP). This paper proposes a PVD-based\nrapid iterative trajectory planning (RITP) method to solve the above\nchallenges. The proposed method effectively balances the necessity for time\nefficiency and precise collision avoidance through a novel collision avoidance\nframework. Moreover, it enhances the overall control feasibility of the planned\ntrajectory by incorporating the vehicle kinematics model and including terminal\nsmoothing constraints (TSC) at GSP during path planning. Specifically, the\nproposed method leverages differential flatness to ensure the planned path\nadheres to the vehicle kinematic model. Additionally, it utilizes TSC to\nmaintain curvature continuity at GSP, thereby enhancing the control feasibility\nof the overall trajectory. The simulation results demonstrate superior time\nefficiency and tracking errors compared to model-integrated and other\niteration-based trajectory planning methods. In the real-world experiment, the\nproposed method was implemented and validated on a ROS-based vehicle,\ndemonstrating the applicability of the RITP method for real vehicles.", "AI": {"tldr": "自动馨车停车轨迹规划方法，通过新的碰撞避免框架和终端平滑约束，解决了轨迹规划中的时间效率与精确性、控制可行性的矛盾", "motivation": "自动馨车停车轨迹规划遇到两大挑战：快速精确的无碰撞轨迹规划，以及在换档点保持轨迹的控制可行性", "method": "提出基于PVD的快速迭代轨迹规划方法(RITP)，采用新型碰撞避免框架平衡时间效率与精确性，通过车辆运动学模型和终端平滑约束提高控制可行性", "result": "模拟结果显示该方法在时间效率和跟踪误差方面都优于模型集成和其他迭代基础方法，并在ROS基础车辆上实际验证了其实际可用性", "conclusion": "该RITP方法有效解决了自动停车轨迹规划中的关键挑战，为实际应用提供了高效、高精度且控制可行的解决方案"}}
{"id": "2508.16947", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16947", "abs": "https://arxiv.org/abs/2508.16947", "authors": ["Fan Ding", "Xuewen Luo", "Hwa Hui Tew", "Ruturaj Reddy", "Xikun Wang", "Junn Yong Loo"], "title": "Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model", "comment": "Has been submitted to AAAI 2026", "summary": "Recent advances in motion planning for autonomous driving have led to models\ncapable of generating high-quality trajectories. However, most existing\nplanners tend to fix their policy after supervised training, leading to\nconsistent but rigid driving behaviors. This limits their ability to reflect\nhuman preferences or adapt to dynamic, instruction-driven demands. In this\nwork, we propose a diffusion-based multi-head trajectory planner(M-diffusion\nplanner). During the early training stage, all output heads share weights to\nlearn to generate high-quality trajectories. Leveraging the probabilistic\nnature of diffusion models, we then apply Group Relative Policy Optimization\n(GRPO) to fine-tune the pre-trained model for diverse policy-specific\nbehaviors. At inference time, we incorporate a large language model (LLM) to\nguide strategy selection, enabling dynamic, instruction-aware planning without\nswitching models. Closed-loop simulation demonstrates that our post-trained\nplanner retains strong planning capability while achieving state-of-the-art\n(SOTA) performance on the nuPlan val14 benchmark. Open-loop results further\nshow that the generated trajectories exhibit clear diversity, effectively\nsatisfying multi-modal driving behavior requirements. The code and related\nexperiments will be released upon acceptance of the paper.", "AI": {"tldr": "提出基于扩散模型的多头轨迹规划器(M-diffusion planner)，通过GRPO策略优化实现多样化驾驶行为，结合LLM进行动态指令感知规划，在nuPlan基准测试中达到SOTA性能。", "motivation": "现有自动驾驶运动规划模型在监督训练后策略固定，导致驾驶行为僵化，无法反映人类偏好或适应动态指令驱动的需求。", "method": "使用扩散模型构建多头轨迹规划器，早期训练共享权重生成高质量轨迹，后期应用GRPO进行策略特定行为微调，推理时结合LLM指导策略选择。", "result": "闭环仿真显示规划器保持强规划能力，在nuPlan val14基准测试中达到SOTA性能；开环结果显示生成轨迹具有明显多样性，有效满足多模态驾驶行为需求。", "conclusion": "该方法成功实现了在保持规划质量的同时提供多样化、可适应动态指令的自动驾驶行为，为自动驾驶规划系统提供了新的解决方案。"}}
{"id": "2508.17466", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17466", "abs": "https://arxiv.org/abs/2508.17466", "authors": ["Dilermando Almeida", "Guilherme Lazzarini", "Juliano Negri", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation", "comment": null, "summary": "Quadruped robots have emerged as highly efficient and versatile platforms,\nexcelling in navigating complex and unstructured terrains where traditional\nwheeled robots might fail. Equipping these robots with manipulator arms unlocks\nthe advanced capability of loco-manipulation to perform complex physical\ninteraction tasks in areas ranging from industrial automation to\nsearch-and-rescue missions. However, achieving precise and adaptable grasping\nin such dynamic scenarios remains a significant challenge, often hindered by\nthe need for extensive real-world calibration and pre-programmed grasp\nconfigurations. This paper introduces a deep learning framework designed to\nenhance the grasping capabilities of quadrupeds equipped with arms, focusing on\nimproved precision and adaptability. Our approach centers on a sim-to-real\nmethodology that minimizes reliance on physical data collection. We developed a\npipeline within the Genesis simulation environment to generate a synthetic\ndataset of grasp attempts on common objects. By simulating thousands of\ninteractions from various perspectives, we created pixel-wise annotated\ngrasp-quality maps to serve as the ground truth for our model. This dataset was\nused to train a custom CNN with a U-Net-like architecture that processes\nmulti-modal input from an onboard RGB and depth cameras, including RGB images,\ndepth maps, segmentation masks, and surface normal maps. The trained model\noutputs a grasp-quality heatmap to identify the optimal grasp point. We\nvalidated the complete framework on a four-legged robot. The system\nsuccessfully executed a full loco-manipulation task: autonomously navigating to\na target object, perceiving it with its sensors, predicting the optimal grasp\npose using our model, and performing a precise grasp. This work proves that\nleveraging simulated training with advanced sensing offers a scalable and\neffective solution for object handling.", "AI": {"tldr": "基于深度学习的模型，通过模拟到实际的方法，为四足机器人提供了高精度和适应性的抓取能力，完成了从自主导航到准确抓取的完整任务。", "motivation": "四足机器人在复杂地形中具有优势，但配备操作臂后的精确抓取仍面临挑战，需要大量实际校准和预编程配置，很难适应动态场景。", "method": "采用模拟到实际的方法，在Genesis模拟环境中生成合成数据集，包含上千次抓取尝试的像素级注释抓取质量地图。训练了一个基于U-Net结构的自定义CNN模型，处理来自RGB和深度摄像头的多模态输入，输出抓取质量热力图来确定最佳抓取点。", "result": "在四足机器人上验证了完整框架，系统成功执行了完整的移动操作任务：自主导航到目标物体、使用传感器感知、使用模型预测最佳抓取姿势并执行精确抓取。", "conclusion": "这项工作证明，利用模拟训练结合先进感知技术可以为物体处理提供可扩展和高效的解决方案。"}}
{"id": "2508.16962", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16962", "abs": "https://arxiv.org/abs/2508.16962", "authors": ["Wendi Li", "Hao Wu", "Han Gao", "Bing Mao", "Fengyuan Xu", "Sheng Zhong"], "title": "LLM-based Human-like Traffic Simulation for Self-driving Tests", "comment": null, "summary": "Ensuring realistic traffic dynamics is a prerequisite for simulation\nplatforms to evaluate the reliability of self-driving systems before deployment\nin the real world. Because most road users are human drivers, reproducing their\ndiverse behaviors within simulators is vital. Existing solutions, however,\ntypically rely on either handcrafted heuristics or narrow data-driven models,\nwhich capture only fragments of real driving behaviors and offer limited\ndriving style diversity and interpretability. To address this gap, we introduce\nHDSim, an HD traffic generation framework that combines cognitive theory with\nlarge language model (LLM) assistance to produce scalable and realistic traffic\nscenarios within simulation platforms. The framework advances the state of the\nart in two ways: (i) it introduces a hierarchical driver model that represents\ndiverse driving style traits, and (ii) it develops a Perception-Mediated\nBehavior Influence strategy, where LLMs guide perception to indirectly shape\ndriver actions. Experiments reveal that embedding HDSim into simulation\nimproves detection of safety-critical failures in self-driving systems by up to\n68% and yields realism-consistent accident interpretability.", "AI": {"tldr": "HDSim是一个结合认知理论和大型语言模型的交通生成框架，通过分层驾驶员模型和感知介导行为影响策略，在仿真平台中生成可扩展且真实的交通场景。", "motivation": "现有仿真平台依赖手工启发式或有限的数据驱动模型，只能捕捉真实驾驶行为的片段，驾驶风格多样性和可解释性有限。需要更真实的交通动态来评估自动驾驶系统的可靠性。", "method": "1. 引入分层驾驶员模型来表示多样化的驾驶风格特征；2. 开发感知介导行为影响策略，使用LLM指导感知来间接塑造驾驶员行为。", "result": "将HDSim嵌入仿真后，自动驾驶系统的安全关键故障检测率提高高达68%，并实现了真实性一致的事故可解释性。", "conclusion": "HDSim框架通过结合认知理论和LLM辅助，显著提升了交通仿真的真实性和有效性，为自动驾驶系统评估提供了更可靠的测试环境。"}}
{"id": "2508.18039", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18039", "abs": "https://arxiv.org/abs/2508.18039", "authors": ["Diego Quevedo", "Sarah Hudson", "Donghoon Kim"], "title": "Modeling and Control Framework for Autonomous Space Manipulator Handover Operations", "comment": "14 pages, submitted to 2025 Astrodynamics Specialists Conference\n  proceedings", "summary": "Autonomous space robotics is poised to play a vital role in future space\nmissions, particularly for In-space Servicing, Assembly, and Manufacturing\n(ISAM). A key capability in such missions is the Robot-to-Robot (R2R) handover\nof mission-critical objects. This work presents a dynamic model of a dual-arm\nspace manipulator system and compares various tracking control laws. The key\ncontributions of this work are the development of a cooperative manipulator\ndynamic model and the comparative analysis of control laws to support\nautonomous R2R handovers in ISAM scenarios.", "AI": {"tldr": "本文开发了双臂空间机械臂系统的动态模型，并比较了不同的跟踪控制律，以支持自主机器人间任务关键对象交接。", "motivation": "自主空间机器人技术在未来的空间任务中至关重要，特别是在空间服务、装配和制造(ISAM)领域。机器人间(R2R)任务关键对象的交接是此类任务中的关键能力。", "method": "开发了协作机械臂动态模型，并对各种跟踪控制律进行了比较分析。", "result": "提出了双臂空间机械臂系统的动态模型，并比较了不同控制律的性能。", "conclusion": "这项工作为自主ISAM场景中的R2R交接提供了动态模型和控制律分析基础，支持未来空间任务中机器人协作能力的发展。"}}
{"id": "2508.17034", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17034", "abs": "https://arxiv.org/abs/2508.17034", "authors": ["Jiayi Li", "Yuxin Yao", "Qiuhang Lu", "Juyong Zhang"], "title": "DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration", "comment": null, "summary": "Rigid registration, aiming to estimate a rigid transformation to align source\nand target data, play a crucial role in applications such as SLAM and 3D\nreconstruction. However, noisy, partially overlapping data and the need for\nreal-time processing pose major challenges for rigid registration. Considering\nthat feature-based matching can handle large transformation differences but\nsuffers from limited accuracy, while local geometry-based matching can achieve\nfine-grained local alignment but relies heavily on a good initial\ntransformation, we propose a novel dual-space paradigm to fully leverage the\nstrengths of both approaches. First, we introduce an efficient filtering\nmechanism that incorporates a computationally lightweight single-point RANSAC\nalgorithm followed by a refinement module to eliminate unreliable feature-based\ncorrespondences. Subsequently, we treat filtered correspondences as anchor\npoints, extract geometric proxies, and formulates an effective objective\nfunction with a tailored solver to estimate the transformation. Experiments\nverify our method's effectiveness, as shown by achieving up to a 32x CPU-time\nspeedup over MAC on KITTI with comparable accuracy.", "AI": {"tldr": "提出了一种新的双空间配准范式，结合特征匹配和几何匹配的优势，通过高效过滤机制和几何代理实现快速准确的刚性配准", "motivation": "刚性配准在SLAM和3D重建中至关重要，但噪声数据、部分重叠和实时处理需求带来挑战。特征匹配能处理大变换但精度有限，几何匹配能精细对齐但依赖好的初始变换", "method": "采用双空间范式：1) 使用轻量级单点RANSAC算法和精炼模块过滤不可靠特征对应；2) 将过滤后的对应作为锚点，提取几何代理，构建目标函数求解变换", "result": "在KITTI数据集上实现高达32倍的CPU时间加速，同时保持可比精度", "conclusion": "提出的双空间方法有效结合了特征匹配和几何匹配的优势，实现了快速准确的刚性配准，适用于实时应用场景"}}
{"id": "2508.17070", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17070", "abs": "https://arxiv.org/abs/2508.17070", "authors": ["Halid Abdulrahim Kadi", "Kasim Terzić"], "title": "LaGarNet: Goal-Conditioned Recurrent State-Space Models for Pick-and-Place Garment Flattening", "comment": "20 pages, 11 figures and 3 tables", "summary": "We present a novel goal-conditioned recurrent state space (GC-RSSM) model\ncapable of learning latent dynamics of pick-and-place garment manipulation. Our\nproposed method LaGarNet matches the state-of-the-art performance of mesh-based\nmethods, marking the first successful application of state-space models on\ncomplex garments. LaGarNet trains on a coverage-alignment reward and a dataset\ncollected through a general procedure supported by a random policy and a\ndiffusion policy learned from few human demonstrations; it substantially\nreduces the inductive biases introduced in the previous similar methods. We\ndemonstrate that a single-policy LaGarNet achieves flattening on four different\ntypes of garments in both real-world and simulation settings.", "AI": {"tldr": "提出了GC-RSSM模型LaGarNet，在衣物抓取放置操作中实现了与基于网格方法相当的性能，是首个在复杂衣物上成功应用状态空间模型的方法", "motivation": "解决衣物操作任务中状态空间模型应用困难的问题，减少先前方法中的归纳偏差，实现更通用的衣物操作策略", "method": "使用目标条件循环状态空间模型(GC-RSSM)，在覆盖对齐奖励和混合数据集（随机策略+扩散策略收集）上训练", "result": "单一策略LaGarNet在四种不同类型衣物的平整化任务中，在真实世界和仿真环境中均表现出色，匹配了基于网格方法的性能", "conclusion": "LaGarNet成功证明了状态空间模型在复杂衣物操作任务中的有效性，为减少归纳偏差和实现通用衣物操作策略提供了新途径"}}
{"id": "2508.17260", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17260", "abs": "https://arxiv.org/abs/2508.17260", "authors": ["Anurag Maurya", "Tashmoy Ghosh", "Anh Nguyen", "Ravi Prakash"], "title": "OVITA: Open-Vocabulary Interpretable Trajectory Adaptations", "comment": "Accepted to Robotics and Automation Letters 2025. Code link:\n  https://github.com/anurag1000101/OVITA", "summary": "Adapting trajectories to dynamic situations and user preferences is crucial\nfor robot operation in unstructured environments with non-expert users. Natural\nlanguage enables users to express these adjustments in an interactive manner.\nWe introduce OVITA, an interpretable, open-vocabulary, language-driven\nframework designed for adapting robot trajectories in dynamic and novel\nsituations based on human instructions. OVITA leverages multiple pre-trained\nLarge Language Models (LLMs) to integrate user commands into trajectories\ngenerated by motion planners or those learned through demonstrations. OVITA\nemploys code as an adaptation policy generated by an LLM, enabling users to\nadjust individual waypoints, thus providing flexible control. Another LLM,\nwhich acts as a code explainer, removes the need for expert users, enabling\nintuitive interactions. The efficacy and significance of the proposed OVITA\nframework is demonstrated through extensive simulations and real-world\nenvironments with diverse tasks involving spatiotemporal variations on\nheterogeneous robotic platforms such as a KUKA IIWA robot manipulator,\nClearpath Jackal ground robot, and CrazyFlie drone.", "AI": {"tldr": "OVITA是一个基于大型语言模型的框架，通过自然语言指令来适应和调整机器人轨迹，支持多种机器人平台和动态环境。", "motivation": "在非结构化环境中，非专业用户需要通过自然语言直观地调整机器人轨迹以适应动态情况和用户偏好。", "method": "利用多个预训练大型语言模型，通过生成代码作为适应策略来调整轨迹路径点，并使用另一个LLM作为代码解释器实现直观交互。", "result": "在模拟和真实环境中进行了广泛测试，证明了框架在具有时空变化的异构机器人平台上的有效性和重要性。", "conclusion": "OVITA提供了一个可解释、开放词汇的语言驱动框架，能够有效适应动态和新颖情况下的机器人轨迹调整。"}}
{"id": "2508.17449", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17449", "abs": "https://arxiv.org/abs/2508.17449", "authors": ["Zezeng Li", "Alexandre Chapin", "Enda Xiang", "Rui Yang", "Bruno Machado", "Na Lei", "Emmanuel Dellandrea", "Di Huang", "Liming Chen"], "title": "Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges", "comment": null, "summary": "Robotic Manipulation (RM) is central to the advancement of autonomous robots,\nenabling them to interact with and manipulate objects in real-world\nenvironments. This survey focuses on RM methodologies that leverage imitation\nlearning, a powerful technique that allows robots to learn complex manipulation\nskills by mimicking human demonstrations. We identify and analyze the most\ninfluential studies in this domain, selected based on community impact and\nintrinsic quality. For each paper, we provide a structured summary, covering\nthe research purpose, technical implementation, hierarchical classification,\ninput formats, key priors, strengths and limitations, and citation metrics.\nAdditionally, we trace the chronological development of imitation learning\ntechniques within RM policy (RMP), offering a timeline of key technological\nadvancements. Where available, we report benchmark results and perform\nquantitative evaluations to compare existing methods. By synthesizing these\ninsights, this review provides a comprehensive resource for researchers and\npractitioners, highlighting both the state of the art and the challenges that\nlie ahead in the field of robotic manipulation through imitation learning.", "AI": {"tldr": "这篇综述论文系统回顾了机器人操作领域中基于模仿学习的方法，分析了该领域最具影响力的研究，提供了结构化总结和技术发展时间线。", "motivation": "机器人操作是自主机器人发展的核心，模仿学习作为一种强大技术，能让机器人通过模仿人类演示来学习复杂操作技能。本文旨在为该领域的研究者和实践者提供全面的资源。", "method": "通过识别和分析基于社区影响力和内在质量筛选出的最具影响力研究，为每篇论文提供结构化总结，包括研究目的、技术实现、层次分类、输入格式、关键先验、优缺点和引用指标等。", "result": "提供了模仿学习技术在机器人操作策略中的时间发展线，报告了可用的基准测试结果，并对现有方法进行了定量评估比较。", "conclusion": "这篇综述综合了机器人操作通过模仿学习领域的最新进展，既展示了当前技术水平，也指出了未来面临的挑战，为该领域的研究提供了重要参考。"}}
{"id": "2508.17464", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.17464", "abs": "https://arxiv.org/abs/2508.17464", "authors": ["Alican Mertan", "Nick Cheney"], "title": "Evolutionary Brain-Body Co-Optimization Consistently Fails to Select for Morphological Potential", "comment": "Accepted to be presented at ALife 2025 as a talk", "summary": "Brain-body co-optimization remains a challenging problem, despite increasing\ninterest from the community in recent years. To understand and overcome the\nchallenges, we propose exhaustively mapping a morphology-fitness landscape to\nstudy it. To this end, we train controllers for each feasible morphology in a\ndesign space of 1,305,840 distinct morphologies, constrained by a computational\nbudget. First, we show that this design space constitutes a good model for\nstudying the brain-body co-optimization problem, and our attempt to\nexhaustively map it roughly captures the landscape. We then proceed to analyze\nhow evolutionary brain-body co-optimization algorithms work in this design\nspace. The complete knowledge of the morphology-fitness landscape facilitates a\nbetter understanding of the results of evolutionary brain-body co-optimization\nalgorithms and how they unfold over evolutionary time in the morphology space.\nThis investigation shows that the experimented algorithms cannot consistently\nfind near-optimal solutions. The search, at times, gets stuck on morphologies\nthat are sometimes one mutation away from better morphologies, and the\nalgorithms cannot efficiently track the fitness gradient in the\nmorphology-fitness landscape. We provide evidence that experimented algorithms\nregularly undervalue the fitness of individuals with newly mutated bodies and,\nas a result, eliminate promising morphologies throughout evolution. Our work\nprovides the most concrete demonstration of the challenges of evolutionary\nbrain-body co-optimization. Our findings ground the trends in the literature\nand provide valuable insights for future work.", "AI": {"tldr": "该研究通过详尽映射包含130万种形态的设计空间，揭示了进化脑体协同优化算法面临的挑战，发现算法无法持续找到接近最优解，且经常低估新突变形态的适应度。", "motivation": "脑体协同优化是一个具有挑战性的问题，研究团队希望通过详尽映射形态-适应度景观来深入理解这一问题的本质和现有算法的局限性。", "method": "在包含1,305,840种不同形态的设计空间中，为每个可行形态训练控制器，并在此空间上详尽映射形态-适应度景观，分析进化脑体协同优化算法的工作原理。", "result": "实验表明现有算法无法持续找到接近最优解，搜索过程经常陷入局部最优，算法无法有效追踪形态-适应度景观中的适应度梯度，且经常低估新突变形态的适应度值。", "conclusion": "这项工作为进化脑体协同优化的挑战提供了最具体的实证，揭示了算法在形态空间中追踪适应度梯度的困难，为未来研究提供了重要见解。"}}
{"id": "2508.17469", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.17469", "abs": "https://arxiv.org/abs/2508.17469", "authors": ["Alican Mertan", "Nick Cheney"], "title": "Morphological Cognition: Classifying MNIST Digits Through Morphological Computation Alone", "comment": "Accepted to be presented at ALife 2025 as a talk", "summary": "With the rise of modern deep learning, neural networks have become an\nessential part of virtually every artificial intelligence system, making it\ndifficult even to imagine different models for intelligent behavior. In\ncontrast, nature provides us with many different mechanisms for intelligent\nbehavior, most of which we have yet to replicate. One of such underinvestigated\naspects of intelligence is embodiment and the role it plays in intelligent\nbehavior. In this work, we focus on how the simple and fixed behavior of\nconstituent parts of a simulated physical body can result in an emergent\nbehavior that can be classified as cognitive by an outside observer.\nSpecifically, we show how simulated voxels with fixed behaviors can be combined\nto create a robot such that, when presented with an image of an MNIST digit\nzero, it moves towards the left; and when it is presented with an image of an\nMNIST digit one, it moves towards the right. Such robots possess what we refer\nto as ``morphological cognition'' -- the ability to perform cognitive behavior\nas a result of morphological processes. To the best of our knowledge, this is\nthe first demonstration of a high-level mental faculty such as image\nclassification performed by a robot without any neural circuitry. We hope that\nthis work serves as a proof-of-concept and fosters further research into\ndifferent models of intelligence.", "AI": {"tldr": "该论文展示了如何通过模拟物理身体的固定行为部件实现形态认知，无需神经网络即可完成MNIST数字图像分类任务", "motivation": "当前深度学习主导AI研究，但自然界存在多种智能机制未被探索。论文关注体现性在智能行为中的作用，研究如何通过简单固定行为的物理部件组合产生认知行为", "method": "使用具有固定行为的模拟体素组合构建机器人，当呈现MNIST数字0时向左移动，数字1时向右移动，实现形态认知", "result": "成功演示了无需神经电路的高层次心智功能（图像分类），这是首次通过纯形态过程实现认知行为的证明", "conclusion": "这项工作作为概念验证，展示了形态认知的可能性，希望促进对不同智能模型的进一步研究"}}
{"id": "2508.17482", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17482", "abs": "https://arxiv.org/abs/2508.17482", "authors": ["S. Talha Bukhari", "Kaivalya Agrawal", "Zachary Kingston", "Aniket Bera"], "title": "Variational Shape Inference for Grasp Diffusion on SE(3)", "comment": null, "summary": "Grasp synthesis is a fundamental task in robotic manipulation which usually\nhas multiple feasible solutions. Multimodal grasp synthesis seeks to generate\ndiverse sets of stable grasps conditioned on object geometry, making the robust\nlearning of geometric features crucial for success. To address this challenge,\nwe propose a framework for learning multimodal grasp distributions that\nleverages variational shape inference to enhance robustness against shape noise\nand measurement sparsity. Our approach first trains a variational autoencoder\nfor shape inference using implicit neural representations, and then uses these\nlearned geometric features to guide a diffusion model for grasp synthesis on\nthe SE(3) manifold. Additionally, we introduce a test-time grasp optimization\ntechnique that can be integrated as a plugin to further enhance grasping\nperformance. Experimental results demonstrate that our shape inference for\ngrasp synthesis formulation outperforms state-of-the-art multimodal grasp\nsynthesis methods on the ACRONYM dataset by 6.3%, while demonstrating\nrobustness to deterioration in point cloud density compared to other\napproaches. Furthermore, our trained model achieves zero-shot transfer to\nreal-world manipulation of household objects, generating 34% more successful\ngrasps than baselines despite measurement noise and point cloud calibration\nerrors.", "AI": {"tldr": "通过变分形状推断与SE(3)流形模型结合，提出了一种多模态捌取合成方法，在形状噪声和点云稀疏情况下显著提升了性能和稳定性", "motivation": "多模态捌取合成需要生成多种稳定捌取方案，对对象几何特征的稳健学习至关重要。但现有方法在形状噪声和测量稀疏性时表现不佳", "method": "首先训练变分自动编码器进行隐式神经表示的形状推断，然后利用这些学到的几何特征来指导SE(3)流形模型进行捌取合成，并提出了测试时捌取优化技术", "result": "在ACRONYM数据集上超过最先进方法6.3%，对点云密度降低具有更好的稳健性。在实际家庭物品操作中实现零样本迁移，成功捌取率比基线提高34%，尽管存在测量噪声和点云检定错误", "conclusion": "该框架通过结合变分形状推断与SE(3)流形模型，有效提升了多模态捌取合成的稳健性和性能，并具有良好的实际应用演示效果"}}
{"id": "2508.17547", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17547", "abs": "https://arxiv.org/abs/2508.17547", "authors": ["Weikang Wan", "Jiawei Fu", "Xiaodi Yuan", "Yifeng Zhu", "Hao Su"], "title": "LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations", "comment": "CoRL 2025", "summary": "Developing robotic systems capable of robustly executing long-horizon\nmanipulation tasks with human-level dexterity is challenging, as such tasks\nrequire both physical dexterity and seamless sequencing of manipulation skills\nwhile robustly handling environment variations. While imitation learning offers\na promising approach, acquiring comprehensive datasets is resource-intensive.\nIn this work, we propose a learning framework and system LodeStar that\nautomatically decomposes task demonstrations into semantically meaningful\nskills using off-the-shelf foundation models, and generates diverse synthetic\ndemonstration datasets from a few human demos through reinforcement learning.\nThese sim-augmented datasets enable robust skill training, with a Skill Routing\nTransformer (SRT) policy effectively chaining the learned skills together to\nexecute complex long-horizon manipulation tasks. Experimental evaluations on\nthree challenging real-world long-horizon dexterous manipulation tasks\ndemonstrate that our approach significantly improves task performance and\nrobustness compared to previous baselines. Videos are available at\nlodestar-robot.github.io.", "AI": {"tldr": "LodeStar是一个机器人学习框架，利用基础模型自动分解任务演示为语义技能，并通过强化学习生成多样化合成演示数据集，显著提升长时程灵巧操作任务的性能和鲁棒性。", "motivation": "开发能够稳健执行长时程灵巧操作任务的机器人系统具有挑战性，需要物理灵巧性和技能无缝衔接，而模仿学习需要大量数据集，获取成本高昂。", "method": "使用现成的基础模型自动将任务演示分解为语义上有意义的技能，通过强化学习从少量人类演示生成多样化合成演示数据集，采用Skill Routing Transformer策略将学习到的技能链接执行复杂任务。", "result": "在三个具有挑战性的真实世界长时程灵巧操作任务上的实验评估表明，该方法相比之前的基线方法显著提高了任务性能和鲁棒性。", "conclusion": "LodeStar框架通过自动技能分解和合成数据增强，有效解决了长时程灵巧操作任务的学习挑战，为机器人系统提供了更高效的模仿学习方案。"}}
{"id": "2508.17600", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17600", "abs": "https://arxiv.org/abs/2508.17600", "authors": ["Guanxing Lu", "Baoxiong Jia", "Puhao Li", "Yixin Chen", "Ziwei Wang", "Yansong Tang", "Siyuan Huang"], "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation", "comment": "Published at ICCV 2025. Project page:\n  https://gaussian-world-model.github.io/", "summary": "Training robot policies within a learned world model is trending due to the\ninefficiency of real-world interactions. The established image-based world\nmodels and policies have shown prior success, but lack robust geometric\ninformation that requires consistent spatial and physical understanding of the\nthree-dimensional world, even pre-trained on internet-scale video sources. To\nthis end, we propose a novel branch of world model named Gaussian World Model\n(GWM) for robotic manipulation, which reconstructs the future state by\ninferring the propagation of Gaussian primitives under the effect of robot\nactions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D\nvariational autoencoder, enabling fine-grained scene-level future state\nreconstruction with Gaussian Splatting. GWM can not only enhance the visual\nrepresentation for imitation learning agent by self-supervised future\nprediction training, but can serve as a neural simulator that supports\nmodel-based reinforcement learning. Both simulated and real-world experiments\ndepict that GWM can precisely predict future scenes conditioned on diverse\nrobot actions, and can be further utilized to train policies that outperform\nthe state-of-the-art by impressive margins, showcasing the initial data scaling\npotential of 3D world model.", "AI": {"tldr": "提出基于高斯原语传播的3D世界模型GWM，通过扩散变换器和3D变分自编码器实现精细场景重建，显著提升机器人操作策略性能", "motivation": "现有基于图像的世界模型缺乏稳健的几何信息，无法提供一致的空间和物理理解，限制了在机器人操作任务中的效果", "method": "使用高斯原语推断机器人动作下的传播，结合潜在扩散变换器(DiT)和3D变分自编码器，通过高斯溅射实现细粒度场景级未来状态重建", "result": "在仿真和真实世界实验中，GWM能够精确预测不同机器人动作条件下的未来场景，训练出的策略性能显著超越现有最优方法", "conclusion": "GWM展示了3D世界模型在数据扩展方面的潜力，既能增强模仿学习的视觉表示，又能作为支持基于模型强化学习的神经模拟器"}}
{"id": "2508.17643", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17643", "abs": "https://arxiv.org/abs/2508.17643", "authors": ["Krishna Vinod", "Prithvi Jai Ramesh", "Pavan Kumar B N", "Bharatesh Chakravarthi"], "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation", "comment": null, "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/", "AI": {"tldr": "开发了一个开源ROS包，用于从Gazebo模拟器的RGB相机生成事件流，并研究了基于事件的机器人策略在导航和操作任务中的性能。", "motivation": "事件相机具有微秒级延迟、高动态范围和低功耗等优势，但在主流机器人模拟器中缺乏合成事件视觉的支持，这阻碍了事件驱动方法在机器人操作和导航任务中的评估。", "method": "提出了一个开源的、用户友好的v2e ROS包，能够从RGB相机源无缝生成事件流。使用基于Transformer的事件机器人策略，通过行为克隆进行训练，并在移动机器人目标跟随和机械臂目标检测抓取两个场景中与RGB基准进行比较。", "result": "实验结果表明，事件引导的策略在各种操作条件下始终提供竞争优势，突显了事件驱动感知在改善实时机器人导航和操作方面的潜力。", "conclusion": "这项工作为事件相机更广泛地集成到机器人策略学习奠定了基础，展示了事件驱动感知在机器人应用中的实际价值。"}}
{"id": "2508.17684", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17684", "abs": "https://arxiv.org/abs/2508.17684", "authors": ["Kento Kawaharazuka", "Shogo Sawaguchi", "Ayumu Iwata", "Keita Yoneda", "Temma Suzuki", "Kei Okada"], "title": "MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding", "comment": "Accepted at IEEE-RAS Humanoids2025, Website -\n  https://haraduka.github.io/mevita-hardware , YouTube -\n  https://youtu.be/_akfHkCne0s", "summary": "Various bipedal robots have been developed to date, and in recent years,\nthere has been a growing trend toward releasing these robots as open-source\nplatforms. This shift is fostering an environment in which anyone can freely\ndevelop bipedal robots and share their knowledge, rather than relying solely on\ncommercial products. However, most existing open-source bipedal robots are\ndesigned to be fabricated using 3D printers, which limits their scalability in\nsize and often results in fragile structures. On the other hand, some\nmetal-based bipedal robots have been developed, but they typically involve a\nlarge number of components, making assembly difficult, and in some cases, the\nparts themselves are not readily available through e-commerce platforms. To\naddress these issues, we developed MEVITA, an open-source bipedal robot that\ncan be built entirely from components available via e-commerce. Aiming for the\nminimal viable configuration for a bipedal robot, we utilized sheet metal\nwelding to integrate complex geometries into single parts, thereby\nsignificantly reducing the number of components and enabling easy assembly for\nanyone. Through reinforcement learning in simulation and Sim-to-Real transfer,\nwe demonstrated robust walking behaviors across various environments,\nconfirming the effectiveness of our approach. All hardware, software, and\ntraining environments can be obtained from https://github.com/haraduka/mevita .", "AI": {"tldr": "开发了MEVITA开源双足机器人，使用电商平台可获取的零件和钣金焊接技术，简化组装过程，并通过强化学习实现了稳健的行走能力", "motivation": "现有开源双足机器人大多依赖3D打印，尺寸扩展性有限且结构脆弱；金属基机器人零件数量多、组装困难且不易获取。需要开发易于组装、零件易得的开源双足机器人平台", "method": "采用钣金焊接技术将复杂几何结构集成到单个零件中，显著减少组件数量；通过仿真中的强化学习和Sim-to-Real迁移技术实现行走控制", "result": "成功开发出MEVITA机器人，实现了在各种环境中的稳健行走行为，验证了方法的有效性", "conclusion": "MEVITA提供了一个可行的开源双足机器人解决方案，硬件、软件和训练环境全部开源，促进了双足机器人技术的普及和发展"}}
{"id": "2508.17753", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17753", "abs": "https://arxiv.org/abs/2508.17753", "authors": ["Theresa Pekarek Rosin", "Julia Gachot", "Henri-Leon Kordt", "Matthias Kerzel", "Stefan Wermter"], "title": "Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications", "comment": "Accepted at the workshop on Foundation Models for Social Robotics\n  (FoMoSR) at ICSR 2025", "summary": "Automatic Speech Recognition (ASR) systems in real-world settings need to\nhandle imperfect audio, often degraded by hardware limitations or environmental\nnoise, while accommodating diverse user groups. In human-robot interaction\n(HRI), these challenges intersect to create a uniquely challenging recognition\nenvironment. We evaluate four state-of-the-art ASR systems on eight publicly\navailable datasets that capture six dimensions of difficulty: domain-specific,\naccented, noisy, age-variant, impaired, and spontaneous speech. Our analysis\ndemonstrates significant variations in performance, hallucination tendencies,\nand inherent biases, despite similar scores on standard benchmarks. These\nlimitations have serious implications for HRI, where recognition errors can\ninterfere with task performance, user trust, and safety.", "AI": {"tldr": "这篇论文评估了4种先进语音识别系统在公开数据集上的性能，发现它们在实际环境中存在显著性能差异、幻觉倾向和偏见问题，这些问题对人机交互的任务执行、用户信任和安全造成严重影响。", "motivation": "实际环境中的语音识别系统需要处理不完美音频和多样化用户群体，而在人机交互环境中，这些挑战更为严峻，因此需要系统性评估现有ASR系统的实际性能。", "method": "使用8个公开数据集来测试4种先进语音识别系统，这些数据集涵盖了六个难度维度：领域特定、口音、噪音、年龄变化、障碍性和自发性语音。", "result": "分析显示了这些ASR系统在性能、幻觉倾向和内在偏见方面存在显著差异，尽管在标准测试集上获得相似的分数。", "conclusion": "这些限制对人机交互产生了严重影响，因为识别错误可能干扰任务执行、损害用户信任和影响安全。研究结果强调了在实际应用场景中更全面评估ASR系统性能的重要性。"}}
{"id": "2508.17797", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17797", "abs": "https://arxiv.org/abs/2508.17797", "authors": ["Yunxiang Liu", "Hongkuo Niu", "Jianlin Zhu"], "title": "Adaptive Output Steps: FlexiSteps Network for Dynamic Trajectory Prediction", "comment": null, "summary": "Accurate trajectory prediction is vital for autonomous driving, robotics, and\nintelligent decision-making systems, yet traditional models typically rely on\nfixed-length output predictions, limiting their adaptability to dynamic\nreal-world scenarios. In this paper, we introduce the FlexiSteps Network (FSN),\na novel framework that dynamically adjusts prediction output time steps based\non varying contextual conditions. Inspired by recent advancements addressing\nobservation length discrepancies and dynamic feature extraction, FSN\nincorporates an pre-trained Adaptive Prediction Module (APM) to evaluate and\nadjust the output steps dynamically, ensuring optimal prediction accuracy and\nefficiency. To guarantee the plug-and-play of our FSN, we also design a Dynamic\nDecoder(DD). Additionally, to balance the prediction time steps and prediction\naccuracy, we design a scoring mechanism, which not only introduces the\nFr\\'echet distance to evaluate the geometric similarity between the predicted\ntrajectories and the ground truth trajectories but the length of predicted\nsteps is also considered. Extensive experiments conducted on benchmark datasets\nincluding Argoverse and INTERACTION demonstrate the effectiveness and\nflexibility of our proposed FSN framework.", "AI": {"tldr": "FlexiSteps Network (FSN) 是一个动态调整预测时间步长的轨迹预测框架，通过自适应预测模块和动态解码器实现灵活输出，在保持精度的同时提高适应性。", "motivation": "传统轨迹预测模型使用固定长度输出，难以适应动态现实场景的需求，需要开发能够根据上下文条件动态调整预测步长的解决方案。", "method": "提出FSN框架，包含预训练的自适应预测模块(APM)评估和动态调整输出步长，设计动态解码器(DD)确保即插即用，并引入结合Fréchet距离和预测步长长度的评分机制。", "result": "在Argoverse和INTERACTION等基准数据集上的大量实验证明了FSN框架的有效性和灵活性。", "conclusion": "FSN框架通过动态调整预测时间步长，在轨迹预测任务中实现了更好的适应性和预测精度平衡，为自动驾驶和智能决策系统提供了更实用的解决方案。"}}
{"id": "2508.17830", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17830", "abs": "https://arxiv.org/abs/2508.17830", "authors": ["Mary Kate Gale", "Kailana Baker-Matsuoka", "Ilana Nisky", "Allison Okamura"], "title": "Effect of Performance Feedback Timing on Motor Learning for a Surgical Training Task", "comment": "Submitted to IEEE Transactions on Biomedical Engineering", "summary": "Objective: Robot-assisted minimally invasive surgery (RMIS) has become the\ngold standard for a variety of surgical procedures, but the optimal method of\ntraining surgeons for RMIS is unknown. We hypothesized that real-time, rather\nthan post-task, error feedback would better increase learning speed and reduce\nerrors. Methods: Forty-two surgical novices learned a virtual version of the\nring-on-wire task, a canonical task in RMIS training. We investigated the\nimpact of feedback timing with multi-sensory (haptic and visual) cues in three\ngroups: (1) real-time error feedback, (2) trial replay with error feedback, and\n(3) no error feedback. Results: Participant performance was evaluated based on\nthe accuracy of ring position and orientation during the task. Participants who\nreceived real-time feedback outperformed other groups in ring orientation.\nAdditionally, participants who received feedback in replay outperformed\nparticipants who did not receive any error feedback on ring orientation during\nlong, straight path sections. There were no significant differences between\ngroups for ring position overall, but participants who received real-time\nfeedback outperformed the other groups in positional accuracy on tightly curved\npath sections. Conclusion: The addition of real-time haptic and visual error\nfeedback improves learning outcomes in a virtual surgical task over error\nfeedback in replay or no error feedback at all. Significance: This work\ndemonstrates that multi-sensory error feedback delivered in real time leads to\nbetter training outcomes as compared to the same feedback delivered after task\ncompletion. This novel method of training may enable surgical trainees to\ndevelop skills with greater speed and accuracy.", "AI": {"tldr": "实时多感官（触觉和视觉）错误反馈相比任务回放反馈或无反馈，能更好地提高机器人辅助微创手术虚拟训练中的学习效果，特别是在环状物定向和弯曲路径段的位置准确性方面。", "motivation": "机器人辅助微创手术已成为多种外科手术的金标准，但最佳的训练方法尚不清楚。研究者假设实时错误反馈比任务后反馈能更好地提高学习速度和减少错误。", "method": "42名手术新手学习虚拟环线任务，分为三组：(1)实时错误反馈组（触觉和视觉提示）、(2)任务回放错误反馈组、(3)无错误反馈组，比较不同反馈时机对学习效果的影响。", "result": "实时反馈组在环状物定向方面表现最佳；回放反馈组在直线路径段的定向表现优于无反馈组；实时反馈组在弯曲路径段的位置准确性方面表现最优；各组在整体位置准确性方面无显著差异。", "conclusion": "实时多感官错误反馈相比回放反馈或无反馈能改善虚拟手术任务的学习效果，这种新型训练方法可能使外科培训生以更快速度和更高准确性发展技能。"}}
{"id": "2508.17831", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17831", "abs": "https://arxiv.org/abs/2508.17831", "authors": ["Yuan Fang", "Fangzhan Shi", "Xijia Wei", "Qingchao Chen", "Kevin Chetty", "Simon Julier"], "title": "CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes", "comment": null, "summary": "As drone use has become more widespread, there is a critical need to ensure\nsafety and security. A key element of this is robust and accurate drone\ndetection and localization. While cameras and other optical sensors like LiDAR\nare commonly used for object detection, their performance degrades under\nadverse lighting and environmental conditions. Therefore, this has generated\ninterest in finding more reliable alternatives, such as millimeter-wave\n(mmWave) radar. Recent research on mmWave radar object detection has\npredominantly focused on 2D detection of road users. Although these systems\ndemonstrate excellent performance for 2D problems, they lack the sensing\ncapability to measure elevation, which is essential for 3D drone detection. To\naddress this gap, we propose CubeDN, a single-stage end-to-end radar object\ndetection network specifically designed for flying drones. CubeDN overcomes\nchallenges such as poor elevation resolution by utilizing a dual radar\nconfiguration and a novel deep learning pipeline. It simultaneously detects,\nlocalizes, and classifies drones of two sizes, achieving decimeter-level\ntracking accuracy at closer ranges with overall $95\\%$ average precision (AP)\nand $85\\%$ average recall (AR). Furthermore, CubeDN completes data processing\nand inference at 10Hz, making it highly suitable for practical applications.", "AI": {"tldr": "CubeDN是一个专门用于无人机3D检测的单阶段端到端雷达物体检测网络，通过双雷达配置和深度学习管道解决毫米波雷达在高度测量方面的限制，实现了分米级跟踪精度和实时处理能力。", "motivation": "随着无人机的广泛使用，需要确保安全和安保。传统光学传感器在恶劣光照和环境条件下性能下降，而现有毫米波雷达系统主要专注于2D道路用户检测，缺乏对无人机3D检测所需的高度测量能力。", "method": "提出CubeDN单阶段端到端雷达物体检测网络，采用双雷达配置来克服高度分辨率差的问题，使用新颖的深度学习管道同时实现检测、定位和分类功能。", "result": "系统能够同时检测和分类两种尺寸的无人机，在近距离实现分米级跟踪精度，总体平均精度(AP)达到95%，平均召回率(AR)达到85%，数据处理和推理速度为10Hz。", "conclusion": "CubeDN通过创新的双雷达配置和深度学习架构，成功解决了毫米波雷达在无人机3D检测中的高度测量挑战，提供了高精度、实时的无人机检测解决方案，非常适合实际应用。"}}
{"id": "2508.17922", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17922", "abs": "https://arxiv.org/abs/2508.17922", "authors": ["Bokai Ji", "Jie Gu", "Xiaokang Ma", "Chu Tang", "Jingmin Chen", "Guangxia Li"], "title": "Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model", "comment": null, "summary": "Affordance is crucial for intelligent robots in the context of object\nmanipulation. In this paper, we argue that affordance should be\ntask-/instruction-dependent, which is overlooked by many previous works. That\nis, different instructions can lead to different manipulation regions and\ndirections even for the same object. According to this observation, we present\na new dataset comprising fifteen thousand object-instruction-affordance\ntriplets. All scenes in the dataset are from an egocentric viewpoint, designed\nto approximate the perspective of a human-like robot. Furthermore, we\ninvestigate how to enable large multimodal models (LMMs) to serve as affordance\npredictors by implementing a ``search against verifiers'' pipeline. An LMM is\nasked to progressively predict affordances, with the output at each step being\nverified by itself during the iterative process, imitating a reasoning process.\nExperiments show that our method not only unlocks new instruction-oriented\naffordance prediction capabilities, but also achieves outstanding performance\nbroadly.", "AI": {"tldr": "本文提出了任务/指令依赖的affordance概念，构建了包含1.5万个物体-指令-affordance三元组的数据集，并开发了基于大型多模态模型的迭代推理预测方法。", "motivation": "传统affordance研究往往忽略了任务/指令依赖性，即同一物体在不同指令下会产生不同的操作区域和方向，这一重要特性需要被重新审视和研究。", "method": "构建了从自我中心视角的物体-指令-affordance三元组数据集；提出了\"search against verifiers\"管道，让大型多模态模型通过迭代推理过程逐步预测affordance，每一步输出都由模型自身验证。", "result": "实验表明该方法不仅解锁了新的指令导向affordance预测能力，而且在广泛范围内取得了出色的性能表现。", "conclusion": "任务/指令依赖的affordance是一个重要研究方向，通过大型多模态模型的迭代推理方法能够有效预测不同指令下的物体操作affordance。"}}
{"id": "2508.17969", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.17969", "abs": "https://arxiv.org/abs/2508.17969", "authors": ["Alexandros Gkillas", "Christos Anagnostopoulos", "Nikos Piperigkos", "Dimitris Tsiktsiris", "Theofilos Christodoulou", "Theofanis Siamatras", "Dimitrios Triantafyllou", "Christos Basdekis", "Theoktisti Marinopoulou", "Panagiotis Lepentsiotis", "Elefterios Blitsis", "Aggeliki Zacharaki", "Nearchos Stylianidis", "Leonidas Katelaris", "Lamberto Salvan", "Aris S. Lalos", "Christos Laoudias", "Antonios Lalas", "Konstantinos Votis"], "title": "A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm", "comment": null, "summary": "This paper introduces a holistic perception system for internal and external\nmonitoring of autonomous vehicles, with the aim of demonstrating a novel\nAI-leveraged self-adaptive framework of advanced vehicle technologies and\nsolutions that optimize perception and experience on-board. Internal monitoring\nsystem relies on a multi-camera setup designed for predicting and identifying\ndriver and occupant behavior through facial recognition, exploiting in addition\na large language model as virtual assistant. Moreover, the in-cabin monitoring\nsystem includes AI-empowered smart sensors that measure air-quality and perform\nthermal comfort analysis for efficient on and off-boarding. On the other hand,\nexternal monitoring system perceives the surrounding environment of vehicle,\nthrough a LiDAR-based cost-efficient semantic segmentation approach, that\nperforms highly accurate and efficient super-resolution on low-quality raw 3D\npoint clouds. The holistic perception framework is developed in the context of\nEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on\na real electric vehicle provided by ALKE. Experimental validation and\nevaluation at the integration site of Joint Research Centre at Ispra, Italy,\nhighlights increased performance and efficiency of the modular blocks of the\nproposed perception architecture.", "AI": {"tldr": "本文提出了一个用于自动驾驶车辆内外监控的整体感知系统，包含基于多摄像头和LLM的内部监控以及基于LiDAR的外部语义分割，在真实电动车上验证了性能提升", "motivation": "开发一个AI驱动的自适应框架来优化自动驾驶车辆的感知和体验，实现内外环境的全面监控", "method": "内部监控：多摄像头面部识别驾驶员行为+LLM虚拟助手+AI传感器空气质量监测；外部监控：基于LiDAR的成本效益语义分割方法，对低质量3D点云进行超分辨率处理", "result": "在欧盟AutoTRUST项目框架下集成到真实电动车上，在意大利Ispra联合研究中心实验验证，显示各模块性能和效率显著提升", "conclusion": "提出的整体感知架构通过内外监控系统的协同工作，为自动驾驶车辆提供了高效可靠的感知解决方案，在实际部署中表现出色"}}
{"id": "2508.17985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17985", "abs": "https://arxiv.org/abs/2508.17985", "authors": ["Abu Shad Ahammed", "Md Shahi Amran Hossain", "Sayeri Mukherjee", "Roman Obermaisser", "Md. Ziaur Rahman"], "title": "Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE", "comment": null, "summary": "Ensuring safety in autonomous driving requires a seamless integration of\nperception and decision making under uncertain conditions. Although computer\nvision (CV) models such as YOLO achieve high accuracy in detecting traffic\nsigns and obstacles, their performance degrades in drift scenarios caused by\nweather variations or unseen objects. This work presents a simulated autonomous\ndriving system that combines a context aware CV model with adaptive control\nusing the ADORE framework. The CARLA simulator was integrated with ADORE via\nthe ROS bridge, allowing real-time communication between perception, decision,\nand control modules. A simulated test case was designed in both clear and drift\nweather conditions to demonstrate the robust detection performance of the\nperception model while ADORE successfully adapted vehicle behavior to speed\nlimits and obstacles with low response latency. The findings highlight the\npotential of coupling deep learning-based perception with rule-based adaptive\ndecision making to improve automotive safety critical system.", "AI": {"tldr": "该论文提出了一个结合上下文感知计算机视觉模型和ADORE自适应控制框架的自动驾驶系统，在CARLA模拟器中验证了其在恶劣天气条件下的鲁棒性能。", "motivation": "解决自动驾驶在天气变化和未知物体等不确定条件下的安全问题，传统计算机视觉模型在这些场景下性能会下降。", "method": "使用CARLA模拟器与ADORE框架通过ROS桥接集成，结合上下文感知CV模型和自适应控制，在清晰和恶劣天气条件下进行测试。", "result": "感知模型在恶劣天气条件下仍保持鲁棒检测性能，ADORE成功适应车速限制和障碍物，响应延迟低。", "conclusion": "深度学习感知与基于规则的自适应决策相结合，能够显著提升汽车安全关键系统的性能。"}}
{"id": "2508.17986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17986", "abs": "https://arxiv.org/abs/2508.17986", "authors": ["Karel Bartunek", "Lukas Rustler", "Matej Hoffmann"], "title": "No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin", "comment": "Submitted for review to ICRA 2026", "summary": "Locating and grasping of objects by robots is typically performed using\nvisual sensors. Haptic feedback from contacts with the environment is only\nsecondary if present at all. In this work, we explored an extreme case of\nsearching for and grasping objects in complete absence of visual input, relying\non haptic feedback only. The main novelty lies in the use of contacts over the\ncomplete surface of a robot manipulator covered with sensitive skin. The search\nis divided into two phases: (1) coarse workspace exploration with the complete\nrobot surface, followed by (2) precise localization using the end-effector\nequipped with a force/torque sensor. We systematically evaluated this method in\nsimulation and on the real robot, demonstrating that diverse objects can be\nlocated, grasped, and put in a basket. The overall success rate on the real\nrobot for one object was 85.7\\% with failures mainly while grasping specific\nobjects. The method using whole-body contacts is six times faster compared to a\nbaseline that uses haptic feedback only on the end-effector. We also show\nlocating and grasping multiple objects on the table. This method is not\nrestricted to our specific setup and can be deployed on any platform with the\nability of sensing contacts over the entire body surface. This work holds\npromise for diverse applications in areas with challenging visual perception\n(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or\nvegetables need to be located inside foliage and picked.", "AI": {"tldr": "本文提出了一种在完全无视觉输入情况下，利用机器人全身触觉皮肤进行物体定位和抓取的方法，通过粗粒度工作空间探索和精确定位两阶段实现，相比仅使用末端执行器触觉反馈的方法快6倍。", "motivation": "传统机器人物体定位和抓取主要依赖视觉传感器，触觉反馈仅作为辅助。本研究探索在完全无视觉输入的情况下，仅依靠触觉反馈来搜索和抓取物体，适用于视觉感知受限的环境（如光照不佳、灰尘、烟雾、遮挡等场景）。", "method": "将搜索分为两个阶段：(1) 使用覆盖敏感皮肤的完整机器人表面进行粗粒度工作空间探索；(2) 使用配备力/力矩传感器的末端执行器进行精确定位。该方法不限于特定设置，可部署在任何具有全身接触感知能力的平台上。", "result": "在仿真和真实机器人上系统评估表明，该方法能够定位、抓取和放置多种物体。真实机器人单物体抓取总体成功率为85.7%，失败主要发生在抓取特定物体时。使用全身接触的方法比仅使用末端执行器触觉反馈的基线方法快6倍。", "conclusion": "这种基于全身触觉感知的方法在视觉感知受限的环境中具有广泛应用前景，如农业中需要在树叶内部定位和采摘水果蔬菜等场景。"}}
{"id": "2508.18066", "categories": ["cs.RO", "cs.AI", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.18066", "abs": "https://arxiv.org/abs/2508.18066", "authors": ["Alberto Silvio Chiappa", "Boshi An", "Merkourios Simos", "Chengkun Li", "Alexander Mathis"], "title": "Arnold: a generalist muscle transformer policy", "comment": "A.S.C. and B.A. contributed equally. Code is available at\n  https://github.com/amathislab/arnold-the-generalist", "summary": "Controlling high-dimensional and nonlinear musculoskeletal models of the\nhuman body is a foundational scientific challenge. Recent machine learning\nbreakthroughs have heralded policies that master individual skills like\nreaching, object manipulation and locomotion in musculoskeletal systems with\nmany degrees of freedom. However, these agents are merely \"specialists\",\nachieving high performance for a single skill. In this work, we develop Arnold,\na generalist policy that masters multiple tasks and embodiments. Arnold\ncombines behavior cloning and fine-tuning with PPO to achieve expert or\nsuper-expert performance in 14 challenging control tasks from dexterous object\nmanipulation to locomotion. A key innovation is Arnold's sensorimotor\nvocabulary, a compositional representation of the semantics of heterogeneous\nsensory modalities, objectives, and actuators. Arnold leverages this vocabulary\nvia a transformer architecture to deal with the variable observation and action\nspaces of each task. This framework supports efficient multi-task,\nmulti-embodiment learning and facilitates rapid adaptation to novel tasks.\nFinally, we analyze Arnold to provide insights into biological motor control,\ncorroborating recent findings on the limited transferability of muscle\nsynergies across tasks.", "AI": {"tldr": "本文提出了Arnold，一个通用策略模型，能够掌握多种任务和身体构型，在14个挑战性控制任务中达到专家或超专家水平。", "motivation": "解决高维非线性人体肌肉骨骼模型的控制难题，现有方法多为单一技能专家，需要开发能够处理多任务和多身体构型的通用策略。", "method": "结合行为克隆和PPO微调，使用创新的感觉运动词汇表（sensorimotor vocabulary）和transformer架构处理可变观测和动作空间。", "result": "在14个挑战性任务中达到专家或超专家性能，支持高效多任务多身体构型学习，并能快速适应新任务。", "conclusion": "Arnold框架为生物运动控制提供了新见解，证实了肌肉协同作用在任务间有限的可迁移性，推动了通用运动控制策略的发展。"}}
{"id": "2508.18074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18074", "abs": "https://arxiv.org/abs/2508.18074", "authors": ["Zhaokun Chen", "Wenshuo Wang", "Wenzhuo Liu", "Yichen Liu", "Junqiang Xi"], "title": "The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation", "comment": null, "summary": "Communication delays in mobile robot teleoperation adversely affect\nhuman-machine collaboration. Understanding delay effects on human operational\nperformance and neurocognition is essential for resolving this issue. However,\nno previous research has explored this. To fill this gap, we conduct a\nhuman-in-the-loop experiment involving 10 participants, integrating\nelectroencephalography (EEG) and robot behavior data under varying delays\n(0-500 ms in 100 ms increments) to systematically investigate these effects.\nBehavior analysis reveals significant performance degradation at 200-300 ms\ndelays, affecting both task efficiency and accuracy. EEG analysis discovers\nfeatures with significant delay dependence: frontal $\\theta/\\beta$-band and\nparietal $\\alpha$-band power. We also identify a threshold window (100-200 ms)\nfor early perception of delay in humans, during which these EEG features first\nexhibit significant differences. When delay exceeds 400 ms, all features\nplateau, indicating saturation of cognitive resource allocation at\nphysiological limits. These findings provide the first evidence of perceptual\nand cognitive delay thresholds during teleoperation tasks in humans, offering\ncritical neurocognitive insights for the design of delay compensation\nstrategies.", "AI": {"tldr": "本研究通过脑电和行为实验首次系统揭示了移动机器人遥操作中通信延迟对人类操作表现和神经认知的影响，发现了200-300ms的性能下降阈值和100-200ms的早期感知窗口。", "motivation": "通信延迟严重影响移动机器人遥操作中的人机协作，但此前缺乏对延迟如何影响人类操作表现和神经认知的系统研究，需要填补这一空白。", "method": "采用10名参与者的人机闭环实验，结合脑电图(EEG)和机器人行为数据，在0-500ms延迟范围内以100ms为增量进行系统研究。", "result": "行为分析显示200-300ms延迟导致性能显著下降；EEG分析发现前额θ/β频带和顶叶α频带功率具有显著延迟依赖性；识别出100-200ms的早期感知窗口；400ms以上所有特征趋于饱和。", "conclusion": "研究首次提供了人类在遥操作任务中感知和认知延迟阈值的证据，为延迟补偿策略的设计提供了关键的神经认知学见解。"}}
{"id": "2508.18139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18139", "abs": "https://arxiv.org/abs/2508.18139", "authors": ["Prathima Ananda Kumar"], "title": "Analysis of Harpy's Constrained Trotting and Jumping Maneuver", "comment": "Master's Thesis", "summary": "This study presents an analysis of experimental data from Harpy, a\nthruster-assisted bipedal robot developed at Northeastern University. The study\nexamines data sets from trotting and jumping experiments to understand the\nfundamental principles governing hybrid leg-thruster locomotion. Through data\nanalysis across multiple locomotion modes, this research reveals that Harpy\nachieves stable locomotion with bounded trajectories and consistent foot\nplacement through strategic leg-thruster synergy. The results demonstrate\ncontrolled joint behavior with low torques and symmetric tracking, accurate\nfoot placement within kinematic constraints despite phase-transition\nperturbations, and underactuated degree-of-freedom stability without\ndivergence. Energy level analysis reveals that legs provide primary propulsion,\nwhile the thrusters enable additional aerial phase control. The analysis\nidentifies critical body-leg coupling dynamics during aerial phases that\nrequire phase-specific control strategies. Consistent repeatability and\nsymmetry across experiments validate the robustness of the hybrid actuation\napproach.", "AI": {"tldr": "对Harpy混合腿-推进器双足机器人的实验数据分析，揭示了腿提供主要推进力而推进器实现空中相位控制的协同机制，实现了稳定的有界轨迹和精确足部放置", "motivation": "研究混合腿-推进器 locomotion 的基本原理，了解腿和推进器在双足机器人运动中的协同作用机制", "method": "分析Harpy机器人在小跑和跳跃实验中的数据集，通过多运动模式的数据分析来理解混合驱动原理", "result": "实现了有界轨迹稳定运动、精确足部放置、低扭矩关节控制、对称跟踪，推进器提供额外空中相位控制，腿提供主要推进力", "conclusion": "混合驱动方法具有鲁棒性，需要针对空中相位的特定控制策略来管理关键的体-腿耦合动力学"}}
{"id": "2508.18153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18153", "abs": "https://arxiv.org/abs/2508.18153", "authors": ["Aalok Patwardhan", "Andrew J. Davison"], "title": "DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation", "comment": null, "summary": "Robot swarms require cohesive collective behaviour to address diverse\nchallenges, including shape formation and decision-making. Existing approaches\noften treat consensus in discrete and continuous decision spaces as distinct\nproblems. We present DANCeRS, a unified, distributed algorithm leveraging\nGaussian Belief Propagation (GBP) to achieve consensus in both domains. By\nrepresenting a swarm as a factor graph our method ensures scalability and\nrobustness in dynamic environments, relying on purely peer-to-peer message\npassing. We demonstrate the effectiveness of our general framework through two\napplications where agents in a swarm must achieve consensus on global behaviour\nwhilst relying on local communication. In the first, robots must perform path\nplanning and collision avoidance to create shape formations. In the second, we\nshow how the same framework can be used by a group of robots to form a\nconsensus over a set of discrete decisions. Experimental results highlight our\nmethod's scalability and efficiency compared to recent approaches to these\nproblems making it a promising solution for multi-robot systems requiring\ndistributed consensus. We encourage the reader to see the supplementary video\ndemo.", "AI": {"tldr": "DANCeRS是一个统一的分布式算法，使用高斯置信传播在离散和连续决策空间中实现群体共识，通过因子图表示群体，依赖纯对等消息传递确保可扩展性和鲁棒性。", "motivation": "现有方法通常将离散和连续决策空间的共识视为不同问题，需要一种统一的分布式算法来解决群体在动态环境中的共识问题。", "method": "使用高斯置信传播(GBP)和因子图表示群体，通过纯对等消息传递实现分布式共识，适用于形状形成和离散决策两种应用场景。", "result": "实验结果表明该方法在可扩展性和效率方面优于现有方法，能够有效处理路径规划、碰撞避免和离散决策共识问题。", "conclusion": "DANCeRS为需要分布式共识的多机器人系统提供了一个有前景的解决方案，统一处理了离散和连续决策空间的共识问题。"}}
{"id": "2508.18249", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18249", "abs": "https://arxiv.org/abs/2508.18249", "authors": ["Zipeng Fang", "Yanbo Wang", "Lei Zhao", "Weidong Chen"], "title": "Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework", "comment": null, "summary": "Traversability estimation is critical for enabling robots to navigate across\ndiverse terrains and environments. While recent self-supervised learning\nmethods achieve promising results, they often fail to capture the\ncharacteristics of non-traversable regions. Moreover, most prior works\nconcentrate on a single modality, overlooking the complementary strengths\noffered by integrating heterogeneous sensory modalities for more robust\ntraversability estimation. To address these limitations, we propose a\nmultimodal self-supervised framework for traversability labeling and\nestimation. First, our annotation pipeline integrates footprint, LiDAR, and\ncamera data as prompts for a vision foundation model, generating traversability\nlabels that account for both semantic and geometric cues. Then, leveraging\nthese labels, we train a dual-stream network that jointly learns from different\nmodalities in a decoupled manner, enhancing its capacity to recognize diverse\ntraversability patterns. In addition, we incorporate sparse LiDAR-based\nsupervision to mitigate the noise introduced by pseudo labels. Finally,\nextensive experiments conducted across urban, off-road, and campus environments\ndemonstrate the effectiveness of our approach. The proposed automatic labeling\nmethod consistently achieves around 88% IoU across diverse datasets. Compared\nto existing self-supervised state-of-the-art methods, our multimodal\ntraversability estimation network yields consistently higher IoU, improving by\n1.6-3.5% on all evaluated datasets.", "AI": {"tldr": "提出了一种多模态自监督框架，通过整合足印、LiDAR和相机数据生成可通行性标签，并训练双流网络进行多模态学习，在多种环境中实现了88%的IoU和1.6-3.5%的性能提升。", "motivation": "现有自监督学习方法往往无法有效捕捉不可通行区域特征，且大多只关注单一模态，忽略了多模态传感器融合的优势。", "method": "1) 使用视觉基础模型整合多模态数据生成可通行性标签；2) 训练解耦的双流网络进行多模态学习；3) 引入稀疏LiDAR监督减少伪标签噪声。", "result": "在城市场景、越野环境和校园环境中广泛实验，自动标注方法达到约88%的IoU，相比现有最优自监督方法IoU提升1.6-3.5%。", "conclusion": "提出的多模态自监督框架有效解决了可通行性估计中的模态单一和伪标签噪声问题，在各种环境中都表现出优越性能。"}}
{"id": "2508.18268", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18268", "abs": "https://arxiv.org/abs/2508.18268", "authors": ["Haoyuan Deng", "Wenkai Guo", "Qianzhun Wang", "Zhenyu Wu", "Ziwei Wang"], "title": "SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation", "comment": "Project website is at: https://denghaoyuan123.github.io/SafeBimanip/", "summary": "Bimanual manipulation has been widely applied in household services and\nmanufacturing, which enables the complex task completion with coordination\nrequirements. Recent diffusion-based policy learning approaches have achieved\npromising performance in modeling action distributions for bimanual\nmanipulation. However, they ignored the physical safety constraints of bimanual\nmanipulation, which leads to the dangerous behaviors with damage to robots and\nobjects. To this end, we propose a test-time trajectory optimization framework\nnamed SafeBimanual for any pre-trained diffusion-based bimanual manipulation\npolicies, which imposes the safety constraints on bimanual actions to avoid\ndangerous robot behaviors with improved success rate. Specifically, we design\ndiverse cost functions for safety constraints in different dual-arm cooperation\npatterns including avoidance of tearing objects and collision between arms and\nobjects, which optimizes the manipulator trajectories with guided sampling of\ndiffusion denoising process. Moreover, we employ a vision-language model (VLM)\nto schedule the cost functions by specifying keypoints and corresponding\npairwise relationship, so that the optimal safety constraint is dynamically\ngenerated in the entire bimanual manipulation process. SafeBimanual\ndemonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase\nin success rate and a 18.8% reduction in unsafe interactions over\nstate-of-the-art diffusion-based methods. Extensive experiments on 4 real-world\ntasks further verify its practical value by improving the success rate by\n32.5%.", "AI": {"tldr": "SafeBimanual是一个测试时轨迹优化框架，为预训练的基于扩散模型的双臂操作策略添加安全约束，避免危险行为并提高成功率", "motivation": "现有的基于扩散模型的双臂操作策略忽略了物理安全约束，导致可能损坏机器人和物体的危险行为", "method": "设计不同双臂协作模式的安全约束成本函数，通过扩散去噪过程的引导采样优化轨迹，并使用视觉语言模型动态调度成本函数", "result": "在8个模拟任务中成功率提高13.7%，不安全交互减少18.8%；在4个真实任务中成功率提高32.5%", "conclusion": "SafeBimanual框架有效提升了双臂操作的安全性和成功率，具有实际应用价值"}}
{"id": "2508.18269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18269", "abs": "https://arxiv.org/abs/2508.18269", "authors": ["Zhide Zhong", "Haodong Yan", "Junfeng Li", "Xiangchen Liu", "Xin Gong", "Wenxuan Song", "Jiayi Chen", "Haoang Li"], "title": "FlowVLA: Thinking in Motion with a Visual Chain of Thought", "comment": null, "summary": "Many Vision-Language-Action (VLA) models rely on an internal world model\ntrained via next-frame prediction. This approach, however, struggles with\nphysical reasoning as it entangles static appearance with dynamic motion, often\nresulting in implausible visual forecasts and inefficient policy learning. To\naddress these limitations, we introduce the Visual Chain of Thought (Visual\nCoT): a pre-training framework that encourages a model to reason about how a\nscene evolves before predicting what it will look like. We instantiate this\nprinciple in FlowVLA, which predicts a future frame ($v_{t+1}$) only after\ngenerating an intermediate optical flow representation ($f_t$) that encodes\nmotion dynamics. This ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'' reasoning\nprocess is implemented within a single autoregressive Transformer, guiding the\nmodel to learn disentangled dynamics. As a result, FlowVLA produces coherent\nvisual predictions and facilitates more efficient policy learning. Experiments\non challenging robotics manipulation benchmarks demonstrate state-of-the-art\nperformance with substantially improved sample efficiency, pointing toward a\nmore principled foundation for world modeling. Project page:\nhttps://irpn-lab.github.io/FlowVLA/", "AI": {"tldr": "FlowVLA通过引入视觉思维链（Visual CoT）框架，在预测未来帧之前先生成中间光流表示来解耦静态外观和动态运动，从而提升物理推理能力和策略学习效率。", "motivation": "现有的VLA模型通过下一帧预测训练内部世界模型，但这种方法将静态外观与动态运动纠缠在一起，导致不合理的视觉预测和低效的策略学习。", "method": "提出Visual CoT框架，在FlowVLA中实现\"当前帧→光流→未来帧\"的推理过程，使用单一自回归Transformer生成中间光流表示来编码运动动态。", "result": "在机器人操作基准测试中实现了最先进的性能，显著提高了样本效率，产生了更连贯的视觉预测。", "conclusion": "该方法为世界建模提供了更原则性的基础，通过解耦动态学习实现了更好的物理推理和策略学习效果。"}}
