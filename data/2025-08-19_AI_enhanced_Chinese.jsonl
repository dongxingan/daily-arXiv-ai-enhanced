{"id": "2508.11759", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11759", "abs": "https://arxiv.org/abs/2508.11759", "authors": ["Peter Lindes", "Kaoutar Skiker"], "title": "Using Natural Language for Human-Robot Collaboration in the Real World", "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6", "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans.", "AI": {"tldr": "本文探讨了如何通过大语言模型提升自主机器人的自然语言理解能力，以实现人机协作的远景。作者提出了一种整合认知代理、物理机器人和LLM的方案，并通过ChatGPT进行了概念验证实验。", "motivation": "传统交互式任务学习系统的语言理解能力有限，而大语言模型的出现为提升机器人语言理解能力提供了机会，但需要解决LLM与物理世界操作机器人的集成挑战。", "method": "提出一种以认知代理为核心的AI系统方案，该代理控制物理机器人、与人类和LLM交互、通过经验积累情境知识。使用ChatGPT进行了三个具体挑战的概念验证实验。", "result": "通过简单的概念验证实验，证明了使用LLM提升机器人自然语言理解的可行性，为构建集成化语言协作机器人助手系统奠定了基础。", "conclusion": "虽然目前只是概念验证，但本文提出的方案为实现具有健壮语言能力的人机协作机器人助手指明了方向，需要进一步研究如何将这些简单实验转化为可操作的集成系统。"}}
{"id": "2508.11802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11802", "abs": "https://arxiv.org/abs/2508.11802", "authors": ["Luigi Penco", "Beomyeong Park", "Stefan Fasano", "Nehar Poddar", "Stephen McCrory", "Nicholas Kitchel", "Tomasz Bialek", "Dexton Anderson", "Duncan Calvert", "Robert Griffin"], "title": "Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots\n  (Humanoids)", "summary": "Achieving seamless synchronization between user and robot motion in\nteleoperation, particularly during high-speed tasks, remains a significant\nchallenge. In this work, we propose a novel approach for transferring stepping\nmotions from the user to the robot in real-time. Instead of directly\nreplicating user foot poses, we retarget user steps to robot footstep\nlocations, allowing the robot to utilize its own dynamics for locomotion,\nensuring better balance and stability. Our method anticipates user footsteps to\nminimize delays between when the user initiates and completes a step and when\nthe robot does it. The step estimates are continuously adapted to converge with\nthe measured user references. Additionally, the system autonomously adjusts the\nrobot's steps to account for its surrounding terrain, overcoming challenges\nposed by environmental mismatches between the user's flat-ground setup and the\nrobot's uneven terrain. Experimental results on the humanoid robot Nadia\ndemonstrate the effectiveness of the proposed system.", "AI": {"tldr": "通过预测用户步伐并重定向到机器人步伐位置，实现了高速任务中用户与机器人运动的实时同步，而非直接复制脚部姿态，从而保持机器人平衡性和稳定性。", "motivation": "解决高速任务中用户与机器人运动同步的挑战，特别是在环境不匹配的情况下保证机器人的平衡和稳定性。", "method": "预测用户步伐并重定向到机器人步伐位置，让机器人利用自身动力学进行移动；持续适应步伐估计以进入测量用户参考；自主调整机器人步伐以适应周围地形。", "result": "在人型机器人Nadia上的实验结果证明了所提系统的有效性。", "conclusion": "该方法能够有效解决高速任务中的同步问题，保证机器人在不平坦地形上的平衡和稳定性，为遥播操作提供了可靠的解决方案。"}}
{"id": "2508.11849", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11849", "abs": "https://arxiv.org/abs/2508.11849", "authors": ["Allen Wang", "Gavin Tao"], "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba", "comment": null, "summary": "We introduce LocoMamba, a vision-driven cross-modal DRL framework built on\nselective state-space models, specifically leveraging Mamba, that achieves\nnear-linear-time sequence modeling, effectively captures long-range\ndependencies, and enables efficient training with longer sequences. First, we\nembed proprioceptive states with a multilayer perceptron and patchify depth\nimages with a lightweight convolutional neural network, producing compact\ntokens that improve state representation. Second, stacked Mamba layers fuse\nthese tokens via near-linear-time selective scanning, reducing latency and\nmemory footprint, remaining robust to token length and image resolution, and\nproviding an inductive bias that mitigates overfitting. Third, we train the\npolicy end-to-end with Proximal Policy Optimization under terrain and\nappearance randomization and an obstacle-density curriculum, using a compact\nstate-centric reward that balances progress, smoothness, and safety. We\nevaluate our method in challenging simulated environments with static and\nmoving obstacles as well as uneven terrain. Compared with state-of-the-art\nbaselines, our method achieves higher returns and success rates with fewer\ncollisions, exhibits stronger generalization to unseen terrains and obstacle\ndensities, and improves training efficiency by converging in fewer updates\nunder the same compute budget.", "AI": {"tldr": "LocoMamba是一个基于Mamba选择性状态空间模型的视觉驱动跨模态深度强化学习框架，能够实现近线性时间序列建模，有效捕获长距离依赖关系，并在具有挑战性的模拟环境中优于现有基线方法。", "motivation": "为了解决传统方法在序列建模中的计算效率问题，同时需要有效处理长距离依赖和跨模态信息融合，特别是在机器人导航任务中需要处理视觉和本体感知信息。", "method": "1) 使用多层感知机嵌入本体感知状态，用轻量级CNN处理深度图像生成紧凑token；2) 堆叠Mamba层通过近线性时间选择性扫描融合token；3) 使用PPO算法在随机化地形和外观下进行端到端策略训练，采用障碍物密度课程学习和紧凑状态中心奖励函数。", "result": "在具有静态和动态障碍物以及不平坦地形的挑战性模拟环境中，相比最先进基线方法，LocoMamba获得更高的回报和成功率，碰撞更少，对未见地形和障碍物密度表现出更强的泛化能力，在相同计算预算下以更少的更新次数收敛。", "conclusion": "LocoMamba框架通过选择性状态空间模型有效解决了长序列建模的计算效率问题，在机器人导航任务中表现出优异的性能和训练效率，为跨模态深度强化学习提供了新的解决方案。"}}
{"id": "2508.11868", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11868", "abs": "https://arxiv.org/abs/2508.11868", "authors": ["Lida Xu"], "title": "Data Shift of Object Detection in Autonomous Driving", "comment": null, "summary": "With the widespread adoption of machine learning technologies in autonomous\ndriving systems, their role in addressing complex environmental perception\nchallenges has become increasingly crucial. However, existing machine learning\nmodels exhibit significant vulnerability, as their performance critically\ndepends on the fundamental assumption that training and testing data satisfy\nthe independent and identically distributed condition, which is difficult to\nguarantee in real-world applications. Dynamic variations in data distribution\ncaused by seasonal changes, weather fluctuations lead to data shift problems in\nautonomous driving systems. This study investigates the data shift problem in\nautonomous driving object detection tasks, systematically analyzing its\ncomplexity and diverse manifestations. We conduct a comprehensive review of\ndata shift detection methods and employ shift detection analysis techniques to\nperform dataset categorization and balancing. Building upon this foundation, we\nconstruct an object detection model. To validate our approach, we optimize the\nmodel by integrating CycleGAN-based data augmentation techniques with the\nYOLOv5 framework. Experimental results demonstrate that our method achieves\nsuperior performance compared to baseline models on the BDD100K dataset.", "AI": {"tldr": "本文研究自动驾驶中的数据偏移问题，通过数据偏移检测和CycleGAN数据增帽技术优化YOLOv5模型，在BDD100K数据集上达到更优性能。", "motivation": "自动驾驶系统中的机器学习模型对训练和测试数据的IID假设敏感，实际应用中季节、天气等因素导致的数据分布变化会引发数据偏移问题，影响目标检测性能。", "method": "系统分析数据偏移的复杂性和多样性，综述数据偏移检测方法，进行数据集分类和平衡处理。基于YOLOv5构建目标检测模型，结合CycleGAN数据增帽技术进行模型优化。", "result": "在BDD100K数据集上的实验结果显示，该方法比基线模型表现更优。", "conclusion": "通过数据偏移检测和数据增帽技术的结合，可有效提升自动驾驶目标检测模型在实际应用中的性能和适应性。"}}
{"id": "2508.11883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11883", "abs": "https://arxiv.org/abs/2508.11883", "authors": ["Lei Li", "Boyang Qin", "Wenzhuo Gao", "Yanyu Li", "Yiyuan Zhang", "Bo Wang", "Shihan Kong", "Jian Wang", "Dekui He", "Junzhi Yu"], "title": "Bioinspired underwater soft robots: from biology to robotics and back", "comment": null, "summary": "The ocean vast unexplored regions and diverse soft-bodied marine organisms\nhave spurred interest in bio-inspired underwater soft robotics. Recent advances\nhave enabled new capabilities in underwater movement, sensing, and interaction.\nHowever, these efforts are largely unidirectional, with biology guiding\nrobotics while insights from robotics rarely feed back into biology. Here we\npropose a holistic, bidirectional framework that integrates biological\nprinciples, robotic implementation, and biological validation. We show that\nsoft robots can serve as experimental tools to probe biological functions and\neven test evolutionary hypotheses. Their inherent compliance also allows them\nto outperform rigid systems in unstructured environments, supporting\napplications in marine exploration, manipulation, and medicine. Looking\nforward, we introduce bio-universal-inspired robotics, a paradigm that\ntranscends species-specific mimicry by identifying convergent principles across\nspecies to inspire more adaptable designs. Despite rapid progress, challenges\npersist in material robustness, actuation efficiency, autonomy, and\nintelligence. By uniting biology and engineering, soft robots can advance ocean\nexploration and deepen scientific discovery.", "AI": {"tldr": "这篇论文提出了一种双向软体机器人研究框架，将生物学原理、机器人实现和生物验证相结合，以推动海洋探索和科学发现。", "motivation": "当前海洋软体机器人研究多为单向影响（生物学引导机器人），缺乏从机器人到生物学的反馈循环。需要一种更全面的双向框架来深化科学发现和推动技术进步。", "method": "提出了一种整体性的双向框架，让软体机器人作为实验工具来探索生物功能和验证进化假说。同时提出\"生物普适源机器人学\"新范式，跨越特定物种模仿，寻找不同物种间的聚合原理来设计更适应性强的机器人。", "result": "软体机器人在非结构化环境中表现出比硬质系统更优异的性能，支撑了海洋探索、操作和医疗等应用领域。通过双向框架，机器人技术不仅能够模仿生物，还能反过来验证生物学理论和假说。", "conclusion": "软体机器人通过联合生物学和工程学科，能够推动海洋探索和深化科学发现。虽然面临材料耐用性、驱动效率、自主性和智能化等挑战，但双向框架和生物普适源机器人学的新范式为未来研究指明了方向。"}}
{"id": "2508.11884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11884", "abs": "https://arxiv.org/abs/2508.11884", "authors": ["Havel Liu", "Mingzhang Zhu", "Arturo Moises Flores Alvarez", "Yuan Hung Lo", "Conrad Ku", "Federico Parres", "Justin Quan", "Colin Togashi", "Aditya Navghare", "Quanyou Wang", "Dennis W. Hong"], "title": "From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics", "comment": "8 pages, 14 figures, accepted by IEEE Humanoids 2025", "summary": "Humanoid robots represent the cutting edge of robotics research, yet their\npotential in entertainment remains largely unexplored. Entertainment as a field\nprioritizes visuals and form, a principle that contrasts with the purely\nfunctional designs of most contemporary humanoid robots. Designing\nentertainment humanoid robots capable of fluid movement presents a number of\nunique challenges. In this paper, we present Kid Cosmo, a research platform\ndesigned for robust locomotion and life-like motion generation while imitating\nthe look and mannerisms of its namesake character from Netflix's movie The\nElectric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall\nand weighing 25 kg. It contains 28 degrees of freedom and primarily uses\nproprioceptive actuators, enabling torque-control walking and lifelike motion\ngeneration. Following worldwide showcases as part of the movie's press tour, we\npresent the system architecture, challenges of a functional entertainment robot\nand unique solutions, and our initial findings on stability during simultaneous\nupper and lower body movement. We demonstrate the viability of\nperformance-oriented humanoid robots that prioritize both character embodiment\nand technical functionality.", "AI": {"tldr": "Kid Cosmo是一个儿童尺寸的娱乐人形机器人平台，专为稳健运动和拟人动作生成而设计，模仿了Netflix电影中的角色形象，展示了娱乐导向人形机器人的可行性", "motivation": "当前人形机器人主要关注功能性设计，而娱乐领域更注重视觉效果和形态，娱乐人形机器人的潜力尚未被充分探索", "method": "开发了1.45米高、25公斤重的28自由度人形机器人Kid Cosmo，使用本体感受执行器实现扭矩控制行走和逼真动作生成", "result": "通过全球展示验证了系统架构，解决了功能性娱乐机器人的挑战，展示了上下半身同时运动时的稳定性", "conclusion": "证明了同时注重角色体现和技术功能的表演导向人形机器人的可行性"}}
{"id": "2508.11885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11885", "abs": "https://arxiv.org/abs/2508.11885", "authors": ["Haixin Gong", "Chen Zhang", "Yanan Sui"], "title": "Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System", "comment": "IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids\n  2025)", "summary": "The human foot serves as the critical interface between the body and\nenvironment during locomotion. Existing musculoskeletal models typically\noversimplify foot-ground contact mechanics, limiting their ability to\naccurately simulate human gait dynamics. We developed a novel contact-rich and\ndeformable model of the human foot integrated within a complete musculoskeletal\nsystem that captures the complex biomechanical interactions during walking. To\novercome the control challenges inherent in modeling multi-point contacts and\ndeformable material, we developed a two-stage policy training strategy to learn\nnatural walking patterns for this interface-enhanced model. Comparative\nanalysis between our approach and conventional rigid musculoskeletal models\ndemonstrated improvements in kinematic, kinetic, and gait stability metrics.\nValidation against human subject data confirmed that our simulation closely\nreproduced real-world biomechanical measurements. This work advances\ncontact-rich interface modeling for human musculoskeletal systems and\nestablishes a robust framework that can be extended to humanoid robotics\napplications requiring precise foot-ground interaction control.", "AI": {"tldr": "开发了一种新的可变形脚部模型，通过两阶段策略训练自然步态，在动态学、动力学和步态稳定性方面显著改善了传统粗糕模型的误差", "motivation": "现有肌骨驱动模型对脚地接触力学过于简化，限制了模拟人类步态动态的准确性", "method": "构建了一个接触丰富的可变形脚部模型，并集成到完整的肌骨驱动系统中，采用两阶段策略训练方法来学习自然步态", "result": "与传统粗糕模型相比，在运动学、力学和步态稳定性指标上都显著改善，通过人体实验数据验证了模拟结果的准确性", "conclusion": "这项工作推进了人体肌骨驱动系统的接触丰富界面模型技术，为需要精确脚地交互控制的人形机器人应用建立了稳健框架"}}
{"id": "2508.12166", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12166", "abs": "https://arxiv.org/abs/2508.12166", "authors": ["Gokul Puthumanaillam", "Aditya Penumarti", "Manav Vora", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Jane Shin", "Melkior Ornik"], "title": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing", "comment": "Accepted to CoRL 2025 (Conference on Robot Learning)", "summary": "Robots equipped with rich sensor suites can localize reliably in\npartially-observable environments, but powering every sensor continuously is\nwasteful and often infeasible. Belief-space planners address this by\npropagating pose-belief covariance through analytic models and switching\nsensors heuristically--a brittle, runtime-expensive approach. Data-driven\napproaches--including diffusion models--learn multi-modal trajectories from\ndemonstrations, but presuppose an accurate, always-on state estimate. We\naddress the largely open problem: for a given task in a mapped environment,\nwhich \\textit{minimal sensor subset} must be active at each location to\nmaintain state uncertainty \\textit{just low enough} to complete the task? Our\nkey insight is that when a diffusion planner is explicitly conditioned on a\npose-belief raster and a sensor mask, the spread of its denoising trajectories\nyields a calibrated, differentiable proxy for the expected localisation error.\nBuilding on this insight, we present Belief-Conditioned One-Step Diffusion\n(B-COD), the first planner that, in a 10 ms forward pass, returns a\nshort-horizon trajectory, per-waypoint aleatoric variances, and a proxy for\nlocalisation error--eliminating external covariance rollouts. We show that this\nsingle proxy suffices for a soft-actor-critic to choose sensors online,\noptimising energy while bounding pose-covariance growth. We deploy B-COD in\nreal-time marine trials on an unmanned surface vehicle and show that it reduces\nsensing energy consumption while matching the goal-reach performance of an\nalways-on baseline.", "AI": {"tldr": "B-COD是一种基于扩散模型的规划器，通过单次前向传播即可生成轨迹和定位误差代理，结合强化学习在线选择最小传感器子集，在保证任务完成的同时显著降低能耗。", "motivation": "现有方法要么需要持续开启所有传感器（浪费能源），要么依赖启发式传感器切换和昂贵的协方差传播计算。需要一种能够在保持足够状态不确定性的前提下，选择最小传感器子集的方法。", "method": "提出Belief-Conditioned One-Step Diffusion (B-COD)规划器，将位姿信念栅格和传感器掩码作为条件输入扩散模型，通过去噪轨迹的扩散程度获得定位误差的校准代理。结合soft-actor-critic在线选择传感器。", "result": "在无人水面艇的真实海洋试验中，B-COD在匹配全开传感器基线性能的同时，显著降低了传感能耗，单次前向传播仅需10毫秒。", "conclusion": "B-COD首次实现了在单次前向传播中同时生成轨迹、方差估计和定位误差代理，为在线传感器选择提供了高效解决方案，在真实环境中验证了其有效性和能效优势。"}}
{"id": "2508.11887", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11887", "abs": "https://arxiv.org/abs/2508.11887", "authors": ["Yousra Shleibik", "Jordan Sinclair", "Kerstin Haring"], "title": "Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards", "comment": null, "summary": "The advent of autonomous driving systems promises to transform transportation\nby enhancing safety, efficiency, and comfort. As these technologies evolve\ntoward higher levels of autonomy, the need for integrated systems that\nseamlessly support human involvement in decision-making becomes increasingly\ncritical. Certain scenarios necessitate human involvement, including those\nwhere the vehicle is unable to identify an object or element in the scene, and\nas such cannot take independent action. Therefore, situational awareness is\nessential to mitigate potential risks during a takeover, where a driver must\nassume control and autonomy from the vehicle. The need for driver attention is\nimportant to avoid collisions with external agents and ensure a smooth\ntransition during takeover operations. This paper explores the integration of\nattention redirection techniques, such as gaze manipulation through targeted\nvisual and auditory cues, to help drivers maintain focus on emerging hazards\nand reduce target fixation in semi-autonomous driving scenarios. We propose a\nconceptual framework that combines real-time gaze tracking, context-aware\nsaliency analysis, and synchronized visual and auditory alerts to enhance\nsituational awareness, proactively address potential hazards, and foster\neffective collaboration between humans and autonomous systems.", "AI": {"tldr": "论文提出了一种通过视觉和听觉线索进行注意力重定向的框架，以增强半自动驾驶场景中的情境意识，帮助驾驶员在接管过程中更好地应对潜在危险。", "motivation": "随着自动驾驶技术向更高自主性发展，需要在无法识别场景元素时进行人工干预。情境意识对于降低接管过程中的风险至关重要，需要保持驾驶员注意力以避免碰撞并确保平稳过渡。", "method": "提出了一个概念框架，结合实时视线追踪、上下文感知的显著性分析和同步的视觉听觉警报，通过目标视觉和听觉线索进行注视操纵来实现注意力重定向。", "result": "该框架旨在帮助驾驶员保持对突发危险的关注，减少目标固定现象，增强情境意识，主动应对潜在危险，促进人与自主系统的有效协作。", "conclusion": "注意力重定向技术对于提升半自动驾驶安全性具有重要意义，提出的集成框架能够有效支持人类在决策过程中的参与，确保自动驾驶系统向更高自主性发展的平稳过渡。"}}
{"id": "2508.12335", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12335", "abs": "https://arxiv.org/abs/2508.12335", "authors": ["Yunfan Gao", "Florian Messerer", "Niels van Duijkeren", "Rashmi Dabir", "Moritz Diehl"], "title": "Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control", "comment": "21 pages, 15 figures", "summary": "This paper presents a novel approach for collision avoidance in optimal and\nmodel predictive control, in which the environment is represented by a large\nnumber of points and the robot as a union of padded polygons. The conditions\nthat none of the points shall collide with the robot can be written in terms of\nan infinite number of constraints per obstacle point. We show that the\nresulting semi-infinite programming (SIP) optimal control problem (OCP) can be\nefficiently tackled through a combination of two methods: local reduction and\nan external active-set method. Specifically, this involves iteratively\nidentifying the closest point obstacles, determining the lower-level distance\nminimizer among all feasible robot shape parameters, and solving the\nupper-level finitely-constrained subproblems.\n  In addition, this paper addresses robust collision avoidance in the presence\nof ellipsoidal state uncertainties. Enforcing constraint satisfaction over all\npossible uncertainty realizations extends the dimension of constraint\ninfiniteness. The infinitely many constraints arising from translational\nuncertainty are handled by local reduction together with the robot shape\nparameterization, while rotational uncertainty is addressed via a backoff\nreformulation.\n  A controller implemented based on the proposed method is demonstrated on a\nreal-world robot running at 20Hz, enabling fast and collision-free navigation\nin tight spaces. An application to 3D collision avoidance is also demonstrated\nin simulation.", "AI": {"tldr": "这篇论文提出了一种新题的最优控制和模型预测控制中的碰撞避免方法，通过半无穷规划和主动集方法高效处理无穷约束，并在实际机器人上实现了20Hz的快速碰撞免遮导航。", "motivation": "解决最优控制和模型预测控制中的碰撞避免问题，特别是当环境由大量点体表示而机器人由多边形组成时，需要处理无穷多个约束的技术挑战。", "method": "采用半无穷规划(SIP)方法，结合局部约化和外部主动集算法，迭代识别最近的障碍点、确定可行机器人形状参数的距离最小化器，并求解有限约束子问题。对于状态不确定性，通过局部约化处理平移不确定性，通过后退重构处理旋转不确定性。", "result": "在实际机器人上实现了20Hz的控制器，能够在窄窄空间中实现快速的碰撞免遮导航。同时在模拟中也展示了3D碰撞避免的应用效果。", "conclusion": "该方法能够高效处理最优控制问题中的无穷约束挑战，并在存在状态不确定性的情况下实现稳健的碰撞避免，为机器人在复杂环境中的安全导航提供了有效解决方案。"}}
{"id": "2508.11890", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11890", "abs": "https://arxiv.org/abs/2508.11890", "authors": ["Sangwoo Jeon", "Juchul Shin", "YeonJe Cho", "Gyeong-Tae Kim", "Seongwoo Kim"], "title": "Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation", "comment": null, "summary": "Modern autonomous drone missions increasingly require software frameworks\ncapable of seamlessly integrating structured symbolic planning with adaptive\nreinforcement learning (RL). Although traditional rule-based architectures\noffer robust structured reasoning for drone autonomy, their capabilities fall\nshort in dynamically complex operational environments that require adaptive\nsymbolic planning. Symbolic RL (SRL), using the Planning Domain Definition\nLanguage (PDDL), explicitly integrates domain-specific knowledge and\noperational constraints, significantly improving the reliability and safety of\nunmanned aerial vehicle (UAV) decision making. In this study, we propose the\nAMAD-SRL framework, an extended and refined version of the Autonomous Mission\nAgents for Drones (AMAD) cognitive multi-agent architecture, enhanced with\nsymbolic reinforcement learning for dynamic mission planning and execution. We\nvalidated our framework in a Software-in-the-Loop (SIL) environment structured\nidentically to an intended Hardware-In-the-Loop Simulation (HILS) platform,\nensuring seamless transition to real hardware. Experimental results demonstrate\nstable integration and interoperability of modules, successful transitions\nbetween BDI-driven and symbolic RL-driven planning phases, and consistent\nmission performance. Specifically, we evaluate a target acquisition scenario in\nwhich the UAV plans a surveillance path followed by a dynamic reentry path to\nsecure the target while avoiding threat zones. In this SIL evaluation, mission\nefficiency improved by approximately 75% over a coverage-based baseline,\nmeasured by travel distance reduction. This study establishes a robust\nfoundation for handling complex UAV missions and discusses directions for\nfurther enhancement and validation.", "AI": {"tldr": "基于AMAD-SRL框架，通过符号强化学习整合结构化规划与适应性决策，在软件在环测试中将无人机任务效率提升75%，为复杂任务提供了可靠解决方案。", "motivation": "现代无人机任务需要能够无缝整合结构化符号规划与适应性强化学习的框架，以应对动态复杂环境中的适应性符号规划需求，提高决策的可靠性和安全性。", "method": "提出AMAD-SRL框架，作为AMAD认知多代理架构的扩展和精化版本，采用基于PDDL语言的符号强化学习技术，在软件在环测试环境中进行验证。", "result": "实验结果显示模块集成稳定、互操作性良好，能够成功在BDI驱动和符号RL驱动规划阶段之间转换，任务效率按行程距离计算提高了75%。", "conclusion": "该研究为处理复杂无人机任务打下了坚实基础，并提出了进一步提升和验证的方向。"}}
{"id": "2508.12395", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12395", "abs": "https://arxiv.org/abs/2508.12395", "authors": ["Zihan Wang"], "title": "PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting", "comment": null, "summary": "This study presents the design and control of a Plasma-propelled\nUltra-silence Blimp (PUB), a novel aerial robot employing plasma vector\npropulsion for ultra-quiet flight without mechanical propellers. The system\nutilizes a helium-lift platform for extended endurance and a four-layer ring\nasymmetric capacitor to generate ionic wind thrust. The modular propulsion\nunits allow flexible configuration to meet mission-specific requirements, while\na two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop\nslip control scheme is implemented for stable maneuvering. Flight experiments\ndemonstrate full-envelope capability, including take-off, climb, hover,\ndescent, and smooth landing, confirming the feasibility of plasma vector\npropulsion, the effectiveness of DOF vector control, and the stability of the\ncontrol system. Owing to its low acoustic signature, structural simplicity, and\nhigh maneuverability, PUB is well suited for noise-sensitive, enclosed, and\nnear-space applications.", "AI": {"tldr": "这篇论文提出了一种采用等离子风推进的超静音氯气航天器(PUB)，通过模块化推进系统和闭环滑移控制实现了高机动性和超低噪音的飞行。", "motivation": "为了解决传统旋翼推进噪音大、结构复杂的问题，提出一种无机械推进器的超静音氯气航天器方案，适用于噪音敏感、封闭式和近空间应用场景。", "method": "采用氯气提供升力平台，四层环形非对称电容器生成离子风推力，模块化推进单元支持灵活配置，两度自由度头部实现推力向量控制，集成闭环滑移控制方案保证稳定机动。", "result": "飞行实验证明系统具备全包线能力，包括起飞、爬升、悬停、降落和平滑着陆，验证了等离子向量推进的可行性、向量控制的有效性以及控制系统的稳定性。", "conclusion": "PUB系统通过等离子推进技术实现了超低噪音、结构简单和高机动性，在噪音敏感、封闭环境和近空间应用中具有重要价值和应用潜力。"}}
{"id": "2508.11898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11898", "abs": "https://arxiv.org/abs/2508.11898", "authors": ["Jilei Mao", "Jiarui Guan", "Yingjuan Tang", "Qirui Hu", "Zhihang Li", "Junjie Yu", "Yongjie Mao", "Yunzhe Sun", "Shuang Liu", "Xiaozhu Ju"], "title": "OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation", "comment": null, "summary": "The visuomotor policy can easily overfit to its training datasets, such as\nfixed camera positions and backgrounds. This overfitting makes the policy\nperform well in the in-distribution scenarios but underperform in the\nout-of-distribution generalization. Additionally, the existing methods also\nhave difficulty fusing multi-view information to generate an effective 3D\nrepresentation. To tackle these issues, we propose Omni-Vision Diffusion Policy\n(OmniD), a multi-view fusion framework that synthesizes image observations into\na unified bird's-eye view (BEV) representation. We introduce a deformable\nattention-based Omni-Feature Generator (OFG) to selectively abstract\ntask-relevant features while suppressing view-specific noise and background\ndistractions. OmniD achieves 11\\%, 17\\%, and 84\\% average improvement over the\nbest baseline model for in-distribution, out-of-distribution, and few-shot\nexperiments, respectively. Training code and simulation benchmark are\navailable: https://github.com/1mather/omnid.git", "AI": {"tldr": "OmniD是一个多视角融合框架，通过可变形注意力机制将多视角图像合成为统一的鸟瞰图表示，显著提升了视觉运动策略在分布内外和少样本场景下的泛化性能。", "motivation": "现有视觉运动策略容易在训练数据上过拟合（如固定相机位置和背景），导致在分布外场景泛化能力差，且难以有效融合多视角信息生成3D表示。", "method": "提出Omni-Vision Diffusion Policy (OmniD)框架，使用基于可变形注意力的Omni-Feature Generator (OFG)选择性提取任务相关特征，抑制视角特定噪声和背景干扰，合成统一的鸟瞰图表示。", "result": "在分布内、分布外和少样本实验中，OmniD相比最佳基线模型分别平均提升了11%、17%和84%的性能。", "conclusion": "OmniD通过有效的多视角融合和特征选择机制，显著提升了视觉运动策略的泛化能力，为解决过拟合和多视角信息融合问题提供了有效方案。"}}
{"id": "2508.12729", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12729", "abs": "https://arxiv.org/abs/2508.12729", "authors": ["Junhao Ye", "Cheng Hu", "Yiqin Wang", "Weizhan Huang", "Nicolas Baumann", "Jie He", "Meixun Qu", "Lei Xie", "Hongye Su"], "title": "MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA", "comment": null, "summary": "In autonomous racing, reactive controllers eliminate the computational burden\nof the full See-Think-Act autonomy stack by directly mapping sensor inputs to\ncontrol actions. This bypasses the need for explicit localization and\ntrajectory planning. A widely adopted baseline in this category is the\nFollow-The-Gap method, which performs trajectory planning using LiDAR data.\nBuilding on FTG, the Delaunay Triangulation-based Racing algorithm introduces\nfurther enhancements. However, DTR's use of circumcircles for trajectory\ngeneration often results in insufficiently smooth paths, ultimately degrading\nperformance. Additionally, the commonly used F1TENTH-simulator for autonomous\nracing competitions lacks support for 3D LiDAR perception, limiting its\neffectiveness in realistic testing. To address these challenges, this work\nproposes the MCTR algorithm. MCTR improves trajectory smoothness through the\nuse of Curvature Corrected Moving Average and implements a digital twin system\nwithin the CARLA simulator to validate the algorithm's robustness under 3D\nLiDAR perception. The proposed algorithm has been thoroughly validated through\nboth simulation and real-world vehicle experiments.", "AI": {"tldr": "MCTR算法通过曲率校正移动平均提高轨迹平滑度，并在CARLA模拟器中实现数字孪生系统，解决了DTR算法轨迹不平滑和F1TENTH模拟器缺乏3D LiDAR支持的问题。", "motivation": "现有Follow-The-Gap和DTR算法在自主赛车中存在轨迹不够平滑的问题，且常用F1TENTH模拟器缺乏3D LiDAR感知支持，限制了算法的真实环境测试效果。", "method": "提出MCTR算法，使用曲率校正移动平均(CCMA)改进轨迹平滑度，并在CARLA模拟器中构建数字孪生系统来验证3D LiDAR感知下的算法鲁棒性。", "result": "算法通过仿真和真实车辆实验得到充分验证，证明了其在3D LiDAR感知环境下的有效性和鲁棒性。", "conclusion": "MCTR算法成功解决了自主赛车中轨迹平滑度和3D感知验证的问题，为反应式控制器提供了更好的性能表现。"}}
{"id": "2508.11917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11917", "abs": "https://arxiv.org/abs/2508.11917", "authors": ["Hossein Keshavarz", "Alejandro Ramirez-Serrano", "Majid Khadiv"], "title": "Control of Legged Robots using Model Predictive Optimized Path Integral", "comment": "8 pages, 13 figures, Humanoid conference", "summary": "Legged robots possess a unique ability to traverse rough terrains and\nnavigate cluttered environments, making them well-suited for complex,\nreal-world unstructured scenarios. However, such robots have not yet achieved\nthe same level as seen in natural systems. Recently, sampling-based predictive\ncontrollers have demonstrated particularly promising results. This paper\ninvestigates a sampling-based model predictive strategy combining model\npredictive path integral (MPPI) with cross-entropy (CE) and covariance matrix\nadaptation (CMA) methods to generate real-time whole-body motions for legged\nrobots across multiple scenarios. The results show that combining the benefits\nof MPPI, CE and CMA, namely using model predictive optimized path integral\n(MPOPI), demonstrates greater sample efficiency, enabling robots to attain\nsuperior locomotion results using fewer samples when compared to typical MPPI\nalgorithms. Extensive simulation experiments in multiple scenarios on a\nquadruped robot show that MPOPI can be used as an anytime control strategy,\nincreasing locomotion capabilities at each iteration.", "AI": {"tldr": "提出MPOPI算法，结合MPPI、CE和CMA方法，用于四足机器人实时全身运动控制，相比传统MPPI算法具有更好的样本效率和运动性能", "motivation": "腿式机器人在复杂非结构化环境中具有独特优势，但目前性能仍不及自然系统。采样预测控制器显示出良好前景，需要提高样本效率和实时控制能力", "method": "采用基于采样的模型预测策略，结合模型预测路径积分(MPPI)、交叉熵(CE)和协方差矩阵自适应(CMA)方法，开发MPOPI算法", "result": "MPOPI算法表现出更高的样本效率，能用更少样本获得优于传统MPPI算法的运动效果。在四足机器人多种场景的仿真实验中证明可作为随时控制策略", "conclusion": "MPOPI结合MPPI、CE和CMA的优势，能够有效提升腿式机器人的运动能力，每次迭代都能提高运动性能，是一种有效的实时控制方法"}}
{"id": "2508.13151", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13151", "abs": "https://arxiv.org/abs/2508.13151", "authors": ["Yuying Zhang", "Joni Pajarinen"], "title": "Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors", "comment": null, "summary": "Mobile manipulation in dynamic environments is challenging due to movable\nobstacles blocking the robot's path. Traditional methods, which treat\nnavigation and manipulation as separate tasks, often fail in such\n'manipulate-to-navigate' scenarios, as obstacles must be removed before\nnavigation. In these cases, active interaction with the environment is required\nto clear obstacles while ensuring sufficient space for movement. To address the\nmanipulate-to-navigate problem, we propose a reinforcement learning-based\napproach for learning manipulation actions that facilitate subsequent\nnavigation. Our method combines manipulability priors to focus the robot on\nhigh manipulability body positions with affordance maps for selecting\nhigh-quality manipulation actions. By focusing on feasible and meaningful\nactions, our approach reduces unnecessary exploration and allows the robot to\nlearn manipulation strategies more effectively. We present two new\nmanipulate-to-navigate simulation tasks called Reach and Door with the Boston\nDynamics Spot robot. The first task tests whether the robot can select a good\nhand position in the target area such that the robot base can move effectively\nforward while keeping the end effector position fixed. The second task requires\nthe robot to move a door aside in order to clear the navigation path. Both of\nthese tasks need first manipulation and then navigating the base forward.\nResults show that our method allows a robot to effectively interact with and\ntraverse dynamic environments. Finally, we transfer the learned policy to a\nreal Boston Dynamics Spot robot, which successfully performs the Reach task.", "AI": {"tldr": "提出基于强化学习的方法解决移动机器人在动态环境中需要先操纵障碍物再导航的问题，结合可操纵性先验和功能映射来选择高质量操纵动作，在模拟和真实Spot机器人上验证有效性", "motivation": "传统方法将导航和操纵作为独立任务处理，在需要先清除障碍物才能导航的'操纵以导航'场景中经常失败，需要主动与环境交互来清理障碍物同时确保足够的移动空间", "method": "强化学习方法结合可操纵性先验（关注高可操纵性体位）和功能映射（选择高质量操纵动作），减少不必要的探索，更有效地学习操纵策略", "result": "在两个新的模拟任务（Reach和Door）中验证方法有效性，并将学习到的策略成功迁移到真实Boston Dynamics Spot机器人上执行Reach任务", "conclusion": "该方法使机器人能够有效与动态环境交互并穿越，解决了操纵以导航的问题"}}
{"id": "2508.11918", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11918", "abs": "https://arxiv.org/abs/2508.11918", "authors": ["Zhichen Lou", "Kechun Xu", "Zhongxiang Zhou", "Rong Xiong"], "title": "ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models", "comment": null, "summary": "The advancement of embodied intelligence is accelerating the integration of\nrobots into daily life as human assistants. This evolution requires robots to\nnot only interpret high-level instructions and plan tasks but also perceive and\nadapt within dynamic environments. Vision-Language Models (VLMs) present a\npromising solution by combining visual understanding and language reasoning.\nHowever, existing VLM-based methods struggle with interactive exploration,\naccurate perception, and real-time plan adaptation. To address these\nchallenges, we propose ExploreVLM, a novel closed-loop task planning framework\npowered by Vision-Language Models (VLMs). The framework is built around a\nstep-wise feedback mechanism that enables real-time plan adjustment and\nsupports interactive exploration. At its core is a dual-stage task planner with\nself-reflection, enhanced by an object-centric spatial relation graph that\nprovides structured, language-grounded scene representations to guide\nperception and planning. An execution validator supports the closed loop by\nverifying each action and triggering re-planning. Extensive real-world\nexperiments demonstrate that ExploreVLM significantly outperforms\nstate-of-the-art baselines, particularly in exploration-centric tasks. Ablation\nstudies further validate the critical role of the reflective planner and\nstructured perception in achieving robust and efficient task execution.", "AI": {"tldr": "ExploreVLM是一个基于视觉语言模型的闭环任务规划框架，通过逐步反馈机制实现实时计划调整和交互式探索，在探索型任务中显著优于现有方法。", "motivation": "随着具身智能的发展，机器人需要理解高级指令、规划任务并在动态环境中感知适应。现有VLM方法在交互探索、精确感知和实时计划调整方面存在不足。", "method": "提出双阶段任务规划器（带自反思机制）、对象中心空间关系图提供结构化场景表示、执行验证器形成闭环系统，支持实时重新规划。", "result": "大量真实世界实验表明，ExploreVLM显著优于最先进基线方法，特别是在探索型任务中表现突出。消融研究验证了反思规划器和结构化感知的关键作用。", "conclusion": "ExploreVLM通过闭环反馈机制和结构化感知表示，有效解决了VLM在机器人任务规划中的交互探索和实时适应挑战，实现了鲁棒高效的任务执行。"}}
{"id": "2508.11929", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11929", "abs": "https://arxiv.org/abs/2508.11929", "authors": ["Mohitvishnu S. Gadde", "Pranay Dugar", "Ashish Malik", "Alan Fern"], "title": "No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain", "comment": null, "summary": "Effective bipedal locomotion in dynamic environments, such as cluttered\nindoor spaces or uneven terrain, requires agile and adaptive movement in all\ndirections. This necessitates omnidirectional terrain sensing and a controller\ncapable of processing such input. We present a learning framework for\nvision-based omnidirectional bipedal locomotion, enabling seamless movement\nusing depth images. A key challenge is the high computational cost of rendering\nomnidirectional depth images in simulation, making traditional sim-to-real\nreinforcement learning (RL) impractical. Our method combines a robust blind\ncontroller with a teacher policy that supervises a vision-based student policy,\ntrained on noise-augmented terrain data to avoid rendering costs during RL and\nensure robustness. We also introduce a data augmentation technique for\nsupervised student training, accelerating training by up to 10 times compared\nto conventional methods. Our framework is validated through simulation and\nreal-world tests, demonstrating effective omnidirectional locomotion with\nminimal reliance on expensive rendering. This is, to the best of our knowledge,\nthe first demonstration of vision-based omnidirectional bipedal locomotion,\nshowcasing its adaptability to diverse terrains.", "AI": {"tldr": "提出了一种基于视觉的全向双足运动学习框架，通过结合盲控制器和教师策略来训练学生策略，避免了仿真渲染的高计算成本，实现了高效的全向地形适应运动。", "motivation": "在动态环境中实现有效的双足运动需要全向地形感知和相应的控制器，但传统仿真到现实的强化学习方法因全向深度图像渲染计算成本过高而不实用。", "method": "结合鲁棒的盲控制器和教师策略来监督基于视觉的学生策略，使用噪声增强的地形数据进行训练，避免RL过程中的渲染成本，并引入数据增强技术加速训练。", "result": "在仿真和真实世界测试中验证了框架有效性，展示了全向运动能力，对多样化地形具有良好的适应性，训练速度比传统方法快10倍。", "conclusion": "这是首个基于视觉的全向双足运动演示，展示了在最小化昂贵渲染依赖的情况下实现有效全向运动的能力，为动态环境中的双足机器人运动提供了实用解决方案。"}}
{"id": "2508.11960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11960", "abs": "https://arxiv.org/abs/2508.11960", "authors": ["Sandeep Kanta", "Mehrdad Tavassoli", "Varun Teja Chirkuri", "Venkata Akhil Kumar", "Santhi Bharath Punati", "Praveen Damacharla", "Sunny Katyara"], "title": "Toward General Physical Intelligence for Resilient Agile Manufacturing Automation", "comment": "Advanced Engineering Informatics", "summary": "Agile and human-centric manufacturing stipulates resilient robotic solutions\ncapable of contextual reasoning and safe interaction in unstructured\nenvironments. Foundation models particularly the Vision Language Action (VLA)\nmodels have emerged to fuse multimodal perception, reasoning and physically\ngrounded action across varied embodiments into unified representation, termed\nas General Physical Intelligence (GPI). While GPI has already been described in\nthe literature but its practical application and evolving role in contemporary\nagile manufacturing processes have yet to be duly explored. To bridge this gap,\nthis practical review systematically surveys recent advancements in VLA models\nwithin GPI context, performs comprehensive comparative analysis of leading\nimplementations and evaluates their readiness for industrial deployment through\nstructured ablation study. Our analysis has organized state-of-the-art into\nfive thematic pillars including multisensory representation learning, sim2real\ntransfer, planning and control, uncertainty and safety measures and\nbenchmarking. Finally, we articulate open research challenges and propose\ndirections to better integrate GPI into next-generation industrial ecosystems\nin line with Industry 5.0.", "AI": {"tldr": "这篇实践性综述系统性分析了基础模型在普遍物理智能(GPI)中的最新进展，尤其是视觉语言动作(VLA)模型在灵活制造中的应用潜力和挑战", "motivation": "尽管普遍物理智能(GPI)已在理论中描述，但其在当代灵活制造过程中的实际应用和发展角色仍未得到充分探索，需要桥接这一空白", "method": "通过系统性综述最近VLA模型在GPI背景下的进展，进行全面的比较分析，并通过结构化的消融研究评估其工业部署准备度", "result": "将最新技术进展组织为五个主题架构：多感知表征学习、模拟到实际转移、规划与控制、不确定性与安全措施、以及基准测试，识别了相关技术的工业应用潜力", "conclusion": "提出了开放性研究挑战，并建议了方向来更好地将GPI集成到符合工业5.0的下一代工业生态系统中"}}
{"id": "2508.12038", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12038", "abs": "https://arxiv.org/abs/2508.12038", "authors": ["Liwen Zhang", "Heng Deng", "Guanghui Sun"], "title": "Fully Spiking Actor-Critic Neural Network for Robotic Manipulation", "comment": null, "summary": "This study proposes a hybrid curriculum reinforcement learning (CRL)\nframework based on a fully spiking neural network (SNN) for 9-degree-of-freedom\nrobotic arms performing target reaching and grasping tasks. To reduce network\ncomplexity and inference latency, the SNN architecture is simplified to include\nonly an input and an output layer, which shows strong potential for\nresource-constrained environments. Building on the advantages of SNNs-high\ninference speed, low energy consumption, and spike-based biological\nplausibility, a temporal progress-partitioned curriculum strategy is integrated\nwith the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy\nconsumption modeling framework is introduced to quantitatively compare the\ntheoretical energy consumption between SNNs and conventional Artificial Neural\nNetworks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized\nobservation space further improve learning efficiency and policy accuracy.\nExperiments on the Isaac Gym simulation platform demonstrate that the proposed\nmethod achieves superior performance under realistic physical constraints.\nComparative evaluations with conventional PPO and ANN baselines validate the\nscalability and energy efficiency of the proposed approach in dynamic robotic\nmanipulation tasks.", "AI": {"tldr": "基于全空间刷新神经网络的混合课程强化学习框架，用于9自由度机器手的目标到达和抓取任务，具有低网络复杂度、低能耗和高效率特点", "motivation": "解决传统人工神经网络在机器手控制中的高能耗问题，利用空间刷新神经网络的低能耗特性来实现高效能的动态操控任务", "method": "简化SNN网络结构仅包含输入输出层，集成时间进度分区课程策略与PPO算法，引入能耗模型框架和动态两阶段奖励调整机制，优化观测空间", "result": "在Isaac Gym模拟平台上达到了优异性能，进行了与传统PPO和ANN基线的对比评估，验证了方法在动态机器人操控任务中的可扩展性和能量效率", "conclusion": "该混合课程强化学习框架通过简化SNN结构和优化策略，在保持高性能的同时显著降低了能耗，为资源受限环境下的机器人控制提供了有效解决方案"}}
{"id": "2508.12043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12043", "abs": "https://arxiv.org/abs/2508.12043", "authors": ["Fei Lin", "Tengchao Zhang", "Qinghua Ni", "Jun Huang", "Siji Ma", "Yonglin Tian", "Yisheng Lv", "Naiqi Wu"], "title": "Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs", "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) in unmanned systems has\nsignificantly enhanced the semantic understanding and autonomous task execution\ncapabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited\ncommunication bandwidth and the need for high-frequency interactions pose\nsevere challenges to semantic information transmission within the swarm. This\npaper explores the feasibility of LLM-driven UAV swarms for autonomous semantic\ncompression communication, aiming to reduce communication load while preserving\ncritical task semantics. To this end, we construct four types of 2D simulation\nscenarios with different levels of environmental complexity and design a\ncommunication-execution pipeline that integrates system prompts with task\ninstruction prompts. On this basis, we systematically evaluate the semantic\ncompression performance of nine mainstream LLMs in different scenarios and\nanalyze their adaptability and stability through ablation studies on\nenvironmental complexity and swarm size. Experimental results demonstrate that\nLLM-based UAV swarms have the potential to achieve efficient collaborative\ncommunication under bandwidth-constrained and multi-hop link conditions.", "AI": {"tldr": "论文探索LLM驱动的无人机群在带宽受限条件下实现语义压缩通信的可行性，通过构建不同复杂度的2D仿真场景，评估9种主流LLM的语义压缩性能。", "motivation": "无人机群采用大语言模型后语义理解和自主任务执行能力显著提升，但有限通信带宽和高频交互需求对语义信息传输构成严峻挑战。", "method": "构建四种不同环境复杂度的2D仿真场景，设计集成系统提示和任务指令提示的通信-执行流水线，系统评估9种主流LLM在不同场景下的语义压缩性能。", "result": "实验结果表明，基于LLM的无人机群有潜力在带宽受限和多跳链路条件下实现高效的协作通信。", "conclusion": "LLM驱动的无人机群能够通过语义压缩有效减少通信负载，同时保持关键任务语义，在复杂环境下表现出良好的适应性和稳定性。"}}
{"id": "2508.12071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12071", "abs": "https://arxiv.org/abs/2508.12071", "authors": ["Amy Phung", "Richard Camilli"], "title": "OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments", "comment": "This paper has been accepted for publication in IROS 2025. Copyright\n  IEEE", "summary": "High resolution underwater 3D scene reconstruction is crucial for various\napplications, including construction, infrastructure maintenance, monitoring,\nexploration, and scientific investigation. Prior work has leveraged the\ncomplementary sensing modalities of imaging sonars and optical cameras for\nopti-acoustic 3D scene reconstruction, demonstrating improved results over\nmethods which rely solely on either sensor. However, while most existing\napproaches focus on offline reconstruction, real-time spatial awareness is\nessential for both autonomous and piloted underwater vehicle operations. This\npaper presents OASIS, an opti-acoustic fusion method that integrates data from\noptical images with voxel carving techniques to achieve real-time 3D\nreconstruction unstructured underwater workspaces. Our approach utilizes an\n\"eye-in-hand\" configuration, which leverages the dexterity of robotic\nmanipulator arms to capture multiple workspace views across a short baseline.\nWe validate OASIS through tank-based experiments and present qualitative and\nquantitative results that highlight its utility for underwater manipulation\ntasks.", "AI": {"tldr": "OASIS是一种实时水下3D重建方法，通过融合光学相机和声纳数据，结合体素雕刻技术，实现非结构化水下工作空间的实时三维重建", "motivation": "现有方法主要关注离线重建，而实时空间感知对于自主和有人驾驶水下车辆操作至关重要，需要开发实时重建技术", "method": "采用\"眼在手\"配置，利用机器人机械臂的灵活性在短基线上捕获多个工作空间视图，融合光学图像和声纳数据，结合体素雕刻技术", "result": "通过水箱实验验证，提供了定性和定量结果，证明其对水下操作任务的有效性", "conclusion": "OASIS方法能够实现实时水下3D重建，为水下操作任务提供了实用的解决方案"}}
{"id": "2508.12075", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12075", "abs": "https://arxiv.org/abs/2508.12075", "authors": ["Shaul Ashkenazi", "Gabriel Skantze", "Jane Stuart-Smith", "Mary Ellen Foster"], "title": "Into the Wild: When Robots Are Not Welcome", "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025 (paper PubRob-Fails/2025/4)", "summary": "Social robots are increasingly being deployed in public spaces, where they\nface not only technological difficulties and unexpected user utterances, but\nalso objections from stakeholders who may not be comfortable with introducing a\nrobot into those spaces. We describe our difficulties with deploying a social\nrobot in two different public settings: 1) Student services center; 2) Refugees\nand asylum seekers drop-in service. Although this is a failure report, in each\nuse case we eventually managed to earn the trust of the staff and form a\nrelationship with them, allowing us to deploy our robot and conduct our\nstudies.", "AI": {"tldr": "社交机器人在公共空间部署遇到的困难和反对声音，但最终通过建立信任关系成功完成部署", "motivation": "研究社交机器人在公共空间部署时遇到的挑战，包括技术难题、意外用户语言和相关方反对", "method": "在两个公共场景中部署社交机器人：1学生服务中心；2雷渌湾和寻求宵报者临时服务点", "result": "尽管遇到困难，最终成功获得员工信任并建立良好关系，完成机器人部署和研究", "conclusion": "在公共空间部署社交机器人需要先获得相关方的信任和支持，建立良好关系是成功部署的关键因素"}}
{"id": "2508.12170", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12170", "abs": "https://arxiv.org/abs/2508.12170", "authors": ["Aryan Gupta"], "title": "Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)", "comment": null, "summary": "This study presents a systematic literature review of software-level\napproaches to energy efficiency in robotics published from 2020 through 2024,\nupdating and extending pre-2020 evidence. An automated-but-audited pipeline\ncombined Google Scholar seeding, backward/forward snowballing, and\nlarge-language-model (LLM) assistance for screening and data extraction, with\n~10% human audits at each automated step and consensus-with-tie-breaks for\nfull-text decisions. The final corpus comprises 79 peer-reviewed studies\nanalyzed across application domain, metrics, evaluation type, energy models,\nmajor energy consumers, software technique families, and energy-quality\ntrade-offs. Industrial settings dominate (31.6%) followed by exploration\n(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of\nstudies, with computing/controllers a distant second (13.9%). Simulation-only\nevaluations remain most common (51.9%), though hybrid evaluations are frequent\n(25.3%). Representational (physics-grounded) energy models predominate (87.3%).\nMotion and trajectory optimization is the leading technique family (69.6%),\noften paired with learning/prediction (40.5%) and computation\nallocation/scheduling (26.6%); power management/idle control (11.4%) and\ncommunication/data efficiency (3.8%) are comparatively underexplored. Reporting\nis heterogeneous: composite objectives that include energy are most common,\nwhile task-normalized and performance-per-energy metrics appear less often,\nlimiting cross-paper comparability. The review offers a minimal reporting\nchecklist (e.g., total energy and average power plus a task-normalized metric\nand clear baselines) and highlights opportunities in cross-layer designs and in\nquantifying non-performance trade-offs (accuracy, stability). A replication\npackage with code, prompts, and frozen datasets accompanies the review.", "AI": {"tldr": "这是一份2020-2024年软件层面机器人能源效率研究的系统性文献综述，分析了79份研究，发现运动/轨迹优化是主要技术，电机/执行器是主要能源消耗者，并提出了最小报告检查单和研究机遇。", "motivation": "更新和扩展2020年之前的证据，系统评估软件层面的机器人能源效率技术，以提高研究可比性和指导未来研究方向。", "method": "采用自动化但审计的流水线，结合Google Scholar种子、向后/向前雪球投放、以及大语言模型辅助进行筛选和数据提取，每个自动步骤有10%的人工审计。", "result": "分析了79份同行审查研究，发现工业设置占主导(31.6%)，电机/执行器是主要能源消耗者(68.4%)，运动和轨迹优化是主要技术(69.6%)，但报告标准异质性限制了可比性。", "conclusion": "研究提出了最小报告检查单（包括总能消耗、平均功率、任务标准化指标和清晰基准），并强调了跨层设计和量化非性能交易的机遇。"}}
{"id": "2508.12184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12184", "abs": "https://arxiv.org/abs/2508.12184", "authors": ["Rhea Malhotra", "William Chong", "Catie Cuan", "Oussama Khatib"], "title": "Humanoid Motion Scripting with Postural Synergies", "comment": null, "summary": "Generating sequences of human-like motions for humanoid robots presents\nchallenges in collecting and analyzing reference human motions, synthesizing\nnew motions based on these reference motions, and mapping the generated motion\nonto humanoid robots. To address these issues, we introduce SynSculptor, a\nhumanoid motion analysis and editing framework that leverages postural\nsynergies for training-free human-like motion scripting. To analyze human\nmotion, we collect 3+ hours of motion capture data across 20 individuals where\na real-time operational space controller mimics human motion on a simulated\nhumanoid robot. The major postural synergies are extracted using principal\ncomponent analysis (PCA) for velocity trajectories segmented by changes in\nrobot momentum, constructing a style-conditioned synergy library for free-space\nmotion generation. To evaluate generated motions using the synergy library, the\nfoot-sliding ratio and proposed metrics for motion smoothness involving total\nmomentum and kinetic energy deviations are computed for each generated motion,\nand compared with reference motions. Finally, we leverage the synergies with a\nmotion-language transformer, where the humanoid, during execution of motion\ntasks with its end-effectors, adapts its posture based on the chosen synergy.\nSupplementary material, code, and videos are available at\nhttps://rhea-mal.github.io/humanoidsynergies.io.", "AI": {"tldr": "SynSculptor是一个基于姿态协同的人形机器人运动分析与编辑框架，通过提取人体运动的主要协同模式，实现无需训练的人形运动生成。", "motivation": "为人形机器人生成类人运动序列面临参考运动采集分析困难、新运动合成复杂以及运动映射到机器人上的挑战，需要一种无需训练的运动生成方法。", "method": "收集3+小时20人的运动捕捉数据，使用PCA提取速度轨迹的主要姿态协同，构建风格条件协同库，通过运动-语言变换器实现基于协同的运动适应。", "result": "开发了基于脚滑动比、总动量和动能偏差的运动平滑度评估指标，生成的运动与参考运动进行比较验证。", "conclusion": "SynSculptor框架成功利用姿态协同实现了人形机器人的类人运动生成和编辑，为实时运动控制提供了有效解决方案。"}}
{"id": "2508.12189", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12189", "abs": "https://arxiv.org/abs/2508.12189", "authors": ["Rhea Malhotra", "Yuejiang Liu", "Chelsea Finn"], "title": "Self-Guided Action Diffusion", "comment": null, "summary": "Recent works have shown the promise of inference-time search over action\nsamples for improving generative robot policies. In particular, optimizing\ncross-chunk coherence via bidirectional decoding has proven effective in\nboosting the consistency and reactivity of diffusion policies. However, this\napproach remains computationally expensive as the diversity of sampled actions\ngrows. In this paper, we introduce self-guided action diffusion, a more\nefficient variant of bidirectional decoding tailored for diffusion-based\npolicies. At the core of our method is to guide the proposal distribution at\neach diffusion step based on the prior decision. Experiments in simulation\ntasks show that the proposed self-guidance enables near-optimal performance at\nnegligible inference cost. Notably, under a tight sampling budget, our method\nachieves up to 70% higher success rates than existing counterparts on\nchallenging dynamic tasks. See project website at\nhttps://rhea-mal.github.io/selfgad.github.io.", "AI": {"tldr": "自我导向动作液散方法，通过在每个液散步骤使用之前决策来导向建议分布，以更高效地提升液散策略的一致性和反应性。", "motivation": "虽然双向解码方法在提升液散策略的一致性和反应性方面显示了潜力，但随着动作采样多样性的增加，该方法计算成本过高。需要一种更高效的推理时搜索方法。", "method": "提出自我导向动作液散方法，在每个液散步骤中，基于之前的决策来导向建议分布，从而减少计算开销。", "result": "在模拟任务中，该方法在可忽略的推理成本下实现了近优性能。在严格的采样预算下，在具有挑战性的动态任务上比现有方法获得了较高成功率。", "conclusion": "自我导向动作液散方法能够在保持高性能的同时显著降低计算成本，为液散基于策略的推理时优化提供了一种高效的解决方案。"}}
{"id": "2508.12211", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12211", "abs": "https://arxiv.org/abs/2508.12211", "authors": ["Cyrus Neary", "Omar G. Younis", "Artur Kuramshin", "Ozgur Aslan", "Glen Berseth"], "title": "Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search", "comment": null, "summary": "Pre-trained vision-language-action (VLA) models offer a promising foundation\nfor generalist robot policies, but often produce brittle behaviours or unsafe\nfailures when deployed zero-shot in out-of-distribution scenarios. We present\nVision-Language-Action Planning & Search (VLAPS) -- a novel framework and\naccompanying algorithms that embed model-based search into the inference\nprocedure of pre-trained VLA policies to improve their performance on robotic\ntasks. Specifically, our method biases a modified Monte Carlo Tree Search\n(MCTS) algorithm -- run using a model of the target environment -- using action\npriors defined by the VLA policy. By using VLA-derived abstractions and priors\nin model-based search, VLAPS efficiently explores language-conditioned robotics\ntasks whose search spaces would otherwise be intractably large. Conversely, by\nintegrating model-based search with the VLA policy's inference procedure, VLAPS\nyields behaviours that are more performant than those obtained by directly\nfollowing the VLA policy's action predictions. VLAPS offers a principled\nframework to: i) control test-time compute in VLA models, ii) leverage a priori\nknowledge of the robotic environment, and iii) integrate established planning\nand reinforcement learning techniques into the VLA inference process. Across\nall experiments, VLAPS significantly outperforms VLA-only baselines on\nlanguage-specified tasks that would otherwise be intractable for uninformed\nsearch algorithms, increasing success rates by as much as 67 percentage points.", "AI": {"tldr": "VLAPS是一个将基于模型的搜索嵌入预训练VLA模型推理过程的新框架，通过结合蒙特卡洛树搜索和环境模型来提升机器人任务性能，显著优于纯VLA基线方法。", "motivation": "预训练的视觉-语言-动作(VLA)模型在零样本部署时容易产生脆弱行为或不安全故障，特别是在分布外场景中。需要一种方法来提高VLA模型在机器人任务中的鲁棒性和性能。", "method": "提出VLAPS框架，将改进的蒙特卡洛树搜索(MCTS)算法嵌入VLA策略推理过程。使用环境模型进行搜索，并用VLA策略定义的动作先验来引导搜索过程。", "result": "在所有实验中，VLAPS显著优于纯VLA基线方法，在语言指定任务上的成功率最高提升了67个百分点，解决了原本对无信息搜索算法不可行的问题。", "conclusion": "VLAPS提供了一个原则性框架，可以控制VLA模型的测试时计算、利用机器人环境的先验知识，并将成熟的规划和强化学习技术集成到VLA推理过程中。"}}
{"id": "2508.12252", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12252", "abs": "https://arxiv.org/abs/2508.12252", "authors": ["Kaizhe Hu", "Haochen Shi", "Yao He", "Weizhuo Wang", "C. Karen Liu", "Shuran Song"], "title": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids", "comment": "Accepted to The Conference on Robot Learning (CoRL) 2025", "summary": "Simulation-based reinforcement learning (RL) has significantly advanced\nhumanoid locomotion tasks, yet direct real-world RL from scratch or adapting\nfrom pretrained policies remains rare, limiting the full potential of humanoid\nrobots. Real-world learning, despite being crucial for overcoming the\nsim-to-real gap, faces substantial challenges related to safety, reward design,\nand learning efficiency. To address these limitations, we propose\nRobot-Trains-Robot (RTR), a novel framework where a robotic arm teacher\nactively supports and guides a humanoid robot student. The RTR system provides\nprotection, learning schedule, reward, perturbation, failure detection, and\nautomatic resets. It enables efficient long-term real-world humanoid training\nwith minimal human intervention. Furthermore, we propose a novel RL pipeline\nthat facilitates and stabilizes sim-to-real transfer by optimizing a single\ndynamics-encoded latent variable in the real world. We validate our method\nthrough two challenging real-world humanoid tasks: fine-tuning a walking policy\nfor precise speed tracking and learning a humanoid swing-up task from scratch,\nillustrating the promising capabilities of real-world humanoid learning\nrealized by RTR-style systems. See https://robot-trains-robot.github.io/ for\nmore info.", "AI": {"tldr": "提出了Robot-Trains-Robot (RTR)框架，通过机械臂教师主动指导人形机器人学生，实现高效的长时真实世界人形机器人训练，并提出了新的RL管道来促进和稳定sim-to-real迁移。", "motivation": "解决真实世界人形机器人强化学习面临的安全性、奖励设计和学习效率等挑战，克服sim-to-real差距，充分发挥人形机器人的潜力。", "method": "RTR框架：机械臂教师提供保护、学习计划、奖励、扰动、故障检测和自动重置；提出新的RL管道，通过优化真实世界中的单个动力学编码潜在变量来促进sim-to-real迁移。", "result": "在两个具有挑战性的真实世界人形任务中验证了方法：精确速度跟踪的行走策略微调，以及从零开始学习人形摆动任务。", "conclusion": "RTR系统展示了实现真实世界人形机器人学习的有前景能力，为克服sim-to-real差距提供了有效解决方案。"}}
{"id": "2508.12274", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12274", "abs": "https://arxiv.org/abs/2508.12274", "authors": ["Jian Zhao", "Yunlong Lian", "Andy M Tyrrell", "Michael Gienger", "Jihong Zhu"], "title": "Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments", "comment": "8 pages, 41 figures", "summary": "Robot-assisted dressing is a popular but challenging topic in the field of\nrobotic manipulation, offering significant potential to improve the quality of\nlife for individuals with mobility limitations. Currently, the majority of\nresearch on robot-assisted dressing focuses on how to put on loose-fitting\nclothing, with little attention paid to tight garments. For the former, since\nthe armscye is larger, a single robotic arm can usually complete the dressing\ntask successfully. However, for the latter, dressing with a single robotic arm\noften fails due to the narrower armscye and the property of diminishing\nrigidity in the armscye, which eventually causes the armscye to get stuck. This\npaper proposes a bimanual dressing strategy suitable for dressing tight-fitting\nclothing. To facilitate the encoding of dressing trajectories that adapt to\ndifferent human arm postures, a spherical coordinate system for dressing is\nestablished. We uses the azimuthal angle of the spherical coordinate system as\na task-relevant feature for bimanual manipulation. Based on this new\ncoordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture\nRegression (GMR) for imitation learning of bimanual dressing trajectories,\ngenerating dressing strategies that adapt to different human arm postures. The\neffectiveness of the proposed method is validated through various experiments.", "AI": {"tldr": "提出了一种适用于穿紧身服装的双手机器人穿衣策略，通过球坐标系和模建方法实现了适应不同人手臂姿势的穿衣轨迹生成", "motivation": "当前机器人辅助穿衣研究主要集中在松身服装，对紧身服装关注少，单手机器人穿紧身服装容易失败", "method": "建立球坐标系统作为穿衣任务的表示，使用方位角作为双手操作的任务相关特征，采用GMM和GMR进行仿真学习生成适应性穿衣轨迹", "result": "通过多种实验验证了所提方法的有效性", "conclusion": "该双手穿衣策略能够有效解决紧身服装穿着的挑战，为机器人辅助穿衣领域提供了新的解决方案"}}
{"id": "2508.12296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12296", "abs": "https://arxiv.org/abs/2508.12296", "authors": ["Bin Wang", "Jiwen Zhang", "Song Wang", "Dan Wu"], "title": "A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts", "comment": null, "summary": "In some high-precision industrial applications, robots are deployed to\nperform precision assembly tasks on mass batches of manufactured pegs and\nholes. If the peg and hole are designed with transition fit, machining errors\nmay lead to either a clearance or an interference fit for a specific pair of\ncomponents, with uncertain fit amounts. This paper focuses on the robotic batch\nprecision assembly task involving components with uncertain fit types and fit\namounts, and proposes an efficient methodology to construct the robust and\ncompliant assembly control strategy. Specifically, the batch precision assembly\ntask is decomposed into multiple deterministic subtasks, and a force-vision\nfusion controller-driven reinforcement learning method and a multi-task\nreinforcement learning training method (FVFC-MTRL) are proposed to jointly\nlearn multiple compliance control strategies for these subtasks. Subsequently,\nthe multi-teacher policy distillation approach is designed to integrate\nmultiple trained strategies into a unified student network, thereby\nestablishing a robust control strategy. Real-world experiments demonstrate that\nthe proposed method successfully constructs the robust control strategy for\nhigh-precision assembly task with different fit types and fit amounts.\nMoreover, the MTRL framework significantly improves training efficiency, and\nthe final developed control strategy achieves superior force compliance and\nhigher success rate compared with many existing methods.", "AI": {"tldr": "提出了一种基于力-视觉融合控制器驱动的强化学习方法(FVFC-MTRL)和多任务强化学习训练方法，用于解决机器人批量精密装配中配合类型和配合量不确定的问题，通过多教师策略蒸馏整合多个控制策略，实现鲁棒装配控制。", "motivation": "在高精度工业应用中，机器人执行精密装配任务时，由于加工误差导致销孔配合类型（间隙配合或过盈配合）和配合量存在不确定性，需要开发鲁棒的顺应性装配控制策略。", "method": "将批量精密装配任务分解为多个确定性子任务，采用力-视觉融合控制器驱动的强化学习方法和多任务强化学习训练方法(FVFC-MTRL)联合学习多个顺应控制策略，然后通过多教师策略蒸馏方法将这些策略整合到统一的学生网络中。", "result": "实验表明该方法成功构建了适用于不同配合类型和配合量的鲁棒控制策略，MTRL框架显著提高了训练效率，最终的控制策略在力顺应性和成功率方面优于现有方法。", "conclusion": "提出的FVFC-MTRL方法能够有效处理装配任务中的不确定性，实现高性能的鲁棒精密装配控制，为工业机器人精密装配提供了有效的解决方案。"}}
{"id": "2508.12312", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12312", "abs": "https://arxiv.org/abs/2508.12312", "authors": ["Marco Leon Rapp"], "title": "Implementation and evaluation of a prediction algorithm for an autonomous vehicle", "comment": "7 pages, 7 figures", "summary": "This paper presents a prediction algorithm that estimates the vehicle\ntrajectory every five milliseconds for an autonomous vehicle. A kinematic and a\ndynamic bicycle model are compared, with the dynamic model exhibiting superior\naccuracy at higher speeds. Vehicle parameters such as mass, center of gravity,\nmoment of inertia, and cornering stiffness are determined experimentally. For\ncornering stiffness, a novel measurement procedure using optical position\ntracking is introduced. The model is incorporated into an extended Kalman\nfilter and implemented in a ROS node in C++. The algorithm achieves a\npositional deviation of only 1.25 cm per meter over the entire test drive and\nis up to 82.6% more precise than the kinematic model.", "AI": {"tldr": "提出了一种基于动态自行车模型的车辆轨迹预测算法，每5毫秒预测一次，比运动学模型精度高82.6%，位置偏差仅为每米1.25厘米", "motivation": "为自动驾驶车辆开发高精度的实时轨迹预测算法，解决传统运动学模型在高速下精度不足的问题", "method": "使用动态自行车模型，通过实验确定车辆参数（质量、重心、转动惯量、侧偏刚度），采用光学位置跟踪测量侧偏刚度，将模型集成到扩展卡尔曼滤波器中，并在ROS中用C++实现", "result": "算法在整个测试过程中位置偏差仅为每米1.25厘米，比运动学模型精度提高82.6%，在高速下动态模型表现更优", "conclusion": "动态自行车模型比运动学模型更适合自动驾驶车辆的轨迹预测，特别是在高速情况下，提出的测量方法和算法实现能够实现高精度的实时预测"}}
{"id": "2508.12394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12394", "abs": "https://arxiv.org/abs/2508.12394", "authors": ["Zichen Yan", "Rui Huang", "Lei He", "Shao Guo", "Lin Zhao"], "title": "SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning", "comment": null, "summary": "Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an\nunknown environment and reaching a location that visually matches a given\ntarget image. While prior works primarily study ImageNav for ground robots,\nenabling this capability for autonomous drones is substantially more\nchallenging due to their need for high-frequency feedback control and global\nlocalization for stable flight. In this paper, we propose a novel sim-to-real\nframework that leverages visual reinforcement learning (RL) to achieve ImageNav\nfor drones. To enhance visual representation ability, our approach trains the\nvision backbone with auxiliary tasks, including image perturbations and future\ntransition prediction, which results in more effective policy training. The\nproposed algorithm enables end-to-end ImageNav with direct velocity control,\neliminating the need for external localization. Furthermore, we integrate a\ndepth-based safety module for real-time obstacle avoidance, allowing the drone\nto safely navigate in cluttered environments. Unlike most existing drone\nnavigation methods that focus solely on reference tracking or obstacle\navoidance, our framework supports comprehensive navigation\nbehaviors--autonomous exploration, obstacle avoidance, and image-goal\nseeking--without requiring explicit global mapping. Code and model checkpoints\nwill be released upon acceptance.", "AI": {"tldr": "通过视觉强化学习和辅助任务训练，实现了无人机的图像目标导航，支持自主探索、障碍避免和目标寻找，无需全局定位或显式地图建模。", "motivation": "现有无人机导航方法主要关注参考跟踪或障碍避免，而图像目标导航对于无人机更具挑战性，需要高频率控制和全局定位来维持稳定飞行。", "method": "提出了一种模拟到实际的框架，利用视觉强化学习训练视觉背榜，包括图像批变和未来过渡预测等辅助任务。集成了深度基于的安全模块用于实时障碍避免。", "result": "算法能够实现端到端的图像目标导航，直接通过速度控制，无需外部定位系统。在杂乱环境中也能安全导航。", "conclusion": "该框架为无人机提供了全面的导航能力，支持自主探索、障碍避免和图像目标寻找，而无需显式的全局地图建模，促进了无人机在复杂环境中的自主导航发展。"}}
{"id": "2508.12435", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12435", "abs": "https://arxiv.org/abs/2508.12435", "authors": ["Deqing Song", "Weimin Yang", "Maryam Rezayati", "Hans Wernher van de Venn"], "title": "Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots", "comment": null, "summary": "While gesture recognition using vision or robot skins is an active research\narea in Human-Robot Collaboration (HRC), this paper explores deep learning\nmethods relying solely on a robot's built-in joint sensors, eliminating the\nneed for external sensors. We evaluated various convolutional neural network\n(CNN) architectures and collected two datasets to study the impact of data\nrepresentation and model architecture on the recognition accuracy. Our results\nshow that spectrogram-based representations significantly improve accuracy,\nwhile model architecture plays a smaller role. We also tested generalization to\nnew robot poses, where spectrogram-based models performed better. Implemented\non a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,\nachieved over 95% accuracy in contact detection and gesture classification.\nThese findings demonstrate the feasibility of external-sensor-free tactile\nrecognition and promote further research toward cost-effective, scalable\nsolutions for HRC.", "AI": {"tldr": "本文探索仅使用机器人内置关节传感器的深度学习手势识别方法，无需外部传感器，通过CNN架构和频谱图表示实现高精度识别。", "motivation": "在HRC领域，传统手势识别依赖视觉或机器人皮肤等外部传感器，成本高且部署复杂。本研究旨在探索仅利用机器人内置关节传感器的解决方案，实现更经济、可扩展的触觉识别。", "method": "评估多种CNN架构，收集两个数据集研究数据表示和模型架构的影响。采用频谱图表示方法，开发STFT2DCNN和STT3DCNN模型，在Franka Emika Research机器人上实现。", "result": "频谱图表示显著提高识别精度（95%以上），模型架构影响较小。在机器人新姿态泛化测试中，频谱图模型表现更优。成功实现接触检测和手势分类。", "conclusion": "证明了无需外部传感器的触觉识别可行性，为HRC提供了成本效益高、可扩展的解决方案，推动了该领域的进一步研究。"}}
{"id": "2508.12439", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12439", "abs": "https://arxiv.org/abs/2508.12439", "authors": ["Sunyu Wang", "Arjun S. Lakshmipathy", "Jean Oh", "Nancy S. Pollard"], "title": "Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation", "comment": null, "summary": "Reasoning about rolling and sliding contact, or roll-slide contact for short,\nis critical for dexterous manipulation tasks that involve intricate geometries.\nBut existing works on roll-slide contact mostly focus on continuous shapes with\ndifferentiable parametrizations. This work extends roll-slide contact modeling\nto manifold meshes. Specifically, we present an integration scheme based on\ngeodesic tracing to first-order time-integrate roll-slide contact directly on\nmeshes, enabling dexterous manipulation to reason over high-fidelity discrete\nrepresentations of an object's true geometry. Using our method, we planned\ndexterous motions of a multi-finger robotic hand manipulating five objects\nin-hand in simulation. The planning was achieved with a least-squares optimizer\nthat strives to maintain the most stable instantaneous grasp by minimizing\ncontact sliding and spinning. Then, we evaluated our method against a baseline\nusing collision detection and a baseline using primitive shapes. The results\nshow that our method performed the best in accuracy and precision, even for\ncoarse meshes. We conclude with a future work discussion on incorporating\nmultiple contacts and contact forces to achieve accurate and robust mesh-based\nsurface contact modeling.", "AI": {"tldr": "扩展滚动滑动接触模型到流形网格，通过海线迹踪积分方案在网格上直接时间积分，支持高保真度的手势操控规划", "motivation": "现有滚动滑动接触研究主要集中在连续可微形状上，需要扩展到流形网格以支持更精细的真实几何表示", "method": "基于海线迹踪的积分方案，在网格上进行一阶时间积分，使用最小二乘优化器规划多指手势操控运动", "result": "在五个对象的手势操推模拟中，方法在准确性和精确度方面都超过了碰撞检测基准和基础形状基准，甚至在粗糕网格上也表现优异", "conclusion": "方法能够在网格上实现高保真度的表面接触模型，未来工作将重点研究多重接触和接触力的集成，以实现更准确和稳健的模型"}}
{"id": "2508.12456", "categories": ["cs.RO", "68T07, 93C85, 86A05", "I.2.6; I.2.9; J.2"], "pdf": "https://arxiv.org/pdf/2508.12456", "abs": "https://arxiv.org/abs/2508.12456", "authors": ["Hadas C. Kuzmenko", "David Ehevich", "Oren Gal"], "title": "Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics", "comment": "30 pages, 40 figures. Framework combining Liquid Time-Constant Neural\n  Networks with autonomous marine robotics for oil spill trajectory prediction\n  and response coordination", "summary": "Marine oil spills pose grave environmental and economic risks, threatening\nmarine ecosystems, coastlines, and dependent industries. Predicting and\nmanaging oil spill trajectories is highly complex, due to the interplay of\nphysical, chemical, and environmental factors such as wind, currents, and\ntemperature, which makes timely and effective response challenging. Accurate\nreal-time trajectory forecasting and coordinated mitigation are vital for\nminimizing the impact of these disasters. This study introduces an integrated\nframework combining a multi-agent swarm robotics system built on the MOOS-IvP\nplatform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system\nfuses adaptive machine learning with autonomous marine robotics, enabling\nreal-time prediction, dynamic tracking, and rapid response to evolving oil\nspills. By leveraging LTCNs--well-suited for modeling complex, time-dependent\nprocesses--the framework achieves real-time, high-accuracy forecasts of spill\nmovement. Swarm intelligence enables decentralized, scalable, and resilient\ndecision-making among robot agents, enhancing collective monitoring and\ncontainment efforts. Our approach was validated using data from the Deepwater\nHorizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,\nsurpassing LSTM approaches by 23%. The integration of advanced neural modeling\nwith autonomous, coordinated robotics demonstrates substantial improvements in\nprediction precision, flexibility, and operational scalability. Ultimately,\nthis research advances the state-of-the-art for sustainable, autonomous oil\nspill management and environmental protection by enhancing both trajectory\nprediction and response coordination.", "AI": {"tldr": "基于MOOS-IvP平台的多机器人群智能系统结合液体时间常数神经网络(LTCNs)，实现油漆澄渍轨迹的高精度实时预测和动态响应", "motivation": "海洋油漆澄渍造成严重环境和经济风险，需要准确的实时轨迹预测和协调减少影响，但因风、流、温度等多因素交互而极其复杂", "method": "集成多机器人群智能系统(MOOS-IvP平台)与液体时间常数神经网络(LTCNs)，结合自适应机器学习与自主海洋机器人技术", "result": "在Deepwater Horizon漆澄数据上，LTC-RK4模型达到0.96空间准确度，超过LSTM方法23%，显著提升了预测精度、灵活性和操作可扩展性", "conclusion": "该研究通过先进神经建模与自主协同机器人技术的集成，大大提高了油漆澄渍管理的预测精度和响应协调能力，推进了可持续自主环境保护技术的发展"}}
{"id": "2508.12469", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12469", "abs": "https://arxiv.org/abs/2508.12469", "authors": ["Abhinav Chalise", "Nimesh Gopal Pradhan", "Nishan Khanal", "Prashant Raj Bista", "Dinesh Baniya Kshatri"], "title": "Mechanical Automation with Vision: A Design for Rubik's Cube Solver", "comment": "Presented at the 15th IOE Graduate Conference, Tribhuvan University,\n  May 2024. Original paper available at\n  https://conference.ioe.edu.np/publications/ioegc15/IOEGC-15-023-C1-2-42.pdf", "summary": "The core mechanical system is built around three stepper motors for physical\nmanipulation, a microcontroller for hardware control, a camera and YOLO\ndetection model for real-time cube state detection. A significant software\ncomponent is the development of a user-friendly graphical user interface (GUI)\ndesigned in Unity. The initial state after detection from real-time YOLOv8\nmodel (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)\nis virtualized on GUI. To get the solution, the system employs the Kociemba's\nalgorithm while physical manipulation with a single degree of freedom is done\nby combination of stepper motors' interaction with the cube achieving the\naverage solving time of ~2.2 minutes.", "AI": {"tldr": "基于YOLOv8实时检测和Kociemba算法的麻将魔方自动解法系统，通过三个步进电机实现物理操控，平均解法时间2.2分钟", "motivation": "开发一个能够自动检测麻将魔方状态并自动解决的系统，通过统一的GUI界面实时显示魔方状态和解法过程", "method": "使用YOLOv8模型进行实时魔斶状态检测（精度0.98443，回归0.98419），通过Kociemba算法求解，使用三个步进电机组合实现单自由度的物理操控", "result": "系统成功实现了麻将魔方的自动检测和解法，平均解法时间约2.2分钟，YOLOv8模型表现优异（Box Loss 0.42051, Class Loss 0.2611）", "conclusion": "该系统成功展示了基于计算机视觉和机械控制的麻将魔方自动解法方案，具有高精度检测和较快解法速度，Unity GUI提供了便捷的用户交互体验"}}
{"id": "2508.12554", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12554", "abs": "https://arxiv.org/abs/2508.12554", "authors": ["Hamza El-Kebir"], "title": "PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions", "comment": "Accepted for presentation at the 2025 IEEE Conference on Decision and\n  Control (CDC)", "summary": "We introduce PROD (Palpative Reconstruction of Deformables), a novel method\nfor reconstructing the shape and mechanical properties of deformable objects\nusing elastostatic signed distance functions (SDFs). Unlike traditional\napproaches that rely on purely geometric or visual data, PROD integrates\npalpative interaction -- measured through force-controlled surface probing --\nto estimate both the static and dynamic response of soft materials. We model\nthe deformation of an object as an elastostatic process and derive a governing\nPoisson equation for estimating its SDF from a sparse set of pose and force\nmeasurements. By incorporating steady-state elastodynamic assumptions, we show\nthat the undeformed SDF can be recovered from deformed observations with\nprovable convergence. Our approach also enables the estimation of material\nstiffness by analyzing displacement responses to varying force inputs. We\ndemonstrate the robustness of PROD in handling pose errors, non-normal force\napplication, and curvature errors in simulated soft body interactions. These\ncapabilities make PROD a powerful tool for reconstructing deformable objects in\napplications ranging from robotic manipulation to medical imaging and haptic\nfeedback systems.", "AI": {"tldr": "PROD是一种通过触觉交互重建可变形物体形状和力学特性的新方法，使用弹性静力学SDF和力控表面探测来估计软材料的静态和动态响应", "motivation": "传统方法依赖纯几何或视觉数据，无法有效估计可变形物体的力学特性，需要一种能够整合触觉交互信息的方法", "method": "将物体变形建模为弹性静力学过程，推导控制泊松方程从稀疏位姿和力测量估计SDF，结合稳态弹性动力学假设恢复未变形SDF", "result": "PROD能够处理位姿误差、非垂直力施加和曲率误差，在模拟软体交互中表现出鲁棒性，并能通过分析位移响应估计材料刚度", "conclusion": "PROD是重建可变形物体的强大工具，适用于机器人操作、医学成像和触觉反馈系统等应用"}}
{"id": "2508.12564", "categories": ["cs.RO", "cs.CV", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.12564", "abs": "https://arxiv.org/abs/2508.12564", "authors": ["Jiayao Mai", "Xiuyuan Lu", "Kuan Dai", "Shaojie Shen", "Yi Zhou"], "title": "Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems", "comment": "8 pages, 5 figures", "summary": "Event cameras generate asynchronous signals in response to pixel-level\nbrightness changes, offering a sensing paradigm with theoretically\nmicrosecond-scale latency that can significantly enhance the performance of\nmulti-sensor systems. Extrinsic calibration is a critical prerequisite for\neffective sensor fusion; however, the configuration that involves event cameras\nremains an understudied topic. In this paper, we propose a motion-based\ntemporal and rotational calibration framework tailored for event-centric\nmulti-sensor systems, eliminating the need for dedicated calibration targets.\nOur method uses as input the rotational motion estimates obtained from event\ncameras and other heterogeneous sensors, respectively. Different from\nconventional approaches that rely on event-to-frame conversion, our method\nefficiently estimates angular velocity from normal flow observations, which are\nderived from the spatio-temporal profile of event data. The overall calibration\npipeline adopts a two-step approach: it first initializes the temporal offset\nand rotational extrinsics by exploiting kinematic correlations in the spirit of\nCanonical Correlation Analysis (CCA), and then refines both temporal and\nrotational parameters through a joint non-linear optimization using a\ncontinuous-time parametrization in SO(3). Extensive evaluations on both\npublicly available and self-collected datasets validate that the proposed\nmethod achieves calibration accuracy comparable to target-based methods, while\nexhibiting superior stability over purely CCA-based methods, and highlighting\nits precision, robustness and flexibility. To facilitate future research, our\nimplementation will be made open-source. Code:\nhttps://github.com/NAIL-HNU/EvMultiCalib.", "AI": {"tldr": "事件相机多传感器系统的无标定物时间和旋转外参检定方法，通过正常流观测估计角速度，结合CCA初始化和连续时间SO(3)优化，达到了与标定物方法相当的精度。", "motivation": "事件相机作为一种低延迟传感器，在多传感器融合中具有重要价值，但相关外参检定方法研究较少。需要一种无需专门标定物的检定方法来支持事件相机为中心的多传感器系统。", "method": "提出基于运动的时间和旋转检定框架：1)通过正常流观测估计角速度，避免事件-帧转换；2)利用动力学相关性通过CCA方法初始化时间偏移和旋转外参；3)使用连续时间SO(3)参数化进行非线性优化精细调整所有参数。", "result": "在公开和自收数据集上的广泛评估显示，该方法达到了与基于标定物方法相当的检定精度，同时比纯CCA方法更稳定，并体现了高精度、验实性和灵活性。", "conclusion": "成功开发了一种无需标定物的事件相机多传感器系统外参检定方法，通过正常流角速度估计和两步优化流程，实现了高精度和稳定性。该方法为事件相机多传感器融合提供了可靠的检定解决方案，并将开源代码以促进相关研究。"}}
{"id": "2508.12681", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12681", "abs": "https://arxiv.org/abs/2508.12681", "authors": ["Johann Licher", "Max Bartholdt", "Henrik Krauss", "Tim-Lukas Habich", "Thomas Seel", "Moritz Schappler"], "title": "Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory", "comment": "20 pages, 15 figures", "summary": "Dynamic control of soft continuum robots (SCRs) holds great potential for\nexpanding their applications, but remains a challenging problem due to the high\ncomputational demands of accurate dynamic models. While data-driven approaches\nlike Koopman-operator-based methods have been proposed, they typically lack\nadaptability and cannot capture the full robot shape, limiting their\napplicability. This work introduces a real-time-capable nonlinear\nmodel-predictive control (MPC) framework for SCRs based on a domain-decoupled\nphysics-informed neural network (DD-PINN) with adaptable bending stiffness. The\nDD-PINN serves as a surrogate for the dynamic Cosserat rod model with a\nspeed-up factor of 44000. It is also used within an unscented Kalman filter for\nestimating the model states and bending compliance from end-effector position\nmeasurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the\nGPU. In simulation, it demonstrates accurate tracking of dynamic trajectories\nand setpoint control with end-effector position errors below 3 mm (2.3% of the\nactuator's length). In real-world experiments, the controller achieves similar\naccuracy and accelerations up to 3.55 m/s2.", "AI": {"tldr": "基于域解耦物理信息神经网络(DD-PINN)的非线性模型预测控制框架，实现了软连续体机器人的高速实时动态控制，稳定性和准确性显著提升。", "motivation": "软连续体机器人(SCRs)的动态控制具有广阔应用前景，但精确动态模型的高计算要求使其面临挑战。现有的数据驱动方法缺乏适应性且无法完整描述机器人形状，限制了实际应用。", "method": "提出基于域解耦物理信息神经网络(DD-PINN)的非线性模型预测控制框架。DD-PINN作为动态Cosserat柱模型的代理模型，速度提升44000倍，并在无味卡尔滤波器中用于估计模型状态和弯曲顺度。在GPU上实现频70Hz运行的非线性进化MPC。", "result": "在模拟中，控制器展现出准确的动态轨迹跟踪能力，终端执行器位置误差低于3mm(活动器长度的2.3%)。在实际实验中，控制器达到了相似的精度，并实现了超过3.55m/s²的加速度。", "conclusion": "该框架成功解决了SCRs动态控制的计算挑战，通过DD-PINN代理模型实现了高效计算，并在实时性和控制精度方面取得了显著成效，为软连续体机器人的应用扩展提供了可靠技术支撑。"}}
{"id": "2508.12916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12916", "abs": "https://arxiv.org/abs/2508.12916", "authors": ["Hecheng Wang", "Jiankun Ren", "Jia Yu", "Lizhe Qi", "Yunquan Sun"], "title": "RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph", "comment": null, "summary": "Humans effortlessly retrieve objects in cluttered, partially observable\nenvironments by combining visual reasoning, active viewpoint adjustment, and\nphysical interaction-with only a single pair of eyes. In contrast, most\nexisting robotic systems rely on carefully positioned fixed or multi-camera\nsetups with complete scene visibility, which limits adaptability and incurs\nhigh hardware costs. We present \\textbf{RoboRetriever}, a novel framework for\nreal-world object retrieval that operates using only a \\textbf{single}\nwrist-mounted RGB-D camera and free-form natural language instructions.\nRoboRetriever grounds visual observations to build and update a \\textbf{dynamic\nhierarchical scene graph} that encodes object semantics, geometry, and\ninter-object relations over time. The supervisor module reasons over this\nmemory and task instruction to infer the target object and coordinate an\nintegrated action module combining \\textbf{active perception},\n\\textbf{interactive perception}, and \\textbf{manipulation}. To enable\ntask-aware scene-grounded active perception, we introduce a novel visual\nprompting scheme that leverages large reasoning vision-language models to\ndetermine 6-DoF camera poses aligned with the semantic task goal and geometry\nscene context. We evaluate RoboRetriever on diverse real-world object retrieval\ntasks, including scenarios with human intervention, demonstrating strong\nadaptability and robustness in cluttered scenes with only one RGB-D camera.", "AI": {"tldr": "RoboRetriever是一个仅使用单个手腕安装RGB-D相机和自然语言指令的机器人对象检索框架，通过动态层次场景图和主动感知实现真实世界的物体检索。", "motivation": "人类能够在杂乱、部分可观察的环境中轻松检索物体，而现有机器人系统通常需要多摄像头设置和完整场景可见性，限制了适应性和增加了硬件成本。", "method": "构建动态层次场景图编码对象语义、几何和关系；使用监督模块推理任务指令；集成主动感知、交互感知和操作；引入视觉提示方案利用大型视觉语言模型确定6-DoF相机位姿。", "result": "在多样化真实世界物体检索任务中表现出强大的适应性和鲁棒性，包括有人类干预的场景，仅使用单个RGB-D相机。", "conclusion": "RoboRetriever展示了仅使用单个相机和自然语言指令就能在杂乱环境中有效检索物体的能力，为机器人系统提供了更灵活和低成本的解决方案。"}}
{"id": "2508.12925", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12925", "abs": "https://arxiv.org/abs/2508.12925", "authors": ["Eetu Laukka", "Evan G. Center", "Timo Ojala", "Steven M. LaValle", "Matti Pouke"], "title": "Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users", "comment": "2025 IEEE Conference on Telepresence", "summary": "Mobile telepresence robots allow users to feel present and explore remote\nenvironments using technology. Traditionally, these systems are implemented\nusing a camera onboard a mobile robot that can be controlled. Although\nhigh-immersion technologies, such as 360-degree cameras, can increase\nsituational awareness and presence, they also introduce significant challenges.\nAdditional processing and bandwidth requirements often result in latencies of\nup to seconds. The current delay with a 360-degree camera streaming over the\ninternet makes real-time control of these systems difficult. Working with\nhigh-latency systems requires some form of assistance to the users.\n  This study presents a novel way to utilize optical flow to create an illusion\nof self-motion to the user during the latency period between user sending\nmotion commands to the robot and seeing the actual motion through the\n360-camera stream. We find no significant benefit of using the self-motion\nillusion to performance or accuracy of controlling a telepresence robot with a\nlatency of 500 ms, as measured by the task completion time and collisions into\nobjects. Some evidence is shown that the method might increase virtual reality\n(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We\nconclude that further adjustments are necessary in order to render the method\nviable.", "AI": {"tldr": "使用光流技术在500ms延迟情况下为移动传真机器人创建自我运动幻觉，但未显著提升性能或准确性，反而可能增加VR舔晕", "motivation": "解决360度摄像头流媒体影音导致的高延迟问题，提升移动传真机器人的实时控制能力", "method": "利用光流技术在用户发送运动命令到看到实际运动的延迟期间创建自我运动幻觉", "result": "在500ms延迟下未发现任务完成时间和碰撞等性能指标的显著改善，反而可能增加了VR舔晕", "conclusion": "该方法需要进一步调整才能成为可行的解决方案"}}
{"id": "2508.12928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12928", "abs": "https://arxiv.org/abs/2508.12928", "authors": ["Victor Dhédin", "Haizhou Zhao", "Majid Khadiv"], "title": "Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion", "comment": null, "summary": "Legged robots have the potential to traverse highly constrained environments\nwith agile maneuvers. However, planning such motions requires solving a highly\nchallenging optimization problem with a mixture of continuous and discrete\ndecision variables. In this paper, we present a full pipeline based on\nMonte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to\nperform simultaneous contact sequence and patch selection on highly challenging\nenvironments. Through extensive simulation experiments, we show that our\nframework can quickly find a diverse set of dynamically consistent plans. We\nexperimentally show that these plans are transferable to a real quadruped\nrobot. We further show that the same framework can find highly complex acyclic\nhumanoid maneuvers. To the best of our knowledge, this is the first\ndemonstration of simultaneous contact sequence and patch selection for acyclic\nmulti-contact locomotion using the whole-body dynamics of a quadruped.", "AI": {"tldr": "基于MCTS和全身轨迹优化的方法，通过同时解决接触序列和接触点选择问题，实现了四足和人形机器人在极端环境中的灵活移动规划", "motivation": "四足机器人在高度约束环境中进行灵活机动时，需要解决混合了连续和离散决策变量的高难度优化问题", "method": "采用蒙特卡洛树搜索(MCTS)算法结合全身轨迹优化(TO)，实现同时的接触序列和接触点选择", "result": "在高难度环境中快速找到多样化的动态一致规划，并成功将规划转移到真实四足机器人，同时支持复杂的非周期性人形机动", "conclusion": "这是首次在四足机器人全身动力学基础上实现同时接触序列和接触点选择的非周期性多接触移动规划案例"}}
{"id": "2508.12946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12946", "abs": "https://arxiv.org/abs/2508.12946", "authors": ["Ann-Sophie Schenk", "Stefan Schiffer", "Heqiu Song"], "title": "Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade", "comment": null, "summary": "In this paper we report on first insights from interviews with teachers and\nstudents on using social robots in computer science class in sixth grade. Our\nfocus is on learning about requirements and potential applications. We are\nparticularly interested in getting both perspectives, the teachers' and the\nlearners' view on how robots could be used and what features they should or\nshould not have. Results show that teachers as well as students are very open\nto robots in the classroom. However, requirements are partially quite\nheterogeneous among the groups. This leads to complex design challenges which\nwe discuss at the end of this paper.", "AI": {"tldr": "对六年级计算机科学课堂使用社交机器人的教师和学生访谈初步研究，探讨需求和应用可能性，发现双方都持开放态度但需求存在差异", "motivation": "了解教师和学习者对在计算机科学课堂中使用社交机器人的需求和潜在应用，特别关注两个群体对机器人使用方式和功能需求的不同视角", "method": "通过对教师和学生进行访谈，收集关于社交机器人在六年级计算机科学课堂中应用的看法和需求", "result": "教师和学生都对在课堂中使用机器人持非常开放的态度，但两个群体的需求存在部分异质性", "conclusion": "研究揭示了复杂的设计挑战，需要在机器人设计中平衡教师和学生的不同需求"}}
{"id": "2508.12980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12980", "abs": "https://arxiv.org/abs/2508.12980", "authors": ["Victor Levé", "João Moura", "Sachiya Fujita", "Tamon Miyake", "Steve Tonneau", "Sethu Vijayakumar"], "title": "Scaling Whole-body Multi-contact Manipulation with Contact Optimization", "comment": "This work has been accepted for publication in IEEE-RAS 24th\n  International Conference on Humanoid Robots (Humanoids 2025). Copyrights to\n  IEEE", "summary": "Daily tasks require us to use our whole body to manipulate objects, for\ninstance when our hands are unavailable. We consider the issue of providing\nhumanoid robots with the ability to autonomously perform similar whole-body\nmanipulation tasks. In this context, the infinite possibilities for where and\nhow contact can occur on the robot and object surfaces hinder the scalability\nof existing planning methods, which predominantly rely on discrete sampling.\nGiven the continuous nature of contact surfaces, gradient-based optimization\noffers a more suitable approach for finding solutions. However, a key remaining\nchallenge is the lack of an efficient representation of robot surfaces. In this\nwork, we propose (i) a representation of robot and object surfaces that enables\nclosed-form computation of proximity points, and (ii) a cost design that\neffectively guides whole-body manipulation planning. Our experiments\ndemonstrate that the proposed framework can solve problems unaddressed by\nexisting methods, and achieves a 77% improvement in planning time over the\nstate of the art. We also validate the suitability of our approach on real\nhardware through the whole-body manipulation of boxes by a humanoid robot.", "AI": {"tldr": "提出了一种新的表面表示方法和成本设计，用于人形机器人的全身操纵规划，解决了现有方法在连续接触表面处理上的挑战", "motivation": "人类日常任务需要使用全身操纵物体，但现有的人形机器人规划方法主要依赖离散采样，在处理连续接触表面时程度性不佳", "method": "提出(i)一种能够进行闭式计算近似点的机器人和物体表面表示方法，(ii)一种有效指导全身操纵规划的成本设计", "result": "实验结果显示该框架能解决现有方法无法处理的问题，并比最先进方法规划时间提高了77%，在真实硬件上通过人形机器人操纵箱子进行了验证", "conclusion": "该研究提供了一种高效的全身操纵规划解决方案，通过新的表面表示和成本设计，有效解决了连续接触表面处理的挑战"}}
{"id": "2508.13052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13052", "abs": "https://arxiv.org/abs/2508.13052", "authors": ["Sourav Raxit", "Abdullah Al Redwan Newaz", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla"], "title": "BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments", "comment": null, "summary": "This paper introduces the BOW Planner, a scalable motion planning algorithm\ndesigned to navigate robots through complex environments using constrained\nBayesian optimization (CBO). Unlike traditional methods, which often struggle\nwith kinodynamic constraints such as velocity and acceleration limits, the BOW\nPlanner excels by concentrating on a planning window of reachable velocities\nand employing CBO to sample control inputs efficiently. This approach enables\nthe planner to manage high-dimensional objective functions and stringent safety\nconstraints with minimal sampling, ensuring rapid and secure trajectory\ngeneration. Theoretical analysis confirms the algorithm's asymptotic\nconvergence to near-optimal solutions, while extensive evaluations in cluttered\nand constrained settings reveal substantial improvements in computation times,\ntrajectory lengths, and solution times compared to existing techniques.\nSuccessfully deployed across various real-world robotic systems, the BOW\nPlanner demonstrates its practical significance through exceptional sample\nefficiency, safety-aware optimization, and rapid planning capabilities, making\nit a valuable tool for advancing robotic applications. The BOW Planner is\nreleased as an open-source package and videos of real-world and simulated\nexperiments are available at https://bow-web.github.io.", "AI": {"tldr": "BOW规划器是一种基于约束贝叶斯优化的可扩展运动规划算法，通过高效采样控制输入来处理动力学约束和安全要求，实现了迅速、安全的轨迹生成。", "motivation": "传统运动规划方法在处理速度、加速度等动力学约束时遇到困难，需要一种能够高效处理高维目标函数和严格安全约束的新方法。", "method": "算法采用约束贝叶斯优化(CBO)技术，通过聚焦于可到达速度的规划窗口来高效采样控制输入，以最少的采样数量管理复杂约束。", "result": "理论分析证明算法进行近优解的趋势收敛，在杂乱和约束环境中的评估显示在计算时间、轨迹长度和解决时间方面都有显著改善，实际部署也证明了其高样本效率、安全优化和快速规划能力。", "conclusion": "BOW规划器作为一种高效、安全的运动规划工具，对推动机器人应用发展具有重要价值，已作为开源包发布。"}}
{"id": "2508.13073", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13073", "abs": "https://arxiv.org/abs/2508.13073", "authors": ["Rui Shao", "Wei Li", "Lingsen Zhang", "Renshan Zhang", "Zhiyang Liu", "Ran Chen", "Liqiang Nie"], "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey", "comment": "Project Page:\n  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation", "summary": "Robotic manipulation, a key frontier in robotics and embodied AI, requires\nprecise motor control and multimodal understanding, yet traditional rule-based\nmethods fail to scale or generalize in unstructured, novel environments. In\nrecent years, Vision-Language-Action (VLA) models, built upon Large\nVision-Language Models (VLMs) pretrained on vast image-text datasets, have\nemerged as a transformative paradigm. This survey provides the first\nsystematic, taxonomy-oriented review of large VLM-based VLA models for robotic\nmanipulation. We begin by clearly defining large VLM-based VLA models and\ndelineating two principal architectural paradigms: (1) monolithic models,\nencompassing single-system and dual-system designs with differing levels of\nintegration; and (2) hierarchical models, which explicitly decouple planning\nfrom execution via interpretable intermediate representations. Building on this\nfoundation, we present an in-depth examination of large VLM-based VLA models:\n(1) integration with advanced domains, including reinforcement learning,\ntraining-free optimization, learning from human videos, and world model\nintegration; (2) synthesis of distinctive characteristics, consolidating\narchitectural traits, operational strengths, and the datasets and benchmarks\nthat support their development; (3) identification of promising directions,\nincluding memory mechanisms, 4D perception, efficient adaptation, multi-agent\ncooperation, and other emerging capabilities. This survey consolidates recent\nadvances to resolve inconsistencies in existing taxonomies, mitigate research\nfragmentation, and fill a critical gap through the systematic integration of\nstudies at the intersection of large VLMs and robotic manipulation. We provide\na regularly updated project page to document ongoing progress:\nhttps://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.", "AI": {"tldr": "这是一份关于基于大型视觉-语言模型的视觉-语言-动作模型在机器人操纵领域的系统性调研报告，包含架构分析、领域整合、特征综述和未来方向的全面评论。", "motivation": "传统的规则基础方法无法在非结构化新环境中扩展或泛化，而基于大型视觉-语言模型的VLA模型成为了变革性的解决方案。本调研旨在系统整理该领域的研究进展，解决现有分类不一致和研究分散问题。", "method": "采用系统化的分类学方法，首先定义大型VLM基础的VLA模型，并划分为两种主要架构范式：单一系统模型（包含不同集成程度的单系统和双系统设计）和层次模型（通过可解释的中间表示显式解耦规划与执行）。进行深入分析包括与各领域的整合、特征综合和未来方向识别。", "result": "本调研提供了对大型VLM基础VLA模型的全面评估，包括它们的架构特征、运作优势以及支持其开发的数据集和测试标准。完整地综合了大型VLMs与机器人操纵交叉领域的研究成果，填补了该领域的关键空白。", "conclusion": "大型VLM基础的VLA模型为机器人操纵领域带来了变革性的可能性，能够在复杂非结构化环境中实现更好的泛化能力。本调研为该领域提供了统一的分类框架和系统化的知识整理，并指明了包括内存机制、4D感知、高效适应、多代理合作等在内的未来研究方向。"}}
{"id": "2508.13103", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13103", "abs": "https://arxiv.org/abs/2508.13103", "authors": ["Tianyi Zhang", "Haonan Duan", "Haoran Hao", "Yu Qiao", "Jifeng Dai", "Zhi Hou"], "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy", "comment": null, "summary": "Vision-Language-Action (VLA) models frequently encounter challenges in\ngeneralizing to real-world environments due to inherent discrepancies between\nobservation and action spaces. Although training data are collected from\ndiverse camera perspectives, the models typically predict end-effector poses\nwithin the robot base coordinate frame, resulting in spatial inconsistencies.\nTo mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)\nframework, which grounds action predictions directly in the camera observation\nspace. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms\nend-effector poses from the robot base coordinate system into the camera\ncoordinate system, thereby unifying prediction targets across heterogeneous\nviewpoints. This lightweight, plug-and-play strategy ensures robust alignment\nbetween perception and action, substantially improving model resilience to\ncamera viewpoint variations. The proposed approach is readily compatible with\nexisting VLA architectures, requiring no substantial modifications.\nComprehensive evaluations on both simulated and real-world robotic manipulation\ntasks demonstrate that OC-VLA accelerates convergence, enhances task success\nrates, and improves cross-view generalization. The code will be publicly\navailable.", "AI": {"tldr": "OC-VLA框架通过将动作预测直接建立在相机观测空间中，解决了VLA模型在观察和动作空间不一致的问题，显著提高了模型对相机视角变化的鲁棒性。", "motivation": "VLA模型在处理真实世界环境时面临观察空间和动作空间不一致的挑战，特别是在不同相机视角下预测末端执行器位姿时存在空间不一致问题。", "method": "利用相机外参标定矩阵，将末端执行器位姿从机器人基坐标系转换到相机坐标系，在观测空间中统一预测目标，这是一个轻量级的即插即用策略。", "result": "在仿真和真实世界机器人操作任务上的综合评估表明，OC-VLA加速了收敛速度，提高了任务成功率，并改善了跨视角泛化能力。", "conclusion": "OC-VLA框架能够有效统一感知和动作之间的对齐，提高模型对相机视角变化的鲁棒性，且与现有VLA架构兼容，无需重大修改。"}}
