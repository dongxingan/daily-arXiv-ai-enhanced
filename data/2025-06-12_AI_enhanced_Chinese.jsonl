{"id": "2506.09098", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09098", "abs": "https://arxiv.org/abs/2506.09098", "authors": ["Yangjie Cui", "Boyang Gao", "Yiwei Zhang", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras", "comment": "https://youtu.be/AQAgVdrx1DE", "summary": "Previous studies on event camera sensing have demonstrated certain detection\nperformance using dense event representations. However, the accumulated noise\nin such dense representations has received insufficient attention, which\ndegrades the representation quality and increases the likelihood of missed\ndetections. To address this challenge, we propose the Wavelet\nDenoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event\ncameras. In particular, a dense event representation is presented first, which\nenables real-time reconstruction of events as tensors. Then, a wavelet\ntransform method is designed to filter noise in the event representations. Such\na method is integrated into the backbone for feature extraction. The extracted\nfeatures are subsequently fed into a transformer-based network for object\nprediction. To further reduce inference time, we incorporate the Dynamic\nReorganization Convolution Block (DRCB) as a fusion module within the hybrid\nencoder. The proposed method has been evaluated on three event-based object\ndetection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that\nWD-DETR outperforms tested state-of-the-art methods. Additionally, we implement\nour approach on a common onboard computer for robots, the NVIDIA Jetson Orin\nNX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,\nwhich is exceptionally well-suited for real-time perception of onboard robotic\nsystems.", "AI": {"tldr": "论文提出了一种基于小波去噪的检测Transformer网络（WD-DETR），用于解决事件相机中密集事件表示中的噪声问题，提升检测性能。", "motivation": "密集事件表示中的噪声问题降低了表示质量并增加了漏检概率，现有研究对此关注不足。", "method": "提出WD-DETR网络，包括密集事件表示、小波去噪方法、Transformer-based网络和动态重组卷积块（DRCB）。", "result": "在DSEC、Gen1和1Mpx数据集上表现优于现有方法，并在NVIDIA Jetson Orin NX上实现35 FPS的高帧率。", "conclusion": "WD-DETR有效解决了事件相机中的噪声问题，适用于实时机器人感知。"}}
{"id": "2506.09169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09169", "abs": "https://arxiv.org/abs/2506.09169", "authors": ["Yuemin Mao", "Bardienus P. Duisterhof", "Moonyoung Lee", "Jeffrey Ichnowski"], "title": "Hearing the Slide: Acoustic-Guided Constraint Learning for Fast Non-Prehensile Transport", "comment": null, "summary": "Object transport tasks are fundamental in robotic automation, emphasizing the\nimportance of efficient and secure methods for moving objects. Non-prehensile\ntransport can significantly improve transport efficiency, as it enables\nhandling multiple objects simultaneously and accommodating objects unsuitable\nfor parallel-jaw or suction grasps. Existing approaches incorporate constraints\nbased on the Coulomb friction model, which is imprecise during fast motions\nwhere inherent mechanical vibrations occur. Imprecise constraints can cause\ntransported objects to slide or even fall off the tray. To address this\nlimitation, we propose a novel method to learn a friction model using acoustic\nsensing that maps a tray's motion profile to a dynamically conditioned friction\ncoefficient. This learned model enables an optimization-based motion planner to\nadjust the friction constraint at each control step according to the planned\nmotion at that step. In experiments, we generate time-optimized trajectories\nfor a UR5e robot to transport various objects with constraints using both the\nstandard Coulomb friction model and the learned friction model. Results suggest\nthat the learned friction model reduces object displacement by up to 86.0%\ncompared to the baseline, highlighting the effectiveness of acoustic sensing in\nlearning real-world friction constraints.", "AI": {"tldr": "论文提出了一种基于声学传感学习摩擦模型的新方法，用于优化非抓取式物体运输任务，显著减少物体位移。", "motivation": "现有基于库仑摩擦模型的约束在快速运动中不精确，导致物体滑动或掉落，需要更精确的摩擦约束方法。", "method": "通过声学传感学习动态摩擦系数，结合优化运动规划器调整摩擦约束。", "result": "实验表明，学习模型比库仑模型减少物体位移高达86.0%。", "conclusion": "声学传感能有效学习真实摩擦约束，提升运输效率与安全性。"}}
{"id": "2506.09182", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.09182", "abs": "https://arxiv.org/abs/2506.09182", "authors": ["Hang Zhou", "Chengyuan Ma", "Shiyu Shen", "Xiaopeng Li"], "title": "Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method", "comment": "NA", "summary": "With the rapid development of automated vehicles (AVs) in recent years,\ncommercially available AVs are increasingly demonstrating high-level automation\ncapabilities. However, most existing AV safety evaluation methods are primarily\ndesigned for simple maneuvers such as car-following and lane-changing. While\nsuitable for basic tests, these methods are insufficient for assessing\nhigh-level automation functions deployed in more complex environments. First,\nthese methods typically use crash rate as the evaluation metric, whose accuracy\nheavily depends on the quality and completeness of naturalistic driving\nenvironment data used to estimate scenario probabilities. Such data is often\ndifficult and expensive to collect. Second, when applied to diverse scenarios,\nthese methods suffer from the curse of dimensionality, making large-scale\nevaluation computationally intractable. To address these challenges, this paper\nproposes a novel framework for full-scenario AV safety evaluation. A unified\nmodel is first introduced to standardize the representation of diverse driving\nscenarios. This modeling approach constrains the dimension of most scenarios to\na regular highway setting with three lanes and six surrounding background\nvehicles, significantly reducing dimensionality. To further avoid the\nlimitations of probability-based method, we propose a volume-based evaluation\nmethod that quantifies the proportion of risky scenarios within the entire\nscenario space. For car-following scenarios, we prove that the set of safe\nscenarios is convex under specific settings, enabling exact volume computation.\nExperimental results validate the effectiveness of the proposed volume-based\nmethod using both AV behavior models from existing literature and six\nproduction AV models calibrated from field-test trajectory data in the Ultra-AV\ndataset. Code and data will be made publicly available upon acceptance of this\npaper.", "AI": {"tldr": "本文提出了一种新的全场景自动驾驶车辆（AV）安全评估框架，解决了现有方法在复杂环境中的不足。", "motivation": "现有AV安全评估方法主要针对简单操作（如跟车和变道），难以评估复杂环境中的高级自动化功能，且依赖高质量自然驾驶数据，计算复杂度高。", "method": "提出统一模型标准化多样化驾驶场景表示，并采用基于体积的评估方法量化风险场景比例，避免概率方法的局限性。", "result": "实验验证了基于体积的方法的有效性，使用现有文献中的AV行为模型和实际测试数据校准的模型。", "conclusion": "新框架显著降低了维度问题，为AV安全评估提供了更高效和可扩展的解决方案。"}}
{"id": "2506.09217", "categories": ["cs.RO", "cs.CV", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.09217", "abs": "https://arxiv.org/abs/2506.09217", "authors": ["Boyu Jiang", "Liang Shi", "Zhengzhi Lin", "Loren Stowe", "Feng Guo"], "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule", "comment": null, "summary": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python.", "AI": {"tldr": "论文提出了一种新的感知评估指标PCD，用于量化自动驾驶系统中对象检测的最远可靠距离，并结合了模型输出的不确定性。同时发布了SensorRainFall数据集，用于评估不同天气条件下的感知性能。", "motivation": "自动驾驶感知系统的性能受对象距离、场景动态和天气等环境因素影响，传统静态评估指标无法捕捉置信度的动态变化。", "method": "引入PCD指标，结合SensorRainFall数据集（包含晴天和雨天场景的精确数据），通过统计分析方法量化检测置信度的变化。", "result": "PCD能够捕捉不同天气条件下感知系统的可靠性差异，而传统静态指标无法做到。", "conclusion": "PCD为感知性能提供了分布感知的评估方法，支持更安全的自动驾驶系统运行，SensorRainFall数据集为评估提供了基准。"}}
{"id": "2506.09383", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.09383", "abs": "https://arxiv.org/abs/2506.09383", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "comment": null, "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "AI": {"tldr": "论文提出了一种分层控制管道，通过全身肌肉骨骼系统模拟人类平衡，揭示了肌肉损伤对平衡行为的影响，并验证了髋部外骨骼辅助对平衡的改善效果。", "motivation": "动态平衡研究较多，但静态平衡和跌倒的定量理解有限，需要更深入的肌肉层面分析。", "method": "采用分层控制管道和全身肌肉骨骼系统模拟人类平衡，分析稳定站立时的时空动态及肌肉损伤影响。", "result": "揭示了平衡行为的时空动态，模拟结果与临床数据一致，外骨骼辅助可改善平衡并减少肌肉努力。", "conclusion": "研究为平衡障碍的干预措施和人形机器人系统的发展提供了肌肉层面的理论基础。"}}
{"id": "2506.09284", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09284", "abs": "https://arxiv.org/abs/2506.09284", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "comment": null, "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "AI": {"tldr": "UAD是一种无需人工标注的方法，通过利用基础模型从视觉和语言模型中提取知识，自动生成大规模任务条件化的视觉功能数据集，并在仿真环境中训练轻量级解码器，展现出对真实场景和人类活动的泛化能力。", "motivation": "现有视觉功能预测方法依赖人工标注或预定义任务集，限制了在开放任务指令下的机器人操作能力。", "method": "UAD利用大型视觉模型和视觉语言模型的互补优势，自动标注大规模数据集，并训练任务条件化的轻量级解码器。", "result": "UAD在仿真环境中训练的模型能够泛化到真实机器人场景和人类活动，且模仿学习策略在少量演示后表现出对未见对象和任务指令的泛化能力。", "conclusion": "UAD提供了一种无需人工标注的高效方法，显著提升了机器人对物体功能的泛化能力。"}}
{"id": "2506.09876", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.09876", "abs": "https://arxiv.org/abs/2506.09876", "authors": ["Jisheng Xu", "Ding Lin", "Pangkit Fong", "Chongrong Fang", "Xiaoming Duan", "Jianping He"], "title": "Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization", "comment": null, "summary": "This paper introduces an underwater multi-robot platform, named Aucamp,\ncharacterized by cost-effective monocular-camera-based sensing, distributed\nprotocol and robust orientation control for localization. We utilize the\nclarity feature to measure the distance, present the monocular imaging model,\nand estimate the position of the target object. We achieve global positioning\nin our platform by designing a distributed update protocol. The distributed\nalgorithm enables the perception process to simultaneously cover a broader\nrange, and greatly improves the accuracy and robustness of the positioning.\nMoreover, the explicit dynamics model of the robot in our platform is obtained,\nbased on which, we propose a robust orientation control framework. The control\nsystem ensures that the platform maintains a balanced posture for each robot,\nthereby ensuring the stability of the localization system. The platform can\nswiftly recover from an forced unstable state to a stable horizontal posture.\nAdditionally, we conduct extensive experiments and application scenarios to\nevaluate the performance of our platform. The proposed new platform may provide\nsupport for extensive marine exploration by underwater sensor networks.", "AI": {"tldr": "本文介绍了一种名为Aucamp的水下多机器人平台，具有低成本单目摄像头感知、分布式协议和鲁棒的方向控制功能，用于定位。", "motivation": "为水下传感器网络提供广泛的海洋探索支持。", "method": "利用清晰度特征测量距离，提出单目成像模型，设计分布式更新协议实现全局定位，并提出鲁棒的方向控制框架。", "result": "平台能够覆盖更广范围，显著提高定位精度和鲁棒性，并能快速从不稳定状态恢复。", "conclusion": "该平台为水下传感器网络的海洋探索提供了新的支持。"}}
{"id": "2506.09366", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09366", "abs": "https://arxiv.org/abs/2506.09366", "authors": ["Yuxuan Kuang", "Haoran Geng", "Amine Elhafsi", "Tan-Dzung Do", "Pieter Abbeel", "Jitendra Malik", "Marco Pavone", "Yue Wang"], "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending", "comment": null, "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.", "AI": {"tldr": "SkillBlender是一种新颖的分层强化学习框架，通过预训练任务无关的原始技能并动态混合这些技能，实现了多功能人形机器人的运动与操作任务。", "motivation": "现有方法需要针对每个任务进行繁琐的调整，限制了其多功能性和可扩展性。SkillBlender旨在减少任务特定的奖励工程，提升机器人在日常场景中的适应性。", "method": "SkillBlender预训练目标条件的任务无关原始技能，并动态混合这些技能以完成复杂的运动与操作任务。", "result": "实验表明，SkillBlender显著优于基线方法，并避免了奖励滥用，实现了更准确和可行的运动。", "conclusion": "SkillBlender为多功能人形机器人的运动与操作任务提供了一种高效且可扩展的解决方案，其代码和基准将开源。"}}
{"id": "2506.09384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09384", "abs": "https://arxiv.org/abs/2506.09384", "authors": ["Chendong Xin", "Mingrui Yu", "Yongpeng Jiang", "Zhefeng Zhang", "Xiang Li"], "title": "Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation", "comment": null, "summary": "Kinematic retargeting from human hands to robot hands is essential for\ntransferring dexterity from humans to robots in manipulation teleoperation and\nimitation learning. However, due to mechanical differences between human and\nrobot hands, completely reproducing human motions on robot hands is impossible.\nExisting works on retargeting incorporate various optimization objectives,\nfocusing on different aspects of hand configuration. However, the lack of\nexperimental comparative studies leaves the significance and effectiveness of\nthese objectives unclear. This work aims to analyze these retargeting\nobjectives for dexterous manipulation through extensive real-world comparative\nexperiments. Specifically, we propose a comprehensive retargeting objective\nformulation that integrates intuitively crucial factors appearing in recent\napproaches. The significance of each factor is evaluated through experimental\nablation studies on the full objective in kinematic posture retargeting and\nreal-world teleoperated manipulation tasks. Experimental results and\nconclusions provide valuable insights for designing more accurate and effective\nretargeting algorithms for real-world dexterous manipulation.", "AI": {"tldr": "该论文研究了从人手到机器人手的运动重定向问题，通过实验比较分析了不同优化目标的有效性，并提出了一种综合重定向目标公式。", "motivation": "由于人手与机器人手之间的机械差异，完全复现人类动作是不可能的。现有研究缺乏对不同优化目标的实验比较，因此这些目标的重要性和有效性尚不明确。", "method": "提出了一种综合重定向目标公式，并通过实验消融研究评估了每个因素的重要性。", "result": "实验结果表明，综合目标公式在运动姿态重定向和实际遥操作任务中表现优异。", "conclusion": "研究结果为设计更准确有效的重定向算法提供了有价值的见解。"}}
{"id": "2506.09406", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09406", "abs": "https://arxiv.org/abs/2506.09406", "authors": ["Minji Kang", "Chanwoo Baek", "Yoonsang Lee"], "title": "Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems", "comment": null, "summary": "Quadruped robots have made significant advances in locomotion, extending\ntheir capabilities from controlled environments to real-world applications.\nBeyond movement, recent work has explored loco-manipulation using the legs to\nperform tasks such as pressing buttons or opening doors. While these efforts\ndemonstrate the feasibility of leg-based manipulation, most have focused on\nrelatively static tasks. In this work, we propose a framework that enables\nquadruped robots to collect objects without additional actuators by leveraging\nthe agility of their legs. By attaching a simple scoop-like add-on to one leg,\nthe robot can scoop objects and toss them into a collection tray mounted on its\nback. Our method employs a hierarchical policy structure comprising two expert\npolicies-one for scooping and tossing, and one for approaching object\npositions-and a meta-policy that dynamically switches between them. The expert\npolicies are trained separately, followed by meta-policy training for\ncoordinated multi-object collection. This approach demonstrates how quadruped\nlegs can be effectively utilized for dynamic object manipulation, expanding\ntheir role beyond locomotion.", "AI": {"tldr": "提出了一种四足机器人利用腿部敏捷性收集物体的框架，无需额外执行器。通过简单的铲状附件和分层策略结构，实现了动态物体操作。", "motivation": "探索四足机器人腿部在动态物体操作中的潜力，超越传统静态任务。", "method": "采用分层策略结构，包括铲取和投掷的专家策略、接近物体位置的专家策略，以及动态切换的元策略。", "result": "展示了四足机器人腿部在动态物体收集中的有效性，扩展了其功能。", "conclusion": "证明了四足机器人腿部可用于动态操作，为未来应用提供了新方向。"}}
{"id": "2506.09422", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09422", "abs": "https://arxiv.org/abs/2506.09422", "authors": ["Ye Niu", "Sanping Zhou", "Yizhe Li", "Ye Den", "Le Wang"], "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation", "comment": null, "summary": "In many complex scenarios, robotic manipulation relies on generative models\nto estimate the distribution of multiple successful actions. As the diffusion\nmodel has better training robustness than other generative models, it performs\nwell in imitation learning through successful robot demonstrations. However,\nthe diffusion-based policy methods typically require significant time to\niteratively denoise robot actions, which hinders real-time responses in robotic\nmanipulation. Moreover, existing diffusion policies model a time-varying action\ndenoising process, whose temporal complexity increases the difficulty of model\ntraining and leads to suboptimal action accuracy. To generate robot actions\nefficiently and accurately, we present the Time-Unified Diffusion Policy\n(TUDP), which utilizes action recognition capabilities to build a time-unified\ndenoising process. On the one hand, we build a time-unified velocity field in\naction space with additional action discrimination information. By unifying all\ntimesteps of action denoising, our velocity field reduces the difficulty of\npolicy learning and speeds up action generation. On the other hand, we propose\nan action-wise training method, which introduces an action discrimination\nbranch to supply additional action discrimination information. Through\naction-wise training, the TUDP implicitly learns the ability to discern\nsuccessful actions to better denoising accuracy. Our method achieves\nstate-of-the-art performance on RLBench with the highest success rate of 82.6%\non a multi-view setup and 83.8% on a single-view setup. In particular, when\nusing fewer denoising iterations, TUDP achieves a more significant improvement\nin success rate. Additionally, TUDP can produce accurate actions for a wide\nrange of real-world tasks.", "AI": {"tldr": "提出了时间统一的扩散策略（TUDP），通过动作识别能力构建时间统一的去噪过程，提高了机器人动作生成的效率和准确性。", "motivation": "扩散模型在机器人模仿学习中表现良好，但现有方法因时间变化的去噪过程导致训练复杂且动作精度不足，且生成速度慢，难以实时响应。", "method": "构建时间统一的速度场，减少策略学习难度；提出动作级训练方法，引入动作判别分支以提高去噪精度。", "result": "在RLBench上达到最高成功率（多视图82.6%，单视图83.8%），且在较少去噪迭代时表现更优。", "conclusion": "TUDP能高效生成准确动作，适用于广泛的实际任务。"}}
{"id": "2506.09444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09444", "abs": "https://arxiv.org/abs/2506.09444", "authors": ["Paul Tucan", "Nadim Al Hajjar", "Calin Vaida", "Alexandru Pusca", "Tiberiu Antal", "Corina Radu", "Daniel Jucan", "Adrian Pisla", "Damien Chablat", "Doina Pisla"], "title": "Design of an innovative robotic surgical instrument for circular stapling", "comment": null, "summary": "Esophageal cancer remains a highly aggressive malignancy with low survival\nrates, requiring advanced surgical interventions like esophagectomy.\nTraditional manual techniques, including circular staplers, face challenges\nsuch as limited precision, prolonged recovery times, and complications like\nleaks and tissue misalignment. This paper presents a novel robotic circular\nstapler designed to enhance the dexterity in confined spaces, improve tissue\nalignment, and reduce post-operative risks. Integrated with a cognitive robot\nthat serves as a surgeon's assistant, the surgical stapler uses three actuators\nto perform anvil motion, cutter/stapler motion and allows a 75-degree bending\nof the cartridge (distal tip). Kinematic analysis is used to compute the\nstapler tip's position, ensuring synchronization with a robotic system.", "AI": {"tldr": "本文提出了一种新型机器人圆形吻合器，旨在提高食管癌手术的精确性，减少术后风险。", "motivation": "传统手动吻合器在食管癌手术中存在精度不足、恢复时间长及并发症多的问题。", "method": "设计了一种集成认知机器人的吻合器，通过三个执行器实现运动，并支持75度弯曲，结合运动学分析确保同步。", "result": "该吻合器提升了在狭窄空间的操作灵活性，改善了组织对齐。", "conclusion": "新型机器人吻合器有望优化食管癌手术效果，降低术后风险。"}}
{"id": "2506.09485", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.09485", "abs": "https://arxiv.org/abs/2506.09485", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "comment": null, "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "AI": {"tldr": "Adv-BMT框架通过双向运动变换器生成多样且真实的对抗性交互，解决了自动驾驶测试中长尾安全关键场景稀缺的问题。", "motivation": "现有数据集缺乏长尾安全关键场景，限制了自动驾驶系统的测试效果。", "method": "提出Adv-BMT框架，包含双向运动变换器（BMT）进行逆时序交通运动预测，无需碰撞数据预训练，分两阶段生成对抗性交互。", "result": "实验表明，Adv-BMT生成的碰撞场景质量高，训练后碰撞率降低20%。", "conclusion": "Adv-BMT能有效生成多样且真实的碰撞场景，提升自动驾驶系统测试效果。"}}
{"id": "2506.09491", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09491", "abs": "https://arxiv.org/abs/2506.09491", "authors": ["Guanghu Xie", "Zhiduo Jiang", "Yonglong Zhang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects", "comment": null, "summary": "Transparent and reflective objects in everyday environments pose significant\nchallenges for depth sensors due to their unique visual properties, such as\nspecular reflections and light transmission. These characteristics often lead\nto incomplete or inaccurate depth estimation, which severely impacts downstream\ngeometry-based vision tasks, including object recognition, scene\nreconstruction, and robotic manipulation. To address the issue of missing depth\ninformation in transparent and reflective objects, we propose DCIRNet, a novel\nmultimodal depth completion network that effectively integrates RGB images and\ndepth maps to enhance depth estimation quality. Our approach incorporates an\ninnovative multimodal feature fusion module designed to extract complementary\ninformation between RGB images and incomplete depth maps. Furthermore, we\nintroduce a multi-stage supervision and depth refinement strategy that\nprogressively improves depth completion and effectively mitigates the issue of\nblurred object boundaries. We integrate our depth completion model into\ndexterous grasping frameworks and achieve a $44\\%$ improvement in the grasp\nsuccess rate for transparent and reflective objects. We conduct extensive\nexperiments on public datasets, where DCIRNet demonstrates superior\nperformance. The experimental results validate the effectiveness of our\napproach and confirm its strong generalization capability across various\ntransparent and reflective objects.", "AI": {"tldr": "DCIRNet是一种新型多模态深度补全网络，通过融合RGB图像和深度图提升透明和反射物体的深度估计质量，显著提高了抓取成功率。", "motivation": "透明和反射物体的独特视觉特性（如镜面反射和光线透射）导致深度传感器估计不完整或不准确，影响下游视觉任务。", "method": "提出DCIRNet，结合RGB图像和深度图，采用多模态特征融合模块和多阶段监督与深度细化策略。", "result": "在公开数据集上表现优异，抓取成功率提升44%，验证了方法的有效性和泛化能力。", "conclusion": "DCIRNet能有效解决透明和反射物体的深度估计问题，提升下游任务性能。"}}
{"id": "2506.09494", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09494", "abs": "https://arxiv.org/abs/2506.09494", "authors": ["Alberto San-Miguel-Tello", "Gennaro Scarati", "Alejandro Hernández", "Mario Cavero-Vidal", "Aakash Maroti", "Néstor García"], "title": "Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications", "comment": "7 pages, 2 figures", "summary": "This paper presents advances on the Universal Manipulation Interface (UMI), a\nlow-cost hand-held gripper for robot Learning from Demonstration (LfD), for\ncomplex in-the-wild scenarios found in agricultural settings. The focus is on\nimproving the acquisition of suitable samples with minimal additional setup.\nFirstly, idle times and user's cognitive load are reduced through the\nextraction of individual samples from a continuous demonstration considering\ntask events. Secondly, reliability on the generation of task sample's\ntrajectories is increased through the combination on-board inertial\nmeasurements and external visual marker localization usage using Extended\nKalman Filtering (EKF). Results are presented for a fruit harvesting task,\noutperforming the default pipeline.", "AI": {"tldr": "本文介绍了通用操作接口（UMI）的改进，用于农业场景中的机器人学习演示（LfD），重点是通过减少空闲时间和用户认知负荷，结合惯性测量和视觉标记定位，提高了任务样本采集的可靠性。", "motivation": "解决农业场景中复杂任务样本采集的挑战，减少用户负担并提高数据可靠性。", "method": "通过任务事件提取连续演示中的样本，结合惯性测量和视觉标记定位（使用EKF）生成轨迹。", "result": "在水果采摘任务中表现优于默认流程。", "conclusion": "UMI改进方法有效提升了复杂农业任务中的样本采集效率和可靠性。"}}
{"id": "2506.09548", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09548", "abs": "https://arxiv.org/abs/2506.09548", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "comment": "Robotics and Automation Letters", "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "AI": {"tldr": "提出了一种紧耦合的LiDAR-IMU-腿里程计方法，通过在线学习腿部运动学模型和不确定性估计，在特征缺失和可变形地形等挑战性环境中表现优异。", "motivation": "解决在特征缺失环境和可变形地形中机器人里程计的鲁棒性问题。", "method": "开发了基于神经网络的腿部运动学模型，结合触觉信息（脚部反作用力），并通过在线训练增强适应性。在统一因子图上联合优化运动学模型和里程计估计。", "result": "在沙地和校园等挑战性环境中，该方法优于现有技术。", "conclusion": "提出的方法在复杂环境中显著提升了里程计的鲁棒性和准确性。"}}
{"id": "2506.09552", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09552", "abs": "https://arxiv.org/abs/2506.09552", "authors": ["Fatemeh Mohammadi Amin", "Darwin G. Caldwell", "Hans Wernher van de Venn"], "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments", "comment": "Preprint, Journal of Intelligent & Robotic Systems", "summary": "The robust interpretation of 3D environments is crucial for human-robot\ncollaboration (HRC) applications, where safety and operational efficiency are\nparamount. Semantic segmentation plays a key role in this context by enabling a\nprecise and detailed understanding of the environment. Considering the intense\ndata hunger for real-world industrial annotated data essential for effective\nsemantic segmentation, this paper introduces a pioneering approach in the\nSim2Real domain adaptation for semantic segmentation of 3D point cloud data,\nspecifically tailored for HRC. Our focus is on developing a network that\nrobustly transitions from simulated environments to real-world applications,\nthereby enhancing its practical utility and impact on a safe HRC.\n  In this work, we propose a dual-stream network architecture (FUSION)\ncombining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional\nNeural Networks (CNN) augmented with residual layers as a Sim2Real domain\nadaptation algorithm for an industrial environment. The proposed model was\nevaluated on real-world HRC setups and simulation industrial point clouds, it\nshowed increased state-of-the-art performance, achieving a segmentation\naccuracy of 97.76%, and superior robustness compared to existing methods.", "AI": {"tldr": "本文提出了一种双流网络架构（FUSION），结合动态图卷积神经网络（DGCNN）和卷积神经网络（CNN），用于3D点云数据的Sim2Real域适应，显著提升了语义分割的准确性和鲁棒性。", "motivation": "在人类-机器人协作（HRC）应用中，安全性和操作效率至关重要，而语义分割是实现环境精确理解的关键。然而，真实工业标注数据稀缺，因此需要一种从模拟环境到真实应用的域适应方法。", "method": "提出了一种双流网络架构（FUSION），结合DGCNN和CNN，并引入残差层，作为Sim2Real域适应算法，专门针对工业环境设计。", "result": "在真实HRC设置和模拟工业点云数据上评估，模型达到了97.76%的分割准确率，性能优于现有方法。", "conclusion": "该方法显著提升了语义分割的准确性和鲁棒性，为安全HRC提供了实用且高效的解决方案。"}}
{"id": "2506.09581", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09581", "abs": "https://arxiv.org/abs/2506.09581", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "David Sobrín-Hidalgo", "Ángel Manuel Guerrero-Higueras", "Vicente MatellÁn-Olivera"], "title": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities", "comment": "10 pages, 4 figures, Submitted to 3rd edition of the Workshop on\n  Ontologies and Standards for Robotics and Automation (WOSRA) at ICRA 2024", "summary": "Large Language Models (LLMs) have experienced great advancements in the last\nyear resulting in an increase of these models in several fields to face natural\nlanguage tasks. The integration of these models in robotics can also help to\nimprove several aspects such as human-robot interaction, navigation, planning\nand decision-making. Therefore, this paper introduces llama\\_ros, a tool\ndesigned to integrate quantized Large Language Models (LLMs) into robotic\nsystems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,\nllama\\_ros enables the efficient execution of quantized LLMs as edge artificial\nintelligence (AI) in robotics systems with resource-constrained environments,\naddressing the challenges of computational efficiency and memory limitations.\nBy deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural\nlanguage understanding and generation for enhanced decision-making and\ninteraction which can be paired with prompt engineering, knowledge graphs,\nontologies or other tools to improve the capabilities of autonomous robots.\nAdditionally, this paper provides insights into some use cases of using\nllama\\_ros for planning and explainability in robotics.", "AI": {"tldr": "llama_ros是一个工具，用于将量化的大型语言模型（LLMs）集成到ROS 2机器人系统中，提升自然语言理解和生成能力。", "motivation": "大型语言模型在机器人领域的应用潜力巨大，但面临计算效率和内存限制的挑战。llama_ros旨在解决这些问题。", "method": "利用llama.cpp高效运行时引擎，部署量化LLMs，结合提示工程、知识图谱等工具增强机器人能力。", "result": "llama_ros成功在资源受限环境中运行量化LLMs，提升机器人决策和交互能力。", "conclusion": "llama_ros为机器人系统提供了高效的LLMs集成方案，展示了在规划和可解释性方面的应用潜力。"}}
{"id": "2506.09583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09583", "abs": "https://arxiv.org/abs/2506.09583", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "Vicente Matellán-Olivera"], "title": "VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots", "comment": "15 pages, 5 figures, Submitted to WAF 2023: Workshop de Agentes\n  Fisicos", "summary": "Localization plays a crucial role in the navigation capabilities of\nautonomous robots, and while indoor environments can rely on wheel odometry and\n2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,\npresent unique challenges that necessitate real-time localization and\nconsistent mapping. Addressing this need, this paper introduces the VAULT\nprototype, a ROS 2-based mobile mapping system (MMS) that combines various\nsensors to enable robust outdoor and indoor localization. The proposed solution\nharnesses the power of Global Navigation Satellite System (GNSS) data,\nvisual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the\nExtended Kalman Filter (EKF) to generate reliable 3D odometry. To further\nenhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting\nin the creation of a comprehensive 3D point cloud map. By leveraging these\nsensor technologies and advanced algorithms, the prototype offers a\ncomprehensive solution for outdoor localization in autonomous mobile robots,\nenabling them to navigate and map their surroundings with confidence and\nprecision.", "AI": {"tldr": "本文介绍了VAULT原型，一个基于ROS 2的移动测绘系统（MMS），结合多种传感器实现室内外定位，利用GNSS、VIO、IMU和EKF生成可靠3D里程计，并通过VSLAM提高定位精度。", "motivation": "解决农业和林业等户外环境中自主机器人实时定位和一致测绘的挑战。", "method": "结合GNSS数据、视觉惯性里程计（VIO）、IMU数据和扩展卡尔曼滤波器（EKF）生成3D里程计，并利用VSLAM创建3D点云地图。", "result": "原型系统提供了户外定位的全面解决方案，使自主移动机器人能够精确导航和测绘。", "conclusion": "VAULT原型通过多传感器融合和先进算法，显著提升了户外自主机器人的定位和测绘能力。"}}
{"id": "2506.09588", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09588", "abs": "https://arxiv.org/abs/2506.09588", "authors": ["Junzhe He", "Chong Zhang", "Fabian Jenelten", "Ruben Grandia", "Moritz BÄcher", "Marco Hutter"], "title": "Attention-Based Map Encoding for Learning Generalized Legged Locomotion", "comment": "Original draft prior to peer review. Significant revisions and new\n  materials are expected after formal publication release", "summary": "Dynamic locomotion of legged robots is a critical yet challenging topic in\nexpanding the operational range of mobile robots. It requires precise planning\nwhen possible footholds are sparse, robustness against uncertainties and\ndisturbances, and generalizability across diverse terrains. While traditional\nmodel-based controllers excel at planning on complex terrains, they struggle\nwith real-world uncertainties. Learning-based controllers offer robustness to\nsuch uncertainties but often lack precision on terrains with sparse steppable\nareas. Hybrid methods achieve enhanced robustness on sparse terrains by\ncombining both methods but are computationally demanding and constrained by the\ninherent limitations of model-based planners. To achieve generalized legged\nlocomotion on diverse terrains while preserving the robustness of\nlearning-based controllers, this paper proposes to learn an attention-based map\nencoding conditioned on robot proprioception, which is trained as part of the\nend-to-end controller using reinforcement learning. We show that the network\nlearns to focus on steppable areas for future footholds when the robot\ndynamically navigates diverse and challenging terrains. We synthesize behaviors\nthat exhibit robustness against uncertainties while enabling precise and agile\ntraversal of sparse terrains. Additionally, our method offers a way to\ninterpret the topographical perception of a neural network. We have trained two\ncontrollers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot\nrespectively and tested the resulting controllers in the real world under\nvarious challenging indoor and outdoor scenarios, including ones unseen during\ntraining.", "AI": {"tldr": "本文提出了一种基于注意力机制的地图编码方法，结合强化学习训练端到端控制器，以实现腿式机器人在多样化地形上的动态运动。", "motivation": "传统基于模型的控制器在复杂地形上表现良好，但对现实世界的不确定性缺乏鲁棒性；而基于学习的控制器虽鲁棒性强，但在稀疏地形上缺乏精确性。本文旨在结合两者优势，实现鲁棒且精确的运动。", "method": "通过强化学习训练一个基于注意力的地图编码器，结合机器人本体感知，形成端到端控制器。", "result": "网络学会在动态导航时关注可踩踏区域，实现了在稀疏地形上的精确运动，并展示了鲁棒性和泛化能力。", "conclusion": "该方法不仅实现了鲁棒且精确的运动，还提供了对神经网络地形感知的可解释性。"}}
{"id": "2506.09623", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09623", "abs": "https://arxiv.org/abs/2506.09623", "authors": ["Lipei Xie", "Yingxin Li", "Huiping Zhuang"], "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models", "comment": null, "summary": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler", "AI": {"tldr": "论文提出了一种名为Analytic Task Scheduler (ATS)的新框架，用于解决具身基础模型在持续学习中的灾难性遗忘问题。", "motivation": "具身基础模型在整合多模态输入以理解人类意图和控制机器人方面表现出色，但在持续学习新技能时容易遗忘旧技能。", "method": "ATS包含一个任务特定模型库和一个基于递归最小二乘法（RLS）的分析调度器，用于动态选择模型并避免参数干扰。", "result": "在真实机器人平台（RM65B）上验证了ATS的抗遗忘能力和任务适应性。", "conclusion": "ATS是一种高效、可扩展且可部署的解决方案，适用于复杂动态环境中的持续学习。"}}
{"id": "2506.09629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09629", "abs": "https://arxiv.org/abs/2506.09629", "authors": ["Maurice Brunner", "Edoardo Ghignone", "Nicolas Baumann", "Michele Magno"], "title": "R-CARLA: High-Fidelity Sensor Simulations with Interchangeable Dynamics for Autonomous Racing", "comment": null, "summary": "Autonomous racing has emerged as a crucial testbed for autonomous driving\nalgorithms, necessitating a simulation environment for both vehicle dynamics\nand sensor behavior. Striking the right balance between vehicle dynamics and\nsensor accuracy is crucial for pushing vehicles to their performance limits.\nHowever, autonomous racing developers often face a trade-off between accurate\nvehicle dynamics and high-fidelity sensor simulations. This paper introduces\nR-CARLA, an enhancement of the CARLA simulator that supports holistic\nfull-stack testing, from perception to control, using a single system. By\nseamlessly integrating accurate vehicle dynamics with sensor simulations,\nopponents simulation as NPCs, and a pipeline for creating digital twins from\nreal-world robotic data, R-CARLA empowers researchers to push the boundaries of\nautonomous racing development. Furthermore, it is developed using CARLA's rich\nsuite of sensor simulations. Our results indicate that incorporating the\nproposed digital-twin framework into R-CARLA enables more realistic full-stack\ntesting, demonstrating a significant reduction in the Sim-to-Real gap of car\ndynamics simulation by 42% and by 82% in the case of sensor simulation across\nvarious testing scenarios.", "AI": {"tldr": "R-CARLA是CARLA模拟器的增强版，支持从感知到控制的全栈测试，通过整合精确的车辆动力学和传感器模拟，显著减少了模拟与现实的差距。", "motivation": "自动驾驶赛车开发中，车辆动力学与传感器模拟的平衡是关键，但现有工具往往难以兼顾。", "method": "R-CARLA通过整合车辆动力学、传感器模拟、NPC对手模拟和数字孪生技术，提供全栈测试能力。", "result": "实验显示，R-CARLA将车辆动力学模拟的模拟与现实差距减少了42%，传感器模拟减少了82%。", "conclusion": "R-CARLA为自动驾驶赛车开发提供了更真实的测试环境，显著提升了模拟的准确性。"}}
{"id": "2506.09697", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09697", "abs": "https://arxiv.org/abs/2506.09697", "authors": ["Paolo Franceschi", "Andrea Bussolan", "Vincenzo Pomponi", "Oliver Avram", "Stefano Baraldo", "Anna Valente"], "title": "Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling", "comment": null, "summary": "Nowadays, industries are showing a growing interest in human-robot\ncollaboration, particularly for shared tasks. This requires intelligent\nstrategies to plan a robot's motions, considering both task constraints and\nhuman-specific factors such as height and movement preferences. This work\nintroduces a novel approach to generate personalized trajectories using Dynamic\nMovement Primitives (DMPs), enhanced with real-time velocity scaling based on\nhuman feedback. The method was rigorously tested in industrial-grade\nexperiments, focusing on the collaborative transport of an engine cowl lip\nsection. Comparative analysis between DMP-generated trajectories and a\nstate-of-the-art motion planner (BiTRRT) highlights their adaptability combined\nwith velocity scaling. Subjective user feedback further demonstrates a clear\npreference for DMP- based interactions. Objective evaluations, including\nphysiological measurements from brain and skin activity, reinforce these\nfindings, showcasing the advantages of DMPs in enhancing human-robot\ninteraction and improving user experience.", "AI": {"tldr": "论文提出了一种基于动态运动基元（DMPs）和实时速度调整的个性化轨迹生成方法，用于人机协作任务。实验表明，该方法优于现有运动规划器，并显著提升了用户体验。", "motivation": "工业对人机协作的需求增长，需要智能策略来规划机器人运动，同时考虑任务约束和人类特定因素（如身高和运动偏好）。", "method": "使用动态运动基元（DMPs）生成个性化轨迹，并结合实时速度调整技术。", "result": "实验表明，DMP生成的轨迹优于BiTRRT运动规划器，用户反馈和生理测量均支持DMP方法的优越性。", "conclusion": "DMP方法显著提升了人机交互效果和用户体验，适用于工业协作任务。"}}
{"id": "2506.09765", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09765", "abs": "https://arxiv.org/abs/2506.09765", "authors": ["Shuai Li", "Azarakhsh Keipour", "Sicong Zhao", "Srinath Rajagopalan", "Charles Swan", "Kostas E. Bekris"], "title": "Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction", "comment": "The 19th International Symposium on Experimental Robotics (ISER\n  2025); 6-10 July 2025, Santa Fe, New Mexico, USA; 10 pages", "summary": "Warehouse automation plays a pivotal role in enhancing operational\nefficiency, minimizing costs, and improving resilience to workforce\nvariability. While prior research has demonstrated the potential of machine\nlearning (ML) models to increase picking success rates in large-scale robotic\nfleets by prioritizing high-probability picks and packages, these efforts\nprimarily focused on predicting success probabilities for picks sampled using\nheuristic methods. Limited attention has been given, however, to leveraging\ndata-driven approaches to directly optimize sampled picks for better\nperformance at scale. In this study, we propose an ML-based framework that\npredicts transform adjustments as well as improving the selection of suction\ncups for multi-suction end effectors for sampled picks to enhance their success\nprobabilities. The framework was integrated and evaluated in test workcells\nthat resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,\nwhich is used for package manipulation. Evaluated on over 2 million picks, the\nproposed method achieves a 20\\% reduction in pick failure rates compared to a\nheuristic-based pick sampling baseline, demonstrating its effectiveness in\nlarge-scale warehouse automation scenarios.", "AI": {"tldr": "本文提出了一种基于机器学习的框架，通过预测调整和优化吸盘选择，显著降低了仓库自动化中的拾取失败率。", "motivation": "仓库自动化对提升效率和降低成本至关重要，但现有研究多依赖启发式方法，缺乏数据驱动的优化方法。", "method": "提出了一种ML框架，用于预测调整和优化吸盘选择，以提升拾取成功率。", "result": "在超过200万次拾取测试中，该方法比启发式基线降低了20%的失败率。", "conclusion": "该框架在大规模仓库自动化场景中表现出显著效果。"}}
{"id": "2506.09800", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09800", "abs": "https://arxiv.org/abs/2506.09800", "authors": ["Haochen Liu", "Tianyu Li", "Haohan Yang", "Li Chen", "Caojun Wang", "Ke Guo", "Haochen Tian", "Hongchen Li", "Hongyang Li", "Chen Lv"], "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\ndirectly mapping sensor inputs to planning maneuvers using learning-based\nmodular integrations. However, existing imitation learning (IL)-based models\nsuffer from generalization to hard cases, and a lack of corrective feedback\nloop under post-deployment. While reinforcement learning (RL) offers a\npotential solution to tackle hard cases with optimality, it is often hindered\nby overfitting to specific driving cases, resulting in catastrophic forgetting\nof generalizable knowledge and sample inefficiency. To overcome these\nchallenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),\na novel learning pipeline that constantly refines hard domain while keeping\ngeneralizable driving policy for model-agnostic end-to-end driving systems.\nThrough reinforcement fine-tuning and policy expansion that facilitates\ncontinuous improvement, R2SE features three key components: 1) Generalist\nPretraining with hard-case allocation trains a generalist imitation learning\n(IL) driving system while dynamically identifying failure-prone cases for\ntargeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes\nresidual corrections using reinforcement learning (RL) to improve performance\nin hard case domain while preserving global driving knowledge; 3) Self-aware\nAdapter Expansion dynamically integrates specialist policies back into the\ngeneralist model, enhancing continuous performance improvement. Experimental\nresults in closed-loop simulation and real-world datasets demonstrate\nimprovements in generalization, safety, and long-horizon policy robustness over\nstate-of-the-art E2E systems, highlighting the effectiveness of reinforce\nrefinement for scalable autonomous driving.", "AI": {"tldr": "论文提出了一种名为R2SE的学习框架，通过强化学习和模仿学习的结合，解决端到端自动驾驶中泛化性和样本效率的问题。", "motivation": "现有模仿学习模型在泛化到复杂场景时表现不佳，且缺乏部署后的反馈机制；强化学习虽能优化复杂场景，但容易过拟合并遗忘通用知识。", "method": "R2SE包含三个关键组件：通用预训练、残差强化微调和自感知适配器扩展，通过动态识别失败案例并针对性优化，同时保持通用驾驶策略。", "result": "实验表明，R2SE在闭环仿真和真实数据中提升了泛化性、安全性和长期策略鲁棒性，优于现有端到端系统。", "conclusion": "R2SE通过强化微调和策略扩展，为可扩展的自动驾驶提供了有效解决方案。"}}
{"id": "2506.09859", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09859", "abs": "https://arxiv.org/abs/2506.09859", "authors": ["Huajian Liu", "Yixuan Feng", "Wei Dong", "Kunpeng Fan", "Chao Wang", "Yongzhuo Gao"], "title": "Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints", "comment": null, "summary": "In this paper, we propose a novel hierarchical framework for robot navigation\nin dynamic environments with heterogeneous constraints. Our approach leverages\na graph neural network trained via reinforcement learning (RL) to efficiently\nestimate the robot's cost-to-go, formulated as local goal recommendations. A\nspatio-temporal path-searching module, which accounts for kinematic\nconstraints, is then employed to generate a reference trajectory to facilitate\nsolving the non-convex optimization problem used for explicit constraint\nenforcement. More importantly, we introduce an incremental action-masking\nmechanism and a privileged learning strategy, enabling end-to-end training of\nthe proposed planner. Both simulation and real-world experiments demonstrate\nthat the proposed method effectively addresses local planning in complex\ndynamic environments, achieving state-of-the-art (SOTA) performance. Compared\nwith existing learning-optimization hybrid methods, our approach eliminates the\ndependency on high-fidelity simulation environments, offering significant\nadvantages in computational efficiency and training scalability. The code will\nbe released as open-source upon acceptance of the paper.", "AI": {"tldr": "提出了一种新颖的分层框架，用于动态环境中机器人导航，结合图神经网络和强化学习，通过增量动作屏蔽机制和特权学习策略实现端到端训练，性能达到SOTA。", "motivation": "解决动态环境中机器人导航的复杂性和非凸优化问题，同时减少对高保真仿真环境的依赖。", "method": "使用图神经网络和强化学习估计成本，结合时空路径搜索模块生成参考轨迹，并引入增量动作屏蔽和特权学习策略。", "result": "仿真和实际实验表明，该方法在复杂动态环境中表现优异，计算效率和训练可扩展性显著提升。", "conclusion": "该方法在动态环境中实现了高效导航，性能优于现有混合方法，且代码将开源。"}}
{"id": "2506.09914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09914", "abs": "https://arxiv.org/abs/2506.09914", "authors": ["Teng Guo"], "title": "From Theory to Practice: Advancing Multi-Robot Path Planning Algorithms and Applications", "comment": "Ph.D. thesis", "summary": "The labeled MRPP (Multi-Robot Path Planning) problem involves routing robots\nfrom start to goal configurations efficiently while avoiding collisions.\nDespite progress in solution quality and runtime, its complexity and industrial\nrelevance continue to drive research.\n  This dissertation introduces scalable MRPP methods with provable guarantees\nand practical heuristics. First, we study dense MRPP on 2D grids, relevant to\nwarehouse and parcel systems. We propose the Rubik Table method, achieving $(1\n+ \\delta)$-optimal makespan (with $\\delta \\in (0, 0.5]$) for up to $\\frac{m_1\nm_2}{2}$ robots, solving large instances efficiently and setting a new\ntheoretical benchmark.\n  Next, we address real-world MRPP. We design optimal layouts for structured\nenvironments (e.g., warehouses, parking systems) and propose a puzzle-based\nsystem for dense, deadlock-free autonomous vehicle parking. We also extend MRPP\nto Reeds-Shepp robots, introducing motion primitives and smoothing techniques\nto ensure feasible, efficient paths under nonholonomic constraints. Simulations\nand real-world tests validate the approach in urban driving and robotic\ntransport scenarios.", "AI": {"tldr": "本文提出了一种可扩展的多机器人路径规划（MRPP）方法，包括理论保证和实用启发式算法，适用于密集和实际场景。", "motivation": "MRPP问题在复杂性和工业应用中的重要性推动了研究，需要高效且可扩展的解决方案。", "method": "提出了Rubik Table方法用于密集2D网格MRPP，设计了结构化环境的最优布局，并扩展了Reeds-Shepp机器人的路径规划。", "result": "Rubik Table方法实现了(1+δ)-最优完成时间，解决了大规模实例；实际测试验证了方法在自动驾驶和机器人运输中的有效性。", "conclusion": "本文的方法为MRPP提供了理论和实践上的突破，适用于多种实际应用场景。"}}
{"id": "2506.09930", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09930", "abs": "https://arxiv.org/abs/2506.09930", "authors": ["Irving Fang", "Juexiao Zhang", "Shengbang Tong", "Chen Feng"], "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "comment": "Under review", "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "AI": {"tldr": "本文介绍了Vision-Language-Action (VLA) 模型的局限性，并提出了一种包含50个模拟任务的评估套件，以标准化VLA模型的性能测试。研究发现，尽管VLA模型在感知和规划方面表现良好，但在动作执行上存在不足，且微调可能损害其泛化能力。", "motivation": "当前对VLA模型的评估不足，缺乏统一的基准测试，且现有研究难以复现。本文旨在填补这一空白，提供一个标准化评估工具。", "method": "引入包含50个模拟任务的评估套件，涵盖语言指令、视觉和物体操作，并系统评估多种VLA架构的泛化能力。", "result": "VLA模型在感知和规划（意图）上表现良好，但在动作执行上不稳定，尤其是面对分布外数据时。微调可能削弱模型的泛化能力。", "conclusion": "本文提出的评估套件为VLA研究提供了标准化基准，并揭示了感知与动作执行之间的差距，呼吁进一步研究以弥合这一差距。"}}
{"id": "2506.09934", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09934", "abs": "https://arxiv.org/abs/2506.09934", "authors": ["Jared Lawson", "Rohan Chitale", "Nabil Simaan"], "title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers", "comment": "8 pages, 5 figures, accepted in Robotics and Automation Letters", "summary": "Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.", "AI": {"tldr": "论文提出了一种通过定制不透射线标记物实现微导管形状和姿态同步估计的方法，以减少神经介入手术中对导管运动的感知负担。", "motivation": "当前神经介入手术中，医生需要从双平面透视图像中重建和预测导管运动，感知负担重。现有导管追踪方法局限于平面分割或笨重的传感设备，不适用于微导管。", "method": "在导管上布置定制不透射线标记物，设计标记物排列以减少对标记物追踪不确定性的敏感性。", "result": "在直径小于2mm的微导管上部署该方法，形状追踪误差小于1mm，导管滚动误差低于40度。", "conclusion": "该方法可实现导管在双平面成像下的自主导航。"}}
{"id": "2506.09937", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09937", "abs": "https://arxiv.org/abs/2506.09937", "authors": ["Qiao Gu", "Yuanliang Ju", "Shengxiang Sun", "Igor Gilitschenski", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "comment": "Project Page: https://vla-safe.github.io/", "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "AI": {"tldr": "本文提出了SAFE，一种用于通用机器人策略（如视觉-语言-动作模型VLA）的故障检测器，能够在未见任务和新环境中检测故障。", "motivation": "现有故障检测器仅针对特定任务训练和测试，而VLA需要检测器在未见任务和新环境中也能泛化。", "method": "通过分析VLA特征空间，发现其具有跨任务通用的任务成功与失败知识，基于此设计SAFE，从VLA内部特征学习并预测任务失败概率。", "result": "SAFE在模拟和真实环境中测试，与多种基线比较，表现出最优的故障检测性能和准确性与检测时间的最佳平衡。", "conclusion": "SAFE是一种高效的通用故障检测器，适用于多种策略架构，并在未见任务中表现优异。"}}
{"id": "2506.09979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09979", "abs": "https://arxiv.org/abs/2506.09979", "authors": ["Zachary Olkin", "Aaron D. Ames"], "title": "Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control", "comment": "Submitted to Humanoids 2025", "summary": "Computing stabilizing and optimal control actions for legged locomotion in\nreal time is difficult due to the nonlinear, hybrid, and high dimensional\nnature of these robots. The hybrid nature of the system introduces a\ncombination of discrete and continuous variables which causes issues for\nnumerical optimal control. To address these challenges, we propose a layered\narchitecture that separates the choice of discrete variables and a smooth Model\nPredictive Controller (MPC). The layered formulation allows for online\nflexibility and optimality without sacrificing real-time performance through a\ncombination of gradient-free and gradient-based methods. The architecture\nleverages a sampling-based method for determining discrete variables, and a\nclassical smooth MPC formulation using these fixed discrete variables. We\ndemonstrate the results on a quadrupedal robot stepping over gaps and onto\nterrain with varying heights. In simulation, we demonstrate the controller on a\nhumanoid robot for gap traversal. The layered approach is shown to be more\noptimal and reliable than common heuristic-based approaches and faster to\ncompute than pure sampling methods.", "AI": {"tldr": "提出了一种分层架构，结合无梯度和基于梯度的方法，解决了足式机器人实时控制中的非线性、混合和高维问题。", "motivation": "足式机器人的非线性、混合和高维特性导致实时稳定和最优控制困难，传统数值最优控制方法难以处理。", "method": "采用分层架构，分离离散变量选择和光滑模型预测控制（MPC），结合采样方法和经典MPC。", "result": "在四足和双足机器人上验证了方法的有效性和实时性，优于启发式方法和纯采样方法。", "conclusion": "分层架构在保证实时性的同时提高了控制的最优性和可靠性。"}}
{"id": "2506.09990", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09990", "abs": "https://arxiv.org/abs/2506.09990", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "comment": null, "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "AI": {"tldr": "Chain-of-Action (CoA) 是一种基于轨迹自回归建模的新型视觉运动策略范式，通过反向推理生成完整轨迹，实现了任务目标的全局到局部约束。", "motivation": "传统方法仅正向预测下一步动作，缺乏对任务目标的全局约束。CoA 旨在通过反向推理和任务目标的显式编码，提升策略的泛化能力和性能。", "method": "CoA 采用自回归结构，首先生成任务目标的关键帧动作，随后自回归生成后续动作。设计了连续动作表示、动态停止、反向时间集成和多令牌预测等机制。", "result": "CoA 在60个RLBench任务和8个真实世界操作任务中实现了最先进的性能。", "conclusion": "CoA 通过反向推理和全局约束，显著提升了视觉运动策略的泛化能力和任务性能。"}}
{"id": "2506.09994", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09994", "abs": "https://arxiv.org/abs/2506.09994", "authors": ["Venkatesh Pattabiraman", "Zizhou Huang", "Daniele Panozzo", "Denis Zorin", "Lerrel Pinto", "Raunaq Bhirangi"], "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures", "comment": null, "summary": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.", "AI": {"tldr": "eFlesh是一种低成本、易定制、易制造的磁性触觉传感器，填补了机器人操作中缺乏通用触觉传感器的空白。", "motivation": "在非结构化环境中（如家庭和办公室），机器人需要感知物理交互中的力，但现有触觉传感器缺乏通用性和可定制性，导致解决方案分散或完全忽略力的感知。", "method": "eFlesh传感器由3D打印的微结构、现成磁铁和磁力计电路板组成，通过开源设计工具将CAD模型转换为可3D打印的STL文件。", "result": "实验显示，eFlesh在接触定位（RMSE 0.5 mm）和力预测（正常力RMSE 0.27 N，剪切力RMSE 0.12 N）方面表现优异，并支持95%准确率的滑动检测模型和40%性能提升的视觉触觉控制策略。", "conclusion": "eFlesh为机器人操作提供了一种低成本、高定制化的触觉感知解决方案，其开源设计工具和模块化框架进一步推动了广泛应用。"}}
