{"id": "2508.13303", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13303", "abs": "https://arxiv.org/abs/2508.13303", "authors": ["Yingfan Zhou", "Philip Sanderink", "Sigurd Jager Lemming", "Cheng Fang"], "title": "Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters", "comment": "8 pages, 7 figures", "summary": "High-fidelity personalized human musculoskeletal models are crucial for\nsimulating realistic behavior of physically coupled human-robot interactive\nsystems and verifying their safety-critical applications in simulations before\nactual deployment, such as human-robot co-transportation and rehabilitation\nthrough robotic exoskeletons. Identifying subject-specific Hill-type muscle\nmodel parameters and bone dynamic parameters is essential for a personalized\nmusculoskeletal model, but very challenging due to the difficulty of measuring\nthe internal biomechanical variables in vivo directly, especially the joint\ntorques. In this paper, we propose using Differentiable MusculoSkeletal Model\n(Diff-MSM) to simultaneously identify its muscle and bone parameters with an\nend-to-end automatic differentiation technique differentiating from the\nmeasurable muscle activation, through the joint torque, to the resulting\nobservable motion without the need to measure the internal joint torques.\nThrough extensive comparative simulations, the results manifested that our\nproposed method significantly outperformed the state-of-the-art baseline\nmethods, especially in terms of accurate estimation of the muscle parameters\n(i.e., initial guess sampled from a normal distribution with the mean being the\nground truth and the standard deviation being 10% of the ground truth could end\nup with an average of the percentage errors of the estimated values as low as\n0.05%). In addition to human musculoskeletal modeling and simulation, the new\nparameter identification technique with the Diff-MSM has great potential to\nenable new applications in muscle health monitoring, rehabilitation, and sports\nscience.", "AI": {"tldr": "提出可微分肌肉骨骼模型(Diff-MSM)，使用端到端自动微分技术同时识别肌肉和骨骼参数，无需测量内部关节扭矩，显著优于现有方法", "motivation": "高保真个性化人体肌肉骨骼模型对于人机交互系统仿真和安全验证至关重要，但传统方法难以直接测量内部生物力学变量特别是关节扭矩", "method": "使用可微分肌肉骨骼模型，通过自动微分技术从可测量的肌肉激活到可观察的运动进行端到端参数识别", "result": "在模拟实验中显著优于现有基线方法，肌肉参数估计误差低至0.05%", "conclusion": "该方法不仅在肌肉骨骼建模方面表现出色，还有望在肌肉健康监测、康复和运动科学等领域开启新的应用"}}
{"id": "2508.13319", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13319", "abs": "https://arxiv.org/abs/2508.13319", "authors": ["Kshitij Kavimandan", "Pooja Mangal", "Devanshi Mehta"], "title": "A Surveillance Based Interactive Robot", "comment": "4 pages, 5 figures", "summary": "We build a mobile surveillance robot that streams video in real time and\nresponds to speech so a user can monitor and steer it from a phone or browser.\nThe system uses two Raspberry Pi 4 units: a front unit on a differential drive\nbase with camera, mic, and speaker, and a central unit that serves the live\nfeed and runs perception. Video is sent with FFmpeg. Objects in the scene are\ndetected using YOLOv3 to support navigation and event awareness. For voice\ninteraction, we use Python libraries for speech recognition, multilingual\ntranslation, and text-to-speech, so the robot can take spoken commands and read\nback responses in the requested language. A Kinect RGB-D sensor provides visual\ninput and obstacle cues. In indoor tests the robot detects common objects at\ninteractive frame rates on CPU, recognises commands reliably, and translates\nthem to actions without manual control. The design relies on off-the-shelf\nhardware and open software, making it easy to reproduce. We discuss limits and\npractical extensions, including sensor fusion with ultrasonic range data, GPU\nacceleration, and adding face and text recognition.", "AI": {"tldr": "建立了一个基于树莓派Raspberry Pi 4的移动监控机器人，支持实时视频流、语音控制和多语言交互，可通过手机或浏览器远程监控和控制", "motivation": "开发一个使用商业硬件和开源软件的易复现监控机器人系统，支持实时视频流、语音控制和多语言交互功能", "method": "使用2个Raspberry Pi 4单元：前端单元负责差动基座、摄像头、麦克风和扬声器，中央单元负责实时流和视觉处理。使用FFmpeg传输视频，YOLOv3进行对象检测，Python语音库进行语音识别、多语言翻译和语音合成，Kinect RGB-D传感器提供视觉输入和障碍物提示", "result": "在室内测试中，机器人在CPU上以交互帧率检测常见物体，可靠识别命令，并将其转换为动作而无需手动控制", "conclusion": "设计依靠商业硬件和开源软件，易于复现。讨论了限制和实用扩展，包括与超声波距离数据的传感器融合、GPU加速以及添加人脸和文本识别功能"}}
{"id": "2508.13392", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13392", "abs": "https://arxiv.org/abs/2508.13392", "authors": ["Sidharth Talia", "Oren Salzman", "Siddhartha Srinivasa"], "title": "Incremental Generalized Hybrid A*", "comment": "8 pages, 7 figures", "summary": "We address the problem of efficiently organizing search over very large\ntrees, which arises in many applications ranging from autonomous driving to\naerial vehicles. Here, we are motivated by off-road autonomy, where real-time\nplanning is essential. Classical approaches use graphs of motion primitives and\nexploit dominance to mitigate the curse of dimensionality and prune expansions\nefficiently. However, for complex dynamics, repeatedly solving two-point\nboundary-value problems makes graph construction too slow for fast kinodynamic\nplanning. Hybrid A* (HA*) addressed this challenge by searching over a tree of\nmotion primitives and introducing approximate pruning using a grid-based\ndominance check. However, choosing the grid resolution is difficult: too coarse\nrisks failure, while too fine leads to excessive expansions and slow planning.\nWe propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search\nframework that dynamically organizes vertex expansions without rigid pruning.\nIGHA* provably matches or outperforms HA*. For both on-road kinematic and\noff-road kinodynamic planning queries for a car-like robot, variants of IGHA*\nuse 6x fewer expansions to the best solution compared to an optimized version\nof HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*\noutperforms HA*M when both are used in the loop with a model predictive\ncontroller. We demonstrate real-time performance both in simulation and on a\nsmall-scale off-road vehicle, enabling fast, robust planning under complex\ndynamics. Code: https://github.com/personalrobotics/IGHAStar", "AI": {"tldr": "提出IGHA*算法，一种增量式广义混合A*方法，通过动态组织顶点扩展而非刚性剪枝，在复杂动力学规划中比传统HA*减少6倍扩展次数，实现实时性能", "motivation": "解决在自动驾驶和越野自主性等应用中，复杂动力学下的实时规划问题。传统混合A*方法在网格分辨率选择上存在困难：太粗可能导致失败，太细则导致扩展过多和规划缓慢", "method": "提出增量式广义混合A*（IGHA*）框架，采用动态顶点扩展组织方式，避免刚性剪枝，可证明匹配或优于传统HA*", "result": "在汽车类机器人的道路运动学和越野动力学规划查询中，IGHA*变体比优化版HA*减少6倍扩展到最佳解的扩展次数。在高保真模拟器中，IGHA*在模型预测控制循环中优于HA*M", "conclusion": "IGHA*在仿真和小规模越野车辆上展示了实时性能，能够在复杂动力学下实现快速、鲁棒的规划"}}
{"id": "2508.13407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13407", "abs": "https://arxiv.org/abs/2508.13407", "authors": ["Jiming Ren", "Xuan Lin", "Roman Mineyev", "Karen M. Feigh", "Samuel Coogan", "Ye Zhao"], "title": "Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition", "comment": "16 pages, 7 figures, 6 tables", "summary": "Task and motion planning under Signal Temporal Logic constraints is known to\nbe NP-hard. A common class of approaches formulates these hybrid problems,\nwhich involve discrete task scheduling and continuous motion planning, as\nmixed-integer programs (MIP). However, in applications for bipedal locomotion,\nintroduction of non-convex constraints such as kinematic reachability and\nfootstep rotation exacerbates the computational complexity of MIPs. In this\nwork, we present a method based on Benders Decomposition to address scenarios\nwhere solving the entire monolithic optimization problem is prohibitively\nintractable. Benders Decomposition proposes an iterative cutting-plane\ntechnique that partitions the problem into a master problem to prototype a plan\nthat meets the task specification, and a series of subproblems for kinematics\nand dynamics feasibility checks. Our experiments demonstrate that this method\nachieves faster planning compared to alternative algorithms for solving the\nresulting optimization program with nonlinear constraints.", "AI": {"tldr": "基于Benders分解的方法来解决双足式行走任务与动作规划中的混合整数规划问题，通过分解为主问题和子问题来提高计算效率", "motivation": "双足式行走中的任务与动作规划问题在混合整数规划中引入非凸约束后计算复杂度更高，单一优化问题解决困难", "method": "采用Benders分解技术，将问题分解为主问题（任务规划）和一系列子问题（动力学和运动学可行性检查），通过迭代切割平面技术进行解决", "result": "实验结果显示，该方法比其他解决非线性约束优化问题的算法更快完成规划", "conclusion": "Benders分解技术能够有效地处理双足式行走中的复杂混合整数规划问题，提高规划效率"}}
{"id": "2508.12681", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12681", "abs": "https://arxiv.org/abs/2508.12681", "authors": ["Johann Licher", "Max Bartholdt", "Henrik Krauss", "Tim-Lukas Habich", "Thomas Seel", "Moritz Schappler"], "title": "Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory", "comment": "20 pages, 15 figures", "summary": "Dynamic control of soft continuum robots (SCRs) holds great potential for\nexpanding their applications, but remains a challenging problem due to the high\ncomputational demands of accurate dynamic models. While data-driven approaches\nlike Koopman-operator-based methods have been proposed, they typically lack\nadaptability and cannot capture the full robot shape, limiting their\napplicability. This work introduces a real-time-capable nonlinear\nmodel-predictive control (MPC) framework for SCRs based on a domain-decoupled\nphysics-informed neural network (DD-PINN) with adaptable bending stiffness. The\nDD-PINN serves as a surrogate for the dynamic Cosserat rod model with a\nspeed-up factor of 44000. It is also used within an unscented Kalman filter for\nestimating the model states and bending compliance from end-effector position\nmeasurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the\nGPU. In simulation, it demonstrates accurate tracking of dynamic trajectories\nand setpoint control with end-effector position errors below 3 mm (2.3% of the\nactuator's length). In real-world experiments, the controller achieves similar\naccuracy and accelerations up to 3.55 m/s2.", "AI": {"tldr": "基于域解耦物理信息神经网络的软连续体机器人实时非线性模型预测控制框架，实现了高精度动态轨迹跟踪", "motivation": "软连续体机器人的动态控制具有广阔应用前景，但精确动态模型计算复杂度高，现有数据驱动方法缺乏适应性且无法完整描述机器人形状", "method": "使用域解耦物理信息神经网络(DD-PINN)作为动态Cosserat柱模型的代理模型，速度提升国44000倍，并结合无病态卡尔滤波估计模型状态和弯曲顺度，实现70Hz的非线性进化模型预测控制", "result": "在模拟中实现了精确的动态轨迹跟踪，端执行器位置误差低于3mm(2.3%行程)；在实际实验中达到类似精度，最高加速度达3.55m/s²", "conclusion": "该框架为软连续体机器人提供了一种高效、准确且实时的动态控制方案，解决了传统模型计算复杂和现有方法适应性不足的问题"}}
{"id": "2508.13444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13444", "abs": "https://arxiv.org/abs/2508.13444", "authors": ["Tianyu Li", "Jeonghwan Kim", "Wontaek Kim", "Donghoon Baek", "Seungeun Rho", "Sehoon Ha"], "title": "Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics", "comment": "Workshop Submission", "summary": "Recent advances in whole-body robot control have enabled humanoid and legged\nrobots to execute increasingly agile and coordinated movements. However,\nstandardized benchmarks for evaluating robotic athletic performance in\nreal-world settings and in direct comparison to humans remain scarce. We\npresent Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable\npipeline that leverages motion-sensing console games to evaluate whole-body\nrobot control policies. Using Just Dance on the Nintendo Switch as a\nrepresentative example, our system captures, reconstructs, and retargets\nin-game choreography for robotic execution. We validate the system on a Unitree\nG1 humanoid with an open-source whole-body controller, establishing a\nquantitative baseline for the robot's performance against a human player. In\nthe paper, we discuss these results, which demonstrate the feasibility of using\ncommercial games platform as physically grounded benchmarks and motivate future\nwork to for benchmarking embodied AI.", "AI": {"tldr": "Switch4EAI是一个利用体感游戏（如Just Dance）来评估全身机器人控制策略的低成本系统，通过捕捉和重定向游戏中的舞蹈动作到机器人执行，为人形机器人性能评估提供量化基准。", "motivation": "当前缺乏在真实环境中评估人形机器人运动性能并与人类进行直接比较的标准化基准，需要一种低成本、易部署的评估方法。", "method": "利用Nintendo Switch的体感游戏Just Dance，开发了一个包含动作捕捉、重建和重定向的流水线系统，在Unitree G1人形机器人上使用开源全身控制器进行验证。", "result": "系统成功实现了机器人对游戏舞蹈动作的执行，建立了机器人相对于人类玩家性能的量化基准，证明了商业游戏平台作为物理基准的可行性。", "conclusion": "商业体感游戏平台可以作为评估具身AI的有效物理基准，为未来机器人性能评估提供了新的方向和方法。"}}
{"id": "2508.13457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13457", "abs": "https://arxiv.org/abs/2508.13457", "authors": ["Xu Yang", "Jun Ni", "Hengyang Feng", "Feiyu Wang", "Tiezhen Wang"], "title": "Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle", "comment": null, "summary": "An all-wheel omni-directional independent steering vehicle (AWOISV) is a\nspecialized all-wheel independent steering vehicle with each wheel capable of\nsteering up to 90{\\deg}, enabling unique maneuvers like yaw and diagonal\nmovement. This paper introduces a theoretical steering radius angle and\nsideslip angle (\\( \\theta_R \\)-\\(\\beta_R \\)) representation, based on the\nposition of the instantaneous center of rotation relative to the wheel rotation\ncenter, defining the motion modes and switching criteria for AWOISVs. A\ngeneralized \\( v\\)-\\(\\beta\\)-\\(r \\) dynamic model is developed with forward\nvelocity \\(v\\), sideslip angle \\(\\beta\\), and yaw rate \\(r\\) as states, and\n\\(\\theta_R\\) and \\(\\beta_R\\) as control inputs. This model decouples\nlongitudinal and lateral motions into forward and rotational motions, allowing\nseamless transitions across all motion modes under specific conditions. A\nfiltered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,\nachieving simultaneous tracking of lateral position and arbitrary heading\nangles, with robustness to model inaccuracies and parameter uncertainties.\nCo-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC\nenables high-precision control of both position and heading while ensuring\nexcellent real-time performance.", "AI": {"tldr": "基于新的轮轮转向半径角和侧滑角表示法，开发了全轮全向独立转向车辆的动态模型和FT-LTVMPC控制策略，实现了高精度位置和航向角同时跟随", "motivation": "解决全轮全向独立转向车辆(AWOISV)在多种运动模式下的平滑转换和精确控制问题，特别是同时跟随位置和任意航向角的挑战", "method": "提出理论轮轮转向半径角和侧滑角表示法，建立广义v-β-r动态模型，设计筛波管基线性时变MPC策略(FT-LTVMPC)", "result": "实验验证FT-LTVMPC能够实现高精度位置和航向角跟随，具有强壁性和良好的实时性能", "conclusion": "所提方法有效解决了AWOISV在全部运动模式下的精确控制问题，为类似车辆控制提供了新思路"}}
{"id": "2508.13446", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13446", "abs": "https://arxiv.org/abs/2508.13446", "authors": ["Catherine Glossop", "William Chen", "Arjun Bhorkar", "Dhruv Shah", "Sergey Levine"], "title": "CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models", "comment": null, "summary": "Generalist robots should be able to understand and follow user instructions,\nbut current vision-language-action (VLA) models struggle with following\nfine-grained commands despite providing a powerful architecture for mapping\nopen-vocabulary natural language instructions to robot actions. One cause for\nthis is a lack of semantic diversity and language grounding in existing robot\ndatasets and, specifically, a lack of fine-grained task diversity for similar\nobservations. To address this, we present a novel method to augment existing\nrobot datasets by leveraging vision language models to create counterfactual\nlabels. Our method improves the language-following capabilities of VLAs by\nincreasing the diversity and granularity of language grounding for robot\ndatasets by generating counterfactual language and actions. We evaluate the\nresulting model's ability to follow language instructions, ranging from simple\nobject-centric commands to complex referential tasks, by conducting visual\nlanguage navigation experiments in 3 different indoor and outdoor environments.\nOur experiments demonstrate that counterfactual relabeling, without any\nadditional data collection, significantly improves instruction-following in VLA\npolicies, making them competitive with state-of-the-art methods and increasing\nsuccess rate by 27% on navigation tasks.", "AI": {"tldr": "通过视觉语言模型生成假想标签，提升机器人数据集语言基础的细粒度和多样性，从而显著提高VLA模型的指令追随能力", "motivation": "当前VLA模型在追随细粒度指令方面遇到困难，主要原因是现有机器人数据集缺乏语言基础和语义多样性，尤其是对类似观测的细粒度任务多样性", "method": "提出一种新的数据增帽方法，利用视觉语言模型为现有机器人数据集生成假想标签，通过生成假想语言和动作来提高语言基础的多样性和细粒度", "result": "在3种不同室内外环境中进行视觉语言导航实验，结果显示假想重标注在不需额外数据收集的情况下，显著提高了VLA策略的指令追随能力，导航任务成功率提高27%", "conclusion": "假想标签方法能够有效提升VLA模型的语言指令理解和执行能力，使其在性能上可与最先进方法竞争，为通用机器人的语言理解提供了有效解决方案"}}
{"id": "2508.13459", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.13459", "abs": "https://arxiv.org/abs/2508.13459", "authors": ["Rohan Chandra", "Shubham Singh", "Abhishek Jha", "Dannon Andrade", "Hriday Sainathuni", "Katia Sycara"], "title": "Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms", "comment": null, "summary": "The ``Last Mile Challenge'' has long been considered an important, yet\nunsolved, challenge for autonomous vehicles, public service robots, and\ndelivery robots. A central issue in this challenge is the ability of robots to\nnavigate constrained and cluttered environments (e.g., doorways, hallways,\ncorridor intersections), often while competing for space with other robots and\nhumans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs\nare tightly coupled, high-agency interactions that arise within general\nmulti-robot navigation (MRN) scenarios. They are identified through certain\ndistinct characteristics and require specialized metrics to evaluate them.\nTraditional navigation approaches designed for MRN do not perform well in SMGs,\nwhich has led to focused research on dedicated SMG solvers (navigation methods\nspecialized to navigate in SMGs), which has flourished in recent years.\nHowever, publications on SMG navigation research make different assumptions (on\ncentralized versus decentralized, observability, communication, cooperation,\netc.), and have different objective functions (safety versus liveness). These\nassumptions and objectives are sometimes implicitly assumed or described\ninformally. This makes it difficult to establish appropriate baselines for\ncomparison in research papers, as well as making it difficult for practitioners\nto find the papers relevant to their concrete application. Such ad-hoc\nrepresentation of the field also presents a barrier to new researchers wanting\nto start research in this area. SMG navigation research requires its own\ntaxonomy, definitions, and evaluation protocols to guide effective research\nmoving forward. This survey is the first to catalog SMG solvers using a\nwell-defined and unified taxonomy and to classify existing methods accordingly.", "AI": {"tldr": "本文是首个对社交小游戏(SMGs)导航方法的系统调研，通过统一分类体系解决领域中偏差偏差和比较困难。", "motivation": "社交小游戏(SMGs)作为自主车达到最后一公里的关键挑战，传统导航方法表现不佳，但现有研究偏差偏差严重且缺乏统一评估标准。", "method": "构建了一个统一的分类体系和定义，对现有SMG解决方法进行系统分类和评估。", "result": "完整地整理了领域内的研究状况，为不同偏差偏差和目标的方法提供了明确的分类标准。", "conclusion": "该研究为SMG导航领域建立了标准化的研究框架，有助于推动领域的可比性研究和实践应用。"}}
{"id": "2508.13488", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13488", "abs": "https://arxiv.org/abs/2508.13488", "authors": ["Jingwen Yu", "Jiayi Yang", "Anjun Hu", "Jiankun Wang", "Ping Tan", "Hong Zhang"], "title": "ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments", "comment": "8 pages, 9 figures", "summary": "Loop closure detection is important for simultaneous localization and mapping\n(SLAM), which associates current observations with historical keyframes,\nachieving drift correction and global relocalization. However, a falsely\ndetected loop can be fatal, and this is especially difficult in repetitive\nenvironments where appearance-based features fail due to the high similarity.\nTherefore, verification of a loop closure is a critical step in avoiding false\npositive detections. Existing works in loop closure verification predominantly\nfocus on learning invariant appearance features, neglecting the prior knowledge\nof the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,\nwe propose ROVER, a loop closure verification method that leverages the\nhistorical trajectory as a prior constraint to reject false loops in\nchallenging repetitive environments. For each loop candidate, it is first used\nto estimate the robot trajectory with pose-graph optimization. This trajectory\nis then submitted to a scoring scheme that assesses its compliance with the\ntrajectory without the loop, which we refer to as the trajectory prior, to\ndetermine if the loop candidate should be accepted. Benchmark comparisons and\nreal-world experiments demonstrate the effectiveness of the proposed method.\nFurthermore, we integrate ROVER into state-of-the-art SLAM systems to verify\nits robustness and efficiency. Our source code and self-collected dataset are\navailable at https://github.com/jarvisyjw/ROVER.", "AI": {"tldr": "ROVER是一种利用历史轨迹作为先验约束来验证闭环检测的方法，专门针对重复环境中的误检问题，通过轨迹优化和评分机制来拒绝错误的闭环候选。", "motivation": "在重复性环境中，基于外观特征的闭环检测容易产生误检，而现有方法忽略了机器人的时空运动轨迹这一重要先验知识。误检的闭环对SLAM系统可能是致命的。", "method": "提出ROVER方法：对于每个闭环候选，首先通过位姿图优化估计机器人轨迹，然后通过评分方案评估该轨迹与无闭环时的轨迹先验的符合程度，从而决定是否接受该闭环候选。", "result": "基准测试和真实世界实验证明了该方法的有效性，集成到最先进的SLAM系统中验证了其鲁棒性和效率。", "conclusion": "利用轨迹先验知识作为约束可以有效解决重复环境中闭环验证的挑战，ROVER方法在避免误检方面表现出色，代码和数据集已开源。"}}
{"id": "2508.13513", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13513", "abs": "https://arxiv.org/abs/2508.13513", "authors": ["Maolin Lei", "Edoardo Romiti", "Arturo Laurenzi", "Cheng Zhou", "Wanli Xing", "Liang Lu", "Nikos G. Tsagarakis"], "title": "Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies", "comment": null, "summary": "This work proposes a unified Hierarchical Model Predictive Control (H-MPC)\nfor modular manipulators across various morphologies, as the controller can\nadapt to different configurations to execute the given task without extensive\nparameter tuning in the controller. The H-MPC divides the control process into\ntwo levels: a high-level MPC and a low-level MPC. The high-level MPC predicts\nfuture states and provides trajectory information, while the low-level MPC\nrefines control actions by updating the predictive model based on this\nhigh-level information. This hierarchical structure allows for the integration\nof kinematic constraints and ensures smooth joint-space trajectories, even near\nsingular configurations. Moreover, the low-level MPC incorporates secondary\nlinearization by leveraging predictive information from the high-level MPC,\neffectively capturing the second-order Taylor expansion information of the\nkinematic model while still maintaining a linearized model formulation. This\napproach not only preserves the simplicity of a linear control model but also\nenhances the accuracy of the kinematic representation, thereby improving\noverall control precision and reliability. To validate the effectiveness of the\ncontrol policy, we conduct extensive evaluations across different manipulator\nmorphologies and demonstrate the execution of pick-and-place tasks in\nreal-world scenarios.", "AI": {"tldr": "统一的层次模型预测控制方法，通过高级和低级MPC的分层结构，实现多种机械手槍形态的适配性控制，无需深度参数调整", "motivation": "解决模块化机械手槍在不同形态下的控制适配性问题，避免每次更改配置都需要重新调整控制器参数", "method": "采用两层模型预测控制结构：高级MPC预测未来状态和提供轨迹信息，低级MPC根据高级信息更新预测模型并精炼控制动作，包含二次线性化技术", "result": "在多种机械手槍形态下进行了广泛评估，并在实际场景中演示了摘放任务的执行，验证了控制策略的有效性", "conclusion": "该层次H-MPC方法不仅保持了线性控制模型的简洁性，还提高了运动学表示的准确性，显著提升了整体控制精度和可靠性"}}
{"id": "2508.13531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13531", "abs": "https://arxiv.org/abs/2508.13531", "authors": ["Bolin Li", "Gewei Zuo", "Zhixiang Wang", "Xiaotian Ke", "Lijun Zhu", "Han Ding"], "title": "A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots", "comment": null, "summary": "This paper presents a control framework designed to enhance the stability and\nrobustness of legged robots in the presence of uncertainties, including model\nuncertainties, external disturbances, and faults. The framework enables the\nfull-state feedback estimator to estimate and compensate for uncertainties in\nwhole-body dynamics of the legged robots. First, we propose a novel moving\nhorizon extended state observer (MH-ESO) to estimate uncertainties and mitigate\nnoise in legged systems, which can be integrated into the framework for\ndisturbance compensation. Second, we introduce a three-level whole-body\ndisturbance rejection control framework (T-WB-DRC). Unlike the previous\ntwo-level approach, this three-level framework considers both the plan based on\nwhole-body dynamics without uncertainties and the plan based on dynamics with\nuncertainties, significantly improving payload transportation, external\ndisturbance rejection, and fault tolerance. Third, simulations of both humanoid\nand quadruped robots in the Gazebo simulator demonstrate the effectiveness and\nversatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped\nrobot validate the robustness and stability of the system when using T-WB-DRC\nunder various disturbance conditions.", "AI": {"tldr": "提出一种三层次全身干扰抱抗控制框架(T-WB-DRC)，通过新颖的移动水平扩展状态观测器(MH-ESO)估计不确定性，提高有脚机器人的稳定性和弹性能力。", "motivation": "解决有脚机器人面临的模型不确定性、外部干扰和故障等挑战，提升运载运输、干扰抱抗和故障宽容性能力。", "method": "设计移动水平扩展状态观测器(MH-ESO)估测不确定性并降低噪声，构建三层全身干扰抱抗控制框架(T-WB-DRC)，同时考虑有无不确定性的动力学规划。", "result": "在Gazebo模拟环境中对人形和四足机器人进行了模拟，并在四足机器人上进行了实验室验证，证明框架在各种干扰条件下具有良好的稳定性和强声性。", "conclusion": "T-WB-DRC框架通过加入不确定性估计和补偿机制，显著提升了有脚机器人的弹性能力，为复杂环境下的可靠运行提供了有效解决方案。"}}
{"id": "2508.13534", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13534", "abs": "https://arxiv.org/abs/2508.13534", "authors": ["Chao Tang", "Anxing Xiao", "Yuhong Deng", "Tianrun Hu", "Wenlong Dong", "Hanbo Zhang", "David Hsu", "Hong Zhang"], "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence", "comment": "Accepted to CoRL 2025", "summary": "Imitating tool manipulation from human videos offers an intuitive approach to\nteaching robots, while also providing a promising and scalable alternative to\nlabor-intensive teleoperation data collection for visuomotor policy learning.\nWhile humans can mimic tool manipulation behavior by observing others perform a\ntask just once and effortlessly transfer the skill to diverse tools for\nfunctionally equivalent tasks, current robots struggle to achieve this level of\ngeneralization. A key challenge lies in establishing function-level\ncorrespondences, considering the significant geometric variations among\nfunctionally similar tools, referred to as intra-function variations. To\naddress this challenge, we propose MimicFunc, a framework that establishes\nfunctional correspondences with function frame, a function-centric local\ncoordinate frame constructed with keypoint-based abstraction, for imitating\ntool manipulation skills. Experiments demonstrate that MimicFunc effectively\nenables the robot to generalize the skill from a single RGB-D human video to\nmanipulating novel tools for functionally equivalent tasks. Furthermore,\nleveraging MimicFunc's one-shot generalization capability, the generated\nrollouts can be used to train visuomotor policies without requiring\nlabor-intensive teleoperation data collection for novel objects. Our code and\nvideo are available at https://sites.google.com/view/mimicfunc.", "AI": {"tldr": "MimicFunc是一个通过功能帧建立功能对应关系的框架，使机器人能够从单个人类RGB-D视频中学习工具操作技能，并泛化到新工具上，无需大量遥操作数据收集。", "motivation": "人类能够通过观察一次工具操作就模仿并泛化到功能等效的不同工具上，而现有机器人难以实现这种级别的泛化能力。主要挑战在于处理功能相似工具之间的几何差异（功能内变异）。", "method": "提出MimicFunc框架，使用基于关键点的抽象构建功能中心局部坐标系（功能帧），建立功能级别的对应关系来模仿工具操作技能。", "result": "实验表明MimicFunc能有效使机器人从单个人类RGB-D视频泛化技能到操作新工具执行功能等效任务，生成的轨迹可用于训练视觉运动策略。", "conclusion": "该框架提供了一种直观且可扩展的方法来教授机器人工具操作技能，避免了劳动密集型的遥操作数据收集需求。"}}
{"id": "2508.13699", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13699", "abs": "https://arxiv.org/abs/2508.13699", "authors": ["Maren Raab", "Linda Miller", "Zhe Zeng", "Pascal Jansen", "Martin Baumann", "Johannes Kraus"], "title": "Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation", "comment": null, "summary": "As autonomous robots become more common in public spaces, spontaneous\nencounters with laypersons are more frequent. For this, robots need to be\nequipped with communication strategies that enhance momentary transparency and\nreduce the probability of critical situations. Adapting these robotic\nstrategies requires consideration of robot movements, environmental conditions,\nand user characteristics and states. While numerous studies have investigated\nthe impact of distraction on pedestrians' movement behavior, limited research\nhas examined this behavior in the presence of autonomous robots. This research\naddresses the impact of robot type and robot movement pattern on distracted and\nundistracted pedestrians' movement behavior. In a field setting, unaware\npedestrians were videotaped while moving past two working, autonomous cleaning\nrobots. Out of N=498 observed pedestrians, approximately 8% were distracted by\nsmartphones. Distracted and undistracted pedestrians did not exhibit\nsignificant differences in their movement behaviors around the robots. Instead,\nboth the larger sweeping robot and the offset rectangular movement pattern\nsignificantly increased the number of lateral adaptations compared to the\nsmaller cleaning robot and the circular movement pattern. The offset\nrectangular movement pattern also led to significantly more close lateral\nadaptations. Depending on the robot type, the movement patterns led to\ndifferences in the distances of lateral adaptations. The study provides initial\ninsights into pedestrian movement behavior around an autonomous cleaning robot\nin public spaces, contributing to the growing field of HRI research.", "AI": {"tldr": "研究探讨了机器人类型和移动模式对分心和未分心行人行为的影响，发现分心状态对行人行为无显著差异，但机器人尺寸和移动模式显著影响行人的横向适应行为。", "motivation": "随着自主机器人在公共空间的普及，需要开发能增强瞬时透明度、减少关键情况发生概率的通信策略，但目前缺乏对行人在自主机器人存在时分心行为的研究。", "method": "在实地环境中，对498名不知情的行人进行录像观察，让他们经过两个工作的自主清洁机器人，分析分心（使用手机）和未分心行人的移动行为差异。", "result": "约8%的行人因手机分心，但分心与未分心行人在机器人周围的行为无显著差异。较大的扫地机器人和偏移矩形移动模式显著增加了横向适应次数，偏移矩形模式还导致更多近距离横向适应。", "conclusion": "机器人类型和移动模式比行人分心状态更能影响行人行为，这为公共空间中人机交互研究提供了重要见解，有助于设计更安全的机器人移动策略。"}}
{"id": "2508.13785", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13785", "abs": "https://arxiv.org/abs/2508.13785", "authors": ["Liyang Liu", "Ehsan Mihankhah", "Nathan Wallace", "Javier Martinez", "Andrew J. Hill"], "title": "Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot", "comment": null, "summary": "In open-pit mining, holes are drilled into the surface of the excavation site\nand detonated with explosives to facilitate digging. These blast holes need to\nbe inspected internally for investigation of downhole material types and\nproperties. Knowing these properties can lead to significant savings in\nmaterial handling costs in downstream processes. Manual hole inspection is slow\nand expensive, with major limitations in revealing the geometric and geological\nproperties of the holes and their contents. This has been the motivation for\nthe development of our autonomous mine-site inspection robot - \"DIPPeR\". In\nthis paper, the automation aspect of the project is explained. We present a\nrobust blast hole seeking and detection framework that enables target-based\nnavigation and accurate down-hole sensor positioning. The pipeline first\nprocesses point-cloud data collected by the on-board LiDAR sensors, extracting\nthe cone-shaped volume of drill-waste above the ground. By projecting the 3D\ncone points into a virtual depth image, segmentation is achieved in the 2D\ndomain, yielding a circular hole at the image centre and a collared cone face.\nWe then identify the hole centre using a robust detection module while\nsuppressing non-maximum candidates, ensuring precise sensor placement for\ndown-hole inspection and avoiding collisions with the cavity wall. To enable\nautonomous hole-seeking, the pipeline automatically adjusts its projection\nparameters during robot navigation to account for variations in point sparsity\nand hole opening size, ensuring a consistent hole appearance in 2D images. This\nallows continuous tracking of the target hole as the robot approaches the goal\npoint. We demonstrate the effectiveness of our navigation and perception system\nin both high-fidelity simulation environments and on-site field tests. A\ndemonstration video is available at\n\"https://www.youtube.com/watch?v=fRNbcBcaSqE\".", "AI": {"tldr": "开采矿山爆破孔自主检测机器人DIPPeR，通过LiDAR点云数据处理和虚拟深度图像分割技术，实现了精确的爆破孔寻找和位置定位", "motivation": "传统人工爆破孔检测速度慢、成本高，在揭示孔洞几何和地质特性方面有显著局限性，下游材料处理成本较高", "method": "使用LiDAR采集点云数据，提取地面上方钻孔废料锥体空间，将3D锥体点投影到虚拟深度图像进行2D分割，识别孔洞中心并压制非最大倾向候选者，自动调整投影参数以适应点云稀疏度和孔漏大小变化", "result": "在高保真模拟环境和现场野外测试中验证了导航和感知系统的有效性", "conclusion": "该自主检测机器人系统能够提供精确的爆破孔检测和位置定位，有助于降低开采矿山的材料处理成本"}}
{"id": "2508.13795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13795", "abs": "https://arxiv.org/abs/2508.13795", "authors": ["Haitham El-Hussieny"], "title": "Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control", "comment": null, "summary": "This paper presents a data-driven control framework for quadrotor systems\nthat integrates a deep Koopman operator with model predictive control (DK-MPC).\nThe deep Koopman operator is trained on sampled flight data to construct a\nhigh-dimensional latent representation in which the nonlinear quadrotor\ndynamics are approximated by linear models. This linearization enables the\napplication of MPC to efficiently optimize control actions over a finite\nprediction horizon, ensuring accurate trajectory tracking and stabilization.\nThe proposed DK-MPC approach is validated through a series of\ntrajectory-following and point-stabilization numerical experiments, where it\ndemonstrates superior tracking accuracy and significantly lower computation\ntime compared to conventional nonlinear MPC. These results highlight the\npotential of Koopman-based learning methods to handle complex quadrotor\ndynamics while meeting the real-time requirements of embedded flight control.\nFuture work will focus on extending the framework to more agile flight\nscenarios and improving robustness against external disturbances.", "AI": {"tldr": "深度Koopman算子结合模型预测控制的四旋翼控制框架，通过线性化潜在空间实现高效控制", "motivation": "解决四旋翼非线性动力学导致的控制复杂性问题，提高控制精度和降低计算时间", "method": "使甦深度Koopman算子学习飞行数据，在高维潜在空间构建线性动力学模型，然后集成模型预测控制（MPC）进行有限预测路径优化", "result": "在轨迹跟随和点稳定实验中，方法显示出更高的跟踪精度和显著更低的计算时间，较传统非线性MPC更优", "conclusion": "Koopman基础学习方法能够处理复杂四旋翼动力学，满足嵌入式飞行控制的实时要求，具有很好的应用前景"}}
{"id": "2508.13877", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13877", "abs": "https://arxiv.org/abs/2508.13877", "authors": ["Rathnam Vidushika Rasanji", "Jin Wei-Kocsis", "Jiansong Zhang", "Dongming Gan", "Ragu Athinarayanan", "Paul Asunda"], "title": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer", "comment": null, "summary": "Reinforcement learning (RL) has demonstrated great potential in robotic\noperations. However, its data-intensive nature and reliance on the Markov\nDecision Process (MDP) assumption limit its practical deployment in real-world\nscenarios involving complex dynamics and long-term temporal dependencies, such\nas multi-robot manipulation. Decision Transformers (DTs) have emerged as a\npromising offline alternative by leveraging causal transformers for sequence\nmodeling in RL tasks. However, their applications to multi-robot manipulations\nstill remain underexplored. To address this gap, we propose a novel framework,\nSymbolically-Guided Decision Transformer (SGDT), which integrates a\nneuro-symbolic mechanism with a causal transformer to enable deployable\nmulti-robot collaboration. In the proposed SGDT framework, a neuro-symbolic\nplanner generates a high-level task-oriented plan composed of symbolic\nsubgoals. Guided by these subgoals, a goal-conditioned decision transformer\n(GCDT) performs low-level sequential decision-making for multi-robot\nmanipulation. This hierarchical architecture enables structured, interpretable,\nand generalizable decision making in complex multi-robot collaboration tasks.\nWe evaluate the performance of SGDT across a range of task scenarios, including\nzero-shot and few-shot scenarios. To our knowledge, this is the first work to\nexplore DT-based technology for multi-robot manipulation.", "AI": {"tldr": "提出了SGDT框架，结合神经符号机制和因果变换器，用于多机器人协作操作任务", "motivation": "强化学习在机器人操作中数据密集且依赖MDP假设，难以处理复杂动态和长期依赖的多机器人操作任务。决策变换器作为离线替代方案，但在多机器人操作中的应用尚未充分探索", "method": "SGDT框架包含神经符号规划器生成符号子目标的高级任务导向计划，以及目标条件决策变换器进行低级序列决策的层次架构", "result": "在零样本和少样本场景下评估了SGDT的性能，表明该框架能够实现结构化、可解释和可泛化的多机器人协作决策", "conclusion": "这是首个探索基于决策变换器技术在多机器人操作中的应用工作，SGDT为解决复杂多机器人协作任务提供了有效解决方案"}}
{"id": "2508.13881", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13881", "abs": "https://arxiv.org/abs/2508.13881", "authors": ["Zhaokun Chen", "Chaopeng Zhang", "Xiaohan Li", "Wenshuo Wang", "Gentiane Venture", "Junqiang Xi"], "title": "Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models", "comment": null, "summary": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.", "AI": {"tldr": "通过大语言模型生成语义特权信息，将人类专家的理解模式融入驾驶风格识别系统，在保持计算效率的同时显著提升识别准确性", "motivation": "现有驾驶风格识别系统依赖低级传感器特征，缺乏人类专家的语义理解能力，导致算法分类与专家判断之间的差异", "method": "首先开发DriBehavGPT模块用于生成自然语言驾驶行为描述，通过文本嵌入和降维抄码为机器学习可处理表征，然后作为特权信息融入SVM+进行训练", "result": "在多种真实驾驶场景中，该框架在F1分数上比传统方法提升7.6%（跟车）和7.9%（变道），训练后推理仅需传感器数据", "conclusion": "语义行为表征在提高识别准确性和推进可解释性方面发挥关键作用，为构建人本中心的驾驶系统提供新方向"}}
{"id": "2508.13901", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13901", "abs": "https://arxiv.org/abs/2508.13901", "authors": ["Yihao Lu", "Hao Tang"], "title": "Multimodal Data Storage and Retrieval for Embodied AI: A Survey", "comment": null, "summary": "Embodied AI (EAI) agents continuously interact with the physical world,\ngenerating vast, heterogeneous multimodal data streams that traditional\nmanagement systems are ill-equipped to handle. In this survey, we first\nsystematically evaluate five storage architectures (Graph Databases,\nMulti-Model Databases, Data Lakes, Vector Databases, and Time-Series\nDatabases), focusing on their suitability for addressing EAI's core\nrequirements, including physical grounding, low-latency access, and dynamic\nscalability. We then analyze five retrieval paradigms (Fusion Strategy-Based\nRetrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based\nRetrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based\nOptimization), revealing a fundamental tension between achieving long-term\nsemantic coherence and maintaining real-time responsiveness. Based on this\ncomprehensive analysis, we identify key bottlenecks, spanning from the\nfoundational Physical Grounding Gap to systemic challenges in cross-modal\nintegration, dynamic adaptation, and open-world generalization. Finally, we\noutline a forward-looking research agenda encompassing physics-aware data\nmodels, adaptive storage-retrieval co-optimization, and standardized\nbenchmarking, to guide future research toward principled data management\nsolutions for EAI. Our survey is based on a comprehensive review of more than\n180 related studies, providing a rigorous roadmap for designing the robust,\nhigh-performance data management frameworks essential for the next generation\nof autonomous embodied systems.", "AI": {"tldr": "本调研调查了体现式AI系统的数据管理挑战，系统评估五种存储架构和五种检索范式，识别了关键瓶颈并提出了未来研究议程。", "motivation": "体现式AI以不断产生巨量、异构的多模态数据流，传统数据管理系统无法有效处理这些数据的存储和检索需求。", "method": "系统性评估五种存储架构（图数据库、多模型数据库、数据湖、向量数据库、时间序列数据库）和五种检索范式，基于对180多份相关研究的全面评审。", "result": "发现了实现长期语义一致性与保持实时响应能力之间的根本强张力，识别了从基础物理基准差距到系统性挑战的关键瓶颈。", "conclusion": "提出了包含物理感知数据模型、适应性存储-检索协同优化和标准化测试基准在内的前瞻性研究议程，为下一代自主体现系统的数据管理框架提供了严谨的路线图。"}}
{"id": "2508.13964", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13964", "abs": "https://arxiv.org/abs/2508.13964", "authors": ["Martijn Cramer", "Yanming Wu", "David De Schepper", "Eric Demeester"], "title": "Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation", "comment": "13 pages, 25 figures", "summary": "Due to high-mix-low-volume production, sheet-metal workshops today are\nchallenged by small series and varying orders. As standard automation solutions\ntend to fall short, SMEs resort to repetitive manual labour impacting\nproduction costs and leading to tech-skilled workforces not being used to their\nfull potential. The COOCK+ ROBUST project aims to transform cobots into mobile\nand reconfigurable production assistants by integrating existing technologies,\nincluding 3D object recognition and localisation. This article explores both\nthe opportunities and challenges of enhancing cobotic systems with these\ntechnologies in an industrial setting, outlining the key steps involved in the\nprocess. Additionally, insights from a past project, carried out by the ACRO\nresearch unit in collaboration with an industrial partner, serves as a concrete\nimplementation example throughout.", "AI": {"tldr": "通过3D物体识别与定位技术提升协作机器人的机动性和重配能力，应对小批量多种类的生产挑战", "motivation": "中小企业面临小批量多种类生产带来的挑战，标准自动化方案无法满足需求，导致人工劳动成本高和技能人才浪费", "method": "集成现有技术（包括3D物体识别和定位）将协作机器人转化为移动和可重配的生产助手，并通过与产业合作伙伴的实际项目进行验证", "result": "提出了在工业环境中增强协作机器人系统的具体方法和关键步骤，并提供了实际实施案例", "conclusion": "通过技术集成和重配能力的提升，协作机器人可以成为有效解决中小企业小批量多种类生产挑战的方案"}}
{"id": "2508.13976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13976", "abs": "https://arxiv.org/abs/2508.13976", "authors": ["Carlo Mazzola", "Hassan Ali", "Kristína Malinovská", "Igor Farkaš"], "title": "Toward an Interaction-Centered Approach to Robot Trustworthiness", "comment": "4 pages, presented at TRUST workshop, organised in conjunction with\n  the IEEE RO-MAN 2025 conference, held in Eindhoven, Netherlands", "summary": "As robots get more integrated into human environments, fostering\ntrustworthiness in embodied robotic agents becomes paramount for an effective\nand safe human-robot interaction (HRI). To achieve that, HRI applications must\npromote human trust that aligns with robot skills and avoid misplaced trust or\novertrust, which can pose safety risks and ethical concerns. To achieve that,\nHRI applications must promote human trust that aligns with robot skills and\navoid misplaced trust or overtrust, which can pose safety risks and ethical\nconcerns. In this position paper, we outline an interaction-based framework for\nbuilding trust through mutual understanding between humans and robots. We\nemphasize two main pillars: human awareness and transparency, referring to the\nrobot ability to interpret human actions accurately and to clearly communicate\nits intentions and goals, respectively. By integrating these two pillars,\nrobots can behave in a manner that aligns with human expectations and needs\nwhile providing their human partners with both comprehension and control over\ntheir actions. We also introduce four components that we think are important\nfor bridging the gap between a human-perceived sense of trust and a robot true\ncapabilities.", "AI": {"tldr": "通过互动框架建立人机信任，强调人类意识和透明性两大支枱，以避免错误信任和过度信任带来的安全风险", "motivation": "随着机器人更多积成到人类环境中，建立可信赖的体现型机器人成为致命关键，以保证有效而安全的人机交互", "method": "提出一种基于互相理解的交互框架，包含人类意识（机器人准确解释人类行为的能力）和透明性（清晰传达机器人意图和目标）两大核心支枱", "result": "让机器人能够按照人类期望和需求行动，同时为人类伙伴提供对机器人行为的理解和控制权，缩小人类感知信任与机器人真实能力之间的差距", "conclusion": "通过人类意识和透明性的集成，可以建立一种基于相互理解的人机信任关系，这对于实现有效、安全的人机交互至关重要"}}
{"id": "2508.13982", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA", "I.2.9; I.2"], "pdf": "https://arxiv.org/pdf/2508.13982", "abs": "https://arxiv.org/abs/2508.13982", "authors": ["Sydney Thompson", "Kate Candon", "Marynel Vázquez"], "title": "The Social Context of Human-Robot Interactions", "comment": "To be published in Annual Review of Control, Robotics, and Autonomous\n  Systems", "summary": "The Human-Robot Interaction (HRI) community often highlights the social\ncontext of an interaction as a key consideration when designing, implementing,\nand evaluating robot behavior. Unfortunately, researchers use the term \"social\ncontext\" in varied ways. This can lead to miscommunication, making it\nchallenging to draw connections between related work on understanding and\nmodeling the social contexts of human-robot interactions. To address this gap,\nwe survey the HRI literature for existing definitions and uses of the term\n\"social context\". Then, we propose a conceptual model for describing the social\ncontext of a human-robot interaction. We apply this model to existing work, and\nwe discuss a range of attributes of social contexts that can help researchers\nplan for interactions, develop behavior models for robots, and gain insights\nafter interactions have taken place. We conclude with a discussion of open\nresearch questions in relation to understanding and modeling the social\ncontexts of human-robot interactions.", "AI": {"tldr": "这篇论文针对HRI领域中\"社会情境\"术语使用混乱的问题，提出了一个概念模型来统一描述人机交互的社会情境，并讨论了相关属性和开放研究问题。", "motivation": "HRI研究中对\"社会情境\"的定义和使用方式不一致，导致沟通困难和研究成果难以整合，需要建立统一的概念框架。", "method": "通过文献调查分析现有\"社会情境\"定义，提出概念模型来描述人机交互的社会情境，并将该模型应用于现有研究工作。", "result": "开发了一个能够帮助研究人员规划交互、开发机器人行为模型以及分析交互后洞察的社会情境概念模型，并识别了相关的社会情境属性。", "conclusion": "论文为解决HRI领域社会情境术语混乱问题提供了概念框架，并指出了未来在理解和建模人机交互社会情境方面的开放研究问题。"}}
{"id": "2508.13998", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13998", "abs": "https://arxiv.org/abs/2508.13998", "authors": ["Yifu Yuan", "Haiqin Cui", "Yaoting Huang", "Yibin Chen", "Fei Ni", "Zibin Dong", "Pengyi Li", "Yan Zheng", "Jianye Hao"], "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation", "comment": "Embodied-R1 technical report", "summary": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.", "AI": {"tldr": "通过统一的指点表示作为中间表征，设计Embodied-R1 3B视觉-语言模型，使用两阶段强化学习诞练训练，在11个体现空间指点测试中达到最佳性能，并显示出凶强的零样本沿生能力。", "motivation": "解决体现AI中的\"看到到做到\"问题，该问题来自于数据稀缺和体现异质性。", "method": "提出指点作为统一的体现无关中间表示，建立Embodied-Points-200K大规模数据集，使用两阶段强化学习诞练(RFT)训练Embodied-R1 3B VLM模型。", "result": "在11个体现空间指点测试中达到最佳性能，在SIMPLEREnv中达到56.2%成功率，在8个实际XArm任务中达到87.5%成功率，比基线提升62%，并显示出对视觉干扰的高稳健性。", "conclusion": "指点为中心的表示结合RFT训练范式，为闭合机器人感知-动作间隔提供了有效且可普遍化的路径。"}}
{"id": "2508.14042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14042", "abs": "https://arxiv.org/abs/2508.14042", "authors": ["Zhuoling Li", "Xiaoyang Wu", "Zhenhua Xu", "Hengshuang Zhao"], "title": "Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation", "comment": null, "summary": "Realizing generalizable dynamic object manipulation is important for\nenhancing manufacturing efficiency, as it eliminates specialized engineering\nfor various scenarios. To this end, imitation learning emerges as a promising\nparadigm, leveraging expert demonstrations to teach a policy manipulation\nskills. Although the generalization of an imitation learning policy can be\nimproved by increasing demonstrations, demonstration collection is\nlabor-intensive. To address this problem, this paper investigates whether\nstrong generalization in dynamic object manipulation is achievable with only a\nfew demonstrations. Specifically, we develop an entropy-based theoretical\nframework to quantify the optimization of imitation learning. Based on this\nframework, we propose a system named Generalizable Entropy-based Manipulation\n(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM\ncan generalize across diverse environment backgrounds, robot embodiments,\nmotion dynamics, and object geometries. Notably, GEM has been deployed in a\nreal canteen for tableware collection. Without any in-scene demonstration, it\nachieves a success rate of over 97% across more than 10,000 operations.", "AI": {"tldr": "提出GEM系统，通过基于熵的理论框架实现仅需少量演示就能在动态物体操作中实现强泛化能力", "motivation": "解决动态物体操作中演示数据收集成本高的问题，探索仅用少量演示就能实现强泛化的可能性", "method": "开发基于熵的理论框架来量化模仿学习的优化，并基于此提出GEM系统", "result": "在仿真和真实任务中，GEM能够泛化到不同的环境背景、机器人形态、运动动力学和物体几何形状。在真实食堂餐具收集任务中，无需场景内演示就实现了超过97%的成功率，完成超过10,000次操作", "conclusion": "GEM系统证明了仅用少量演示就能在动态物体操作中实现强泛化能力，为提升制造效率提供了有效解决方案"}}
