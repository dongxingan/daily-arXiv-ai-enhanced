<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 52]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Robust control for multi-legged elongate robots in noisy environments](https://arxiv.org/abs/2506.15788)
*Baxi Chong,Juntao He,Daniel Irvine,Tianyu Wang,Esteban Flores,Daniel Soto,Jianfeng Lin,Zhaochen Xu,Vincent R Nienhusser,Grigoriy Blekherman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 提出了一种新范式，通过将机器人-环境交互与通信理论结合，开发了多足细长机器人（MERs）的稳健控制方案，实现了在复杂地形中的高效运动。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统依赖高带宽传感和计算，且训练泛化性差，需要一种更通用、稳健的控制方法。

Method: 将每条腿与地面的接触视为基本主动接触（bac），类比信号传输中的比特，通过冗余和被动机械响应实现稳健运动，并结合反馈控制（CI）增强性能。

Result: MERs在复杂地形中表现出高效可靠的运动性能（每周期约半个身长），地形噪声超过机器人高度的两倍。

Conclusion: 该研究为MERs控制提供了系统化开发基础，推动了地形无关、敏捷且稳健的机器人系统发展。

Abstract: Modern two and four legged robots exhibit impressive mobility on complex
terrain, largely attributed to advancement in learning algorithms. However,
these systems often rely on high-bandwidth sensing and onboard computation to
perceive/respond to terrain uncertainties. Further, current locomotion
strategies typically require extensive robot-specific training, limiting their
generalizability across platforms. Building on our prior research connecting
robot-environment interaction and communication theory, we develop a new
paradigm to construct robust and simply controlled multi-legged elongate robots
(MERs) capable of operating effectively in cluttered, unstructured
environments. In this framework, each leg-ground contact is thought of as a
basic active contact (bac), akin to bits in signal transmission. Reliable
locomotion can be achieved in open-loop on "noisy" landscapes via sufficient
redundancy in bacs. In such situations, robustness is achieved through passive
mechanical responses. We term such processes as those displaying mechanical
intelligence (MI) and analogize these processes to forward error correction
(FEC) in signal transmission. To augment MI, we develop feedback control
schemes, which we refer to as computational intelligence (CI) and such
processes analogize automatic repeat request (ARQ) in signal transmission.
Integration of these analogies between locomotion and communication theory
allow analysis, design, and prediction of embodied intelligence control schemes
(integrating MI and CI) in MERs, showing effective and reliable performance
(approximately half body lengths per cycle) on complex landscapes with terrain
"noise" over twice the robot's height. Our work provides a foundation for
systematic development of MER control, paving the way for terrain-agnostic,
agile, and resilient robotic systems capable of operating in extreme
environments.

</details>


### [2] [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
*Andrew Wagenmaker,Mitsuhiko Nakamoto,Yunchu Zhang,Seohong Park,Waleed Yagoub,Anusha Nagabandi,Abhishek Gupta,Sergey Levine*

Main category: cs.RO

TL;DR: 该论文提出了一种名为DSRL的方法，通过强化学习在行为克隆（BC）策略的潜在噪声空间中进行调整，以实现快速自主适应。DSRL高效且无需修改基础策略权重。


<details>
  <summary>Details</summary>
Motivation: 行为克隆策略在初始性能不足时需要额外的人类演示，成本高且耗时。强化学习虽能自主改进策略，但样本效率低。DSRL旨在结合两者优势。

Method: 提出DSRL方法，在扩散策略（一种先进的BC方法）的潜在噪声空间中运行强化学习，实现高效自主适应。

Result: DSRL在模拟和真实机器人任务中表现出高样本效率和有效性能改进，且无需修改基础策略权重。

Conclusion: DSRL为行为克隆策略的快速自主适应提供了一种高效且实用的解决方案。

Abstract: Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.

</details>


### [3] [Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning](https://arxiv.org/abs/2506.15828)
*Emanuele Musumeci,Michele Brienza,Francesco Argenziano,Vincenzo Suriani,Daniele Nardi,Domenico D. Bloisi*

Main category: cs.RO

TL;DR: 论文提出了一种结合经典规划与大型语言模型（LLMs）的方法，以解决机器人规划中的适应性和可行性问题。


<details>
  <summary>Details</summary>
Motivation: 传统规划方法（如PDDL）在真实场景中因感知限制和难以将感知映射到规划谓词而表现不佳，而LLMs虽然能利用常识推理，但常生成不可行或不安全的计划。

Method: 通过分层规划框架，结合LLMs的常识推理能力，逐步放松任务目标，使其适应机器人具体环境。

Result: 方法在3D场景图中表现出高效的任务适应和执行能力，优于其他基准方法。

Conclusion: 该方法有效整合了经典规划与LLMs的优势，提升了机器人在复杂场景中的规划能力。

Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting
from imperative to declarative approaches (e.g., PDDL). However, these methods
often fail in real scenarios due to limited robot perception and the need to
ground perceptions to planning predicates. This often results in heavily
hard-coded behaviors that struggle to adapt, even with scenarios where goals
can be achieved through relaxed planning. Meanwhile, Large Language Models
(LLMs) lead to planning systems that leverage commonsense reasoning but often
at the cost of generating unfeasible and/or unsafe plans. To address these
limitations, we present an approach integrating classical planning with LLMs,
leveraging their ability to extract commonsense knowledge and ground actions.
We propose a hierarchical formulation that enables robots to make unfeasible
tasks tractable by defining functionally equivalent goals through gradual
relaxation. This mechanism supports partial achievement of the intended
objective, suited to the agent's specific context. Our method demonstrates its
ability to adapt and execute tasks effectively within environments modeled
using 3D Scene Graphs through comprehensive qualitative and quantitative
evaluations. We also show how this method succeeds in complex scenarios where
other benchmark methods are more likely to fail. Code, dataset, and additional
material are released to the community.

</details>


### [4] [SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation](https://arxiv.org/abs/2506.15847)
*Arpit Bahety,Arnav Balaji,Ben Abbatematteo,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: SafeMimic框架通过单次人类视频演示，让机器人安全自主地学习移动操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从人类视频中学习新任务时的视角转换、形态适应和安全执行问题。

Method: 解析视频为语义和动作段，转换为第一人称视角，采样候选动作并通过安全验证执行。

Result: 实验表明，SafeMimic能在不同环境和用户下安全高效地学习任务，优于现有方法。

Conclusion: SafeMimic为机器人学习复杂任务提供了一种安全、高效的解决方案。

Abstract: For robots to become efficient helpers in the home, they must learn to
perform new mobile manipulation tasks simply by watching humans perform them.
Learning from a single video demonstration from a human is challenging as the
robot needs to first extract from the demo what needs to be done and how,
translate the strategy from a third to a first-person perspective, and then
adapt it to be successful with its own morphology. Furthermore, to mitigate the
dependency on costly human monitoring, this learning process should be
performed in a safe and autonomous manner. We present SafeMimic, a framework to
learn new mobile manipulation skills safely and autonomously from a single
third-person human video. Given an initial human video demonstration of a
multi-step mobile manipulation task, SafeMimic first parses the video into
segments, inferring both the semantic changes caused and the motions the human
executed to achieve them and translating them to an egocentric reference. Then,
it adapts the behavior to the robot's own morphology by sampling candidate
actions around the human ones, and verifying them for safety before execution
in a receding horizon fashion using an ensemble of safety Q-functions trained
in simulation. When safe forward progression is not possible, SafeMimic
backtracks to previous states and attempts a different sequence of actions,
adapting both the trajectory and the grasping modes when required for its
morphology. As a result, SafeMimic yields a strategy that succeeds in the
demonstrated behavior and learns task-specific actions that reduce exploration
in future attempts. Our experiments show that our method allows robots to
safely and efficiently learn multi-step mobile manipulation behaviors from a
single human demonstration, from different users, and in different
environments, with improvements over state-of-the-art baselines across seven
tasks

</details>


### [5] [PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps](https://arxiv.org/abs/2506.15849)
*Kirill Muravyev,Vasily Yuryev,Oleg Bulichev,Dmitry Yudin,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: PRISM-Loc是一种基于拓扑地图的定位方法，适用于大范围环境，通过全局地点识别和局部位姿估计实现高效定位。


<details>
  <summary>Details</summary>
Motivation: 在长距离导航中，实时定位和全局激光雷达地图的高内存需求是挑战，拓扑地图提供了一种解决方案。

Method: 采用双重定位流程：全局地点识别和局部位姿估计，后者使用基于2D特征和点优化的激光雷达扫描匹配算法。

Result: 在3公里路线的ITLP-Campus数据集上，PRISM-Loc在质量和计算效率上均优于现有方法。

Conclusion: PRISM-Loc是一种高效且高质量的定位方法，适用于大范围环境。

Abstract: Localization in the environment is one of the crucial tasks of navigation of
a mobile robot or a self-driving vehicle. For long-range routes, performing
localization within a dense global lidar map in real time may be difficult, and
the creation of such a map may require much memory. To this end, leveraging
topological maps may be useful. In this work, we propose PRISM-Loc -- a
topological map-based approach for localization in large environments. The
proposed approach leverages a twofold localization pipeline, which consists of
global place recognition and estimation of the local pose inside the found
location. For local pose estimation, we introduce an original lidar scan
matching algorithm, which is based on 2D features and point-based optimization.
We evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and
compare it against the state-of-the-art metric map-based and place
recognition-based competitors. The results of the experiments show that the
proposed method outperforms its competitors both quality-wise and
computationally-wise.

</details>


### [6] [A Small-Scale Robot for Autonomous Driving: Design, Challenges, and Best Practices](https://arxiv.org/abs/2506.15870)
*Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: 本文探讨了六分之一比例的小型自动驾驶车辆平台的设计、硬件与软件集成，以及开发中的常见挑战，旨在提升其可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 小型自动驾驶车辆平台成本低，但某些配置未被充分研究，限制了其潜力。本文希望通过分享经验，推动该领域的研究。

Method: 介绍了六分之一比例平台的设计、硬件与软件集成方法，并提出了解决机械和电子问题的方案。

Result: 提出了提升小型车辆平台可靠性和性能的指导原则。

Conclusion: 通过分享经验，本文旨在扩展小型车辆平台在自动驾驶算法测试中的应用，并鼓励进一步研究。

Abstract: Small-scale autonomous vehicle platforms provide a cost-effective environment
for developing and testing advanced driving systems. However, specific
configurations within this scale are underrepresented, limiting full awareness
of their potential. This paper focuses on a one-sixth-scale setup, offering a
high-level overview of its design, hardware and software integration, and
typical challenges encountered during development. We discuss methods for
addressing mechanical and electronic issues common to this scale and propose
guidelines for improving reliability and performance. By sharing these
insights, we aim to expand the utility of small-scale vehicles for testing
autonomous driving algorithms and to encourage further research in this domain.

</details>


### [7] [Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles](https://arxiv.org/abs/2506.15851)
*Qiyuan Wu,Mark Campbell*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶视觉定位的轻量级传感器误差模型，通过学习图像特征和语义信息映射到二维误差分布，实现不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传感器测量与深度学习网络的不确定性量化对机器人系统（如自动驾驶汽车）至关重要，尤其是在安全关键应用中。

Method: 开发了一种基于图像特征和语义信息的轻量级传感器误差模型，用于预测测量误差分布。

Result: 在Ithaca365数据集上验证了方法的准确性，结果显示在恶劣天气和光照条件下，测量误差不符合高斯分布，而高斯混合模型表现更好。

Conclusion: 该方法能够隐式捕捉未标注的关键因素（如场景类型、季节变化），为自动驾驶提供更准确的不确定性估计。

Abstract: The uncertainty quantification of sensor measurements coupled with deep
learning networks is crucial for many robotics systems, especially for
safety-critical applications such as self-driving cars. This paper develops an
uncertainty quantification approach in the context of visual localization for
autonomous driving, where locations are selected based on images. Key to our
approach is to learn the measurement uncertainty using light-weight sensor
error model, which maps both image feature and semantic information to
2-dimensional error distribution. Our approach enables uncertainty estimation
conditioned on the specific context of the matched image pair, implicitly
capturing other critical, unannotated factors (e.g., city vs highway, dynamic
vs static scenes, winter vs summer) in a latent manner. We demonstrate the
accuracy of our uncertainty prediction framework using the Ithaca365 dataset,
which includes variations in lighting and weather (sunny, night, snowy). Both
the uncertainty quantification of the sensor+network is evaluated, along with
Bayesian localization filters using unique sensor gating method. Results show
that the measurement error does not follow a Gaussian distribution with poor
weather and lighting conditions, and is better predicted by our Gaussian
Mixture model.

</details>


### [8] [Advancing Autonomous Racing: A Comprehensive Survey of the RoboRacer (F1TENTH) Platform](https://arxiv.org/abs/2506.15899)
*Israel Charles,Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: 本文综述了RoboRacer（F1TENTH）平台作为自动驾驶研究的领先测试平台，分析了其硬件和软件架构、研究应用及教育价值，并探讨了其在仿真到现实（Sim2Real）转换、算法开发和竞赛中的作用。


<details>
  <summary>Details</summary>
Motivation: RoboRacer平台因其可扩展性、成本效益和社区驱动特性成为自动驾驶研究的重要工具，本文旨在全面评估其贡献和潜力。

Method: 通过分析平台的模块化架构、仿真环境集成、标准化数据集及算法进展，结合全球竞赛和合作研究的案例，进行综合调查。

Result: 研究发现RoboRacer是一个多功能框架，能加速创新并弥合理论研究与实际部署之间的差距。

Conclusion: RoboRacer在自动驾驶赛车和机器人领域具有重要推动作用，为未来研究提供了坚实基础。

Abstract: The RoboRacer (F1TENTH) platform has emerged as a leading testbed for
advancing autonomous driving research, offering a scalable, cost-effective, and
community-driven environment for experimentation. This paper presents a
comprehensive survey of the platform, analyzing its modular hardware and
software architecture, diverse research applications, and role in autonomous
systems education. We examine critical aspects such as bridging the
simulation-to-reality (Sim2Real) gap, integration with simulation environments,
and the availability of standardized datasets and benchmarks. Furthermore, the
survey highlights advancements in perception, planning, and control algorithms,
as well as insights from global competitions and collaborative research
efforts. By consolidating these contributions, this study positions RoboRacer
as a versatile framework for accelerating innovation and bridging the gap
between theoretical research and real-world deployment. The findings underscore
the platform's significance in driving forward developments in autonomous
racing and robotics.

</details>


### [9] [Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples](https://arxiv.org/abs/2506.15865)
*Viral Rasik Galaiya*

Main category: cs.RO

TL;DR: 论文探讨了在非结构化环境中使用触觉传感和强化学习来改进机器人抓取和移动物体的方法。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需要更高的环境感知能力以应对不确定性。尽管相机在机器人任务中占主导地位，但其局限性（如遮挡和视野限制）促使研究转向触觉传感。

Method: 利用触觉传感通过时间特征确定物体姿态；结合强化学习和触觉碰撞减少抓取尝试次数；使用触觉信息指导强化学习代理规划轨迹，并通过人类示例减少训练时间。

Result: 方法有效减少了因相机估计位置不确定性导致的抓取尝试次数，并优化了物体移动轨迹的训练效率。

Conclusion: 触觉传感与强化学习的结合为机器人在复杂环境中的操作提供了更高效的解决方案。

Abstract: To use robots in more unstructured environments, we have to accommodate for
more complexities. Robotic systems need more awareness of the environment to
adapt to uncertainty and variability. Although cameras have been predominantly
used in robotic tasks, the limitations that come with them, such as occlusion,
visibility and breadth of information, have diverted some focus to tactile
sensing. In this thesis, we explore the use of tactile sensing to determine the
pose of the object using the temporal features. We then use reinforcement
learning with tactile collisions to reduce the number of attempts required to
grasp an object resulting from positional uncertainty from camera estimates.
Finally, we use information provided by these tactile sensors to a
reinforcement learning agent to determine the trajectory to take to remove an
object from a restricted passage while reducing training time by pertaining
from human examples.

</details>


### [10] [Full-Pose Tracking via Robust Control for Over-Actuated Multirotors](https://arxiv.org/abs/2506.16427)
*Mohamad Hachem,Clément Roos,Thierry Miquel,Murat Bronz*

Main category: cs.RO

TL;DR: 本文提出了一种针对过驱动多旋翼的级联控制架构，结合INDI和结构化H_inf控制，通过几何引导控制分配实现精确跟踪。


<details>
  <summary>Details</summary>
Motivation: 扩展INDI和H_inf控制的应用范围至过驱动多旋翼，解决姿态和位置跟踪的精确性与鲁棒性问题。

Method: 采用加权最小二乘几何引导控制分配方法，将其建模为二次优化问题，实现全姿态跟踪。

Result: 数值模拟验证了方法在过驱动六旋翼上的有效性，适应性强且具备实际应用潜力。

Conclusion: 该方法成功解决了不可行姿态参考和抗干扰等挑战，适用于多样化任务场景。

Abstract: This paper presents a robust cascaded control architecture for over-actuated
multirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)
control combined with structured H_inf control, initially proposed for
under-actuated multirotors, to a broader range of multirotor configurations. To
achieve precise and robust attitude and position tracking, we employ a weighted
least-squares geometric guidance control allocation method, formulated as a
quadratic optimization problem, enabling full-pose tracking. The proposed
approach effectively addresses key challenges, such as preventing infeasible
pose references and enhancing robustness against disturbances, as well as
considering multirotor's actual physical limitations. Numerical simulations
with an over-actuated hexacopter validate the method's effectiveness,
demonstrating its adaptability to diverse mission scenarios and its potential
for real-world aerial applications.

</details>


### [11] [CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction](https://arxiv.org/abs/2506.15868)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jia Hu,Jiaqi Ma*

Main category: cs.RO

TL;DR: 论文提出了一种基于V2X的风险量化框架CooperRisk，通过多智能体感知信息融合和未来时间戳的风险量化，解决了单车辆系统在复杂场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 单车辆系统在复杂和密集场景中感知范围有限且易受遮挡，而V2X技术虽能共享感知信息，但如何确保风险可解释性并理解多智能体交互仍是一个开放问题。

Method: 设计了基于Transformer的风险导向预测模型，融合多模态和多智能体信息，确保场景一致性和避免冲突预测，生成时间风险地图以指导规划。

Result: 在真实V2X数据集V2XPnP上评估，冲突率降低了44.35%，证明了其在风险量化中的优越性能。

Conclusion: CooperRisk框架通过多智能体协作显著提升了风险量化的准确性和可解释性，为自动驾驶安全提供了有效解决方案。

Abstract: Risk quantification is a critical component of safe autonomous driving,
however, constrained by the limited perception range and occlusion of
single-vehicle systems in complex and dense scenarios. Vehicle-to-everything
(V2X) paradigm has been a promising solution to sharing complementary
perception information, nevertheless, how to ensure the risk interpretability
while understanding multi-agent interaction with V2X remains an open question.
In this paper, we introduce the first V2X-enabled risk quantification pipeline,
CooperRisk, to fuse perception information from multiple agents and quantify
the scenario driving risk in future multiple timestamps. The risk is
represented as a scenario risk map to ensure interpretability based on risk
severity and exposure, and the multi-agent interaction is captured by the
learning-based cooperative prediction model. We carefully design a
risk-oriented transformer-based prediction model with multi-modality and
multi-agent considerations. It aims to ensure scene-consistent future behaviors
of multiple agents and avoid conflicting predictions that could lead to overly
conservative risk quantification and cause the ego vehicle to become overly
hesitant to drive. Then, the temporal risk maps could serve to guide a model
predictive control planner. We evaluate the CooperRisk pipeline in a real-world
V2X dataset V2XPnP, and the experiments demonstrate its superior performance in
risk quantification, showing a 44.35% decrease in conflict rate between the ego
vehicle and background traffic participants.

</details>


### [12] [eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles](https://arxiv.org/abs/2506.16535)
*Tyler Landle,Jordan Rapp,Dean Blank,Chandramouli Amarnath,Abhijit Chatterjee,Alex Daglis,Umakishore Ramachandran*

Main category: cs.RO

TL;DR: 论文提出eCAV平台，用于高效评估自动驾驶车辆的控制算法和V2X技术，支持256辆无感知车辆或64辆有感知车辆的模拟，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆普及，提升道路安全（如避免碰撞和减少附带损害）变得至关重要，而V2X技术是实现这一目标的关键。目前缺乏能有效评估大规模自动驾驶车辆场景的框架。

Method: 提出eCAV平台，该平台高效、模块化且可扩展，支持功能验证和性能预测，包括V2X技术和未来车辆到边缘控制算法。

Result: eCAV可模拟256辆无感知车辆或64辆有感知车辆，性能分别提升8倍和4倍，且速度快1.5倍。

Conclusion: eCAV为自动驾驶车辆控制算法和V2X技术的评估提供了高效且可扩展的解决方案，填补了现有框架的不足。

Abstract: As autonomous vehicles edge closer to widespread adoption, enhancing road
safety through collision avoidance and minimization of collateral damage
becomes imperative. Vehicle-to-everything (V2X) technologies, which include
vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud
(V2C), are being proposed as mechanisms to achieve this safety improvement.
  Simulation-based testing is crucial for early-stage evaluation of Connected
Autonomous Vehicle (CAV) control systems, offering a safer and more
cost-effective alternative to real-world tests. However, simulating large 3D
environments with many complex single- and multi-vehicle sensors and
controllers is computationally intensive. There is currently no evaluation
framework that can effectively evaluate realistic scenarios involving large
numbers of autonomous vehicles.
  We propose eCAV -- an efficient, modular, and scalable evaluation platform to
facilitate both functional validation of algorithmic approaches to increasing
road safety, as well as performance prediction of algorithms of various V2X
technologies, including a futuristic Vehicle-to-Edge control plane and
correspondingly designed control algorithms. eCAV can model up to 256 vehicles
running individual control algorithms without perception enabled, which is
$8\times$ more vehicles than what is possible with state-of-the-art
alternatives. %faster than state-of-the-art alternatives that can simulate
$8\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64
vehicles with a step time under 800ms, which is $4\times$ more and $1.5\times$
faster than the state-of-the-art OpenCDA framework.

</details>


### [13] [Agile, Autonomous Spacecraft Constellations with Disruption Tolerant Networking to Monitor Precipitation and Urban Floods](https://arxiv.org/abs/2506.16537)
*Sreeja Roy-Singh,Alan P. Li,Vinay Ravindra,Roderick Lammers,Marc Sanchez Net*

Main category: cs.RO

TL;DR: 论文提出了一种基于轨道力学、姿态控制和卫星间通信的算法框架，用于调度小型敏捷卫星星座的动态重定向，显著提高了对瞬态或演化现象的响应能力。


<details>
  <summary>Details</summary>
Motivation: 利用商业技术支持的完全可重定向小型航天器，结合改进的机载处理和星座内通信，实现对瞬态或演化现象的高效观测。

Method: 结合轨道力学、姿态控制、卫星间通信、智能预测和规划，开发了地面和机载算法框架，动态调度卫星星座的重定向。

Result: 在24颗卫星的星座中，信息交换延迟低（平均在可用时间的1/3内），机载调度器观测到的洪水幅度比地面实现多7%，性能比非敏捷星座高98%。

Conclusion: 该框架通过智能规划和实时通信，显著提升了卫星星座对动态现象的观测能力，验证了敏捷卫星星座的实用价值。

Abstract: Fully re-orientable small spacecraft are now supported by commercial
technologies, allowing them to point their instruments in any direction and
capture images, with short notice. When combined with improved onboard
processing, and implemented on a constellation of inter-communicable
satellites, this intelligent agility can significantly increase responsiveness
to transient or evolving phenomena. We demonstrate a ground-based and onboard
algorithmic framework that combines orbital mechanics, attitude control,
inter-satellite communication, intelligent prediction and planning to schedule
the time-varying, re-orientation of agile, small satellites in a constellation.
Planner intelligence is improved by updating the predictive value of future
space-time observations based on shared observations of evolving episodic
precipitation and urban flood forecasts. Reliable inter-satellite communication
within a fast, dynamic constellation topology is modeled in the physical,
access control and network layer. We apply the framework on a representative
24-satellite constellation observing 5 global regions. Results show
appropriately low latency in information exchange (average within 1/3rd
available time for implicit consensus), enabling the onboard scheduler to
observe ~7% more flood magnitude than a ground-based implementation. Both
onboard and offline versions performed ~98% better than constellations without
agility.

</details>


### [14] [Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene](https://arxiv.org/abs/2506.15890)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy,David Merrick*

Main category: cs.RO

TL;DR: 本文总结了在飓风Debby和Helene中使用小型无人机（sUAS）进行机器学习（ML）损害评估时遇到的四个主要挑战，并提出了未来实际部署的三个研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在真实灾害响应中首次部署基于sUAS的ML损害评估系统时遇到的挑战，以提升其操作有效性。

Method: 方法包括在佛罗里达州和宾夕法尼亚州使用Wintra WingtraOne sUAS收集图像数据，并应用同一ML模型进行分析。

Result: 结果揭示了四个主要挑战（输入图像空间分辨率变化、图像与地理空间数据不对齐、无线连接问题和数据产品格式），并提出了三个改进建议。

Conclusion: 结论是这些建议有望提升sUAS和基于sUAS的ML损害评估系统在灾害响应中的实际应用效果。

Abstract: This paper details four principal challenges encountered with machine
learning (ML) damage assessment using small uncrewed aerial systems (sUAS) at
Hurricanes Debby and Helene that prevented, degraded, or delayed the delivery
of data products during operations and suggests three research directions for
future real-world deployments. The presence of these challenges is not
surprising given that a review of the literature considering both datasets and
proposed ML models suggests this is the first sUAS-based ML system for disaster
damage assessment actually deployed as a part of real-world operations. The
sUAS-based ML system was applied by the State of Florida to Hurricanes Helene
(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne
sUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a
Wintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial
imagery of inland flood damage resulting from post-tropical remnants of
Hurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing
further insights into the advantages and limitations of sUAS for disaster
response. The four challenges (variationin spatial resolution of input imagery,
spatial misalignment between imagery and geospatial data, wireless
connectivity, and data product format) lead to three recommendations that
specify research needed to improve ML model capabilities to accommodate the
wide variation of potential spatial resolutions used in practice, handle
spatial misalignment, and minimize the dependency on wireless connectivity.
These recommendations are expected to improve the effective operational use of
sUAS and sUAS-based ML damage assessment systems for disaster response.

</details>


### [15] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Main category: cs.RO

TL;DR: 论文提出了一种双层交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）和深度强化学习（DRL），以提升自动驾驶车辆在动态交通场景中的交互合理性、效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在复杂的真实交通环境中，自动驾驶车辆需要与其他交通参与者交互并做出实时且安全关键决策，而人类行为的不可预测性带来了显著挑战。

Method: 采用三种DRL算法构建可靠的价值网络和策略网络，指导交互式MCTS的在线推理过程，并通过动态轨迹规划器和轨迹跟踪控制器在CARLA中实现。

Result: 实验表明，BIDA不仅提升了交互推理能力并降低了计算成本，还在不同交通条件下表现出优于其他基准的安全性和效率。

Conclusion: BIDA算法在动态关键交通场景中显著提升了自动驾驶车辆的交互合理性、效率和安全性。

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


### [16] [An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation](https://arxiv.org/abs/2506.16555)
*Melih Özcan,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: 提出了一种结合力控制和优化运动规划的多模态控制框架，用于复杂机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 力控制在高频适应和柔顺交互中表现优异，但在远距离运动和稳定姿态上受限；优化运动规划擅长全局轨迹生成，但难以处理动态接触力。

Method: 将任务分解为子任务，动态分配三种控制模式：纯优化、纯力控制或混合控制。

Result: 在单臂、双臂和多臂操作任务中展示了方法的鲁棒性和精确性。

Conclusion: 该框架能无缝切换控制模式，适用于自由空间运动和接触丰富的操作任务。

Abstract: Robotic manipulation demands precise control over both contact forces and
motion trajectories. While force control is essential for achieving compliant
interaction and high-frequency adaptation, it is limited to operations in close
proximity to the manipulated object and often fails to maintain stable
orientation during extended motion sequences. Conversely, optimization-based
motion planning excels in generating collision-free trajectories over the
robot's configuration space but struggles with dynamic interactions where
contact forces play a crucial role. To address these limitations, we propose a
multi-modal control framework that combines force control and
optimization-augmented motion planning to tackle complex robotic manipulation
tasks in a sequential manner, enabling seamless switching between control modes
based on task requirements. Our approach decomposes complex tasks into
subtasks, each dynamically assigned to one of three control modes: Pure
optimization for global motion planning, pure force control for precise
interaction, or hybrid control for tasks requiring simultaneous trajectory
tracking and force regulation. This framework is particularly advantageous for
bimanual and multi-arm manipulation, where synchronous motion and coordination
among arms are essential while considering both the manipulated object and
environmental constraints. We demonstrate the versatility of our method through
a range of long-horizon manipulation tasks, including single-arm, bimanual, and
multi-arm applications, highlighting its ability to handle both free-space
motion and contact-rich manipulation with robustness and precision.

</details>


### [17] [Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency](https://arxiv.org/abs/2506.15920)
*Liang Qin,Weiwei Wan,Jun Takahashi,Ryo Negishi,Masaki Matsushita,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出一种基于能量模型的学习方法，通过预测共享抓取加速机器人抓取-放置任务规划。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法需单独评估每个抓取候选，计算开销大。

Method: 引入能量模型（EBM），结合初始和目标物体位姿的可行抓取能量，预测共享抓取。

Result: 实验表明，该方法提升了抓取选择性能，数据效率更高，且能泛化到未见过的抓取和类似形状物体。

Conclusion: 该方法显著减少了搜索空间，提高了机器人任务规划的效率。

Abstract: This work proposes a learning method to accelerate robotic pick-and-place
planning by predicting shared grasps. Shared grasps are defined as grasp poses
feasible to both the initial and goal object configurations in a pick-and-place
task. Traditional analytical methods for solving shared grasps evaluate grasp
candidates separately, leading to substantial computational overhead as the
candidate set grows. To overcome the limitation, we introduce an Energy-Based
Model (EBM) that predicts shared grasps by combining the energies of feasible
grasps at both object poses. This formulation enables early identification of
promising candidates and significantly reduces the search space. Experiments
show that our method improves grasp selection performance, offers higher data
efficiency, and generalizes well to unseen grasps and similarly shaped objects.

</details>


### [18] [DRIVE Through the Unpredictability:From a Protocol Investigating Slip to a Metric Estimating Command Uncertainty](https://arxiv.org/abs/2506.16593)
*Nicolas Samson,William Larrivée-Hardy,William Dubois,Élie Roy-Brouard,Edith Brotherton,Dominic Baril,Julien Lépine,François Pomerleau*

Main category: cs.RO

TL;DR: 论文提出使用DRIVE协议标准化数据收集，用于系统识别和滑移状态空间表征，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 越野自主导航依赖运动模型的准确性，但运动模型受限于对地形与无人地面车辆（UGV）交互的预测能力。

Method: 采用DRIVE协议收集数据，在两平台（75 kg至470 kg）和六种地形上验证，评估协议对速度命令空间的探索能力。

Result: 通过数据分析了速度命令空间与稳态滑移的传递函数，提出了不可预测性度量以评估风险。

Conclusion: DRIVE协议有效支持系统识别，并分享了大型UGV系统识别的经验。

Abstract: Off-road autonomous navigation is a challenging task as it is mainly
dependent on the accuracy of the motion model. Motion model performances are
limited by their ability to predict the interaction between the terrain and the
UGV, which an onboard sensor can not directly measure. In this work, we propose
using the DRIVE protocol to standardize the collection of data for system
identification and characterization of the slip state space. We validated this
protocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on
six terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9
hours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to
explore the velocity command space and identify the reachable velocities for
terrain-robot interactions. We investigated the transfer function between the
command velocity space and the resulting steady-state slip for an SSMR. An
unpredictability metric is proposed to estimate command uncertainty and help
assess risk likelihood and severity in deployment. Finally, we share our
lessons learned on running system identification on large UGV to help the
community.

</details>


### [19] [KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping](https://arxiv.org/abs/2506.15945)
*Kowndinya Boyalakuntla,Abdeslam Boularias,Jingjin Yu*

Main category: cs.RO

TL;DR: KARL是一种结合卡尔曼滤波和强化学习的动态目标跟踪与抓取系统，显著提升眼在手（EoH）系统在复杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态目标跟踪与抓取在复杂环境中的挑战，提升系统在目标短暂消失或快速运动时的鲁棒性。

Method: 1. 六阶段强化学习课程扩展运动范围；2. 卡尔曼滤波层增强6D姿态估计；3. 引入失败恢复机制。

Result: 在仿真和实际实验中，KARL表现出更高的抓取成功率和更快的执行速度。

Conclusion: KARL通过结合卡尔曼滤波和强化学习，显著提升了动态目标跟踪与抓取的性能。

Abstract: We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic
object tracking and grasping over eye-on-hand (EoH) systems, significantly
expanding such systems capabilities in challenging, realistic environments. In
comparison to the previous state-of-the-art, KARL (1) incorporates a novel
six-stage RL curriculum that doubles the system's motion range, thereby greatly
enhancing the system's grasping performance, (2) integrates a robust Kalman
filter layer between the perception and reinforcement learning (RL) control
modules, enabling the system to maintain an uncertain but continuous 6D pose
estimate even when the target object temporarily exits the camera's
field-of-view or undergoes rapid, unpredictable motion, and (3) introduces
mechanisms to allow retries to gracefully recover from unavoidable policy
execution failures. Extensive evaluations conducted in both simulation and
real-world experiments qualitatively and quantitatively corroborate KARL's
advantage over earlier systems, achieving higher grasp success rates and faster
robot execution speed. Source code and supplementary materials for KARL will be
made available at: https://github.com/arc-l/karl.

</details>


### [20] [Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform](https://arxiv.org/abs/2506.16892)
*Partha Chowdhury,Harsha M,Ayush Gupta,Sanat K Biswas*

Main category: cs.RO

TL;DR: OrCo是一个基于网络的平台，用于通过TLE数据预测空间物体的碰撞概率，以增强空间态势感知。


<details>
  <summary>Details</summary>
Motivation: 地球轨道环境日益拥挤，主要由空间碎片和失效卫星引起，增加了碰撞风险。

Method: 采用多种方法传播轨道不确定性并计算碰撞概率。

Result: 通过准确性和效率评估验证了平台性能。

Conclusion: 该平台有助于改善空间物体跟踪，确保卫星在拥挤空间中的安全。

Abstract: This work presents an indigenous web based platform Orbital Collision (OrCo),
created by the Space Systems Laboratory at IIIT Delhi, to enhance Space
Situational Awareness (SSA) by predicting collision probabilities of space
objects using Two Line Elements (TLE) data. The work highlights the growing
challenges of congestion in the Earth's orbital environment, mainly due to
space debris and defunct satellites, which increase collision risks. It employs
several methods for propagating orbital uncertainty and calculating the
collision probability. The performance of the platform is evaluated through
accuracy assessments and efficiency metrics, in order to improve the tracking
of space objects and ensure the safety of the satellite in congested space.

</details>


### [21] [ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation](https://arxiv.org/abs/2506.15953)
*Liang Heng,Haoran Geng,Kaifeng Zhang,Pieter Abbeel,Jitendra Malik*

Main category: cs.RO

TL;DR: ViTacFormer是一种结合视觉和触觉的表示学习方法，通过跨模态融合和自回归预测提升机器人灵巧操作的精度和鲁棒性，在真实任务中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作是机器人实现类人交互的关键能力，但现有视觉方法在非结构化或视觉遮挡场景中表现不足，触觉感知对精细控制至关重要。

Method: 提出ViTacFormer，结合跨注意力编码器融合视觉和触觉信号，并设计自回归触觉预测头。采用由易到难的课程学习策略优化多模态表示。

Result: 在真实任务中，成功率比现有技术高50%，首次实现长时序灵巧操作任务（11个连续阶段，持续2.5分钟）。

Conclusion: ViTacFormer通过跨模态学习和课程训练显著提升了机器人灵巧操作的性能，为复杂任务提供了新解决方案。

Abstract: Dexterous manipulation is a cornerstone capability for robotic systems aiming
to interact with the physical world in a human-like manner. Although
vision-based methods have advanced rapidly, tactile sensing remains crucial for
fine-grained control, particularly in unstructured or visually occluded
settings. We present ViTacFormer, a representation-learning approach that
couples a cross-attention encoder to fuse high-resolution vision and touch with
an autoregressive tactile prediction head that anticipates future contact
signals. Building on this architecture, we devise an easy-to-challenging
curriculum that steadily refines the visual-tactile latent space, boosting both
accuracy and robustness. The learned cross-modal representation drives
imitation learning for multi-fingered hands, enabling precise and adaptive
manipulation. Across a suite of challenging real-world benchmarks, our method
achieves approximately 50% higher success rates than prior state-of-the-art
systems. To our knowledge, it is also the first to autonomously complete
long-horizon dexterous manipulation tasks that demand highly precise control
with an anthropomorphic hand, successfully executing up to 11 sequential stages
and sustaining continuous operation for 2.5 minutes.

</details>


### [22] [Judo: A User-Friendly Open-Source Package for Sampling-Based Model Predictive Control](https://arxiv.org/abs/2506.17184)
*Albert H. Li,Brandon Hung,Aaron D. Ames,Jiuguang Wang,Simon Le Cleac'h,Preston Culbertson*

Main category: cs.RO

TL;DR: Judo是一个用于快速原型设计和评估采样基于MPC算法的软件包，提供标准化任务和易用接口。


<details>
  <summary>Details</summary>
Motivation: 机器人社区需要通用工具来支持采样基于控制器的开发、评估和部署。

Method: Judo提供常见采样基于MPC算法的实现、标准化任务、易用接口和异步执行功能。

Result: Judo在消费级和服务器级硬件上实现实时性能。

Conclusion: Judo是一个高效、易用的工具，支持采样基于控制器的快速开发和部署。

Abstract: Recent advancements in parallel simulation and successful robotic
applications are spurring a resurgence in sampling-based model predictive
control. To build on this progress, however, the robotics community needs
common tooling for prototyping, evaluating, and deploying sampling-based
controllers. We introduce Judo, a software package designed to address this
need. To facilitate rapid prototyping and evaluation, Judo provides robust
implementations of common sampling-based MPC algorithms and standardized
benchmark tasks. It further emphasizes usability with simple but extensible
interfaces for controller and task definitions, asynchronous execution for
straightforward simulation-to-hardware transfer, and a highly customizable
interactive GUI for tuning controllers interactively. While written in Python,
the software leverages MuJoCo as its physics backend to achieve real-time
performance, which we validate across both consumer and server-grade hardware.
Code at https://github.com/bdaiinstitute/judo.

</details>


### [23] [A Low-Cost Portable Lidar-based Mobile Mapping System on an Android Smartphone](https://arxiv.org/abs/2506.15983)
*Jianzhu Huai,Yuxin Shao,Yujia Zhang,Alper Yilmaz*

Main category: cs.RO

TL;DR: 提出了一种低成本、便携的移动测绘系统，结合激光雷达、智能手机和RTK-GNSS，适用于元宇宙和数字孪生等领域。


<details>
  <summary>Details</summary>
Motivation: 当前移动测绘解决方案成本高或性能有限，需要更经济便携的方案。

Method: 系统集成激光雷达、Android手机和RTK-GNSS，通过NDK实现激光雷达惯性里程计，记录多传感器数据。

Result: 系统成本低于2000美元，重量约1公斤，性能平衡。

Conclusion: 系统设计开源，为社区提供低成本测绘解决方案。

Abstract: The rapid advancement of the metaverse, digital twins, and robotics
underscores the demand for low-cost, portable mapping systems for reality
capture. Current mobile solutions, such as the Leica BLK2Go and lidar-equipped
smartphones, either come at a high cost or are limited in range and accuracy.
Leveraging the proliferation and technological evolution of mobile devices
alongside recent advancements in lidar technology, we introduce a novel,
low-cost, portable mobile mapping system. Our system integrates a lidar unit,
an Android smartphone, and an RTK-GNSS stick. Running on the Android platform,
it features lidar-inertial odometry built with the NDK, and logs data from the
lidar, wide-angle camera, IMU, and GNSS. With a total bill of materials (BOM)
cost under 2,000 USD and a weight of about 1 kilogram, the system achieves a
good balance between affordability and portability. We detail the system
design, multisensor calibration, synchronization, and evaluate its performance
for tracking and mapping. To further contribute to the community, the system's
design and software are made open source at:
https://github.com/OSUPCVLab/marslogger_android/releases/tag/v2.1

</details>


### [24] [DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning](https://arxiv.org/abs/2506.16012)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: DualTHOR是一个基于物理的仿真平台，专为复杂双臂人形机器人设计，旨在解决现有仿真平台在真实世界机器人应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前仿真平台依赖简化的机器人形态并忽略低级执行的随机性，限制了其在真实世界机器人中的可迁移性。

Method: 扩展AI2-THOR，引入真实机器人资产、双臂协作任务套件、逆运动学求解器及基于物理的低级执行故障机制。

Result: 评估显示当前视觉语言模型在双臂协调和真实环境中的鲁棒性表现不佳。

Conclusion: DualTHOR为开发更强大的视觉语言模型提供了更全面的评估工具，有助于提升其在真实世界任务中的能力。

Abstract: Developing embodied agents capable of performing complex interactive tasks in
real-world scenarios remains a fundamental challenge in embodied AI. Although
recent advances in simulation platforms have greatly enhanced task diversity to
train embodied Vision Language Models (VLMs), most platforms rely on simplified
robot morphologies and bypass the stochastic nature of low-level execution,
which limits their transferability to real-world robots. To address these
issues, we present a physics-based simulation platform DualTHOR for complex
dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our
simulator includes real-world robot assets, a task suite for dual-arm
collaboration, and inverse kinematics solvers for humanoid robots. We also
introduce a contingency mechanism that incorporates potential failures through
physics-based low-level execution, bridging the gap to real-world scenarios.
Our simulator enables a more comprehensive evaluation of the robustness and
generalization of VLMs in household environments. Extensive evaluations reveal
that current VLMs struggle with dual-arm coordination and exhibit limited
robustness in realistic environments with contingencies, highlighting the
importance of using our simulator to develop more capable VLMs for embodied
tasks. The code is available at https://github.com/ds199895/DualTHOR.git.

</details>


### [25] [Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments](https://arxiv.org/abs/2506.16050)
*Jiawen Yu,Jieji Ren,Yang Chang,Qiaojun Yu,Xuan Tong,Boyang Wang,Yan Song,You Li,Xinji Mai,Wenqiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于异构教师网络（HetNet）的新方法，用于复杂工业环境中的异常检测与定位，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂、非结构化的工业环境中难以准确检测工件缺陷，亟需一种能适应环境波动的方法。

Method: 采用异构教师网络（HetNet）、自适应局部-全局特征融合模块和局部多元高斯噪声生成模块。

Result: 在主流基准测试中表现优异，MSC-AD指标提升约10%，并在其他数据集上达到最优。

Conclusion: HetNet能有效提升工业异常检测系统的可靠性，适用于多样化场景，并已在实际生产环境中验证。

Abstract: Anomaly detection and localization in automated industrial manufacturing can
significantly enhance production efficiency and product quality. Existing
methods are capable of detecting surface defects in pre-defined or controlled
imaging environments. However, accurately detecting workpiece defects in
complex and unstructured industrial environments with varying views, poses and
illumination remains challenging. We propose a novel anomaly detection and
localization method specifically designed to handle inputs with perturbative
patterns. Our approach introduces a new framework based on a collaborative
distillation heterogeneous teacher network (HetNet), an adaptive local-global
feature fusion module, and a local multivariate Gaussian noise generation
module. HetNet can learn to model the complex feature distribution of normal
patterns using limited information about local disruptive changes. We conducted
extensive experiments on mainstream benchmarks. HetNet demonstrates superior
performance with approximately 10% improvement across all evaluation metrics on
MSC-AD under industrial conditions, while achieving state-of-the-art results on
other datasets, validating its resilience to environmental fluctuations and its
capability to enhance the reliability of industrial anomaly detection systems
across diverse scenarios. Tests in real-world environments further confirm that
HetNet can be effectively integrated into production lines to achieve robust
and real-time anomaly detection. Codes, images and videos are published on the
project website at: https://zihuatanejoyu.github.io/HetNet/

</details>


### [26] [Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion](https://arxiv.org/abs/2506.16079)
*Prakrut Kotecha,Aditya Shirwatkar,Shishir Kolathaya*

Main category: cs.RO

TL;DR: Lagrangian Neural Networks (LNNs) 通过利用归纳偏置学习系统动力学，在四足机器人无限视野规划中表现出高效性和准确性，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统动力学模型在长时间视野下存在误差累积问题，LNNs 通过保持物理定律的完整性，提供更准确和稳定的预测，适用于可持续运动。

Method: 评估了四种动力学模型：全阶正向动力学、质量矩阵对角化表示、全阶逆向动力学训练与正向推理、以及基于躯干质心的降阶模型。

Result: LNNs 在样本效率（10倍）和预测准确性（2-10倍）上显著优于基线方法，对角化方法降低了计算复杂度并保留可解释性。

Conclusion: LNNs 在四足机器人动力学建模中表现出优越性，提高了运动规划与控制的性能，并展示了实时部署潜力。

Abstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable
framework for learning the system dynamics by utilizing inductive biases. While
traditional dynamics models struggle with compounding errors over long
horizons, LNNs intrinsically preserve the physical laws governing any system,
enabling accurate and stable predictions essential for sustainable locomotion.
This work evaluates LNNs for infinite horizon planning in quadrupedal robots
through four dynamics models: (1) full-order forward dynamics (FD) training and
inference, (2) diagonalized representation of Mass Matrix in full order FD, (3)
full-order inverse dynamics (ID) training with FD inference, (4) reduced-order
modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that
LNNs bring improvements in sample efficiency (10x) and superior prediction
accuracy (up to 2-10x) compared to baseline methods. Notably, the
diagonalization approach of LNNs reduces computational complexity while
retaining some interpretability, enabling real-time receding horizon control.
These findings highlight the advantages of LNNs in capturing the underlying
structure of system dynamics in quadrupeds, leading to improved performance and
efficiency in locomotion planning and control. Additionally, our approach
achieves a higher control frequency than previous LNN methods, demonstrating
its potential for real-world deployment on quadrupeds.

</details>


### [27] [From Theory to Practice: Identifying the Optimal Approach for Offset Point Tracking in the Context of Agricultural Robotics](https://arxiv.org/abs/2506.16143)
*Stephane Ngnepiepaye Wembe,Vincent Rousseau,Johann Laconte,Roland Lenain*

Main category: cs.RO

TL;DR: 本文提出了一种针对农业机器人工具的预测控制策略，通过预测工具的运动来提升跟踪性能，解决了传统控制策略忽视工具实际工作点的问题。


<details>
  <summary>Details</summary>
Motivation: 现代农业面临劳动力短缺和环境压力，农业机器人成为解决方案。然而，现有控制策略多关注机器人本体，忽视了工具的实际工作点，特别是在非直线作物行的情况下。

Method: 提出了一种预测控制策略，专注于工具参考点的运动预测，避免因工具偏移导致的转向过冲问题。

Result: 该方法显著提升了工具在复杂农业环境中的跟踪性能。

Conclusion: 通过优化工具控制策略，农业机器人的精确性和适应性得到提升，为现代农业实践提供了更高效的解决方案。

Abstract: Modern agriculture faces escalating challenges: increasing demand for food,
labor shortages, and the urgent need to reduce environmental impact.
Agricultural robotics has emerged as a promising response to these pressures,
enabling the automation of precise and suitable field operations. In
particular, robots equipped with implements for tasks such as weeding or sowing
must interact delicately and accurately with the crops and soil. Unlike robots
in other domains, these agricultural platforms typically use rigidly mounted
implements, where the implement's position is more critical than the robot's
center in determining task success. Yet, most control strategies in the
literature focus on the vehicle body, often neglecting the acctual working
point of the system. This is particularly important when considering new
agriculture practices where crops row are not necessary straights. This paper
presents a predictive control strategy targeting the implement's reference
point. The method improves tracking performance by anticipating the motion of
the implement, which, due to its offset from the vehicle's center of rotation,
is prone to overshooting during turns if not properly accounted for.

</details>


### [28] [Single-Microphone-Based Sound Source Localization for Mobile Robots in Reverberant Environments](https://arxiv.org/abs/2506.16173)
*Jiang Wang,Runwu Shi,Benjamin Yen,He Kong,Kazuhiro Nakadai*

Main category: cs.RO

TL;DR: 提出了一种基于单麦克风的在线声源定位方法，适用于移动机器人在混响环境中实时定位声源。


<details>
  <summary>Details</summary>
Motivation: 现有声源定位方法通常依赖多麦克风阵列，限制了其应用范围，因此需要一种单麦克风解决方案。

Method: 使用轻量级神经网络（43k参数）提取混响信号中的时间信息进行实时距离估计，并结合扩展卡尔曼滤波器实现在线定位。

Result: 实验证明该方法有效且性能优越，填补了单麦克风移动机器人声源定位的空白。

Conclusion: 该方法为机器人听觉系统提供了一种更灵活、实用的解决方案，并开源了代码以促进研究。

Abstract: Accurately estimating sound source positions is crucial for robot audition.
However, existing sound source localization methods typically rely on a
microphone array with at least two spatially preconfigured microphones. This
requirement hinders the applicability of microphone-based robot audition
systems and technologies. To alleviate these challenges, we propose an online
sound source localization method that uses a single microphone mounted on a
mobile robot in reverberant environments. Specifically, we develop a
lightweight neural network model with only 43k parameters to perform real-time
distance estimation by extracting temporal information from reverberant
signals. The estimated distances are then processed using an extended Kalman
filter to achieve online sound source localization. To the best of our
knowledge, this is the first work to achieve online sound source localization
using a single microphone on a moving robot, a gap that we aim to fill in this
work. Extensive experiments demonstrate the effectiveness and merits of our
approach. To benefit the broader research community, we have open-sourced our
code at https://github.com/JiangWAV/single-mic-SSL.

</details>


### [29] [FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation](https://arxiv.org/abs/2506.16201)
*Sen Wang,Le Wang,Sanping Zhou,Jingyi Tian,Jiayi Li,Haowen Sun,Wei Tang*

Main category: cs.RO

TL;DR: FlowRAM是一种基于生成模型的新型框架，通过动态半径调度和条件流匹配，实现了高效的多模态信息处理和动作生成，显著提升了高精度任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的策略学习方法在推理过程中计算效率低，且未充分利用生成模型在3D环境中的信息探索潜力。

Method: 提出FlowRAM框架，结合动态半径调度实现自适应感知，集成状态空间模型处理多模态信息，并使用条件流匹配学习动作位姿。

Result: 在RLBench基准测试中，FlowRAM的平均成功率提升了12.0%，且推理速度显著提高（少于4个时间步骤）。

Conclusion: FlowRAM在高精度任务中表现出色，为机器人操作提供了高效且性能优越的解决方案。

Abstract: Robotic manipulation in high-precision tasks is essential for numerous
industrial and real-world applications where accuracy and speed are required.
Yet current diffusion-based policy learning methods generally suffer from low
computational efficiency due to the iterative denoising process during
inference. Moreover, these methods do not fully explore the potential of
generative models for enhancing information exploration in 3D environments. In
response, we propose FlowRAM, a novel framework that leverages generative
models to achieve region-aware perception, enabling efficient multimodal
information processing. Specifically, we devise a Dynamic Radius Schedule,
which allows adaptive perception, facilitating transitions from global scene
comprehension to fine-grained geometric details. Furthermore, we integrate
state space models to integrate multimodal information, while preserving linear
computational complexity. In addition, we employ conditional flow matching to
learn action poses by regressing deterministic vector fields, simplifying the
learning process while maintaining performance. We verify the effectiveness of
the FlowRAM in the RLBench, an established manipulation benchmark, and achieve
state-of-the-art performance. The results demonstrate that FlowRAM achieves a
remarkable improvement, particularly in high-precision tasks, where it
outperforms previous methods by 12.0% in average success rate. Additionally,
FlowRAM is able to generate physically plausible actions for a variety of
real-world tasks in less than 4 time steps, significantly increasing inference
speed.

</details>


### [30] [ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models](https://arxiv.org/abs/2506.16211)
*Puhao Li,Yingying Wu,Ziheng Xi,Wanlin Li,Yuzhe Huang,Zhiyuan Zhang,Yinghan Chen,Jianan Wang,Song-Chun Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.RO

TL;DR: ControlVLA是一个新框架，通过结合预训练的视觉语言动作（VLA）模型和对象中心表示，实现了在少量演示下高效微调机器人操作策略，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少量演示下难以适应真实世界机器人操作任务，且依赖仿真数据或预建模块，存在仿真到现实的差距和扩展性问题。

Method: ControlVLA通过ControlNet风格的架构，零初始化投影层，逐步调整预训练策略，实现对象中心条件的引入。

Result: 在6个多样化任务中，仅需10-20次演示即达到76.7%的成功率，显著优于传统方法。

Conclusion: ControlVLA展示了在少量数据下高效适应特定任务的能力，并具备扩展性和鲁棒性。

Abstract: Learning real-world robotic manipulation is challenging, particularly when
limited demonstrations are available. Existing methods for few-shot
manipulation often rely on simulation-augmented data or pre-built modules like
grasping and pose estimation, which struggle with sim-to-real gaps and lack
extensibility. While large-scale imitation pre-training shows promise, adapting
these general-purpose policies to specific tasks in data-scarce settings
remains unexplored. To achieve this, we propose ControlVLA, a novel framework
that bridges pre-trained VLA models with object-centric representations via a
ControlNet-style architecture for efficient fine-tuning. Specifically, to
introduce object-centric conditions without overwriting prior knowledge,
ControlVLA zero-initializes a set of projection layers, allowing them to
gradually adapt the pre-trained manipulation policies. In real-world
experiments across 6 diverse tasks, including pouring cubes and folding
clothes, our method achieves a 76.7% success rate while requiring only 10-20
demonstrations -- a significant improvement over traditional approaches that
require more than 100 demonstrations to achieve comparable success. Additional
experiments highlight ControlVLA's extensibility to long-horizon tasks and
robustness to unseen objects and backgrounds.

</details>


### [31] [Probabilistic Collision Risk Estimation for Pedestrian Navigation](https://arxiv.org/abs/2506.16219)
*Amine Tourki,Paul Prevel,Nils Einecke,Tim Puphal,Alexandre Alahi*

Main category: cs.RO

TL;DR: 将自动驾驶中的风险模型技术应用于视觉障碍辅助设备，实验证明其警告准确性优于传统距离和时间测量方法。


<details>
  <summary>Details</summary>
Motivation: 智能视觉障碍辅助设备的发展滞后于智能驾驶辅助系统，本研究旨在将自动驾驶中的风险模型技术引入视觉障碍辅助领域。

Method: 将用于自动驾驶的风险模型技术整合到视觉障碍辅助设备中，计算物体轨迹的碰撞风险概率。

Result: 风险模型的警告准确性为67%，而距离和时间测量方法的准确性仅为51%。

Conclusion: 风险模型技术在视觉障碍辅助设备中表现优于传统方法，具有实际应用潜力。

Abstract: Intelligent devices for supporting persons with vision impairment are
becoming more widespread, but they are lacking behind the advancements in
intelligent driver assistant system. To make a first step forward, this work
discusses the integration of the risk model technology, previously used in
autonomous driving and advanced driver assistance systems, into an assistance
device for persons with vision impairment. The risk model computes a
probabilistic collision risk given object trajectories which has previously
been shown to give better indications of an object's collision potential
compared to distance or time-to-contact measures in vehicle scenarios. In this
work, we show that the risk model is also superior in warning persons with
vision impairment about dangerous objects. Our experiments demonstrate that the
warning accuracy of the risk model is 67% while both distance and
time-to-contact measures reach only 51% accuracy for real-world data.

</details>


### [32] [CapsDT: Diffusion-Transformer for Capsule Robot Manipulation](https://arxiv.org/abs/2506.16263)
*Xiting He,Mingwu Su,Xinqi Jiang,Long Bai,Jiewen Lai,Hongliang Ren*

Main category: cs.RO

TL;DR: CapsDT是一种基于扩散变换器的模型，用于胶囊机器人在胃部的操作，结合视觉和文本输入生成控制信号，提升内窥镜任务的效率。


<details>
  <summary>Details</summary>
Motivation: 探索VLA模型在内窥镜机器人中的应用潜力，以改善人机交互和医疗效果。

Method: 设计CapsDT模型，处理视觉和文本输入，生成机器人控制信号，并开发胶囊内窥镜机器人系统。

Result: CapsDT在多种内窥镜任务中表现优异，真实模拟操作成功率达26.25%。

Conclusion: CapsDT作为视觉语言通用模型，在内窥镜任务中展现出卓越性能和应用前景。

Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research
area, showcasing significant potential across a variety of applications.
However, their performance in endoscopy robotics, particularly endoscopy
capsule robots that perform actions within the digestive system, remains
unexplored. The integration of VLA models into endoscopy robots allows more
intuitive and efficient interactions between human operators and medical
devices, improving both diagnostic accuracy and treatment outcomes. In this
work, we design CapsDT, a Diffusion Transformer model for capsule robot
manipulation in the stomach. By processing interleaved visual inputs, and
textual instructions, CapsDT can infer corresponding robotic control signals to
facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot
system, a capsule robot controlled by a robotic arm-held magnet, addressing
different levels of four endoscopy tasks and creating corresponding capsule
robot datasets within the stomach simulator. Comprehensive evaluations on
various robotic tasks indicate that CapsDT can serve as a robust
vision-language generalist, achieving state-of-the-art performance in various
levels of endoscopy tasks while achieving a 26.25% success rate in real-world
simulation manipulation.

</details>


### [33] [M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing](https://arxiv.org/abs/2506.16301)
*Nadine Imholz,Maurice Brunner,Nicolas Baumann,Edoardo Ghignone,Michele Magno*

Main category: cs.RO

TL;DR: 该论文提出了一种基于KF的多对手跟踪器和GPR的方法，用于多智能体赛车场景，实现了高成功率的超车和安全性提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体赛车决策在机器人极限操作能力下具有挑战性，现有方法忽略时空信息或仅适用于单对手场景。

Method: 使用KF多对手跟踪器进行对手ReID，结合空间和速度GPR预测对手轨迹，计算超车策略。

Result: 在1:10比例自主赛车实验中，超车成功率高达91.65%，安全性平均提升10.13%。

Conclusion: 该方法在高性能自主赛车中具有潜力。

Abstract: Unrestricted multi-agent racing presents a significant research challenge,
requiring decision-making at the limits of a robot's operational capabilities.
While previous approaches have either ignored spatiotemporal information in the
decision-making process or been restricted to single-opponent scenarios, this
work enables arbitrary multi-opponent head-to-head racing while considering the
opponents' future intent. The proposed method employs a KF-based multi-opponent
tracker to effectively perform opponent ReID by associating them across
observations. Simultaneously, spatial and velocity GPR is performed on all
observed opponent trajectories, providing predictive information to compute the
overtaking maneuvers. This approach has been experimentally validated on a
physical 1:10 scale autonomous racing car, achieving an overtaking success rate
of up to 91.65% and demonstrating an average 10.13%-point improvement in safety
at the same speed as the previous SotA. These results highlight its potential
for high-performance autonomous racing.

</details>


### [34] [Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections](https://arxiv.org/abs/2506.16336)
*Yiou Huang*

Main category: cs.RO

TL;DR: 提出了一种基于目标条件碰撞预测（GCCP）模块的分层强化学习（HRL）框架，用于自动驾驶任务，提高了样本效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在复杂场景中难以高效且安全地训练策略，需要一种更优的解决方案。

Method: 采用分层结构，高层决策器选择安全子目标，低层运动规划器执行；GCCP模块预测碰撞风险。

Result: 相比传统方法，算法收敛更快，安全性更高，且子目标策略可跨任务复用。

Conclusion: 提出的HRL框架显著提升了自动驾驶任务的样本效率和安全性。

Abstract: Reinforcement learning (RL) exhibits remarkable potential in addressing
autonomous driving tasks. However, it is difficult to train a sample-efficient
and safe policy in complex scenarios. In this article, we propose a novel
hierarchical reinforcement learning (HRL) framework with a goal-conditioned
collision prediction (GCCP) module. In the hierarchical structure, the GCCP
module predicts collision risks according to different potential subgoals of
the ego vehicle. A high-level decision-maker choose the best safe subgoal. A
low-level motion-planner interacts with the environment according to the
subgoal. Compared to traditional RL methods, our algorithm is more
sample-efficient, since its hierarchical structure allows reusing the policies
of subgoals across similar tasks for various navigation scenarios. In
additional, the GCCP module's ability to predict both the ego vehicle's and
surrounding vehicles' future actions according to different subgoals, ensures
the safety of the ego vehicle throughout the decision-making process.
Experimental results demonstrate that the proposed method converges to an
optimal policy faster and achieves higher safety than traditional RL methods.

</details>


### [35] [Comparison between External and Internal Single Stage Planetary gearbox actuators for legged robots](https://arxiv.org/abs/2506.16356)
*Aman Singh,Deepak Kapa,Prasham Chedda,Shishir N. Y. Kolathaya*

Main category: cs.RO

TL;DR: 论文提出了一个设计框架，用于优化选择执行器参数，比较了内单级行星齿轮箱（ISSPG）和外单级行星齿轮箱（ESSPG）的性能，并验证了其方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对ISSPG和ESSPG两种行星齿轮箱架构的客观比较，且现有设计依赖启发式方法而非系统优化。

Method: 提出了一个基于性能要求和电机规格的设计框架，用于优化执行器参数，并生成和分析两种架构的优化齿轮箱设计。

Result: 对于T-motor U12，ISSPG在5:1至7:1的低传动比范围内表现更优，而ESSPG在7:1至11:1范围内更优。优化设计的执行器质量与模型预测一致。

Conclusion: 提出的设计框架有效，能够为不同传动比范围选择最优的行星齿轮箱架构。

Abstract: Legged robots, such as quadrupeds and humanoids, require high-performance
actuators for efficient locomotion. Quasi-Direct-Drive (QDD) actuators with
single-stage planetary gearboxes offer low inertia, high efficiency, and
transparency. Among planetary gearbox architectures, Internal (ISSPG) and
External Single-Stage Planetary Gearbox (ESSPG) are the two predominant
designs. While ISSPG is often preferred for its compactness and high torque
density at certain gear ratios, no objective comparison between the two
architectures exists. Additionally, existing designs rely on heuristics rather
than systematic optimization. This paper presents a design framework for
optimally selecting actuator parameters based on given performance requirements
and motor specifications. Using this framework, we generate and analyze various
optimized gearbox designs for both architectures. Our results demonstrate that
for the T-motor U12, ISSPG is the superior choice within the lower gear ratio
range of 5:1 to 7:1, offering a lighter design. However, for gear ratios
exceeding 7:1, ISSPG becomes infeasible, making ESSPG the better option in the
7:1 to 11:1 range. To validate our approach, we designed and optimized two
actuators for manufacturing: an ISSPG with a 6.0:1 gear ratio and an ESSPG with
a 7.2:1 gear ratio. Their respective masses closely align with our optimization
model predictions, confirming the effectiveness of our methodology.

</details>


### [36] [CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance](https://arxiv.org/abs/2506.16386)
*Leesai Park,Keunwoo Jang,Sanghyun Kim*

Main category: cs.RO

TL;DR: CSC-MPPI是一种改进的MPPI方法，通过结合原始对偶梯度方法和DBSCAN聚类，在轨迹优化中严格满足约束条件，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI在满足约束条件和生成最优轨迹方面存在不足，CSC-MPPI旨在解决这些问题。

Method: 结合原始对偶梯度方法确保轨迹可行性，使用DBSCAN聚类选择代表性控制输入，从中选取最优动作。

Result: CSC-MPPI在避障任务中表现优于传统MPPI，提高了可靠性和效率。

Conclusion: CSC-MPPI通过严格约束和聚类优化，显著提升了轨迹优化的性能和鲁棒性。

Abstract: This paper proposes Constrained Sampling Cluster Model Predictive Path
Integral (CSC-MPPI), a novel constrained formulation of MPPI designed to
enhance trajectory optimization while enforcing strict constraints on system
states and control inputs. Traditional MPPI, which relies on a probabilistic
sampling process, often struggles with constraint satisfaction and generates
suboptimal trajectories due to the weighted averaging of sampled trajectories.
To address these limitations, the proposed framework integrates a primal-dual
gradient-based approach and Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) to steer sampled input trajectories into feasible regions
while mitigating risks associated with weighted averaging. First, to ensure
that sampled trajectories remain within the feasible region, the primal-dual
gradient method is applied to iteratively shift sampled inputs while enforcing
state and control constraints. Then, DBSCAN groups the sampled trajectories,
enabling the selection of representative control inputs within each cluster.
Finally, among the representative control inputs, the one with the lowest cost
is chosen as the optimal action. As a result, CSC-MPPI guarantees constraint
satisfaction, improves trajectory selection, and enhances robustness in complex
environments. Simulation and real-world experiments demonstrate that CSC-MPPI
outperforms traditional MPPI in obstacle avoidance, achieving improved
reliability and efficiency. The experimental videos are available at
https://cscmppi.github.io

</details>


### [37] [Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining](https://arxiv.org/abs/2506.16475)
*Yaru Niu,Yunzhe Zhang,Mingyang Yu,Changyi Lin,Chenhao Li,Yikai Wang,Yuxiang Yang,Wenhao Yu,Tingnan Zhang,Bingqing Chen,Jonathan Francis,Zhenzhen Li,Jie Tan,Ding Zhao*

Main category: cs.RO

TL;DR: 该论文提出了一种跨具身模仿学习系统，用于四足机器人的多功能操作，通过结合人类和机器人数据，显著提升了操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂环境中的运动能力已很出色，但实现自主多功能操作仍具挑战性。

Method: 开发了统一的遥操作和数据收集流程，提出模块化架构支持跨具身数据训练和预训练。

Result: 在六项真实操作任务中，系统平均成功率提升41.9%，OOD设置下提升79.7%。

Conclusion: 结合人类数据预训练可显著提升性能，且仅需一半机器人数据即可实现更好效果。

Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in
complex environments, but equipping them with autonomous versatile manipulation
skills in a scalable way remains a significant challenge. In this work, we
introduce a cross-embodiment imitation learning system for quadrupedal
manipulation, leveraging data collected from both humans and LocoMan, a
quadruped equipped with multiple manipulation modes. Specifically, we develop a
teleoperation and data collection pipeline, which unifies and modularizes the
observation and action spaces of the human and the robot. To effectively
leverage the collected data, we propose an efficient modularized architecture
that supports co-training and pretraining on structured modality-aligned data
across different embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks in both
unimanual and bimanual modes, supplemented by a corresponding human dataset. We
validate our system on six real-world manipulation tasks, where it achieves an
average success rate improvement of 41.9% overall and 79.7% under
out-of-distribution (OOD) settings compared to the baseline. Pretraining with
human data contributes a 38.6% success rate improvement overall and 82.7% under
OOD settings, enabling consistently better performance with only half the
amount of robot data. Our code, hardware, and data are open-sourced at:
https://human2bots.github.io.

</details>


### [38] [Grounding Language Models with Semantic Digital Twins for Robotic Planning](https://arxiv.org/abs/2506.16493)
*Mehreen Naeem,Andrew Melnik,Michael Beetz*

Main category: cs.RO

TL;DR: 提出了一种结合语义数字孪生（SDT）和大语言模型（LLM）的框架，用于动态环境中机器人任务的适应性执行。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人任务执行的高层推理和语义环境理解的结合问题。

Method: 将自然语言指令分解为结构化动作三元组，并通过SDT提供环境数据支持语义理解，LLM用于错误恢复和动作计划修订。

Result: 在ALFRED基准测试中表现稳健，适用于多种家庭场景。

Conclusion: 该框架成功结合了高层推理与语义环境理解，在不确定性和失败情况下实现了可靠的任务完成。

Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs)
with Large Language Models (LLMs) to enable adaptive and goal-driven robotic
task execution in dynamic environments. The system decomposes natural language
instructions into structured action triplets, which are grounded in contextual
environmental data provided by the SDT. This semantic grounding allows the
robot to interpret object affordances and interaction rules, enabling action
planning and real-time adaptability. In case of execution failures, the LLM
utilizes error feedback and SDT insights to generate recovery strategies and
iteratively revise the action plan. We evaluate our approach using tasks from
the ALFRED benchmark, demonstrating robust performance across various household
scenarios. The proposed framework effectively combines high-level reasoning
with semantic environment understanding, achieving reliable task completion in
the face of uncertainty and failure.

</details>


### [39] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Main category: cs.RO

TL;DR: 论文提出ReOI方法，通过检测并干预视觉干扰物，提升世界模型在开放环境中的预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在面对训练中未见的视觉干扰物时表现脆弱，影响动作预测和下游任务。

Method: 提出ReOI方法，检测并移除干扰物，修正观测后重新预测未来结果。

Result: ReOI显著提升任务成功率，最高达3倍，优于未干预的模型。

Conclusion: ReOI是一种简单有效的测试时策略，适用于开放世界中的机器人任务。

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [40] [History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation](https://arxiv.org/abs/2506.16623)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 本文提出了一种新的零样本目标导航框架，通过动态历史感知提示深度整合视觉语言模型（VLM）推理，解决了现有方法在上下文理解和导航行为上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前目标导航方法对视觉语言模型（VLM）的利用较浅，仅用于对象-场景相似性检查，缺乏深度推理能力，导致上下文理解不足和重复导航行为。

Method: 采用动态历史感知提示技术，为VLM提供动作历史上下文，生成导航动作的语义指导分数，并引入VLM辅助的路径点生成机制。

Result: 在HM3D数据集上的实验显示，成功率为46%，路径长度加权的成功率为24.8%，与现有零样本方法相当。

Conclusion: 历史增强的VLM提示策略显著提升了机器人导航的鲁棒性和上下文感知能力。

Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in
unseen environments, demanding sophisticated reasoning. While Vision-Language
Models (VLMs) show potential, current ObjectNav methods often employ them
superficially, primarily using vision-language embeddings for object-scene
similarity checks rather than leveraging deeper reasoning. This limits
contextual understanding and leads to practical issues like repetitive
navigation behaviors. This paper introduces a novel zero-shot ObjectNav
framework that pioneers the use of dynamic, history-aware prompting to more
deeply integrate VLM reasoning into frontier-based exploration. Our core
innovation lies in providing the VLM with action history context, enabling it
to generate semantic guidance scores for navigation actions while actively
avoiding decision loops. We also introduce a VLM-assisted waypoint generation
mechanism for refining the final approach to detected objects. Evaluated on the
HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and
24.8% Success weighted by Path Length (SPL). These results are comparable to
state-of-the-art zero-shot methods, demonstrating the significant potential of
our history-augmented VLM prompting strategy for more robust and context-aware
robotic navigation.

</details>


### [41] [See What I Mean? Expressiveness and Clarity in Robot Display Design](https://arxiv.org/abs/2506.16643)
*Matthew Ebisu,Hang Yu,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: 研究探讨了非语言视觉符号在人与机器人协作中的作用，发现动画显示能提升信任感，而静态图标更易解读。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索不同类型的非语言提示如何影响动态环境中人与机器人协作的任务表现。

Method: 设计了一个协作导航任务，参与者与机器人使用动画或静态的非语言符号进行交流，并在公共空间招募了37名参与者进行实验。

Result: 结果显示，动画显示提升了信任感，静态图标更易解读，而静态眼睛的机器人任务完成率最高。

Conclusion: 结论是动画能增强信任，但结合熟悉的静态图标可以优化人机交流效果。

Abstract: Nonverbal visual symbols and displays play an important role in communication
when humans and robots work collaboratively. However, few studies have
investigated how different types of non-verbal cues affect objective task
performance, especially in a dynamic environment that requires real time
decision-making. In this work, we designed a collaborative navigation task
where the user and the robot only had partial information about the map on each
end and thus the users were forced to communicate with a robot to complete the
task. We conducted our study in a public space and recruited 37 participants
who randomly passed by our setup. Each participant collaborated with a robot
utilizing either animated anthropomorphic eyes and animated icons, or static
anthropomorphic eyes and static icons. We found that participants that
interacted with a robot with animated displays reported the greatest level of
trust and satisfaction; that participants interpreted static icons the best;
and that participants with a robot with static eyes had the highest completion
success. These results suggest that while animation can foster trust with
robots, human-robot communication can be optimized by the addition of familiar
static icons that may be easier for users to interpret. We published our code,
designed symbols, and collected results online at:
https://github.com/mattufts/huamn_Cozmo_interaction.

</details>


### [42] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: 论文提出了一种新型机器人操作框架，通过视觉语言模型（VLM）解析自然语言指令中的模糊概念，并生成可执行代码，以解决指令歧义问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令在机器人操作任务中常存在模糊性，现有端到端模型因缺乏模块化和可解释性导致性能不佳。

Method: 框架结合VLM生成任务特定代码，通过感知模块生成3D注意力图，整合空间与语义信息以消除指令歧义。

Result: 实验表明，该方法在语言模糊性、接触丰富操作和多物体交互任务中表现优异。

Conclusion: 该框架有效解决了自然语言指令的模糊性问题，提升了机器人操作的适应性和性能。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [43] [Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections](https://arxiv.org/abs/2506.16685)
*Xiaomeng Xu,Yifan Hou,Zeyi Liu,Shuran Song*

Main category: cs.RO

TL;DR: 论文提出了一种改进的DAgger方法（CR-DAgger），通过合规干预接口和合规残差策略，显著提升了接触密集型操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决在真实世界接触密集型操作中，如何收集有效的人类校正数据以及如何利用这些数据更新策略的关键挑战。

Method: 引入合规干预接口和合规残差策略，结合力反馈和力控制，从人类校正中学习。

Result: 在书籍翻页和皮带组装任务中，基础策略成功率提升超过50%，优于从头训练和微调方法。

Conclusion: CR-DAgger为真实世界机器人学习任务提供了有效的DAgger实现指南。

Abstract: We address key challenges in Dataset Aggregation (DAgger) for real-world
contact-rich manipulation: how to collect informative human correction data and
how to effectively update policies with this new data. We introduce Compliant
Residual DAgger (CR-DAgger), which contains two novel components: 1) a
Compliant Intervention Interface that leverages compliance control, allowing
humans to provide gentle, accurate delta action corrections without
interrupting the ongoing robot policy execution; and 2) a Compliant Residual
Policy formulation that learns from human corrections while incorporating force
feedback and force control. Our system significantly enhances performance on
precise contact-rich manipulation tasks using minimal correction data,
improving base policy success rates by over 50\% on two challenging tasks (book
flipping and belt assembly) while outperforming both retraining-from-scratch
and finetuning approaches. Through extensive real-world experiments, we provide
practical guidance for implementing effective DAgger in real-world robot
learning tasks. Result videos are available at:
https://compliant-residual-dagger.github.io/

</details>


### [44] [VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation](https://arxiv.org/abs/2506.16703)
*Sinuo Cheng,Ruyi Zhou,Wenhao Feng,Huaiguang Yang,Haibo Gao,Zongquan Deng,Liang Ding*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型（VLM）的多模式导航系统，用于行星探测车的高效安全自主导航。


<details>
  <summary>Details</summary>
Motivation: 行星探测环境的复杂性和多样性要求更灵活适应的导航策略。

Method: 利用VLM解析场景信息，根据地形复杂度分类切换不同导航模式（感知、建图、规划模块）。

Result: 多模式系统在模拟环境中效率提升79.5%，同时保持对地形危险的规避能力。

Conclusion: 该系统显著提升了行星探测车在复杂环境中的导航效率和安全性。

Abstract: The increasingly complex and diverse planetary exploration environment
requires more adaptable and flexible rover navigation strategy. In this study,
we propose a VLM-empowered multi-mode system to achieve efficient while safe
autonomous navigation for planetary rovers. Vision-Language Model (VLM) is used
to parse scene information by image inputs to achieve a human-level
understanding of terrain complexity. Based on the complexity classification,
the system switches to the most suitable navigation mode, composing of
perception, mapping and planning modules designed for different terrain types,
to traverse the terrain ahead before reaching the next waypoint. By integrating
the local navigation system with a map server and a global waypoint generation
module, the rover is equipped to handle long-distance navigation tasks in
complex scenarios. The navigation system is evaluated in various simulation
environments. Compared to the single-mode conservative navigation method, our
multi-mode system is able to bootstrap the time and energy efficiency in a
long-distance traversal with varied type of obstacles, enhancing efficiency by
79.5%, while maintaining its avoidance capabilities against terrain hazards to
guarantee rover safety. More system information is shown at
https://chengsn1234.github.io/multi-mode-planetary-navigation/.

</details>


### [45] [Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms](https://arxiv.org/abs/2506.16710)
*Aditya Bhatt,Mary Katherine Corra,Franklin Merlo,Prajit KrisshnaKumar,Souma Chowdhury*

Main category: cs.RO

TL;DR: 本文提出了一种新的实验室规模物理设置和开源软件管道，用于评估和基准测试多机器人搜索算法，填补了算法在真实物理环境中性能测试的空白。


<details>
  <summary>Details</summary>
Motivation: 多机器人信号源定位在搜索救援和危险定位中有广泛应用，但现有算法多在仿真中测试，缺乏真实物理环境中的性能对比。

Method: 使用声源和小型地面机器人（e-pucks）在标准运动捕捉环境中构建物理设置，开发开源软件管道支持多机器人搜索算法的分布式实现。

Result: 通过评估两种先进的多机器人搜索算法（群优化和批量贝叶斯优化）以及随机行走基线，验证了该设置的实用性。

Conclusion: 该物理设置和软件管道为多机器人搜索算法的真实环境测试提供了便捷工具，有助于缩小仿真与现实的差距。

Abstract: Signal source localization has been a problem of interest in the multi-robot
systems domain given its applications in search \& rescue and hazard
localization in various industrial and outdoor settings. A variety of
multi-robot search algorithms exist that usually formulate and solve the
associated autonomous motion planning problem as a heuristic model-free or
belief model-based optimization process. Most of these algorithms however
remains tested only in simulation, thereby losing the opportunity to generate
knowledge about how such algorithms would compare/contrast in a real physical
setting in terms of search performance and real-time computing performance. To
address this gap, this paper presents a new lab-scale physical setup and
associated open-source software pipeline to evaluate and benchmark multi-robot
search algorithms. The presented physical setup innovatively uses an acoustic
source (that is safe and inexpensive) and small ground robots (e-pucks)
operating in a standard motion-capture environment. This setup can be easily
recreated and used by most robotics researchers. The acoustic source also
presents interesting uncertainty in terms of its noise-to-signal ratio, which
is useful to assess sim-to-real gaps. The overall software pipeline is designed
to readily interface with any multi-robot search algorithm with minimal effort
and is executable in parallel asynchronous form. This pipeline includes a
framework for distributed implementation of multi-robot or swarm search
algorithms, integrated with a ROS (Robotics Operating System)-based software
stack for motion capture supported localization. The utility of this novel
setup is demonstrated by using it to evaluate two state-of-the-art multi-robot
search algorithms, based on swarm optimization and batch-Bayesian Optimization
(called Bayes-Swarm), as well as a random walk baseline.

</details>


### [46] [DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy](https://arxiv.org/abs/2506.16720)
*Weitao Zhou,Bo Zhang,Zhong Cao,Xiang Li,Qian Cheng,Chunyang Liu,Yaqin Zhang,Diange Yang*

Main category: cs.RO

TL;DR: 论文提出了一种基于脱钩原因增强的强化学习方法（DRARL），通过识别脱钩原因来优化自动驾驶策略，避免无效数据干扰。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在开放道路上的脱钩案例日益增多，但数据稀缺且部分脱钩并非策略失败导致，直接利用这些数据效果有限。

Method: 使用OOD状态估计模型识别脱钩原因，区分无效脱钩案例，并在原因增强的想象环境中更新策略。

Result: 实验表明，该方法能准确识别策略相关脱钩原因，提升策略性能，同时避免策略过于保守。

Conclusion: DRARL为利用脱钩案例优化驾驶策略提供了高效方法。

Abstract: With the increasing presence of automated vehicles on open roads under driver
supervision, disengagement cases are becoming more prevalent. While some
data-driven planning systems attempt to directly utilize these disengagement
cases for policy improvement, the inherent scarcity of disengagement data
(often occurring as a single instances) restricts training effectiveness.
Furthermore, some disengagement data should be excluded since the disengagement
may not always come from the failure of driving policies, e.g. the driver may
casually intervene for a while. To this end, this work proposes
disengagement-reason-augmented reinforcement learning (DRARL), which enhances
driving policy improvement process according to the reason of disengagement
cases. Specifically, the reason of disengagement is identified by a
out-of-distribution (OOD) state estimation model. When the reason doesn't
exist, the case will be identified as a casual disengagement case, which
doesn't require additional policy adjustment. Otherwise, the policy can be
updated under a reason-augmented imagination environment, improving the policy
performance of disengagement cases with similar reasons. The method is
evaluated using real-world disengagement cases collected by autonomous driving
robotaxi. Experimental results demonstrate that the method accurately
identifies policy-related disengagement reasons, allowing the agent to handle
both original and semantically similar cases through reason-augmented training.
Furthermore, the approach prevents the agent from becoming overly conservative
after policy adjustments. Overall, this work provides an efficient way to
improve driving policy performance with disengagement cases.

</details>


### [47] [A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT](https://arxiv.org/abs/2506.16748)
*Arjo Chakravarty,Michael X. Grey,M. A. Viraj J. Muthugala,Mohan Rajesh Elara*

Main category: cs.RO

TL;DR: 提出了一种结合优先级继承回溯（PiBT）和安全感知路径平滑的混合规划框架，用于大规模自由空间多智能体路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模场景下难以兼顾最优性和实时性，或依赖网格假设。

Method: 扩展PiBT至8连通网格，结合安全感知路径平滑和局部交互感知，使用SIPP解决碰撞。

Result: 方法支持500+智能体，实时性能优于现有方法，路径接近最优。

Conclusion: 该框架为机器人系统在大规模自由空间中的实时导航提供了可行方案。

Abstract: Free-space multi-agent path planning remains challenging at large scales.
Most existing methods either offer optimality guarantees but do not scale
beyond a few dozen agents, or rely on grid-world assumptions that do not
generalize well to continuous space. In this work, we propose a hybrid,
rule-based planning framework that combines Priority Inheritance with
Backtracking (PiBT) with a novel safety-aware path smoothing method. Our
approach extends PiBT to 8-connected grids and selectively applies
string-pulling based smoothing while preserving collision safety through local
interaction awareness and a fallback collision resolution step based on Safe
Interval Path Planning (SIPP). This design allows us to reduce overall path
lengths while maintaining real-time performance. We demonstrate that our method
can scale to over 500 agents in large free-space environments, outperforming
existing any-angle and optimal methods in terms of runtime, while producing
near-optimal trajectories in sparse domains. Our results suggest this framework
is a promising building block for scalable, real-time multi-agent navigation in
robotics systems operating beyond grid constraints.

</details>


### [48] [Learning Dexterous Object Handover](https://arxiv.org/abs/2506.16822)
*Daniel Frau-Alfaro,Julio Castaño-Amoros,Santiago Puente,Pablo Gil,Roberto Calandra*

Main category: cs.RO

TL;DR: 使用强化学习（RL）实现多指手之间的灵巧物体交接，提出基于双四元数的新奖励函数以减少旋转距离，实验证明其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在协作环境中（如家庭），机器人需要安全高效地交接物体，这是日常人机交互的重要技能。

Method: 采用强化学习方法，引入基于双四元数的奖励函数优化旋转距离，优于欧拉角和旋转矩阵。

Result: 训练策略在未见过物体和扰动情况下表现良好，最佳成功率94%，扰动下性能仅下降13.8%。

Conclusion: 该方法在物体交接任务中表现出高效性和鲁棒性，适用于实际场景。

Abstract: Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.

</details>


### [49] [SDDiff: Boost Radar Perception via Spatial-Doppler Diffusion](https://arxiv.org/abs/2506.16936)
*Shengpeng Wang,Xin Luo,Yulong Xie,Wei Wang*

Main category: cs.RO

TL;DR: 论文提出了一种名为SDDiff的模型，通过结合空间和多普勒特征，同时优化点云提取（PCE）和自车速度估计（EVE），显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将PCE和EVE视为独立任务，忽略了雷达空间和多普勒特征的相互作用，可能引入偏差。论文发现两者存在潜在关联，可以相互促进。

Method: 设计了Spatial-Doppler Diffusion（SDDiff）模型，改进传统潜在扩散过程：1）引入结合空间和多普勒特征的表示；2）设计基于雷达先验的方向性扩散；3）提出迭代多普勒细化以增强适应性。

Result: SDDiff在EVE准确性上提升59%，生成密度提高4倍，同时显著提升了PCE的有效性和可靠性。

Conclusion: SDDiff通过结合空间和多普勒特征，为雷达感知任务提供了更高效的解决方案，展示了联合建模的潜力。

Abstract: Point cloud extraction (PCE) and ego velocity estimation (EVE) are key
capabilities gaining attention in 3D radar perception. However, existing work
typically treats these two tasks independently, which may neglect the interplay
between radar's spatial and Doppler domain features, potentially introducing
additional bias. In this paper, we observe an underlying correlation between 3D
points and ego velocity, which offers reciprocal benefits for PCE and EVE. To
fully unlock such inspiring potential, we take the first step to design a
Spatial-Doppler Diffusion (SDDiff) model for simultaneously dense PCE and
accurate EVE. To seamlessly tailor it to radar perception, SDDiff improves the
conventional latent diffusion process in three major aspects. First, we
introduce a representation that embodies both spatial occupancy and Doppler
features. Second, we design a directional diffusion with radar priors to
streamline the sampling. Third, we propose Iterative Doppler Refinement to
enhance the model's adaptability to density variations and ghosting effects.
Extensive evaluations show that SDDiff significantly outperforms
state-of-the-art baselines by achieving 59% higher in EVE accuracy, 4X greater
in valid generation density while boosting PCE effectiveness and reliability.

</details>


### [50] [Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration](https://arxiv.org/abs/2506.16986)
*Yuntao Ma,Yang Liu,Kaixian Qu,Marco Hutter*

Main category: cs.RO

TL;DR: 论文提出了一种结合学习和模型控制的方法，用于腿式移动机械臂的投掷任务，实现了较高的准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 投掷是机器人扩展操作范围的重要技能，但现有方法在动态全身操作中的准确性不足。

Method: 框架包括末端执行器的名义跟踪策略、高频残差策略和基于优化的加速度控制模块。

Result: 投掷6米目标时平均着陆误差0.28米，与大学生对比实验中成功率达56.8%，远超人类的15.2%。

Conclusion: 该工作展示了动态全身操作中投掷任务的硬件实现，为相关领域提供了进展。

Abstract: Throwing is a fundamental skill that enables robots to manipulate objects in
ways that extend beyond the reach of their arms. We present a control framework
that combines learning and model-based control for prehensile whole-body
throwing with legged mobile manipulators. Our framework consists of three
components: a nominal tracking policy for the end-effector, a high-frequency
residual policy to enhance tracking accuracy, and an optimization-based module
to improve end-effector acceleration control. The proposed controller achieved
the average of 0.28 m landing error when throwing at targets located 6 m away.
Furthermore, in a comparative study with university students, the system
achieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,
hitting small targets randomly placed at distances of 3-5 m while throwing at a
specified speed of 6 m/s. In contrast, humans have a success rate of only
15.2%. This work provides an early demonstration of prehensile throwing with
quantified accuracy on hardware, contributing to progress in dynamic whole-body
manipulation.

</details>


### [51] [Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping](https://arxiv.org/abs/2506.17110)
*Teng Guo,Baichuan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: 论文提出了一种名为MOMA的单目RGB图像度量深度估计框架，通过一次适应性调整实现高精度深度估计，适用于透明物体，并在实际机器人操作任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前6D物体姿态估计依赖昂贵的深度传感器，且对透明物体效果不佳；单目深度估计模型（MDEMs）虽能提供深度信息，但无法直接度量。因此，需要一种低成本、高精度的解决方案。

Method: 提出MOMA框架，通过一次适应性调整（基于稀疏真实深度点）实现尺度-旋转-平移对齐，无需额外数据收集或模型重训练。

Result: MOMA在透明物体上表现良好，并在实际机器人操作任务（如夹取和吸盘式分拣）中取得高成功率。

Conclusion: MOMA是一种高效、通用的单目度量深度估计方法，适用于复杂场景和透明物体，具有实际应用潜力。

Abstract: Accurate 6D object pose estimation is a prerequisite for successfully
completing robotic prehensile and non-prehensile manipulation tasks. At
present, 6D pose estimation for robotic manipulation generally relies on depth
sensors based on, e.g., structured light, time-of-flight, and stereo-vision,
which can be expensive, produce noisy output (as compared with RGB cameras),
and fail to handle transparent objects. On the other hand, state-of-the-art
monocular depth estimation models (MDEMs) provide only affine-invariant depths
up to an unknown scale and shift. Metric MDEMs achieve some successful
zero-shot results on public datasets, but fail to generalize. We propose a
novel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover
metric depth from a single RGB image, through a one-shot adaptation building on
MDEM techniques. MOMA performs scale-rotation-shift alignments during camera
calibration, guided by sparse ground-truth depth points, enabling accurate
depth estimation without additional data collection or model retraining on the
testing setup. MOMA supports fine-tuning the MDEM on transparent objects,
demonstrating strong generalization capabilities. Real-world experiments on
tabletop 2-finger grasping and suction-based bin-picking applications show MOMA
achieves high success rates in diverse tasks, confirming its effectiveness.

</details>


### [52] [Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation](https://arxiv.org/abs/2506.17198)
*Jianglong Ye,Keyi Wang,Chengjing Yuan,Ruihan Yang,Yiquan Li,Jiyue Zhu,Yuzhe Qin,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 论文介绍了Dex1B数据集，通过生成模型创建大规模、多样化的灵巧手操作演示，并在仿真和实际机器人实验中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 灵巧手操作的大规模演示生成具有挑战性，生成模型为解决这一问题提供了新思路。

Method: 提出一种生成模型，结合几何约束提升可行性，并通过附加条件增强多样性，构建了包含10亿演示的Dex1B数据集。

Result: 在仿真和实际实验中，该方法显著优于现有技术，表现出高效性和鲁棒性。

Conclusion: Dex1B数据集和生成模型为灵巧手操作提供了高质量演示，推动了相关研究的发展。

Abstract: Generating large-scale demonstrations for dexterous hand manipulation remains
challenging, and several approaches have been proposed in recent years to
address this. Among them, generative models have emerged as a promising
paradigm, enabling the efficient creation of diverse and physically plausible
demonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and
high-quality demonstration dataset produced with generative models. The dataset
contains one billion demonstrations for two fundamental tasks: grasping and
articulation. To construct it, we propose a generative model that integrates
geometric constraints to improve feasibility and applies additional conditions
to enhance diversity. We validate the model on both established and newly
introduced simulation benchmarks, where it significantly outperforms prior
state-of-the-art methods. Furthermore, we demonstrate its effectiveness and
robustness through real-world robot experiments. Our project page is at
https://jianglongye.com/dex1b

</details>
