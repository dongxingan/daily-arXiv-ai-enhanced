<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 44]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning](https://arxiv.org/abs/2506.13867)
*Yunchu Zhang,Shubham Mittal,Zhengyu Zhang,Liyiming Ke,Siddhartha Srinivasa,Abhishek Gupta*

Main category: cs.RO

TL;DR: 论文提出了一种名为ATK的方法，通过自动选择任务相关的2D关键点，提升视觉运动策略的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉运动策略在训练和评估环境中的视觉差异可能导致性能下降，现有方法（如6D姿态估计）难以扩展，而基于原始传感器的策略对视觉干扰缺乏鲁棒性。

Method: 利用2D关键点作为状态表示，提出ATK方法自动选择任务相关的关键点，并通过专家数据训练策略。

Result: 实验表明，ATK选择的关键点能显著提升策略对视觉干扰和环境变化的鲁棒性。

Conclusion: ATK方法通过任务驱动的关键点选择，有效解决了视觉运动策略的鲁棒性和泛化问题。

Abstract: Visuomotor policies often suffer from perceptual challenges, where visual differences between training and evaluation environments degrade policy performance. Policies relying on state estimations, like 6D pose, require task-specific tracking and are difficult to scale, while raw sensor-based policies may lack robustness to small visual disturbances.In this work, we leverage 2D keypoints - spatially consistent features in the image frame - as a flexible state representation for robust policy learning and apply it to both sim-to-real transfer and real-world imitation learning. However, the choice of which keypoints to use can vary across objects and tasks. We propose a novel method, ATK, to automatically select keypoints in a task-driven manner so that the chosen keypoints are predictive of optimal behavior for the given task. Our proposal optimizes for a minimal set of keypoints that focus on task-relevant parts while preserving policy performance and robustness. We distill expert data (either from an expert policy in simulation or a human expert) into a policy that operates on RGB images while tracking the selected keypoints. By leveraging pre-trained visual modules, our system effectively encodes states and transfers policies to the real-world evaluation scenario despite wide scene variations and perceptual challenges such as transparent objects, fine-grained tasks, and deformable objects manipulation. We validate ATK on various robotic tasks, demonstrating that these minimal keypoint representations significantly improve robustness to visual disturbances and environmental variations. See all experiments and more details on our website.

</details>


### [2] [Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis](https://arxiv.org/abs/2506.13915)
*Katherine Mao,Hongzhan Yu,Ruipeng Zhang,Igor Spasojevic,M Ani Hsieh,Sicun Gao,Vijay Kumar*

Main category: cs.RO

TL;DR: 该论文提出了一种基于学习的方法，用于快速生成四旋翼无人机的时间最优轨迹，通过模仿模型规划器并引入数据增强提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统时间最优轨迹规划方法因非凸优化问题计算成本高，难以实时应用，因此需要更高效的解决方案。

Method: 利用学习模型模仿基于模型的时间最优轨迹规划器，通过数据集训练模型，并引入数据增强技术提高鲁棒性。

Result: 实验表明，该方法显著加速了轨迹生成，并在硬件平台上验证了实时可行性，且能泛化到未见过的路径长度。

Conclusion: 基于学习的方法在时间最优轨迹规划中具有高效性和鲁棒性，适用于实时应用。

Abstract: Time-optimal trajectories drive quadrotors to their dynamic limits, but computing such trajectories involves solving non-convex problems via iterative nonlinear optimization, making them prohibitively costly for real-time applications. In this work, we investigate learning-based models that imitate a model-based time-optimal trajectory planner to accelerate trajectory generation. Given a dataset of collision-free geometric paths, we show that modeling architectures can effectively learn the patterns underlying time-optimal trajectories. We introduce a quantitative framework to analyze local analytic properties of the learned models, and link them to the Backward Reachable Tube of the geometric tracking controller. To enhance robustness, we propose a data augmentation scheme that applies random perturbations to the input paths. Compared to classical planners, our method achieves substantial speedups, and we validate its real-time feasibility on a hardware quadrotor platform. Experiments demonstrate that the learned models generalize to previously unseen path lengths. The code for our approach can be found here: https://github.com/maokat12/lbTOPPQuad

</details>


### [3] [DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance](https://arxiv.org/abs/2506.13922)
*Maximilian Du,Shuran Song*

Main category: cs.RO

TL;DR: DynaGuide是一种用于扩散策略的引导方法，通过外部动力学模型在扩散去噪过程中提供指导，无需预先训练多种目标。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法（如目标条件）需预先训练多种目标的限制，提供更灵活的策略调整能力。

Method: 利用外部动力学模型在扩散去噪过程中提供分离的引导信号，支持多目标调整和增强基础策略行为。

Result: 在模拟和真实实验中表现优异，平均引导成功率为70%，在低质量目标下表现优于目标条件方法5.4倍。

Conclusion: DynaGuide提供了一种灵活、高效的策略引导方法，适用于多种目标调整和增强行为，且兼容预训练策略。

Abstract: Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io

</details>


### [4] [TUM Teleoperation: Open Source Software for Remote Driving and Assistance of Automated Vehicles](https://arxiv.org/abs/2506.13933)
*Tobias Kerbl,David Brecht,Nils Gehrke,Nijinshan Karunainayagam,Niklas Krauss,Florian Pfab,Richard Taupitz,Ines Trautmannsheimer,Xiyan Su,Maria-Magdalena Wolf,Frank Diermeyer*

Main category: cs.RO

TL;DR: 本文提出了一种模块化、开源的远程操作软件栈，支持远程驾驶和远程辅助，填补了现有开源软件的空白。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏结合远程驾驶、远程辅助并与实际车辆集成的开源软件，阻碍了未来移动性的发展。

Method: 开发了一个模块化的开源软件栈，支持与自动驾驶软件交互，提供标准化接口，便于集成和扩展。

Result: 软件在仿真和实际平台上进行了延迟和性能评估，证明了其适用性。

Conclusion: 该软件为远程操作提供了灵活、可扩展的基础，支持未来协作开发和用户研究。

Abstract: Teleoperation is a key enabler for future mobility, supporting Automated Vehicles in rare and complex scenarios beyond the capabilities of their automation. Despite ongoing research, no open source software currently combines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance through high-level interaction with automated driving software modules, and integration with a real-world vehicle for practical testing. To address this gap, we present a modular, open source teleoperation software stack that can interact with an automated driving software, e.g., Autoware, enabling Remote Assistance and Remote Driving. The software featuresstandardized interfaces for seamless integration with various real-world and simulation platforms, while allowing for flexible design of the human-machine interface. The system is designed for modularity and ease of extension, serving as a foundation for collaborative development on individual software components as well as realistic testing and user studies. To demonstrate the applicability of our software, we evaluated the latency and performance of different vehicle platforms in simulation and real-world. The source code is available on GitHub

</details>


### [5] [Quadrotor Morpho-Transition: Learning vs Model-Based Control Strategies](https://arxiv.org/abs/2506.14039)
*Ioannis Mandralis,Richard M. Murray,Morteza Gharib*

Main category: cs.RO

TL;DR: 论文研究了四旋翼飞行器在空中变形（Morpho-Transition）时的控制问题，通过强化学习（RL）训练控制器，并成功在硬件上实现。RL控制器在考虑电机动态和观测延迟时表现良好，而基线MPC控制器无需这些信息但鲁棒性较差。


<details>
  <summary>Details</summary>
Motivation: 四旋翼飞行器在空-地转换时涉及复杂的气动相互作用和接近执行器饱和的操作，传统模型控制方法因未建模动态和接触规划需求而受限。

Method: 训练端到端强化学习（RL）控制器，学习变形转换策略，并在硬件上验证。同时对比基线MPC控制器的表现。

Result: RL控制器实现了敏捷着陆，但需考虑电机动态和观测延迟才能成功迁移到硬件；MPC控制器无需这些信息但鲁棒性较差。

Conclusion: 研究为需要空中变形的四旋翼飞行器提供了更鲁棒的控制方法，展示了RL在复杂动态任务中的潜力。

Abstract: Quadrotor Morpho-Transition, or the act of transitioning from air to ground through mid-air transformation, involves complex aerodynamic interactions and a need to operate near actuator saturation, complicating controller design. In recent work, morpho-transition has been studied from a model-based control perspective, but these approaches remain limited due to unmodeled dynamics and the requirement for planning through contacts. Here, we train an end-to-end Reinforcement Learning (RL) controller to learn a morpho-transition policy and demonstrate successful transfer to hardware. We find that the RL control policy achieves agile landing, but only transfers to hardware if motor dynamics and observation delays are taken into account. On the other hand, a baseline MPC controller transfers out-of-the-box without knowledge of the actuator dynamics and delays, at the cost of reduced recovery from disturbances in the event of unknown actuator failures. Our work opens the way for more robust control of agile in-flight quadrotor maneuvers that require mid-air transformation.

</details>


### [6] [Beyond the Plane: A 3D Representation of Human Personal Space for Socially-Aware Robotics](https://arxiv.org/abs/2506.13937)
*Caio C. G. Ribeiro,Douglas G. Macharet*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的三维个人空间模型，结合高度和水平距离来量化机器人对人类的不适感。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在人类环境中的普及，需要其行为符合社会规范，尤其是尊重个人空间。现有研究多集中于二维空间，忽略了垂直维度的影响。

Method: 提出了一种三维个人空间模型，通过Z轴的不适函数和XY平面的经典公式，计算机器人任意位置对人类的不适感。

Result: 该模型首次实现了在三维空间中考虑人体配置和高度，量化机器人位置对人类的不适感。

Conclusion: 该研究填补了三维个人空间建模的空白，为机器人行为设计提供了更全面的理论支持。

Abstract: The increasing presence of robots in human environments requires them to exhibit socially appropriate behavior, adhering to social norms. A critical aspect in this context is the concept of personal space, a psychological boundary around an individual that influences their comfort based on proximity. This concept extends to human-robot interaction, where robots must respect personal space to avoid causing discomfort. While much research has focused on modeling personal space in two dimensions, almost none have considered the vertical dimension. In this work, we propose a novel three-dimensional personal space model that integrates both height (introducing a discomfort function along the Z-axis) and horizontal proximity (via a classic XY-plane formulation) to quantify discomfort. To the best of our knowledge, this is the first work to compute discomfort in 3D space at any robot component's position, considering the person's configuration and height.

</details>


### [7] [A Hierarchical Test Platform for Vision Language Model (VLM)-Integrated Real-World Autonomous Driving](https://arxiv.org/abs/2506.14100)
*Yupeng Zhou,Can Cui,Juntong Peng,Zichong Yang,Juanwu Lu,Jitesh H Panchal,Bin Yao,Ziran Wang*

Main category: cs.RO

TL;DR: 本文提出了一种专为评估视觉语言模型（VLM）集成的自动驾驶系统设计的层次化真实世界测试平台，解决了现有方法在复杂性和可重复性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于仿真和数据集的方法难以全面捕捉真实场景的复杂性，且无法灵活支持闭环测试。

Method: 提出模块化、低延迟的车载中间件，支持多种VLM集成；采用感知-规划-控制分离架构；设计可配置的真实测试场景。

Result: 通过案例研究展示了平台在多样化条件下的有效测试和评估能力。

Conclusion: 该平台为VLM在自动驾驶中的安全应用提供了可靠的测试框架。

Abstract: Vision-Language Models (VLMs) have demonstrated notable promise in autonomous driving by offering the potential for multimodal reasoning through pretraining on extensive image-text pairs. However, adapting these models from broad web-scale data to the safety-critical context of driving presents a significant challenge, commonly referred to as domain shift. Existing simulation-based and dataset-driven evaluation methods, although valuable, often fail to capture the full complexity of real-world scenarios and cannot easily accommodate repeatable closed-loop testing with flexible scenario manipulation. In this paper, we introduce a hierarchical real-world test platform specifically designed to evaluate VLM-integrated autonomous driving systems. Our approach includes a modular, low-latency on-vehicle middleware that allows seamless incorporation of various VLMs, a clearly separated perception-planning-control architecture that can accommodate both VLM-based and conventional modules, and a configurable suite of real-world testing scenarios on a closed track that facilitates controlled yet authentic evaluations. We demonstrate the effectiveness of the proposed platform`s testing and evaluation ability with a case study involving a VLM-enabled autonomous vehicle, highlighting how our test framework supports robust experimentation under diverse conditions.

</details>


### [8] [Socially-aware Object Transportation by a Mobile Manipulator in Static Planar Environments with Obstacles](https://arxiv.org/abs/2506.13953)
*Caio C. G. Ribeiro,Leonardo R. D. Paes,Douglas G. Macharet*

Main category: cs.RO

TL;DR: 本文提出了一种基于Risk-RRT*框架的方法，用于移动机械臂在静态人群环境中的社交感知导航与物体操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对移动机器人，而移动机械臂在携带负载时的社交导航研究存在空白。本文旨在填补这一空白。

Method: 采用Risk-RRT*框架，协调移动底座和机械臂的动作，实现无碰撞且符合社交偏好的导航。

Result: 在模拟环境中，该方法优于仅针对移动机器人的方法，能有效导航、运输物体、避免碰撞并减少社交不适。

Conclusion: 移动机械臂需要特定技术，本文提出的方法在社交感知导航中表现优异。

Abstract: Socially-aware robotic navigation is essential in environments where humans and robots coexist, ensuring both safety and comfort. However, most existing approaches have been primarily developed for mobile robots, leaving a significant gap in research that addresses the unique challenges posed by mobile manipulators. In this paper, we tackle the challenge of navigating a robotic mobile manipulator, carrying a non-negligible load, within a static human-populated environment while adhering to social norms. Our goal is to develop a method that enables the robot to simultaneously manipulate an object and navigate between locations in a socially-aware manner. We propose an approach based on the Risk-RRT* framework that enables the coordinated actuation of both the mobile base and manipulator. This approach ensures collision-free navigation while adhering to human social preferences. We compared our approach in a simulated environment to socially-aware mobile-only methods applied to a mobile manipulator. The results highlight the necessity for mobile manipulator-specific techniques, with our method outperforming mobile-only approaches. Our method enabled the robot to navigate, transport an object, avoid collisions, and minimize social discomfort effectively.

</details>


### [9] [Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control](https://arxiv.org/abs/2506.14186)
*Anselm Paulus,A. René Geist,Pierre Schumacher,Vít Musil,Georg Martius*

Main category: cs.RO

TL;DR: 论文分析了基于惩罚的模拟器中接触力梯度计算的问题，提出了DiffMJX和CFD方法以改进梯度质量。


<details>
  <summary>Details</summary>
Motivation: 接触力在机器人动力学梯度优化中引入速度跳跃，而基于惩罚的模拟器（如MuJoCo）通过软化接触力简化梯度计算，但硬接触的模拟需要高刚度设置，导致自动微分时梯度错误。

Method: 分析了惩罚模拟器的接触计算问题，提出DiffMJX（结合自适应积分与MuJoCo XLA）改进梯度质量，并引入CFD机制在物体未接触时生成有用梯度。

Result: DiffMJX显著提高了硬接触下的梯度质量，CFD在物体未接触时也能提供信息性梯度。

Conclusion: DiffMJX和CFD方法有效解决了接触力梯度计算的问题，同时保持了物理真实性。

Abstract: Contact forces pose a major challenge for gradient-based optimization of robot dynamics as they introduce jumps in the system's velocities. Penalty-based simulators, such as MuJoCo, simplify gradient computation by softening the contact forces. However, realistically simulating hard contacts requires very stiff contact settings, which leads to incorrect gradients when using automatic differentiation. On the other hand, using non-stiff settings strongly increases the sim-to-real gap. We analyze the contact computation of penalty-based simulators to identify the causes of gradient errors. Then, we propose DiffMJX, which combines adaptive integration with MuJoCo XLA, to notably improve gradient quality in the presence of hard contacts. Finally, we address a key limitation of contact gradients: they vanish when objects do not touch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism that enables the simulator to generate informative contact gradients even before objects are in contact. To preserve physical realism, we apply CFD only in the backward pass using a straight-through trick, allowing us to compute useful gradients without modifying the forward simulation.

</details>


### [10] [A Cooperative Contactless Object Transport with Acoustic Robots](https://arxiv.org/abs/2506.13957)
*Narsimlu Kemsaram,Akin Delibasi,James Hardwick,Bonot Gautam,Diego Martinez Plasencia,Sriram Subramanian*

Main category: cs.RO

TL;DR: 提出了一种基于声学的机器人系统，用于无接触空中物体运输，受生物系统启发，分为独立和协作运输策略，实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 受蚂蚁等生物协作运输的启发，研究旨在开发一种高效、适应动态环境的无接触物体运输系统。

Method: 利用相控超声换能器和机器人控制系统生成局部声压场，实现空中粒子和机器人的精确操控，分为独立和协作运输策略。

Result: 实验验证了系统的可行性，包括悬浮稳定性、运输效率和时钟同步精度。

Conclusion: 该研究为声学机器人领域提供了新方法，有望应用于无接触材料处理、微组装和生物医学领域。

Abstract: Cooperative transport, the simultaneous movement of an object by multiple agents, has been widely observed in biological systems such as ant colonies, which improve efficiency and adaptability in dynamic environments. Inspired by these natural phenomena, we present a novel acoustic robotic system for the transport of contactless objects in mid-air. Our system leverages phased ultrasonic transducers and a robotic control system onboard to generate localized acoustic pressure fields, enabling precise manipulation of airborne particles and robots. We categorize contactless object-transport strategies into independent transport (uncoordinated) and forward-facing cooperative transport (coordinated), drawing parallels with biological systems to optimize efficiency and robustness. The proposed system is experimentally validated by evaluating levitation stability using a microphone in the measurement lab, transport efficiency through a phase-space motion capture system, and clock synchronization accuracy via an oscilloscope. The results demonstrate the feasibility of both independent and cooperative airborne object transport. This research contributes to the field of acoustophoretic robotics, with potential applications in contactless material handling, micro-assembly, and biomedical applications.

</details>


### [11] [Pose State Perception of Interventional Robot for Cardio-cerebrovascular Procedures](https://arxiv.org/abs/2506.14201)
*Shunhan Ji,Yanxi Chen,Zhongyu Yang,Quan Zhang,Xiaohang Nie,Jingqian Sun,Yichao Tang*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的无标记方法，用于精确感知介入机器人的姿态状态，通过多任务U-Net和几何特征实现高可靠性。


<details>
  <summary>Details</summary>
Motivation: 心血管介入手术需求增加，精确控制机器人姿态状态是关键，传统方法依赖传感器或标记物，本文提出无标记视觉方案。

Method: 采用三部分框架：双头多任务U-Net检测血管和机器人；骨架提取优化算法；基于几何特征的姿态感知系统。

Result: 实验证明方法在轨迹跟踪和姿态感知中具有高可靠性和准确性。

Conclusion: 无标记视觉方法为介入机器人控制提供了高效可靠的解决方案。

Abstract: In response to the increasing demand for cardiocerebrovascular interventional surgeries, precise control of interventional robots has become increasingly important. Within these complex vascular scenarios, the accurate and reliable perception of the pose state for interventional robots is particularly crucial. This paper presents a novel vision-based approach without the need of additional sensors or markers. The core of this paper's method consists of a three-part framework: firstly, a dual-head multitask U-Net model for simultaneous vessel segment and interventional robot detection; secondly, an advanced algorithm for skeleton extraction and optimization; and finally, a comprehensive pose state perception system based on geometric features is implemented to accurately identify the robot's pose state and provide strategies for subsequent control. The experimental results demonstrate the proposed method's high reliability and accuracy in trajectory tracking and pose state perception.

</details>


### [12] [Diffusion-based Inverse Observation Model for Artificial Skin](https://arxiv.org/abs/2506.13986)
*Ante Maric,Julius Jankowski,Giammarco Caroleo,Alessandro Albini,Perla Maiolino,Sylvain Calinon*

Main category: cs.RO

TL;DR: 论文提出了一种基于扩散模型的触觉测量方法，用于高效采样多模态接触假设以估计物体姿态。


<details>
  <summary>Details</summary>
Motivation: 由于接触约束下的观测具有不连续性和多模态性，传统方法难以高效采样有效假设。

Method: 利用扩散模型学习基于分布式人工皮肤触觉测量的逆观测模型。

Result: 模拟实验表明该方法能高效采样接触假设以估计物体姿态。

Conclusion: 扩散模型为多模态接触假设的采样提供了一种有效方法。

Abstract: Contact-based estimation of object pose is challenging due to discontinuities and ambiguous observations that can correspond to multiple possible system states. This multimodality makes it difficult to efficiently sample valid hypotheses while respecting contact constraints. Diffusion models can learn to generate samples from such multimodal probability distributions through denoising algorithms. We leverage these probabilistic modeling capabilities to learn an inverse observation model conditioned on tactile measurements acquired from a distributed artificial skin. We present simulated experiments demonstrating efficient sampling of contact hypotheses for object pose estimation through touch.

</details>


### [13] [Barrier Method for Inequality Constrained Factor Graph Optimization with Application to Model Predictive Control](https://arxiv.org/abs/2506.14341)
*Anas Abdelkarim,Holger Voos,Daniel Görges*

Main category: cs.RO

TL;DR: 本文提出了一种将Barrier Interior Point Method (BIPM)与因子图结合的新方法，解决了传统因子图在约束处理中的局限性，并通过实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 因子图在机器人感知任务中表现优异，但在最优控制问题（如MPC）中应用受限，主要由于约束处理的挑战。

Method: 通过引入编码对数障碍函数的专用不等式因子节点，将BIPM与因子图结合，扩展了g2o框架。

Result: 在多目标自适应巡航控制应用中验证了方法的有效性，相比现有技术，收敛更快且计算效率更高。

Conclusion: 该方法首次在g2o框架中实现了统一处理等式和不等式约束，为最优控制问题提供了高效解决方案。

Abstract: Factor graphs have demonstrated remarkable efficiency for robotic perception tasks, particularly in localization and mapping applications. However, their application to optimal control problems -- especially Model Predictive Control (MPC) -- has remained limited due to fundamental challenges in constraint handling. This paper presents a novel integration of the Barrier Interior Point Method (BIPM) with factor graphs, implemented as an open-source extension to the widely adopted g2o framework. Our approach introduces specialized inequality factor nodes that encode logarithmic barrier functions, thereby overcoming the quadratic-form limitations of conventional factor graph formulations. To the best of our knowledge, this is the first g2o-based implementation capable of efficiently handling both equality and inequality constraints within a unified optimization backend. We validate the method through a multi-objective adaptive cruise control application for autonomous vehicles. Benchmark comparisons with state-of-the-art constraint-handling techniques demonstrate faster convergence and improved computational efficiency. (Code repository: https://github.com/snt-arg/bipm_g2o)

</details>


### [14] [GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics](https://arxiv.org/abs/2506.14009)
*Qianzhong Chen,Naixiang Gao,Suning Huang,JunEn Low,Timothy Chen,Jiankai Sun,Mac Schwager*

Main category: cs.RO

TL;DR: GRaD-Nav++是一个轻量级的视觉-语言-动作（VLA）框架，能够在无人机上实时执行自然语言指令，通过高效学习和自适应计算提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工技能或计算密集型模型，无法在无人机上高效运行，因此需要一种轻量级且自适应的解决方案。

Method: 使用3D高斯泼溅模拟器和可微分强化学习（DiffRL）训练策略，结合混合专家（MoE）动作头实现自适应计算。

Result: 在模拟环境中，训练任务成功率83%，未见任务75%；真实硬件上分别为67%和50%。多环境适应实验中，模拟环境平均成功率81%，真实环境67%。

Conclusion: GRaD-Nav++为完全机载的VLA飞行设定了新基准，证明了轻量高效模型可实现可靠的语言引导导航。

Abstract: Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.

</details>


### [15] [A Point Cloud Completion Approach for the Grasping of Partially Occluded Objects and Its Applications in Robotic Strawberry Harvesting](https://arxiv.org/abs/2506.14066)
*Ali Abouzeid,Malak Mansour,Chengsong Hu,Dezhen Song*

Main category: cs.RO

TL;DR: 提出了一种用于草莓采摘的端到端框架，通过点云去噪、分割和补全技术解决遮挡问题，显著提高了抓取成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，水果采摘中的物体遮挡问题对抓取算法设计提出了挑战，尤其是草莓采摘。

Method: 采用点云去噪和分割定位水果，使用点云补全模型重建遮挡部分，生成占用图进行碰撞感知运动规划。

Result: 形状重建精度高（Chamfer Distance 1.10 mm），抓取成功率79.17%，障碍物碰撞率从43.33%降至13.95%。

Conclusion: 该方法显著提升了草莓采摘的效率和可靠性，为机器人水果采摘系统提供了更优解决方案。

Abstract: In robotic fruit picking applications, managing object occlusion in unstructured settings poses a substantial challenge for designing grasping algorithms. Using strawberry harvesting as a case study, we present an end-to-end framework for effective object detection, segmentation, and grasp planning to tackle this issue caused by partially occluded objects. Our strategy begins with point cloud denoising and segmentation to accurately locate fruits. To compensate for incomplete scans due to occlusion, we apply a point cloud completion model to create a dense 3D reconstruction of the strawberries. The target selection focuses on ripe strawberries while categorizing others as obstacles, followed by converting the refined point cloud into an occupancy map for collision-aware motion planning. Our experimental results demonstrate high shape reconstruction accuracy, with the lowest Chamfer Distance compared to state-of-the-art methods with 1.10 mm, and significantly improved grasp success rates of 79.17%, yielding an overall success-to-attempt ratio of 89.58\% in real-world strawberry harvesting. Additionally, our method reduces the obstacle hit rate from 43.33% to 13.95%, highlighting its effectiveness in improving both grasp quality and safety compared to prior approaches. This pipeline substantially improves autonomous strawberry harvesting, advancing more efficient and reliable robotic fruit picking systems.

</details>


### [16] [ReLCP: Scalable Complementarity-Based Collision Resolution for Smooth Rigid Bodies](https://arxiv.org/abs/2506.14097)
*Bryce Palmer,Hasan Metin Aktulga,Tong Gao*

Main category: cs.RO

TL;DR: 提出了一种基于互补性的碰撞解决算法，适用于光滑非球形刚体，通过递归生成的线性互补问题（ReLCP）自适应识别碰撞位置，显著减少了约束数量，提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统离散表面表示方法在提高分辨率时需要细分几何体，导致约束数量超线性增长，计算效率低下。本文旨在解决这一问题，提出一种自适应方法，避免过度采样。

Method: 采用递归生成的线性互补问题（ReLCP）自适应识别碰撞位置，仅在有重叠风险时引入新约束，减少约束数量。

Result: 相比离散方法，ReLCP方法减少了1-2个数量级的约束，计算速度提升10-100倍，并在多种测试场景中验证了其高效性和稳定性。

Conclusion: ReLCP方法在保持高精度表面表示的同时，显著提升了计算效率和可扩展性，适用于密集场景。

Abstract: We present a complementarity-based collision resolution algorithm for smooth, non-spherical, rigid bodies. Unlike discrete surface representation approaches, which approximate surfaces using discrete elements (e.g., tessellations or sub-spheres) with constraints between nearby faces, edges, nodes, or sub-objects, our algorithm solves a recursively generated linear complementarity problem (ReLCP) to adaptively identify potential collision locations during the collision resolution procedure. Despite adaptively and in contrast to Newton-esque schemes, we prove conditions under which the resulting solution exists and the center of mass translational and rotational dynamics are unique. Our ReLCP also converges to classical LCP-based collision resolution for sufficiently small timesteps. Because increasing the surface resolution in discrete representation methods necessitates subdividing geometry into finer elements -- leading to a super-linear increase in the number of collision constraints -- these approaches scale poorly with increased surface resolution. In contrast, our adaptive ReLCP framework begins with a single constraint per pair of nearby bodies and introduces new constraints only when unconstrained motion would lead to overlap, circumventing the oversampling required by discrete methods. By requiring one to two orders of magnitude fewer collision constraints to achieve the same surface resolution, we observe 10-100x speedup in densely packed applications. We validate our ReLCP method against multisphere and single-constraint methods, comparing convergence in a two-ellipsoid collision test, scalability and performance in a compacting ellipsoid suspension and growing bacterial colony, and stability in a taut chainmail network, highlighting our ability to achieve high-fidelity surface representations without suffering from poor scalability or artificial surface roughness.

</details>


### [17] [Haptic-Based User Authentication for Tele-robotic System](https://arxiv.org/abs/2506.14116)
*Rongyu Yu,Kan Chen,Zeyu Deng,Chen Wang,Burak Kizilkaya,Liying Emma Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于触觉反馈的用户行为认证方法，通过深度学习模型实现高精度身份识别和任务分类。


<details>
  <summary>Details</summary>
Motivation: 传统认证方法（如密码和静态生物特征）在高风险、连续交互的远程机器人任务中易受欺骗和重放攻击，因此需要更安全的认证方案。

Method: 利用触觉反馈提取用户行为特征，并开发基于Transformer的深度学习模型分析时间序列力反馈数据。

Result: 实验表明，该方法在用户识别和任务分类中准确率超过90%。

Conclusion: 该方法为远程机器人系统的访问控制和身份验证提供了潜在解决方案。

Abstract: Tele-operated robots rely on real-time user behavior mapping for remote tasks, but ensuring secure authentication remains a challenge. Traditional methods, such as passwords and static biometrics, are vulnerable to spoofing and replay attacks, particularly in high-stakes, continuous interactions. This paper presents a novel anti-spoofing and anti-replay authentication approach that leverages distinctive user behavioral features extracted from haptic feedback during human-robot interactions. To evaluate our authentication approach, we collected a time-series force feedback dataset from 15 participants performing seven distinct tasks. We then developed a transformer-based deep learning model to extract temporal features from the haptic signals. By analyzing user-specific force dynamics, our method achieves over 90 percent accuracy in both user identification and task classification, demonstrating its potential for enhancing access control and identity assurance in tele-robotic systems.

</details>


### [18] [GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation](https://arxiv.org/abs/2506.14135)
*Ying Chai,Litao Deng,Ruizhi Shao,Jiajun Zhang,Liangjun Xing,Hongwen Zhang,Yebin Liu*

Main category: cs.RO

TL;DR: 提出了一种基于4D高斯动作场（GAF）的V-4D-A框架，用于直接从运动感知的4D表示中推理动作，显著提升了机器人操作的准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（V-A或V-3D-A）在复杂动态场景中动作推理不准确，需要一种更高效且动态感知的框架。

Method: 通过扩展3D高斯泼溅（3DGS）引入可学习运动属性，构建GAF模型，支持场景重建、未来帧预测和动作估计，并结合扩散模型优化动作。

Result: 实验显示GAF在重建质量（PSNR +11.5385 dB，LPIPS -0.5574）和任务成功率（提升10.33%）上显著优于现有方法。

Conclusion: GAF通过4D动态建模和动作感知优化，为机器人操作提供了更准确和高效的解决方案。

Abstract: Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/

</details>


### [19] [Lasso Gripper: A String Shooting-Retracting Mechanism for Shape-Adaptive Grasping](https://arxiv.org/abs/2506.14163)
*Qiyuan Qiao,Yu Wang,Xiyu Fan,Peng Lu*

Main category: cs.RO

TL;DR: 提出了一种新型抓取器Lasso Gripper，通过发射和收回绳子来抓取物体，适用于形状、大小不一的物体，解决了传统抓取器的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统抓取器因形状和尺寸限制，难以处理超大、形状多变或易碎物体。

Method: 受套索和uurga启发，设计了一种由四电机控制的抓取器，通过调节电机速度调整绳圈大小，并加入防缠绕机制和动态模型分析。

Result: 实验显示，Lasso Gripper成功抓取并运输了多种物体，包括动物模型和易碎蔬菜。

Conclusion: Lasso Gripper提供了一种更灵活、温和的抓取方案，适用于多样化物体。

Abstract: Handling oversized, variable-shaped, or delicate objects in transportation, grasping tasks is extremely challenging, mainly due to the limitations of the gripper's shape and size. This paper proposes a novel gripper, Lasso Gripper. Inspired by traditional tools like the lasso and the uurga, Lasso Gripper captures objects by launching and retracting a string. Contrary to antipodal grippers, which concentrate force on a limited area, Lasso Gripper applies uniform pressure along the length of the string for a more gentle grasp. The gripper is controlled by four motors-two for launching the string inward and two for launching it outward. By adjusting motor speeds, the size of the string loop can be tuned to accommodate objects of varying sizes, eliminating the limitations imposed by the maximum gripper separation distance. To address the issue of string tangling during rapid retraction, a specialized mechanism was incorporated. Additionally, a dynamic model was developed to estimate the string's curve, providing a foundation for the kinematic analysis of the workspace. In grasping experiments, Lasso Gripper, mounted on a robotic arm, successfully captured and transported a range of objects, including bull and horse figures as well as delicate vegetables. The demonstration video is available here: https://youtu.be/PV1J76mNP9Y.

</details>


### [20] [TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Indoor Localization and Mapping](https://arxiv.org/abs/2506.14178)
*Jeewon Kim,Minho Oh,Hyun Myung*

Main category: cs.RO

TL;DR: 论文提出了一种名为TACS-Graphs的新框架，通过结合地面机器人可通行性和房间分割，解决了传统3D室内场景图中房间层分割不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D室内场景图在结构复杂环境中存在房间层分割不足或过度的问题，导致语义和拓扑不一致。

Method: 提出TACS-Graphs框架，利用可通行性作为定义房间边界的关键因素，实现更语义化和拓扑一致的分割。

Result: 实验结果表明，该方法在场景图一致性和位姿图优化性能上优于现有方法。

Conclusion: TACS-Graphs通过可通行性感知的分割，显著提升了复杂环境中的场景图一致性和机器人任务规划能力。

Abstract: Scene graphs have emerged as a powerful tool for robots, providing a structured representation of spatial and semantic relationships for advanced task planning. Despite their potential, conventional 3D indoor scene graphs face critical limitations, particularly under- and over-segmentation of room layers in structurally complex environments. Under-segmentation misclassifies non-traversable areas as part of a room, often in open spaces, while over-segmentation fragments a single room into overlapping segments in complex environments. These issues stem from naive voxel-based map representations that rely solely on geometric proximity, disregarding the structural constraints of traversable spaces and resulting in inconsistent room layers within scene graphs. To the best of our knowledge, this work is the first to tackle segmentation inconsistency as a challenge and address it with Traversability-Aware Consistent Scene Graphs (TACS-Graphs), a novel framework that integrates ground robot traversability with room segmentation. By leveraging traversability as a key factor in defining room boundaries, the proposed method achieves a more semantically meaningful and topologically coherent segmentation, effectively mitigating the inaccuracies of voxel-based scene graph approaches in complex environments. Furthermore, the enhanced segmentation consistency improves loop closure detection efficiency in the proposed Consistent Scene Graph-leveraging Loop Closure Detection (CoSG-LCD) leading to higher pose estimation accuracy. Experimental results confirm that the proposed approach outperforms state-of-the-art methods in terms of scene graph consistency and pose graph optimization performance.

</details>


### [21] [Non-Overlap-Aware Egocentric Pose Estimation for Collaborative Perception in Connected Autonomy](https://arxiv.org/abs/2506.14180)
*Hong Huang,Dongkuan Xu,Hao Zhang,Peng Gao*

Main category: cs.RO

TL;DR: 提出了一种新颖的非重叠感知自我中心姿态估计方法（NOPE），用于多机器人协作感知，解决了视图不重叠和通信带宽限制的问题。


<details>
  <summary>Details</summary>
Motivation: 在多机器人协作中，机器人需要知道自身与队友的相对姿态，但由于视图差异和通信带宽限制，传统方法难以实现准确估计。

Method: NOPE采用分层学习框架，结合高层深度图匹配（用于识别视图重叠）和低层位置感知交叉注意力图学习（用于姿态估计）。

Result: 实验表明，NOPE实现了非重叠感知的姿态估计，性能优于现有方法。

Conclusion: NOPE为多机器人协作提供了一种高效且实用的姿态估计解决方案。

Abstract: Egocentric pose estimation is a fundamental capability for multi-robot collaborative perception in connected autonomy, such as connected autonomous vehicles. During multi-robot operations, a robot needs to know the relative pose between itself and its teammates with respect to its own coordinates. However, different robots usually observe completely different views that contains similar objects, which leads to wrong pose estimation. In addition, it is unrealistic to allow robots to share their raw observations to detect overlap due to the limited communication bandwidth constraint. In this paper, we introduce a novel method for Non-Overlap-Aware Egocentric Pose Estimation (NOPE), which performs egocentric pose estimation in a multi-robot team while identifying the non-overlap views and satifying the communication bandwidth constraint. NOPE is built upon an unified hierarchical learning framework that integrates two levels of robot learning: (1) high-level deep graph matching for correspondence identification, which allows to identify if two views are overlapping or not, (2) low-level position-aware cross-attention graph learning for egocentric pose estimation. To evaluate NOPE, we conduct extensive experiments in both high-fidelity simulation and real-world scenarios. Experimental results have demonstrated that NOPE enables the novel capability for non-overlapping-aware egocentric pose estimation and achieves state-of-art performance compared with the existing methods. Our project page at https://hongh0.github.io/NOPE/.

</details>


### [22] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Main category: cs.RO

TL;DR: AMPLIFY框架利用大规模无动作视频数据，通过关键点轨迹生成紧凑的运动标记，分离视觉运动预测与动作推理，提升机器人策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 机器人领域的有标记动作数据稀缺且昂贵，而无动作视频数据丰富但难以转化为有效策略。

Method: AMPLIFY将视觉动态编码为离散运动标记，分别训练前向动态模型（无动作视频）和逆向动态模型（少量标记数据）。

Result: 动态模型准确性显著提升（MSE提高3.7倍，像素预测精度提高2.5倍），下游策略学习效果提升1.2-2.2倍。

Conclusion: AMPLIFY通过异构数据源构建高效、通用的世界模型，为机器人控制与视频预测提供新范式。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.

</details>


### [23] [Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments](https://arxiv.org/abs/2506.14233)
*Amirreza Payandeh,Anuj Pokhrel,Daeun Song,Marcos Zampieri,Xuesu Xiao*

Main category: cs.RO

TL;DR: Narrate2Nav是一种新型实时视觉-动作模型，通过自监督学习框架嵌入自然语言推理和社交线索，显著提升了移动机器人在人机环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLMs）在移动机器人导航中表现出潜力，但计算复杂性和对连续数值数据的敏感性限制了实时性能和精确控制。

Method: 采用基于Barlow Twins冗余减少损失的自监督学习框架，结合RGB输入、运动命令和场景上下文文本信号，训练模型从观察到低层运动命令的映射。

Result: 在离线和真实世界实验中，Narrate2Nav分别比最佳基线提升了52.94%和41.67%，并在视觉注意力上表现出对导航关键元素的增强关注。

Conclusion: Narrate2Nav通过嵌入推理和社交线索，显著提升了人机导航任务的性能，证明了其有效性。

Abstract: Large Vision-Language Models (VLMs) have demonstrated potential in enhancing mobile robot navigation in human-centric environments by understanding contextual cues, human intentions, and social dynamics while exhibiting reasoning capabilities. However, their computational complexity and limited sensitivity to continuous numerical data impede real-time performance and precise motion control. To this end, we propose Narrate2Nav, a novel real-time vision-action model that leverages a novel self-supervised learning framework based on the Barlow Twins redundancy reduction loss to embed implicit natural language reasoning, social cues, and human intentions within a visual encoder-enabling reasoning in the model's latent space rather than token space. The model combines RGB inputs, motion commands, and textual signals of scene context during training to bridge from robot observations to low-level motion commands for short-horizon point-goal navigation during deployment. Extensive evaluation of Narrate2Nav across various challenging scenarios in both offline unseen dataset and real-world experiments demonstrates an overall improvement of 52.94 percent and 41.67 percent, respectively, over the next best baseline. Additionally, qualitative comparative analysis of Narrate2Nav's visual encoder attention map against four other baselines demonstrates enhanced attention to navigation-critical scene elements, underscoring its effectiveness in human-centric navigation tasks.

</details>


### [24] [Robust Adaptive Time-Varying Control Barrier Function with Application to Robotic Surface Treatment](https://arxiv.org/abs/2506.14249)
*Yitaek Kim,Christoffer Sloth*

Main category: cs.RO

TL;DR: 本文提出了一种基于鲁棒自适应控制屏障函数（RaCBFs）的方法，用于处理模型不确定性并满足时变约束，同时结合输入到状态安全（ISSf）和集合成员识别以减少保守性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理时变约束时往往忽略模型不确定性，导致实际应用中的性能下降。本文旨在解决这一问题。

Method: 提出RaCBFs结合ISSf处理模型不确定性和输入扰动，并通过集合成员识别减少保守性。

Result: 在机器人表面处理任务中，数值模拟和实际实验表明，该方法能确保质量在可接受范围内。

Conclusion: 该方法有效解决了时变约束下的模型不确定性问题，并在实际应用中验证了其性能。

Abstract: Set invariance techniques such as control barrier functions (CBFs) can be used to enforce time-varying constraints such as keeping a safe distance from dynamic objects. However, existing methods for enforcing time-varying constraints often overlook model uncertainties. To address this issue, this paper proposes a CBFs-based robust adaptive controller design endowing time-varying constraints while considering parametric uncertainty and additive disturbances. To this end, we first leverage Robust adaptive Control Barrier Functions (RaCBFs) to handle model uncertainty, along with the concept of Input-to-State Safety (ISSf) to ensure robustness towards input disturbances. Furthermore, to alleviate the inherent conservatism in robustness, we also incorporate a set membership identification scheme. We demonstrate the proposed method on robotic surface treatment that requires time-varying force bounds to ensure uniform quality, in numerical simulation and real robotic setup, showing that the quality is formally guaranteed within an acceptable range.

</details>


### [25] [Public Acceptance of Cybernetic Avatars in the service sector: Evidence from a Large-Scale Survey in Dubai](https://arxiv.org/abs/2506.14268)
*Laura Aymerich-Franch,Tarek Taha,Takahiro Miyashita,Hiroko Kamide,Hiroshi Ishiguro,Paolo Dario*

Main category: cs.RO

TL;DR: 研究探讨了迪拜多元文化社会中赛博格化身的接受度，重点关注客户服务机器人，分析了外观、场景和功能对接受度的影响。


<details>
  <summary>Details</summary>
Motivation: 研究赛博格化身在多元文化社会中的接受度，以优化其设计和部署。

Method: 通过大规模调查（1000+参与者）分析不同外观、场景和任务对接受度的影响。

Result: 物理机器人接受度高于数字化身；高度拟人化机器人外观最受欢迎；信息提供任务最受重视；购物中心等场景接受度高，医疗场景较低；不同文化群体偏好不同。

Conclusion: 早期融入用户反馈对提高赛博格化身的社会接受度至关重要。

Abstract: Cybernetic avatars are hybrid interaction robots or digital representations that combine autonomous capabilities with teleoperated control. This study investigates the acceptance of cybernetic avatars in the highly multicultural society of Dubai, with particular emphasis on robotic avatars for customer service. Specifically, we explore how acceptance varies as a function of robot appearance (e.g., android, robotic-looking, cartoonish), deployment settings (e.g., shopping malls, hotels, hospitals), and functional tasks (e.g., providing information, patrolling). To this end, we conducted a large-scale survey with over 1,000 participants. Overall, cybernetic avatars received a high level of acceptance, with physical robot avatars receiving higher acceptance than digital avatars. In terms of appearance, robot avatars with a highly anthropomorphic robotic appearance were the most accepted, followed by cartoonish designs and androids. Animal-like appearances received the lowest level of acceptance. Among the tasks, providing information and guidance was rated as the most valued. Shopping malls, airports, public transport stations, and museums were the settings with the highest acceptance, whereas healthcare-related spaces received lower levels of support. An analysis by community cluster revealed among others that Emirati respondents showed significantly greater acceptance of android appearances compared to the overall sample, while participants from the 'Other Asia' cluster were significantly more accepting of cartoonish appearances. Our study underscores the importance of incorporating citizen feedback into the design and deployment of cybernetic avatars from the early stages to enhance acceptance of this technology in society.

</details>


### [26] [Whole-Body Control Framework for Humanoid Robots with Heavy Limbs: A Model-Based Approach](https://arxiv.org/abs/2506.14278)
*Tianlin Zhang,Linzhu Yue,Hongbo Zhang,Lingwei Zhang,Xuanqi Zeng,Zhitao Song,Yun-Hui Liu*

Main category: cs.RO

TL;DR: 提出了一种基于模型的全身体控制框架，用于解决人形机器人因重型肢体运动导致的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在动态运动或不规则地形中常因重型肢体运动而面临平衡挑战。

Method: 结合运动动力学规划器（MPC）和分层优化问题（HQP），简化系统动力学以实现实时运动规划。

Result: 实验证明，该框架能使机器人达到1.2 m/s的动态行走速度，抵抗60 N的外部干扰，并在复杂地形中保持平衡。

Conclusion: 该框架有效解决了重型肢体人形机器人的平衡问题，适用于动态和复杂环境。

Abstract: Humanoid robots often face significant balance issues due to the motion of their heavy limbs. These challenges are particularly pronounced when attempting dynamic motion or operating in environments with irregular terrain. To address this challenge, this manuscript proposes a whole-body control framework for humanoid robots with heavy limbs, using a model-based approach that combines a kino-dynamics planner and a hierarchical optimization problem. The kino-dynamics planner is designed as a model predictive control (MPC) scheme to account for the impact of heavy limbs on mass and inertia distribution. By simplifying the robot's system dynamics and constraints, the planner enables real-time planning of motion and contact forces. The hierarchical optimization problem is formulated using Hierarchical Quadratic Programming (HQP) to minimize limb control errors and ensure compliance with the policy generated by the kino-dynamics planner. Experimental validation of the proposed framework demonstrates its effectiveness. The humanoid robot with heavy limbs controlled by the proposed framework can achieve dynamic walking speeds of up to 1.2~m/s, respond to external disturbances of up to 60~N, and maintain balance on challenging terrains such as uneven surfaces, and outdoor environments.

</details>


### [27] [Steering Robots with Inference-Time Interactions](https://arxiv.org/abs/2506.14287)
*Yanwei Wang*

Main category: cs.RO

TL;DR: 论文提出了一种无需微调预训练策略的方法，通过用户交互在推理时引导行为生成，以纠正策略错误。


<details>
  <summary>Details</summary>
Motivation: 预训练策略在部署时可能因泛化能力不足而犯错，传统方法需额外数据微调，效率低下。

Method: 提出两种框架：(1) 推理时引导，通过用户交互切换离散技能；(2) 任务与动作模仿，允许用户编辑连续动作以满足任务约束。

Result: 无需额外训练即可纠正策略预测偏差，提升预训练模型的实用性。

Conclusion: 通过用户交互在推理时引导行为生成，是一种高效且无需微调的解决方案。

Abstract: Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.

</details>


### [28] [Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation](https://arxiv.org/abs/2506.14294)
*Prashant Kumar Rai,Elham Kowsari,Nataliya Strokina,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 本文提出了一种结合高分辨率成像雷达和惯性测量单元（IMU）的自导航方法，通过神经网络处理雷达数据并估计瞬时线速度及其不确定性，再通过扩展卡尔曼滤波器与IMU数据融合，显著提高了运动估计的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统雷达运动估计方法存在局限性，本文旨在通过结合雷达和IMU数据，利用神经网络处理复杂雷达数据，提升运动估计的精度和鲁棒性。

Method: 使用神经网络处理原始雷达数据，估计瞬时线速度及不确定性，再通过扩展卡尔曼滤波器融合IMU数据，优化噪声和偏差参数。

Result: 在ColoRadar数据集上验证，本文方法显著优于现有公开方法，且在瞬时和扫描匹配技术上表现更优。

Conclusion: 结合雷达与IMU的神经网络方法有效提升了自导航中的运动估计精度和鲁棒性。

Abstract: We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.

</details>


### [29] [Socially Aware Robot Crowd Navigation via Online Uncertainty-Driven Risk Adaptation](https://arxiv.org/abs/2506.14305)
*Zhirui Sun,Xingrong Diao,Yao Wang,Bi-Ke Zhu,Jiankun Wang*

Main category: cs.RO

TL;DR: LR-MPC是一种数据驱动的导航算法，通过离线风险学习和在线自适应推理，平衡效率、安全性和社交意识，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在人与机器人共享的拥挤环境中导航具有挑战性，现有方法常忽视社交意识。

Method: LR-MPC包括离线风险学习（使用PENN训练）和在线自适应推理（通过Multi-RRT规划器引导）。

Result: 实验表明，LR-MPC在成功率和社交意识上优于基线方法。

Conclusion: LR-MPC能高效导航复杂人群，具有高适应性和低干扰性。

Abstract: Navigation in human-robot shared crowded environments remains challenging, as robots are expected to move efficiently while respecting human motion conventions. However, many existing approaches emphasize safety or efficiency while overlooking social awareness. This article proposes Learning-Risk Model Predictive Control (LR-MPC), a data-driven navigation algorithm that balances efficiency, safety, and social awareness. LR-MPC consists of two phases: an offline risk learning phase, where a Probabilistic Ensemble Neural Network (PENN) is trained using risk data from a heuristic MPC-based baseline (HR-MPC), and an online adaptive inference phase, where local waypoints are sampled and globally guided by a Multi-RRT planner. Each candidate waypoint is evaluated for risk by PENN, and predictions are filtered using epistemic and aleatoric uncertainty to ensure robust decision-making. The safest waypoint is selected as the MPC input for real-time navigation. Extensive experiments demonstrate that LR-MPC outperforms baseline methods in success rate and social awareness, enabling robots to navigate complex crowds with high adaptability and low disruption. A website about this work is available at https://sites.google.com/view/lr-mpc.

</details>


### [30] [ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes](https://arxiv.org/abs/2506.14317)
*Zeyuan Chen,Qiyang Yan,Yuanpei Chen,Tianhao Wu,Jiyao Zhang,Zihan Ding,Jinzhou Li,Yaodong Yang,Hao Dong*

Main category: cs.RO

TL;DR: 论文提出了一种名为ClutterDexGrasp的两阶段师生框架，用于在杂乱场景中实现零样本的闭环目标导向灵巧抓取。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单物体抓取或无交互的抓取姿势预测，难以应对复杂杂乱场景。视觉-语言-动作模型虽具潜力，但需要大量真实世界演示，成本高且难以扩展。

Method: 采用模拟到现实的迁移管道，结合密度课程学习训练教师策略，并通过模仿学习将知识蒸馏到学生3D扩散策略（DP3）中。

Result: 实现了首个零样本模拟到现实的闭环系统，在多样化物体和布局中表现出鲁棒性能。

Conclusion: ClutterDexGrasp为杂乱场景中的灵巧抓取提供了一种高效且可扩展的解决方案。

Abstract: Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a novel geometry and spatially-embedded scene representation and a comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target-oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts. More details and videos are available at https://clutterdexgrasp.github.io/.

</details>


### [31] [Data Driven Approach to Input Shaping for Vibration Suppression in a Flexible Robot Arm](https://arxiv.org/abs/2506.14405)
*Jarkko Kotaniemi,Janne Saukkoriipi,Shuai Li,Markku Suomalainen*

Main category: cs.RO

TL;DR: 提出了一种基于数据驱动的方法，用于自适应调整柔性机械臂输入整形器的参数，以抑制残余振动。


<details>
  <summary>Details</summary>
Motivation: 柔性机械臂的残余振动问题影响其精度和性能，传统方法难以适应不同材料和工况。

Method: 通过插值先前测量的残余振动数据，自适应调整输入整形器的参数。输入整形技术通过卷积脉冲序列与输入命令生成减振命令。

Result: 在多种材料的3D打印柔性机械臂上验证，显著减少了残余振动。

Conclusion: 该方法简单有效，适用于不同材料的柔性机械臂，具有实际应用潜力。

Abstract: This paper presents a simple and effective method for setting parameters for an input shaper to suppress the residual vibrations in flexible robot arms using a data-driven approach. The parameters are adaptively tuned in the workspace of the robot by interpolating previously measured data of the robot's residual vibrations. Input shaping is a simple and robust technique to generate vibration-reduced shaped commands by a convolution of an impulse sequence with the desired input command. The generated impulses create waves in the material countering the natural vibrations of the system. The method is demonstrated with a flexible 3D-printed robot arm with multiple different materials, achieving a significant reduction in the residual vibrations.

</details>


### [32] [Enhancing Object Search in Indoor Spaces via Personalized Object-factored Ontologies](https://arxiv.org/abs/2506.14422)
*Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: 论文提出了一种个性化框架和自适应推理策略，以提高机器人在室内环境中长期对象搜索的能力。


<details>
  <summary>Details</summary>
Motivation: 个性化对服务机器人的发展至关重要，机器人需要适应环境变化并长期部署，以执行复杂任务。

Method: 提出个性化本体框架和自适应推理策略，结合动态信念更新，实现多对象搜索。

Result: 实验证明该方法优于现有技术，个性化能进一步提升性能。

Conclusion: 个性化与自适应推理的结合显著提升了机器人在长期对象搜索中的能力。

Abstract: Personalization is critical for the advancement of service robots. Robots need to develop tailored understandings of the environments they are put in. Moreover, they need to be aware of changes in the environment to facilitate long-term deployment. Long-term understanding as well as personalization is necessary to execute complex tasks like prepare dinner table or tidy my room. A precursor to such tasks is that of Object Search. Consequently, this paper focuses on locating and searching multiple objects in indoor environments. In this paper, we propose two crucial novelties. Firstly, we propose a novel framework that can enable robots to deduce Personalized Ontologies of indoor environments. Our framework consists of a personalization schema that enables the robot to tune its understanding of ontologies. Secondly, we propose an Adaptive Inferencing strategy. We integrate Dynamic Belief Updates into our approach which improves performance in multi-object search tasks. The cumulative effect of personalization and adaptive inferencing is an improved capability in long-term object search. This framework is implemented on top of a multi-layered semantic map. We conduct experiments in real environments and compare our results against various state-of-the-art (SOTA) methods to demonstrate the effectiveness of our approach. Additionally, we show that personalization can act as a catalyst to enhance the performance of SOTAs. Video Link: https://bit.ly/3WHk9i9

</details>


### [33] [Automatic Cannulation of Femoral Vessels in a Porcine Shock Model](https://arxiv.org/abs/2506.14467)
*Nico Zevallos,Cecilia G. Morales,Andrew Orekhov,Tejas Rane,Hernando Gomez,Francis X. Guyette,Michael R. Pinsky,John Galeotti,Artur Dubrawski,Howie Choset*

Main category: cs.RO

TL;DR: 论文提出了一种全自动机器人超声引导的中央血管通路技术，成功在猪出血性休克模型中实现了股静脉和动脉插管。


<details>
  <summary>Details</summary>
Motivation: 在创伤和重症监护中，快速可靠的血管通路至关重要，但中央血管通路需要专业的超声引导技能，且传统技术复杂，导致其在前线环境中应用受限。

Method: 研究采用机器人超声引导技术，实现了全自动的股静脉和动脉插管，目标是解决传统技术对专业技能的依赖问题。

Result: 在猪出血性休克模型中，成功实现了股静脉和动脉的插管，验证了全自动技术的可行性。

Conclusion: 全自动机器人超声引导技术有望提升中央血管通路的效率和安全性，尤其在资源有限的环境中具有重要应用前景。

Abstract: Rapid and reliable vascular access is critical in trauma and critical care. Central vascular catheterization enables high-volume resuscitation, hemodynamic monitoring, and advanced interventions like ECMO and REBOA. While peripheral access is common, central access is often necessary but requires specialized ultrasound-guided skills, posing challenges in prehospital settings. The complexity arises from deep target vessels and the precision needed for needle placement. Traditional techniques, like the Seldinger method, demand expertise to avoid complications. Despite its importance, ultrasound-guided central access is underutilized due to limited field expertise. While autonomous needle insertion has been explored for peripheral vessels, only semi-autonomous methods exist for femoral access. This work advances toward full automation, integrating robotic ultrasound for minimally invasive emergency procedures. Our key contribution is the successful femoral vein and artery cannulation in a porcine hemorrhagic shock model.

</details>


### [34] [ros2 fanuc interface: Design and Evaluation of a Fanuc CRX Hardware Interface in ROS2](https://arxiv.org/abs/2506.14487)
*Paolo Franceschi,Marco Faroni,Stefano Baraldo,Anna Valente*

Main category: cs.RO

TL;DR: 本文介绍了ROS2控制与Fanuc CRX机器人系列的硬件接口集成，详细说明了实现细节、通信协议及其与Moveit2运动规划库的集成。通过实验评估了机器人性能，结果表明机器人能够准确跟踪路径并实现避障。


<details>
  <summary>Details</summary>
Motivation: 为Fanuc CRX机器人系列开发ROS2控制接口，并验证其在多种机器人任务中的性能表现。

Method: 实现ROS2硬件接口，集成Moveit2，并通过四种机器人任务（阶跃响应、轨迹跟踪、避障和动态速度缩放）进行实验评估。

Result: 尽管存在命令与反馈的延迟，机器人仍能准确跟踪路径（符合关节速度限制）并实现避障。

Conclusion: 开发的ros2_fanuc_interface性能良好，代码已开源。

Abstract: This paper introduces the ROS2 control and the Hardware Interface (HW) integration for the Fanuc CRX- robot family. It explains basic implementation details and communication protocols, and its integration with the Moveit2 motion planning library. We conducted a series of experiments to evaluate relevant performances in the robotics field. We tested the developed ros2_fanuc_interface for four relevant robotics cases: step response, trajectory tracking, collision avoidance integrated with Moveit2, and dynamic velocity scaling, respectively. Results show that, despite a non-negligible delay between command and feedback, the robot can track the defined path with negligible errors (if it complies with joint velocity limits), ensuring collision avoidance. Full code is open source and available at https://github.com/paolofrance/ros2_fanuc_interface.

</details>


### [35] [Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?](https://arxiv.org/abs/2506.14507)
*Nitesh Subedi,Adam Haroon,Shreyan Ganguly,Samuel T. K. Tetteh,Prajwal Koirala,Cody Fleming,Soumik Sarkar*

Main category: cs.RO

TL;DR: 论文探讨了预训练的视觉语言模型（VLMs）是否能在无需微调或专用模块的情况下成功指导导航任务，并提出了一个直接基于冻结嵌入的行为克隆框架。


<details>
  <summary>Details</summary>
Motivation: 研究预训练嵌入是否能独立支持导航任务，避免复杂架构和微调的需求。

Method: 训练行为克隆策略，直接使用冻结的视觉语言嵌入，基于专家演示数据。

Result: 导航成功率为74%，但步数比专家多3.2倍，显示预训练嵌入在长期规划和空间推理上的局限性。

Conclusion: 预训练嵌入支持基础语言理解，但在复杂任务中表现不足，为机器人设计提供了权衡参考。

Abstract: Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training. While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional fine-tuning or specialized modules? We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert. Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average. This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning. By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios. Our code is available at https://github.com/oadamharoon/text2nav

</details>


### [36] [GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments](https://arxiv.org/abs/2506.14513)
*Farha Abdul Wasay,Mohammed Abdul Rahman,Hania Ghouse*

Main category: cs.RO

TL;DR: GAMORA是一种基于VR的机器人系统，通过手势控制远程执行高风险实验室任务，结合数字孪生和逆向运动学，提高精度和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着生物危害复杂性增加，减少直接人类接触并保持操作精度成为关键需求。

Method: 系统整合Oculus Quest 2、NVIDIA Jetson Nano和ROS，实现实时沉浸式控制、数字孪生模拟和逆向运动学操作。

Result: GAMORA实现了2.2毫米的位置误差、0.2毫升的移液精度和1.2毫米的重复性，能耗降低50%。

Conclusion: GAMORA为生物医学研究提供了一种可扩展、沉浸式的机器人控制解决方案，提升了生物安全性。

Abstract: The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.

</details>


### [37] [NetRoller: Interfacing General and Specialized Models for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.14589)
*Ren Xin,Hongji Liu,Xiaodong Mei,Wenru Liu,Maosheng Ye,Zhili Chen,Jun Ma*

Main category: cs.RO

TL;DR: NetRoller是一个适配器，通过三阶段机制无缝集成通用模型（GMs）和专用驾驶模型（SMs），解决异步系统问题，提升自动驾驶任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有专用驾驶模型在数据多样性和模型能力上存在不足，而通用模型（如LLMs）的引入虽具潜力，但会引发异步系统问题。

Method: NetRoller通过早期停止机制提取LLMs的语义表示，利用可学习查询嵌入和位置层嵌入实现跨模态翻译，并通过查询和特征偏移机制优化SMs性能。

Result: 在nuScenes数据集上，NetRoller显著提升了规划任务的人机相似性和安全性，并在检测与建图任务中实现了精度提升。

Conclusion: NetRoller成功解决了GMs与SMs的集成问题，为自动驾驶任务提供了高效且性能优越的解决方案。

Abstract: Integrating General Models (GMs) such as Large Language Models (LLMs), with Specialized Models (SMs) in autonomous driving tasks presents a promising approach to mitigating challenges in data diversity and model capacity of existing specialized driving models. However, this integration leads to problems of asynchronous systems, which arise from the distinct characteristics inherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an adapter that incorporates a set of novel mechanisms to facilitate the seamless integration of GMs and specialized driving models. Specifically, our mechanisms for interfacing the asynchronous GMs and SMs are organized into three key stages. NetRoller first harvests semantically rich and computationally efficient representations from the reasoning processes of LLMs using an early stopping mechanism, which preserves critical insights on driving context while maintaining low overhead. It then applies learnable query embeddings, nonsensical embeddings, and positional layer embeddings to facilitate robust and efficient cross-modality translation. At last, it employs computationally efficient Query Shift and Feature Shift mechanisms to enhance the performance of SMs through few-epoch fine-tuning. Based on the mechanisms formalized in these three stages, NetRoller enables specialized driving models to operate at their native frequencies while maintaining situational awareness of the GM. Experiments conducted on the nuScenes dataset demonstrate that integrating GM through NetRoller significantly improves human similarity and safety in planning tasks, and it also achieves noticeable precision improvements in detection and mapping tasks for end-to-end autonomous driving. The code and models are available at https://github.com/Rex-sys-hk/NetRoller .

</details>


### [38] [Latent Action Diffusion for Cross-Embodiment Manipulation](https://arxiv.org/abs/2506.14608)
*Erik Bauer,Elvis Nava,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 论文提出了一种基于扩散策略的潜在动作空间方法，用于统一不同末端执行器的动作空间，实现跨机器人形态的技能迁移和多机器人控制。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中数据稀缺和不同末端执行器动作空间异构性带来的跨形态学习和技能迁移障碍。

Method: 通过学习语义对齐的潜在动作空间，使用对比损失训练的编码器，并在该空间中进行多机器人数据的联合训练。

Result: 在跨形态控制中，单一策略实现了13%的操作成功率提升，表明技能迁移成功。

Conclusion: 潜在跨形态策略为统一不同动作空间提供了新方法，减少了数据收集需求，加速了跨形态泛化，推动了机器人学习的可扩展性和效率。

Abstract: End-to-end learning approaches offer great potential for robotic manipulation, but their impact is constrained by data scarcity and heterogeneity across different embodiments. In particular, diverse action spaces across different end-effectors create barriers for cross-embodiment learning and skill transfer. We address this challenge through diffusion policies learned in a latent action space that unifies diverse end-effector actions. We first show that we can learn a semantically aligned latent action space for anthropomorphic robotic hands, a human hand, and a parallel jaw gripper using encoders trained with a contrastive loss. Second, we show that by using our proposed latent action space for co-training on manipulation data from different end-effectors, we can utilize a single policy for multi-robot control and obtain up to 13% improved manipulation success rates, indicating successful skill transfer despite a significant embodiment gap. Our approach using latent cross-embodiment policies presents a new method to unify different action spaces across embodiments, enabling efficient multi-robot control and data sharing across robot setups. This unified representation significantly reduces the need for extensive data collection for each new robot morphology, accelerates generalization across embodiments, and ultimately facilitates more scalable and efficient robotic learning.

</details>


### [39] [SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning](https://arxiv.org/abs/2506.14648)
*Hexian Ni,Tao Lu,Haoyuan Hu,Yinghao Cai,Shuo Wang*

Main category: cs.RO

TL;DR: SENIOR是一种基于偏好的强化学习方法，通过高效查询选择和偏好引导探索，提升反馈和样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统PbRL方法存在反馈和样本效率低的问题，限制了其应用。

Method: 提出MDS方案选择易比较的行为片段对，以及PGE方法引导探索高偏好状态。

Result: 在六项复杂机器人任务中，SENIOR在反馈效率和策略收敛速度上优于五种现有方法。

Conclusion: SENIOR通过MDS和PGE的协同作用，显著加速了奖励和策略学习。

Abstract: Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.

</details>


### [40] [Factor-Graph-Based Passive Acoustic Navigation for Decentralized Cooperative Localization Using Bearing Elevation Depth Difference](https://arxiv.org/abs/2506.14690)
*Kalliyan Velasco,Timothy W. McLain,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种基于因子图的多智能体水下定位框架，结合方位角、仰角和深度差（BEDD），利用倒置超短基线（inverted-USBL）测量实现AUV团队的协作定位。


<details>
  <summary>Details</summary>
Motivation: 水下通信限制导致多智能体定位难以实现高精度和可扩展性。

Method: 使用因子图表示，结合BEDD测量，利用inverted-USBL获取方位角和仰角数据，以及相对深度测量。

Result: 在HoloOcean模拟器中验证，定位精度优于航位推算，并发现方位角和仰角测量异常值的影响。

Conclusion: 需开发鲁棒的异常值剔除技术以提升声学信号定位的可靠性。

Abstract: Accurate and scalable underwater multi-agent localization remains a critical challenge due to the constraints of underwater communication. In this work, we propose a multi-agent localization framework using a factor-graph representation that incorporates bearing, elevation, and depth difference (BEDD). Our method leverages inverted ultra-short baseline (inverted-USBL) derived azimuth and elevation measurements from incoming acoustic signals and relative depth measurements to enable cooperative localization for a multi-robot team of autonomous underwater vehicles (AUVs). We validate our approach in the HoloOcean underwater simulator with a fleet of AUVs, demonstrating improved localization accuracy compared to dead reckoning. Additionally, we investigate the impact of azimuth and elevation measurement outliers, highlighting the need for robust outlier rejection techniques for acoustic signals.

</details>


### [41] [Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models](https://arxiv.org/abs/2506.14727)
*Huihan Liu,Rutav Shah,Shuijing Liu,Jack Pittenger,Mingyo Seo,Yuchen Cui,Yonatan Bisk,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: Casper是一个辅助遥操作系统，利用预训练视觉语言模型的常识知识进行实时意图推断和灵活技能执行，提升任务表现并降低用户认知负担。


<details>
  <summary>Details</summary>
Motivation: 现实世界辅助遥操作中，机器人需从用户控制输入推断广泛意图并提供正确协助，现有方法局限于简单场景或任务特定数据分布。

Method: Casper结合开放世界感知模块、基于VLM的意图推断机制和技能库，支持多样化的移动操作任务。

Result: 实验表明，Casper在任务表现、认知负荷和用户满意度上优于直接遥操作和其他基线方法。

Conclusion: Casper通过常识推理和灵活技能执行，显著提升了辅助遥操作的实用性和用户体验。

Abstract: Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.

</details>


### [42] [Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation](https://arxiv.org/abs/2506.14754)
*Carolina Higuera,Akash Sharma,Taosha Fan,Chaithanya Krishna Bodduluri,Byron Boots,Michael Kaess,Mike Lambeta,Tingfan Wu,Zixi Liu,Francois Robert Hogan,Mustafa Mukadam*

Main category: cs.RO

TL;DR: Sparsh-X是一种多感官触觉表示方法，整合了图像、音频、运动和压力四种触觉模态，通过自监督学习提升机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效整合多感官触觉信号以提升机器人操作的鲁棒性和成功率。

Method: 利用Digit 360传感器收集的约100万次接触交互数据，通过自监督学习融合四种触觉模态为统一表示。

Result: Sparsh-X在模仿学习和触觉适应任务中，策略成功率提升63%，鲁棒性提升90%，物理属性推断准确率提升48%。

Conclusion: 多感官预训练能有效捕捉灵巧操作所需的关键特征，优于端到端方法。

Abstract: We present Sparsh-X, the first multisensory touch representations across four tactile modalities: image, audio, motion, and pressure. Trained on ~1M contact-rich interactions collected with the Digit 360 sensor, Sparsh-X captures complementary touch signals at diverse temporal and spatial scales. By leveraging self-supervised learning, Sparsh-X fuses these modalities into a unified representation that captures physical properties useful for robot manipulation tasks. We study how to effectively integrate real-world touch representations for both imitation learning and tactile adaptation of sim-trained policies, showing that Sparsh-X boosts policy success rates by 63% over an end-to-end model using tactile images and improves robustness by 90% in recovering object states from touch. Finally, we benchmark Sparsh-X ability to make inferences about physical properties, such as object-action identification, material-quantity estimation, and force estimation. Sparsh-X improves accuracy in characterizing physical properties by 48% compared to end-to-end approaches, demonstrating the advantages of multisensory pretraining for capturing features essential for dexterous manipulation.

</details>


### [43] [RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills](https://arxiv.org/abs/2506.14763)
*Chunru Lin,Haotian Yuan,Yian Wang,Xiaowen Qiu,Tsun-Hsuan Wang,Minghao Guo,Bohan Wang,Yashraj Narang,Dieter Fox,Chuang Gan*

Main category: cs.RO

TL;DR: 论文提出RobotSmith，一种自动化工具设计管道，结合视觉语言模型（VLM）和物理仿真，优化工具几何和使用方式，显著提升机器人任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有工具设计方法依赖预定义模板或通用3D生成方法，未能针对机器人操作优化。机器人直接使用人类设计的工具可能不理想，需自动化工具设计能力。

Method: 系统通过协作VLM代理迭代提出工具设计，生成低级机器人轨迹，并联合优化工具几何和使用方式。

Result: 实验显示，方法在多种任务中表现优异，平均成功率50.0%，远超3D生成（21.4%）和工具检索（11.1%）。

Conclusion: RobotSmith验证了自动化工具设计的实用性和泛化能力，成功应用于实际场景。

Abstract: Endowing robots with tool design abilities is critical for enabling them to solve complex manipulation tasks that would otherwise be intractable. While recent generative frameworks can automatically synthesize task settings, such as 3D scenes and reward functions, they have not yet addressed the challenge of tool-use scenarios. Simply retrieving human-designed tools might not be ideal since many tools (e.g., a rolling pin) are difficult for robotic manipulators to handle. Furthermore, existing tool design approaches either rely on predefined templates with limited parameter tuning or apply generic 3D generation methods that are not optimized for tool creation. To address these limitations, we propose RobotSmith, an automated pipeline that leverages the implicit physical knowledge embedded in vision-language models (VLMs) alongside the more accurate physics provided by physics simulations to design and use tools for robotic manipulation. Our system (1) iteratively proposes tool designs using collaborative VLM agents, (2) generates low-level robot trajectories for tool use, and (3) jointly optimizes tool geometry and usage for task performance. We evaluate our approach across a wide range of manipulation tasks involving rigid, deformable, and fluid objects. Experiments show that our method consistently outperforms strong baselines in terms of both task success rate and overall performance. Notably, our approach achieves a 50.0\% average success rate, significantly surpassing other baselines such as 3D generation (21.4%) and tool retrieval (11.1%). Finally, we deploy our system in real-world settings, demonstrating that the generated tools and their usage plans transfer effectively to physical execution, validating the practicality and generalization capabilities of our approach.

</details>


### [44] [GMT: General Motion Tracking for Humanoid Whole-Body Control](https://arxiv.org/abs/2506.14770)
*Zixuan Chen,Mazeyu Ji,Xuxin Cheng,Xuanbin Peng,Xue Bin Peng,Xiaolong Wang*

Main category: cs.RO

TL;DR: GMT是一个通用的运动跟踪框架，通过自适应采样和运动混合专家架构，训练单一策略使类人机器人跟踪多样化运动。


<details>
  <summary>Details</summary>
Motivation: 解决类人机器人在真实世界中跟踪多样化运动时面临的时空和运动学多样性、策略能力及上下半身协调问题。

Method: 采用自适应采样策略平衡训练中的难易运动，结合运动混合专家架构优化运动流形的不同区域。

Result: 在仿真和真实实验中验证了GMT的有效性，实现了统一策略下的最广泛运动跟踪性能。

Conclusion: GMT通过创新架构和策略，成功实现了类人机器人多样化运动跟踪的通用性和可扩展性。

Abstract: The ability to track general whole-body motions in the real world is a useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policy's capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across a broad spectrum of motions using a unified general policy. Videos and additional information can be found at https://gmt-humanoid.github.io.

</details>
