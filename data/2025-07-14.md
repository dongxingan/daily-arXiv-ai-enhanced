<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 12]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Noise-Enabled Goal Attainment in Crowded Collectives](https://arxiv.org/abs/2507.08100)
*Lucy Liu,Justin Werfel,Federico Toschi,L. Mahadevan*

Main category: cs.RO

TL;DR: 研究通过模拟、理论和机器人实验，探讨噪声运动如何破坏交通堵塞并促进流动，发现噪声水平超过临界值时堵塞消失，提出了优化目标达成率的方法。


<details>
  <summary>Details</summary>
Motivation: 研究拥挤环境中个体导航和交通流控制，为机器人群体协调和基础设施设计提供理论支持。

Method: 结合模拟、理论和机器人实验，分析噪声对交通流的影响，并比较局部导航与中央规划器的性能。

Result: 噪声水平超过临界值时交通堵塞消失，简单反应性导航在中等密度下表现优异且计算高效。

Conclusion: 简单反应性导航方案在现实问题中具有高效性和实用性，适用于中等密度环境。

Abstract: In crowded environments, individuals must navigate around other occupants to
reach their destinations. Understanding and controlling traffic flows in these
spaces is relevant to coordinating robot swarms and designing infrastructure
for dense populations. Here, we combine simulations, theory, and robotic
experiments to study how noisy motion can disrupt traffic jams and enable flow
as agents travel to individual goals. Above a critical noise level, large jams
do not persist. From this observation, we analytically approximate the goal
attainment rate as a function of the noise level, then solve for the optimal
agent density and noise level that maximize the swarm's goal attainment rate.
We perform robotic experiments to corroborate our simulated and theoretical
results. Finally, we compare simple, local navigation approaches with a
sophisticated but computationally costly central planner. A simple reactive
scheme performs well up to moderate densities and is far more computationally
efficient than a planner, suggesting lessons for real-world problems.

</details>


### [2] [Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion](https://arxiv.org/abs/2507.08112)
*Lamiaa H. Zain,Hossam H. Ammar,Raafat E. Shalaby*

Main category: cs.RO

TL;DR: 研究设计并测试了两种自定义CNN，用于移动机器人的避障导航，通过传感器融合输出角速度作为转向指令。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在已知和未知环境中的避障导航至关重要。

Method: 使用深度相机的彩色和深度图像作为输入，设计并训练两种CNN，通过传感器融合生成角速度输出。数据收集通过Wi-Fi和ROS实现同步记录。

Result: 通过多种评估指标（如均方误差、方差分数和前馈时间）比较了两种网络的性能。

Conclusion: 研究明确了哪种网络更适合实际应用。

Abstract: Obstacle avoidance is crucial for mobile robots' navigation in both known and
unknown environments. This research designs, trains, and tests two custom
Convolutional Neural Networks (CNNs), using color and depth images from a depth
camera as inputs. Both networks adopt sensor fusion to produce an output: the
mobile robot's angular velocity, which serves as the robot's steering command.
A newly obtained visual dataset for navigation was collected in diverse
environments with varying lighting conditions and dynamic obstacles. During
data collection, a communication link was established over Wi-Fi between a
remote server and the robot, using Robot Operating System (ROS) topics.
Velocity commands were transmitted from the server to the robot, enabling
synchronized recording of visual data and the corresponding steering commands.
Various evaluation metrics, such as Mean Squared Error, Variance Score, and
Feed-Forward time, provided a clear comparison between the two networks and
clarified which one to use for the application.

</details>


### [3] [Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning](https://arxiv.org/abs/2507.08224)
*Chan Young Park,Jillian Fisher,Marius Memmel,Dipika Khullar,Andy Yun,Abhishek Gupta,Yejin Choi*

Main category: cs.RO

TL;DR: SelfReVision是一个轻量级、可扩展的自改进框架，用于提升视觉语言模型在机器人程序规划中的表现，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在机器人程序规划中表现良好，但缺乏低层次细节；视觉语言模型（VLMs）虽有潜力，但现有方法依赖昂贵的大模型或局限于模拟环境。

Method: SelfReVision通过自蒸馏循环（自我批评、修订和验证）提升小型VLMs的规划能力，无需外部监督。

Result: SelfReVision显著提升了小型VLMs的性能，甚至优于规模大100倍的模型，并改善了下游具体任务的控制能力。

Conclusion: SelfReVision为轻量级视觉语言模型提供了一种高效的自改进方法，适用于实际机器人任务。

Abstract: Large language models (LLMs) have shown promise in robotic procedural
planning, yet their human-centric reasoning often omits the low-level, grounded
details needed for robotic execution. Vision-language models (VLMs) offer a
path toward more perceptually grounded plans, but current methods either rely
on expensive, large-scale models or are constrained to narrow simulation
settings. We introduce SelfReVision, a lightweight and scalable
self-improvement framework for vision-language procedural planning.
SelfReVision enables small VLMs to iteratively critique, revise, and verify
their own plans-without external supervision or teacher models-drawing
inspiration from chain-of-thought prompting and self-instruct paradigms.
Through this self-distillation loop, models generate higher-quality,
execution-ready plans that can be used both at inference and for continued
fine-tuning. Using models varying from 3B to 72B, our results show that
SelfReVision not only boosts performance over weak base VLMs but also
outperforms models 100X the size, yielding improved control in downstream
embodied tasks.

</details>


### [4] [CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations](https://arxiv.org/abs/2507.08262)
*Wenbo Cui,Chengyang Zhao,Yuhui Chen,Haoran Li,Zhizheng Zhang,Dongbin Zhao,He Wang*

Main category: cs.RO

TL;DR: CL3R是一种新型3D预训练框架，通过结合点云掩码自编码器和对比学习，提升机器人操作策略的空间感知和语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练2D基础模型，但缺乏3D空间信息和对多样化相机视角的泛化能力，限制了机器人精细操作的效果。

Method: 提出CL3R框架，利用点云掩码自编码器学习3D表征，并通过对比学习结合预训练2D模型的语义知识。统一数据集坐标系并融合多视角点云，提升泛化能力。

Result: 在仿真和真实环境中的实验表明，CL3R在机器人操作任务中表现优越，尤其在处理新视角时具有鲁棒性。

Conclusion: CL3R通过增强3D感知能力，显著提升了机器人操作策略的效果，为视觉运动策略学习提供了新方向。

Abstract: Building a robust perception module is crucial for visuomotor policy
learning. While recent methods incorporate pre-trained 2D foundation models
into robotic perception modules to leverage their strong semantic
understanding, they struggle to capture 3D spatial information and generalize
across diverse camera viewpoints. These limitations hinder the policy's
effectiveness, especially in fine-grained robotic manipulation scenarios. To
address these challenges, we propose CL3R, a novel 3D pre-training framework
designed to enhance robotic manipulation policies. Our method integrates both
spatial awareness and semantic understanding by employing a point cloud Masked
Autoencoder to learn rich 3D representations while leveraging pre-trained 2D
foundation models through contrastive learning for efficient semantic knowledge
transfer. Additionally, we propose a 3D visual representation pre-training
framework for robotic tasks. By unifying coordinate systems across datasets and
introducing random fusion of multi-view point clouds, we mitigate camera view
ambiguity and improve generalization, enabling robust perception from novel
viewpoints at test time. Extensive experiments in both simulation and the real
world demonstrate the superiority of our method, highlighting its effectiveness
in visuomotor policy learning for robotic manipulation.

</details>


### [5] [Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots](https://arxiv.org/abs/2507.08303)
*Yang Zhang,Zhanxiang Cao,Buqing Nie,Haoyang Li,Yue Gao*

Main category: cs.RO

TL;DR: 提出了一种新的对抗训练范式，通过可学习的对抗攻击网络增强人形机器人运动策略的鲁棒性，实验证明其显著提升了机器人在真实环境中的运动能力。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的运动策略因仿真与现实的动态差异导致鲁棒性下降，影响机器人的敏捷性。

Method: 引入可学习的对抗攻击网络，识别运动策略的脆弱点并施加针对性扰动，通过动态对抗训练提升鲁棒性。

Result: 在Unitree G1人形机器人上的实验表明，该方法显著提升了机器人在真实环境中的运动鲁棒性，成功应对复杂地形和敏捷全身轨迹跟踪。

Conclusion: 提出的对抗训练范式有效解决了仿真与现实的动态差异问题，显著提升了人形机器人在真实环境中的运动能力。

Abstract: Humanoid robots show significant potential in daily tasks. However,
reinforcement learning-based motion policies often suffer from robustness
degradation due to the sim-to-real dynamics gap, thereby affecting the agility
of real robots. In this work, we propose a novel robust adversarial training
paradigm designed to enhance the robustness of humanoid motion policies in real
worlds. The paradigm introduces a learnable adversarial attack network that
precisely identifies vulnerabilities in motion policies and applies targeted
perturbations, forcing the motion policy to enhance its robustness against
perturbations through dynamic adversarial training. We conduct experiments on
the Unitree G1 humanoid robot for both perceptive locomotion and whole-body
control tasks. The results demonstrate that our proposed method significantly
enhances the robot's motion robustness in real world environments, enabling
successful traversal of challenging terrains and highly agile whole-body
trajectory tracking.

</details>


### [6] [Joint Optimization-based Targetless Extrinsic Calibration for Multiple LiDARs and GNSS-Aided INS of Ground Vehicles](https://arxiv.org/abs/2507.08349)
*Junhui Wang,Yan Qiao,Chao Gao,Naiqi Wu*

Main category: cs.RO

TL;DR: 提出了一种无需外部目标或重叠视野的多LiDAR与GINS系统的外参标定方法，适用于智能采矿环境。


<details>
  <summary>Details</summary>
Motivation: 现有标定方法依赖人工目标、重叠视野或精确轨迹估计，实际中难以满足；采矿车辆的平面运动导致标定参数不可观测。

Method: 基于GINS安装高度的观测模型约束不可观测参数，结合几何对应和运动一致性的联合优化框架。

Result: 在仿真和真实数据集上验证了方法的准确性、鲁棒性和适用性。

Conclusion: 该方法解决了采矿环境中多LiDAR与GINS系统的标定问题，适用于异构传感器配置。

Abstract: Accurate extrinsic calibration between multiple LiDAR sensors and a
GNSS-aided inertial navigation system (GINS) is essential for achieving
reliable sensor fusion in intelligent mining environments. Such calibration
enables vehicle-road collaboration by aligning perception data from
vehicle-mounted sensors to a unified global reference frame. However, existing
methods often depend on artificial targets, overlapping fields of view, or
precise trajectory estimation, which are assumptions that may not hold in
practice. Moreover, the planar motion of mining vehicles leads to observability
issues that degrade calibration performance. This paper presents a targetless
extrinsic calibration method that aligns multiple onboard LiDAR sensors to the
GINS coordinate system without requiring overlapping sensor views or external
targets. The proposed approach introduces an observation model based on the
known installation height of the GINS unit to constrain unobservable
calibration parameters under planar motion. A joint optimization framework is
developed to refine both the extrinsic parameters and GINS trajectory by
integrating multiple constraints derived from geometric correspondences and
motion consistency. The proposed method is applicable to heterogeneous LiDAR
configurations, including both mechanical and solid-state sensors. Extensive
experiments on simulated and real-world datasets demonstrate the accuracy,
robustness, and practical applicability of the approach under diverse sensor
setups.

</details>


### [7] [Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework](https://arxiv.org/abs/2507.08364)
*Deteng Zhang,Junjie Zhang,Yan Sun,Tao Li,Hao Yin,Hongzhao Xie,Jie Yin*

Main category: cs.RO

TL;DR: 论文提出了M3DGR数据集和Ground-Fusion++框架，填补了SLAM算法在复杂环境下评估和自适应传感器融合的空白。


<details>
  <summary>Details</summary>
Motivation: 当前SLAM方法在结构化环境中表现良好，但在极端情况下鲁棒性不足，且缺乏标准化评估和多传感器自适应融合策略。

Method: 1. 提出M3DGR数据集，包含多种传感器退化场景；2. 评估40种SLAM系统；3. 开发Ground-Fusion++框架，融合多种传感器。

Result: M3DGR数据集为SLAM评估提供了标准化基准，Ground-Fusion++在复杂条件下表现鲁棒。

Conclusion: 论文通过数据集和框架填补了SLAM领域的评估和自适应融合空白，推动了该领域的发展。

Abstract: Considerable advancements have been achieved in SLAM methods tailored for
structured environments, yet their robustness under challenging corner cases
remains a critical limitation. Although multi-sensor fusion approaches
integrating diverse sensors have shown promising performance improvements, the
research community faces two key barriers: On one hand, the lack of
standardized and configurable benchmarks that systematically evaluate SLAM
algorithms under diverse degradation scenarios hinders comprehensive
performance assessment. While on the other hand, existing SLAM frameworks
primarily focus on fusing a limited set of sensor types, without effectively
addressing adaptive sensor selection strategies for varying environmental
conditions.
  To bridge these gaps, we make three key contributions: First, we introduce
M3DGR dataset: a sensor-rich benchmark with systematically induced degradation
patterns including visual challenge, LiDAR degeneracy, wheel slippage and GNSS
denial. Second, we conduct a comprehensive evaluation of forty SLAM systems on
M3DGR, providing critical insights into their robustness and limitations under
challenging real-world conditions. Third, we develop a resilient modular
multi-sensor fusion framework named Ground-Fusion++, which demonstrates robust
performance by coupling GNSS, RGB-D, LiDAR, IMU (Inertial Measurement Unit) and
wheel odometry. Codes and datasets are publicly available.

</details>


### [8] [Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning](https://arxiv.org/abs/2507.08366)
*Ghaith El-Dalahmeh,Mohammad Reza Jabbarpour,Bao Quoc Vo,Ryszard Kowalczyk*

Main category: cs.RO

TL;DR: 论文提出了一种基于深度强化学习（DRL）的卫星姿态控制策略TD3-HD，结合TD3、HER和DWC技术，显著提升了卫星在故障条件下的适应性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 卫星在动态和不确定环境中自主运行时，传统PD控制器和现有DRL算法（如TD3、PPO、A2C）在实时适应性和容错性方面表现不足，亟需改进。

Method: 提出TD3-HD方法，整合Twin Delayed Deep Deterministic Policy Gradient（TD3）、Hindsight Experience Replay（HER）和Dimension Wise Clipping（DWC），以增强稀疏奖励环境下的学习能力。

Result: 实验表明，TD3-HD在姿态误差、角速度调节和故障条件下的稳定性方面显著优于PD控制器和其他DRL算法。

Conclusion: TD3-HD是一种强大的、具有容错能力的机载AI解决方案，适用于自主卫星姿态控制。

Abstract: Reliable satellite attitude control is essential for the success of space
missions, particularly as satellites increasingly operate autonomously in
dynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role
in attitude control, and maintaining control resilience during RW faults is
critical to preserving mission objectives and system stability. However,
traditional Proportional Derivative (PD) controllers and existing deep
reinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall
short in providing the real time adaptability and fault tolerance required for
autonomous satellite operations. This study introduces a DRL-based control
strategy designed to improve satellite resilience and adaptability under fault
conditions. Specifically, the proposed method integrates Twin Delayed Deep
Deterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and
Dimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in
sparse reward environments and maintain satellite stability during RW failures.
The proposed approach is benchmarked against PD control and leading DRL
algorithms. Experimental results show that TD3-HD achieves significantly lower
attitude error, improved angular velocity regulation, and enhanced stability
under fault conditions. These findings underscore the proposed method potential
as a powerful, fault tolerant, onboard AI solution for autonomous satellite
attitude control.

</details>


### [9] [LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps](https://arxiv.org/abs/2507.08420)
*Haitian Wang,Hezam Albaqami,Xinyu Wang,Muhammad Ibrahim,Zainy M. Malakan,Abdullah M. Algamdi,Mohammed H. Alghamdi,Ajmal Mian*

Main category: cs.RO

TL;DR: 提出了一种融合LiDAR、GNSS和IMU数据的统一框架，用于解决GNSS受限环境下的3D地图全局漂移问题，并引入了一个大规模多模态数据集。


<details>
  <summary>Details</summary>
Motivation: LiDAR-based 3D mapping在GNSS受限环境下存在累积漂移问题，导致全局对齐误差。

Method: 结合Dynamic Time Warping进行时间对齐，使用扩展卡尔曼滤波优化GNSS和IMU信号，并通过NDT注册和位姿图优化构建局部地图，最后利用GNSS约束锚点实现全局一致性。

Result: 全局对齐误差从3.32米降至1.24米，提升61.4%。

Conclusion: 该方法在GNSS受限环境下显著提升了3D城市地图的精度，并为未来研究提供了新基准。

Abstract: LiDAR-based 3D mapping suffers from cumulative drift causing global
misalignment, particularly in GNSS-constrained environments. To address this,
we propose a unified framework that fuses LiDAR, GNSS, and IMU data for
high-resolution city-scale mapping. The method performs velocity-based temporal
alignment using Dynamic Time Warping and refines GNSS and IMU signals via
extended Kalman filtering. Local maps are built using Normal Distributions
Transform-based registration and pose graph optimization with loop closure
detection, while global consistency is enforced using GNSS-constrained anchors
followed by fine registration of overlapping segments. We also introduce a
large-scale multimodal dataset captured in Perth, Western Australia to
facilitate future research in this direction. Our dataset comprises 144{,}000
frames acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS
trajectories, and MEMS-IMU measurements across 21 urban loops. To assess
geometric consistency, we evaluated our method using alignment metrics based on
road centerlines and intersections to capture both global and local accuracy.
Our method reduces the average global alignment error from 3.32\,m to 1.24\,m,
achieving a 61.4\% improvement. The constructed high-fidelity map supports a
wide range of applications, including smart city planning, geospatial data
integration, infrastructure monitoring, and GPS-free navigation. Our method,
and dataset together establish a new benchmark for evaluating 3D city mapping
in GNSS-constrained environments. The dataset and code will be released
publicly.

</details>


### [10] [Robotic Calibration Based on Haptic Feedback Improves Sim-to-Real Transfer](https://arxiv.org/abs/2507.08572)
*Juraj Gavura,Michal Vavrecka,Igor Farkas,Connor Gade*

Main category: cs.RO

TL;DR: 提出了一种基于触觉反馈校准的新方法，通过触摸屏获取真实环境中末端执行器位置信息，利用线性变换和神经网络构建转换函数，显著减少定位误差。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模拟到现实转移中末端执行器位置不一致的问题。

Method: 使用触摸屏校准末端执行器位置，构建基于线性变换和神经网络的转换函数。

Result: 非线性神经网络模型表现最佳，显著减少定位误差。

Conclusion: 该方法有效解决了模拟与现实末端执行器位置不一致的问题，提升了机器人控制的准确性。

Abstract: When inverse kinematics (IK) is adopted to control robotic arms in
manipulation tasks, there is often a discrepancy between the end effector (EE)
position of the robot model in the simulator and the physical EE in reality. In
most robotic scenarios with sim-to-real transfer, we have information about
joint positions in both simulation and reality, but the EE position is only
available in simulation. We developed a novel method to overcome this
difficulty based on haptic feedback calibration, using a touchscreen in front
of the robot that provides information on the EE position in the real
environment. During the calibration procedure, the robot touches specific
points on the screen, and the information is stored. In the next stage, we
build a transformation function from the data based on linear transformation
and neural networks that is capable of outputting all missing variables from
any partial input (simulated/real joint/EE position). Our results demonstrate
that a fully nonlinear neural network model performs best, significantly
reducing positioning errors.

</details>


### [11] [Multi-critic Learning for Whole-body End-effector Twist Tracking](https://arxiv.org/abs/2507.08656)
*Aravind Elanjimattathil Vijayan,Andrei Cramariuc,Mattia Risiglione,Christian Gehring,Marco Hutter*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的框架，用于动态、速度感知的全身末端执行器控制，解决了运动与手臂动作的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 运动与手臂动作的目标冲突（如水平基座与末端执行器跟踪的需求矛盾），以及现有RL方法无法直接控制末端执行器速度的问题。

Method: 采用多批评者-行动者架构，解耦运动与操作的奖励信号，并设计了基于扭转的末端执行器任务公式。

Result: 在四足机器人上的仿真和硬件实验中，控制器能同时行走和移动末端执行器，并表现出全身协作行为。

Conclusion: 提出的方法有效解决了任务冲突，实现了动态、速度感知的全身控制，并展示了无需显式公式的全身协作行为。

Abstract: Learning whole-body control for locomotion and arm motions in a single policy
has challenges, as the two tasks have conflicting goals. For instance,
efficient locomotion typically favors a horizontal base orientation, while
end-effector tracking may benefit from base tilting to extend reachability.
Additionally, current Reinforcement Learning (RL) approaches using a pose-based
task specification lack the ability to directly control the end-effector
velocity, making smoothly executing trajectories very challenging. To address
these limitations, we propose an RL-based framework that allows for dynamic,
velocity-aware whole-body end-effector control. Our method introduces a
multi-critic actor architecture that decouples the reward signals for
locomotion and manipulation, simplifying reward tuning and allowing the policy
to resolve task conflicts more effectively. Furthermore, we design a
twist-based end-effector task formulation that can track both discrete poses
and motion trajectories. We validate our approach through a set of simulation
and hardware experiments using a quadruped robot equipped with a robotic arm.
The resulting controller can simultaneously walk and move its end-effector and
shows emergent whole-body behaviors, where the base assists the arm in
extending the workspace, despite a lack of explicit formulations.

</details>


### [12] [Learning human-to-robot handovers through 3D scene reconstruction](https://arxiv.org/abs/2507.08726)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出了一种仅从RGB图像学习机器人交接任务的方法，无需真实机器人训练或数据收集，利用稀疏视角高斯泼溅重建生成机器人演示。


<details>
  <summary>Details</summary>
Motivation: 解决从仿真到真实环境的视觉域差距问题，减少对真实机器人训练的需求。

Method: 使用稀疏视角高斯泼溅重建人类-机器人交接场景，生成图像-动作对，训练机器人策略。

Result: 在16种家庭物品上验证，策略可直接部署到真实环境，效果显著。

Conclusion: H2RH-SGS为人类-机器人交接任务提供了新的有效表示方法。

Abstract: Learning robot manipulation policies from raw, real-world image data requires
a large number of robot-action trials in the physical environment. Although
training using simulations offers a cost-effective alternative, the visual
domain gap between simulation and robot workspace remains a major limitation.
Gaussian Splatting visual reconstruction methods have recently provided new
directions for robot manipulation by generating realistic environments. In this
paper, we propose the first method for learning supervised-based robot
handovers solely from RGB images without the need of real-robot training or
real-robot data collection. The proposed policy learner, Human-to-Robot
Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view
Gaussian Splatting reconstruction of human-to-robot handover scenes to generate
robot demonstrations containing image-action pairs captured with a camera
mounted on the robot gripper. As a result, the simulated camera pose changes in
the reconstructed scene can be directly translated into gripper pose changes.
We train a robot policy on demonstrations collected with 16 household objects
and {\em directly} deploy this policy in the real environment. Experiments in
both Gaussian Splatting reconstructed scene and real-world human-to-robot
handover experiments demonstrate that H2RH-SGS serves as a new and effective
representation for the human-to-robot handover task.

</details>
