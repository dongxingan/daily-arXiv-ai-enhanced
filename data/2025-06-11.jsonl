{"id": "2506.08039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08039", "abs": "https://arxiv.org/abs/2506.08039", "authors": ["Ray Wai Man Kong"], "title": "AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production", "comment": "12 pages, 9 Figures", "summary": "Efficiency, speed, and precision are essential in modern manufacturing. AI\nMaglev Conveyor system, combining magnetic levitation (maglev) technology with\nartificial intelligence (AI), revolutionizes automated production processes.\nThis system reduces maintenance costs and downtime by eliminating friction,\nenhancing operational efficiency. It transports goods swiftly with minimal\nenergy consumption, optimizing resource use and supporting sustainability. AI\nintegration enables real-time monitoring and adaptive control, allowing\nbusinesses to respond to production demand fluctuations and streamline supply\nchain operations.\n  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse\nproduct types and sizes for flexible manufacturing without extensive\nreconfiguration. AI algorithms optimize routing, reduce cycle times, and\nimprove throughput, creating an agile production line adaptable to market\nchanges.\n  This applied research paper introduces the Maglev Conveyor system, featuring\nan electromagnetic controller and multiple movers to enhance automation. It\noffers cost savings as an alternative to setups using six-axis robots or linear\nmotors, with precise adjustments for robotic arm loading. Operating at high\nspeeds minimizes treatment time for delicate components while maintaining\nprecision. Its adaptable design accommodates various materials, facilitating\nintegration of processing stations alongside electronic product assembly.\nPositioned between linear-axis and robotic systems in cost, the Maglev Conveyor\nis ideal for flat parts requiring minimal travel, transforming production\nefficiency across industries. It explores its technical advantages,\nflexibility, cost reductions, and overall benefits."}
{"id": "2506.08045", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08045", "abs": "https://arxiv.org/abs/2506.08045", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs", "comment": "40 pages, 6 Figures", "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence,\nintegrating perception, decision-making, memory, and collaborative planning to\noperate adaptively in complex, real-world environments. Driven by recent\nadvances in Agentic AI, these systems surpass traditional UAVs by exhibiting\ngoal-driven behavior, contextual reasoning, and interactive autonomy. We\nprovide a comprehensive foundation for understanding the architectural\ncomponents and enabling technologies that distinguish Agentic UAVs from\ntraditional autonomous UAVs. Furthermore, a detailed comparative analysis\nhighlights advancements in autonomy with AI agents, learning, and mission\nflexibility. This study explores seven high-impact application domains\nprecision agriculture, construction & mining, disaster response, environmental\nmonitoring, infrastructure inspection, logistics, security, and wildlife\nconservation, illustrating the broad societal value of agentic aerial\nintelligence. Furthermore, we identify key challenges in technical constraints,\nregulatory limitations, and data-model reliability, and we present emerging\nsolutions across hardware innovation, learning architectures, and human-AI\ninteraction. Finally, a future roadmap is proposed, outlining pathways toward\nself-evolving aerial ecosystems, system-level collaboration, and sustainable,\nequitable deployments. This survey establishes a foundational framework for the\nfuture development, deployment, and governance of agentic aerial systems\n(Agentic UAVs) across diverse societal and industrial domains."}
{"id": "2506.08061", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08061", "abs": "https://arxiv.org/abs/2506.08061", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments."}
{"id": "2506.08149", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08149", "abs": "https://arxiv.org/abs/2506.08149", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "comment": null, "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}."}
{"id": "2506.08291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08291", "abs": "https://arxiv.org/abs/2506.08291", "authors": ["Won Kyung Do", "Matthew Strong", "Aiden Swann", "Boshu Lei", "Monroe Kennedy III"], "title": "TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation", "comment": null, "summary": "Advanced dexterous manipulation involving multiple simultaneous contacts\nacross different surfaces, like pinching coins from ground or manipulating\nintertwined objects, remains challenging for robotic systems. Such tasks exceed\nthe capabilities of vision and proprioception alone, requiring high-resolution\ntactile sensing with calibrated physical metrics. Raw optical tactile sensor\nimages, while information-rich, lack interpretability and cross-sensor\ntransferability, limiting their real-world utility. TensorTouch addresses this\nchallenge by integrating finite element analysis with deep learning to extract\ncomprehensive contact information from optical tactile sensors, including\nstress tensors, deformation fields, and force distributions at pixel-level\nresolution. The TensorTouch framework achieves sub-millimeter position accuracy\nand precise force estimation while supporting large sensor deformations crucial\nfor manipulating soft objects. Experimental validation demonstrates 90% success\nin selectively grasping one of two strings based on detected motion, enabling\nnew contact-rich manipulation capabilities previously inaccessible to robotic\nsystems."}
{"id": "2506.08296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08296", "abs": "https://arxiv.org/abs/2506.08296", "authors": ["Hongjun Wu", "Heng Zhang", "Pengsong Zhang", "Jin Wang", "Cong Wang"], "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation", "comment": "31 pages,5 figures", "summary": "Recent advances in multimodal vision-language-action (VLA) models have\nrevolutionized traditional robot learning, enabling systems to interpret\nvision, language, and action in unified frameworks for complex task planning.\nHowever, mastering complex manipulation tasks remains an open challenge,\nconstrained by limitations in persistent contextual memory, multi-agent\ncoordination under uncertainty, and dynamic long-horizon planning across\nvariable sequences. To address this challenge, we propose \\textbf{HiBerNAC}, a\n\\textbf{Hi}erarchical \\textbf{B}rain-\\textbf{e}mulated \\textbf{r}obotic\n\\textbf{N}eural \\textbf{A}gent \\textbf{C}ollective, inspired by breakthroughs\nin neuroscience, particularly in neural circuit mechanisms and hierarchical\ndecision-making. Our framework combines: (1) multimodal VLA planning and\nreasoning with (2) neuro-inspired reflection and multi-agent mechanisms,\nspecifically designed for complex robotic manipulation tasks. By leveraging\nneuro-inspired functional modules with decentralized multi-agent collaboration,\nour approach enables robust and enhanced real-time execution of complex\nmanipulation tasks. In addition, the agentic system exhibits scalable\ncollective intelligence via dynamic agent specialization, adapting its\ncoordination strategy to variable task horizons and complexity. Through\nextensive experiments on complex manipulation tasks compared with\nstate-of-the-art VLA models, we demonstrate that \\textbf{HiBerNAC} reduces\naverage long-horizon task completion time by 23\\%, and achieves non-zero\nsuccess rates (12\\textendash 31\\%) on multi-path tasks where prior\nstate-of-the-art VLA models consistently fail. These results provide indicative\nevidence for bridging biological cognition and robotic learning mechanisms."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Neşet Ünver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Taşkın Padır"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08416", "abs": "https://arxiv.org/abs/2506.08416", "authors": ["Bolin Li", "Linwei Sun", "Xuecong Huang", "Yuzhi Jiang", "Lijun Zhu"], "title": "Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots", "comment": null, "summary": "This paper presents a periodic bipedal gait learning method using reward\ncomposition, integrated with a real-time gait planner for humanoid robots.\nFirst, we introduce a novel gait planner that incorporates dynamics to design\nthe desired joint trajectory. In the gait design process, the 3D robot model is\ndecoupled into two 2D models, which are then approximated as hybrid inverted\npendulums (H-LIP) for trajectory planning. The gait planner operates in\nparallel in real time within the robot's learning environment. Second, based on\nthis gait planner, we design three effective reward functions within a\nreinforcement learning framework, forming a reward composition to achieve\nperiodic bipedal gait. This reward composition reduces the robot's learning\ntime and enhances locomotion performance. Finally, a gait design example and\nperformance comparison are presented to demonstrate the effectiveness of the\nproposed method."}
{"id": "2506.08434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08434", "abs": "https://arxiv.org/abs/2506.08434", "authors": ["Rui Zhao", "Xingjian Zhang", "Yuhong Cao", "Yizhuo Wang", "Guillaume Sartoretti"], "title": "Attention-based Learning for 3D Informative Path Planning", "comment": null, "summary": "In this work, we propose an attention-based deep reinforcement learning\napproach to address the adaptive informative path planning (IPP) problem in 3D\nspace, where an aerial robot equipped with a downward-facing sensor must\ndynamically adjust its 3D position to balance sensing footprint and accuracy,\nand finally obtain a high-quality belief of an underlying field of interest\nover a given domain (e.g., presence of specific plants, hazardous gas,\ngeological structures, etc.). In adaptive IPP tasks, the agent is tasked with\nmaximizing information collected under time/distance constraints, continuously\nadapting its path based on newly acquired sensor data. To this end, we leverage\nattention mechanisms for their strong ability to capture global spatial\ndependencies across large action spaces, allowing the agent to learn an\nimplicit estimation of environmental transitions. Our model builds a contextual\nbelief representation over the entire domain, guiding sequential movement\ndecisions that optimize both short- and long-term search objectives.\nComparative evaluations against state-of-the-art planners demonstrate that our\napproach significantly reduces environmental uncertainty within constrained\nbudgets, thus allowing the agent to effectively balance exploration and\nexploitation. We further show our model generalizes well to environments of\nvarying sizes, highlighting its potential for many real-world applications."}
{"id": "2506.08440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08440", "abs": "https://arxiv.org/abs/2506.08440", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO"}
{"id": "2506.08459", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08459", "abs": "https://arxiv.org/abs/2506.08459", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "comment": null, "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections."}
{"id": "2506.08578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08578", "abs": "https://arxiv.org/abs/2506.08578", "authors": ["Boyang Chen", "Xizhe Zang", "Chao Song", "Yue Zhang", "Xuehe Zhang", "Jie Zhao"], "title": "Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot", "comment": null, "summary": "The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design\ninspired by the rollover shape of the human foot, significantly enhances the\nenergy efficiency of the robot walking gait. However, due to the tilt of the\nsupporting leg, the error of the contact model are amplified, making robot\nstate estimation more challenging. Therefore, this paper focuses on the noise\nanalysis and state estimation for robot walking with the ESVC foot. First,\nthrough physical robot experiments, we investigate the effect of the ESVC foot\non robot measurement noise and process noise. and a noise-time regression model\nusing sliding window strategy is developed. Then, a hierarchical adaptive state\nestimator for biped robots with the ESVC foot is proposed. The state estimator\nconsists of two stages: pre-estimation and post-estimation. In the\npre-estimation stage, a data fusion-based estimation is employed to process the\nsensory data. During post-estimation, the acceleration of center of mass is\nfirst estimated, and then the noise covariance matrices are adjusted based on\nthe regression model. Following that, an EKF(Extended Kalman Filter) based\napproach is applied to estimate the centroid state during robot walking.\nPhysical experiments demonstrate that the proposed adaptive state estimator for\nbiped robot walking with the ESVC foot not only provides higher precision than\nboth EKF and Adaptive EKF, but also converges faster under varying noise\nconditions."}
{"id": "2506.08639", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08639", "abs": "https://arxiv.org/abs/2506.08639", "authors": ["Amir Hossein Barjini", "Seyed Adel Alizadeh Kolagar", "Sadeq Yaqubi", "Jouni Mattila"], "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators", "comment": null, "summary": "This article presents a motion planning and control framework for flexible\nrobotic manipulators, integrating deep reinforcement learning (DRL) with a\nnonlinear partial differential equation (PDE) controller. Unlike conventional\napproaches that focus solely on control, we demonstrate that the desired\ntrajectory significantly influences endpoint vibrations. To address this, a DRL\nmotion planner, trained using the soft actor-critic (SAC) algorithm, generates\noptimized trajectories that inherently minimize vibrations. The PDE nonlinear\ncontroller then computes the required torques to track the planned trajectory\nwhile ensuring closed-loop stability using Lyapunov analysis. The proposed\nmethodology is validated through both simulations and real-world experiments,\ndemonstrating superior vibration suppression and tracking accuracy compared to\ntraditional methods. The results underscore the potential of combining\nlearning-based motion planning with model-based control for enhancing the\nprecision and stability of flexible robotic manipulators."}
{"id": "2506.08706", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.08706", "abs": "https://arxiv.org/abs/2506.08706", "authors": ["Tomasz Winiarski", "Jan Kaniuka", "Daniel Giełdowski", "Jakub Ostrysz", "Krystian Radlak", "Dmytro Kushnir"], "title": "ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel", "comment": "19 pages", "summary": "As robotic systems grow increasingly complex, heterogeneous, and\nsafety-critical, the need for structured development methodologies becomes\nparamount. Although frameworks like the Robot Operating System (ROS) and\nModel-Based Systems Engineering (MBSE) offer foundational tools, they often\nlack integration when used together. This paper addresses that gap by aligning\nthe widely recognized V-model development paradigm with the MeROS metamodel\nSysML-based modeling language tailored for ROS-based systems.\n  We propose a domain-specific methodology that bridges ROS-centric modelling\nwith systems engineering practices. Our approach formalises the structure,\nbehaviour, and validation processes of robotic systems using MeROS, while\nextending it with a generalized, adaptable V-model compatible with both ROS and\nROS 2. Rather than prescribing a fixed procedure, the approach supports\nproject-specific flexibility and reuse, offering guidance across all stages of\ndevelopment.\n  The approach is validated through a comprehensive case study on HeROS, a\nheterogeneous multi-robot platform comprising manipulators, mobile units, and\ndynamic test environments. This example illustrates how the MeROS-compatible\nV-model enhances traceability and system consistency while remaining accessible\nand extensible for future adaptation. The work contributes a structured,\ntool-agnostic foundation for developers and researchers seeking to apply MBSE\npractices in ROS-based projects."}
{"id": "2506.08708", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving."}
{"id": "2506.08756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08756", "abs": "https://arxiv.org/abs/2506.08756", "authors": ["Octavio Arriaga", "Rebecca Adam", "Melvin Laux", "Lisa Gutzeit", "Marco Ragni", "Jan Peters", "Frank Kirchner"], "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning", "comment": null, "summary": "Real-world robotic applications, from autonomous exploration to assistive\ntechnologies, require adaptive, interpretable, and data-efficient learning\nparadigms. While deep learning architectures and foundation models have driven\nsignificant advances in diverse robotic applications, they remain limited in\ntheir ability to operate efficiently and reliably in unknown and dynamic\nenvironments. In this position paper, we critically assess these limitations\nand introduce a conceptual framework for combining data-driven learning with\ndeliberate, structured reasoning. Specifically, we propose leveraging\ndifferentiable physics for efficient world modeling, Bayesian inference for\nuncertainty-aware decision-making, and meta-learning for rapid adaptation to\nnew tasks. By embedding physical symbolic reasoning within neural models,\nrobots could generalize beyond their training data, reason about novel\nsituations, and continuously expand their knowledge. We argue that such hybrid\nneuro-symbolic architectures are essential for the next generation of\nautonomous systems, and to this end, we provide a research roadmap to guide and\naccelerate their development."}
{"id": "2506.08795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08795", "abs": "https://arxiv.org/abs/2506.08795", "authors": ["Kaijie Shi", "Wanglong Lu", "Hanli Zhao", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning", "comment": null, "summary": "Limb loss affects millions globally, impairing physical function and reducing\nquality of life. Most traditional surface electromyographic (sEMG) and\nsemi-autonomous methods require users to generate myoelectric signals for each\ncontrol, imposing physically and mentally taxing demands. This study aims to\ndevelop a fully autonomous control system that enables a prosthetic hand to\nautomatically grasp and release objects of various shapes using only a camera\nattached to the wrist. By placing the hand near an object, the system will\nautomatically execute grasping actions with a proper grip force in response to\nthe hand's movements and the environment. To release the object being grasped,\njust naturally place the object close to the table and the system will\nautomatically open the hand. Such a system would provide individuals with limb\nloss with a very easy-to-use prosthetic control interface and greatly reduce\nmental effort while using. To achieve this goal, we developed a teleoperation\nsystem to collect human demonstration data for training the prosthetic hand\ncontrol model using imitation learning, which mimics the prosthetic hand\nactions from human. Through training the model using only a few objects' data\nfrom one single participant, we have shown that the imitation learning\nalgorithm can achieve high success rates, generalizing to more individuals and\nunseen objects with a variation of weights. The demonstrations are available at\n\\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}"}
{"id": "2506.08822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08822", "abs": "https://arxiv.org/abs/2506.08822", "authors": ["Yifei Su", "Ning Liu", "Dong Chen", "Zhen Zhao", "Kun Wu", "Meng Li", "Zhiyuan Xu", "Zhengping Che", "Jian Tang"], "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency", "comment": null, "summary": "Generative modeling-based visuomotor policies have been widely adopted in\nrobotic manipulation attributed to their ability to model multimodal action\ndistributions. However, the high inference cost of multi-step sampling limits\ntheir applicability in real-time robotic systems. To address this issue,\nexisting approaches accelerate the sampling process in generative\nmodeling-based visuomotor policies by adapting acceleration techniques\noriginally developed for image generation. Despite this progress, a major\ndistinction remains: image generation typically involves producing independent\nsamples without temporal dependencies, whereas robotic manipulation involves\ngenerating time-series action trajectories that require continuity and temporal\ncoherence. To effectively exploit temporal information in robotic manipulation,\nwe propose FreqPolicy, a novel approach that first imposes frequency\nconsistency constraints on flow-based visuomotor policies. Our work enables the\naction model to capture temporal structure effectively while supporting\nefficient, high-quality one-step action generation. We introduce a frequency\nconsistency constraint that enforces alignment of frequency-domain action\nfeatures across different timesteps along the flow, thereby promoting\nconvergence of one-step action generation toward the target distribution. In\naddition, we design an adaptive consistency loss to capture structural temporal\nvariations inherent in robotic manipulation tasks. We assess FreqPolicy on 53\ntasks across 3 simulation benchmarks, proving its superiority over existing\none-step action generators. We further integrate FreqPolicy into the\nvision-language-action (VLA) model and achieve acceleration without performance\ndegradation on the 40 tasks of Libero. Besides, we show efficiency and\neffectiveness in real-world robotic scenarios with an inference frequency\n93.5Hz. The code will be publicly available."}
{"id": "2506.08840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08840", "abs": "https://arxiv.org/abs/2506.08840", "authors": ["Dewei Wang", "Xinmiao Wang", "Xinzhe Liu", "Jiyuan Shi", "Yingnan Zhao", "Chenjia Bai", "Xuelong Li"], "title": "MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains", "comment": "9 pages, 5 figures", "summary": "Humanoid robots have demonstrated robust locomotion capabilities using\nReinforcement Learning (RL)-based approaches. Further, to obtain human-like\nbehaviors, existing methods integrate human motion-tracking or motion prior in\nthe RL framework. However, these methods are limited in flat terrains with\nproprioception only, restricting their abilities to traverse challenging\nterrains with human-like gaits. In this work, we propose a novel framework\nusing a mixture of latent residual experts with multi-discriminators to train\nan RL policy, which is capable of traversing complex terrains in controllable\nlifelike gaits with exteroception. Our two-stage training pipeline first\nteaches the policy to traverse complex terrains using a depth camera, and then\nenables gait-commanded switching between human-like gait patterns. We also\ndesign gait rewards to adjust human-like behaviors like robot base height.\nSimulation and real-world experiments demonstrate that our framework exhibits\nexceptional performance in traversing complex terrains, and achieves seamless\ntransitions between multiple human-like gait patterns."}
{"id": "2506.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08851", "abs": "https://arxiv.org/abs/2506.08851", "authors": ["Sepehr Samavi", "Garvish Bhutani", "Florian Shkurti", "Angela P. Schoellig"], "title": "Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics\n  (non-archival)", "summary": "Safe and efficient navigation in crowded environments remains a critical\nchallenge for robots that provide a variety of service tasks such as food\ndelivery or autonomous wheelchair mobility. Classical robot crowd navigation\nmethods decouple human motion prediction from robot motion planning, which\nneglects the closed-loop interactions between humans and robots. This lack of a\nmodel for human reactions to the robot plan (e.g. moving out of the way) can\ncause the robot to get stuck. Our proposed Safe and Interactive Crowd\nNavigation (SICNav) method is a bilevel Model Predictive Control (MPC)\nframework that combines prediction and planning into one optimization problem,\nexplicitly modeling interactions among agents. In this paper, we present a\nsystems overview of the crowd navigation platform we use to deploy SICNav in\npreviously unseen indoor and outdoor environments. We provide a preliminary\nanalysis of the system's operation over the course of nearly 7 km of autonomous\nnavigation over two hours in both indoor and outdoor environments."}
{"id": "2506.08856", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08856", "abs": "https://arxiv.org/abs/2506.08856", "authors": ["Jonathan P. King", "Harnoor Ahluwalia", "Michael Zhang", "Nancy S. Pollard"], "title": "Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation", "comment": "Submitted to IEEE Conference on Humanoid Robots", "summary": "This work presents a fast anytime algorithm for computing globally optimal\nindependent contact regions (ICRs). ICRs are regions such that one contact\nwithin each region enables a valid grasp. Locations of ICRs can provide\nguidance for grasp and manipulation planning, learning, and policy transfer.\nHowever, ICRs for modern applications have been little explored, in part due to\nthe expense of computing them, as they have a search space exponential in the\nnumber of contacts. We present a divide and conquer algorithm based on\nincremental n-dimensional Delaunay triangulation that produces results with\nbounded suboptimality in times sufficient for real-time planning. This paper\npresents the base algorithm for grasps where contacts lie within a plane. Our\nexperiments show substantial benefits over competing grasp quality metrics and\nspeedups of 100X and more for competing approaches to computing ICRs. We\nexplore robustness of a policy guided by ICRs and outline a path to general 3D\nimplementation. Code will be released on publication to facilitate further\ndevelopment and applications."}
{"id": "2506.08868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08868", "abs": "https://arxiv.org/abs/2506.08868", "authors": ["Marco Ruggia"], "title": "MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation", "comment": "12 pages, 12 figures, preprint", "summary": "MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone\nthat is fully actuated, meaning it can control its orientation independently of\nits position. MOMAV is also highly symmetrical, making its flight efficiency\nlargely unaffected by its current orientation. These characteristics are\nachieved by a novel drone design where six rotor arms align with the vertices\nof an octahedron, and where each arm can actively rotate along its long axis.\nVarious standout features of MOMAV are presented: The high flight efficiency\ncompared to arm configuration of other fully-actuated drones, the design of an\noriginal rotating arm assembly featuring slip-rings used to enable continuous\narm rotation, and a novel control allocation algorithm based on sequential\nquadratic programming (SQP) used to calculate throttle and arm-angle setpoints\nin flight. Flight tests have shown that MOMAV is able to achieve remarkably low\nmean position/orientation errors of 6.6mm, 2.1{\\deg} ({\\sigma}: 3.0mm,\n1.0{\\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\\deg} ({\\sigma}:\n8.6mm, 2.0{\\deg}) when sweeping orientation setpoints."}
{"id": "2506.08890", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08890", "abs": "https://arxiv.org/abs/2506.08890", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.08931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08931", "abs": "https://arxiv.org/abs/2506.08931", "authors": ["Yixuan Li", "Yutang Lin", "Jieming Cui", "Tengyu Liu", "Wei Liang", "Yixin Zhu", "Siyuan Huang"], "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks", "comment": "18 pages, 13 figures", "summary": "Humanoid teleoperation plays a vital role in demonstrating and collecting\ndata for complex humanoid-scene interactions. However, current teleoperation\nsystems face critical limitations: they decouple upper- and lower-body control\nto maintain stability, restricting natural coordination, and operate open-loop\nwithout real-time position feedback, leading to accumulated drift. The\nfundamental challenge is achieving precise, coordinated whole-body\nteleoperation over extended durations while maintaining accurate global\npositioning. Here we show that an MoE-based teleoperation system, CLONE, with\nclosed-loop error correction enables unprecedented whole-body teleoperation\nfidelity, maintaining minimal positional drift over long-range trajectories\nusing only head and hand tracking from an MR headset. Unlike previous methods\nthat either sacrifice coordination for stability or suffer from unbounded\ndrift, CLONE learns diverse motion skills while preventing tracking error\naccumulation through real-time feedback, enabling complex coordinated movements\nsuch as ``picking up objects from the ground.'' These results establish a new\nmilestone for whole-body humanoid teleoperation for long-horizon humanoid-scene\ninteraction tasks."}
{"id": "2506.08039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08039", "abs": "https://arxiv.org/abs/2506.08039", "authors": ["Ray Wai Man Kong"], "title": "AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production", "comment": "12 pages, 9 Figures", "summary": "Efficiency, speed, and precision are essential in modern manufacturing. AI\nMaglev Conveyor system, combining magnetic levitation (maglev) technology with\nartificial intelligence (AI), revolutionizes automated production processes.\nThis system reduces maintenance costs and downtime by eliminating friction,\nenhancing operational efficiency. It transports goods swiftly with minimal\nenergy consumption, optimizing resource use and supporting sustainability. AI\nintegration enables real-time monitoring and adaptive control, allowing\nbusinesses to respond to production demand fluctuations and streamline supply\nchain operations.\n  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse\nproduct types and sizes for flexible manufacturing without extensive\nreconfiguration. AI algorithms optimize routing, reduce cycle times, and\nimprove throughput, creating an agile production line adaptable to market\nchanges.\n  This applied research paper introduces the Maglev Conveyor system, featuring\nan electromagnetic controller and multiple movers to enhance automation. It\noffers cost savings as an alternative to setups using six-axis robots or linear\nmotors, with precise adjustments for robotic arm loading. Operating at high\nspeeds minimizes treatment time for delicate components while maintaining\nprecision. Its adaptable design accommodates various materials, facilitating\nintegration of processing stations alongside electronic product assembly.\nPositioned between linear-axis and robotic systems in cost, the Maglev Conveyor\nis ideal for flat parts requiring minimal travel, transforming production\nefficiency across industries. It explores its technical advantages,\nflexibility, cost reductions, and overall benefits."}
{"id": "2506.08045", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08045", "abs": "https://arxiv.org/abs/2506.08045", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs", "comment": "40 pages, 6 Figures", "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence,\nintegrating perception, decision-making, memory, and collaborative planning to\noperate adaptively in complex, real-world environments. Driven by recent\nadvances in Agentic AI, these systems surpass traditional UAVs by exhibiting\ngoal-driven behavior, contextual reasoning, and interactive autonomy. We\nprovide a comprehensive foundation for understanding the architectural\ncomponents and enabling technologies that distinguish Agentic UAVs from\ntraditional autonomous UAVs. Furthermore, a detailed comparative analysis\nhighlights advancements in autonomy with AI agents, learning, and mission\nflexibility. This study explores seven high-impact application domains\nprecision agriculture, construction & mining, disaster response, environmental\nmonitoring, infrastructure inspection, logistics, security, and wildlife\nconservation, illustrating the broad societal value of agentic aerial\nintelligence. Furthermore, we identify key challenges in technical constraints,\nregulatory limitations, and data-model reliability, and we present emerging\nsolutions across hardware innovation, learning architectures, and human-AI\ninteraction. Finally, a future roadmap is proposed, outlining pathways toward\nself-evolving aerial ecosystems, system-level collaboration, and sustainable,\nequitable deployments. This survey establishes a foundational framework for the\nfuture development, deployment, and governance of agentic aerial systems\n(Agentic UAVs) across diverse societal and industrial domains."}
{"id": "2506.08061", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08061", "abs": "https://arxiv.org/abs/2506.08061", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments."}
{"id": "2506.08149", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08149", "abs": "https://arxiv.org/abs/2506.08149", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "comment": null, "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}."}
{"id": "2506.08291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08291", "abs": "https://arxiv.org/abs/2506.08291", "authors": ["Won Kyung Do", "Matthew Strong", "Aiden Swann", "Boshu Lei", "Monroe Kennedy III"], "title": "TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation", "comment": null, "summary": "Advanced dexterous manipulation involving multiple simultaneous contacts\nacross different surfaces, like pinching coins from ground or manipulating\nintertwined objects, remains challenging for robotic systems. Such tasks exceed\nthe capabilities of vision and proprioception alone, requiring high-resolution\ntactile sensing with calibrated physical metrics. Raw optical tactile sensor\nimages, while information-rich, lack interpretability and cross-sensor\ntransferability, limiting their real-world utility. TensorTouch addresses this\nchallenge by integrating finite element analysis with deep learning to extract\ncomprehensive contact information from optical tactile sensors, including\nstress tensors, deformation fields, and force distributions at pixel-level\nresolution. The TensorTouch framework achieves sub-millimeter position accuracy\nand precise force estimation while supporting large sensor deformations crucial\nfor manipulating soft objects. Experimental validation demonstrates 90% success\nin selectively grasping one of two strings based on detected motion, enabling\nnew contact-rich manipulation capabilities previously inaccessible to robotic\nsystems."}
{"id": "2506.08296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08296", "abs": "https://arxiv.org/abs/2506.08296", "authors": ["Hongjun Wu", "Heng Zhang", "Pengsong Zhang", "Jin Wang", "Cong Wang"], "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation", "comment": "31 pages,5 figures", "summary": "Recent advances in multimodal vision-language-action (VLA) models have\nrevolutionized traditional robot learning, enabling systems to interpret\nvision, language, and action in unified frameworks for complex task planning.\nHowever, mastering complex manipulation tasks remains an open challenge,\nconstrained by limitations in persistent contextual memory, multi-agent\ncoordination under uncertainty, and dynamic long-horizon planning across\nvariable sequences. To address this challenge, we propose \\textbf{HiBerNAC}, a\n\\textbf{Hi}erarchical \\textbf{B}rain-\\textbf{e}mulated \\textbf{r}obotic\n\\textbf{N}eural \\textbf{A}gent \\textbf{C}ollective, inspired by breakthroughs\nin neuroscience, particularly in neural circuit mechanisms and hierarchical\ndecision-making. Our framework combines: (1) multimodal VLA planning and\nreasoning with (2) neuro-inspired reflection and multi-agent mechanisms,\nspecifically designed for complex robotic manipulation tasks. By leveraging\nneuro-inspired functional modules with decentralized multi-agent collaboration,\nour approach enables robust and enhanced real-time execution of complex\nmanipulation tasks. In addition, the agentic system exhibits scalable\ncollective intelligence via dynamic agent specialization, adapting its\ncoordination strategy to variable task horizons and complexity. Through\nextensive experiments on complex manipulation tasks compared with\nstate-of-the-art VLA models, we demonstrate that \\textbf{HiBerNAC} reduces\naverage long-horizon task completion time by 23\\%, and achieves non-zero\nsuccess rates (12\\textendash 31\\%) on multi-path tasks where prior\nstate-of-the-art VLA models consistently fail. These results provide indicative\nevidence for bridging biological cognition and robotic learning mechanisms."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Neşet Ünver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Taşkın Padır"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08416", "abs": "https://arxiv.org/abs/2506.08416", "authors": ["Bolin Li", "Linwei Sun", "Xuecong Huang", "Yuzhi Jiang", "Lijun Zhu"], "title": "Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots", "comment": null, "summary": "This paper presents a periodic bipedal gait learning method using reward\ncomposition, integrated with a real-time gait planner for humanoid robots.\nFirst, we introduce a novel gait planner that incorporates dynamics to design\nthe desired joint trajectory. In the gait design process, the 3D robot model is\ndecoupled into two 2D models, which are then approximated as hybrid inverted\npendulums (H-LIP) for trajectory planning. The gait planner operates in\nparallel in real time within the robot's learning environment. Second, based on\nthis gait planner, we design three effective reward functions within a\nreinforcement learning framework, forming a reward composition to achieve\nperiodic bipedal gait. This reward composition reduces the robot's learning\ntime and enhances locomotion performance. Finally, a gait design example and\nperformance comparison are presented to demonstrate the effectiveness of the\nproposed method."}
{"id": "2506.08434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08434", "abs": "https://arxiv.org/abs/2506.08434", "authors": ["Rui Zhao", "Xingjian Zhang", "Yuhong Cao", "Yizhuo Wang", "Guillaume Sartoretti"], "title": "Attention-based Learning for 3D Informative Path Planning", "comment": null, "summary": "In this work, we propose an attention-based deep reinforcement learning\napproach to address the adaptive informative path planning (IPP) problem in 3D\nspace, where an aerial robot equipped with a downward-facing sensor must\ndynamically adjust its 3D position to balance sensing footprint and accuracy,\nand finally obtain a high-quality belief of an underlying field of interest\nover a given domain (e.g., presence of specific plants, hazardous gas,\ngeological structures, etc.). In adaptive IPP tasks, the agent is tasked with\nmaximizing information collected under time/distance constraints, continuously\nadapting its path based on newly acquired sensor data. To this end, we leverage\nattention mechanisms for their strong ability to capture global spatial\ndependencies across large action spaces, allowing the agent to learn an\nimplicit estimation of environmental transitions. Our model builds a contextual\nbelief representation over the entire domain, guiding sequential movement\ndecisions that optimize both short- and long-term search objectives.\nComparative evaluations against state-of-the-art planners demonstrate that our\napproach significantly reduces environmental uncertainty within constrained\nbudgets, thus allowing the agent to effectively balance exploration and\nexploitation. We further show our model generalizes well to environments of\nvarying sizes, highlighting its potential for many real-world applications."}
{"id": "2506.08440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08440", "abs": "https://arxiv.org/abs/2506.08440", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO"}
{"id": "2506.08459", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08459", "abs": "https://arxiv.org/abs/2506.08459", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "comment": null, "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections."}
{"id": "2506.08578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08578", "abs": "https://arxiv.org/abs/2506.08578", "authors": ["Boyang Chen", "Xizhe Zang", "Chao Song", "Yue Zhang", "Xuehe Zhang", "Jie Zhao"], "title": "Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot", "comment": null, "summary": "The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design\ninspired by the rollover shape of the human foot, significantly enhances the\nenergy efficiency of the robot walking gait. However, due to the tilt of the\nsupporting leg, the error of the contact model are amplified, making robot\nstate estimation more challenging. Therefore, this paper focuses on the noise\nanalysis and state estimation for robot walking with the ESVC foot. First,\nthrough physical robot experiments, we investigate the effect of the ESVC foot\non robot measurement noise and process noise. and a noise-time regression model\nusing sliding window strategy is developed. Then, a hierarchical adaptive state\nestimator for biped robots with the ESVC foot is proposed. The state estimator\nconsists of two stages: pre-estimation and post-estimation. In the\npre-estimation stage, a data fusion-based estimation is employed to process the\nsensory data. During post-estimation, the acceleration of center of mass is\nfirst estimated, and then the noise covariance matrices are adjusted based on\nthe regression model. Following that, an EKF(Extended Kalman Filter) based\napproach is applied to estimate the centroid state during robot walking.\nPhysical experiments demonstrate that the proposed adaptive state estimator for\nbiped robot walking with the ESVC foot not only provides higher precision than\nboth EKF and Adaptive EKF, but also converges faster under varying noise\nconditions."}
{"id": "2506.08639", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08639", "abs": "https://arxiv.org/abs/2506.08639", "authors": ["Amir Hossein Barjini", "Seyed Adel Alizadeh Kolagar", "Sadeq Yaqubi", "Jouni Mattila"], "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators", "comment": null, "summary": "This article presents a motion planning and control framework for flexible\nrobotic manipulators, integrating deep reinforcement learning (DRL) with a\nnonlinear partial differential equation (PDE) controller. Unlike conventional\napproaches that focus solely on control, we demonstrate that the desired\ntrajectory significantly influences endpoint vibrations. To address this, a DRL\nmotion planner, trained using the soft actor-critic (SAC) algorithm, generates\noptimized trajectories that inherently minimize vibrations. The PDE nonlinear\ncontroller then computes the required torques to track the planned trajectory\nwhile ensuring closed-loop stability using Lyapunov analysis. The proposed\nmethodology is validated through both simulations and real-world experiments,\ndemonstrating superior vibration suppression and tracking accuracy compared to\ntraditional methods. The results underscore the potential of combining\nlearning-based motion planning with model-based control for enhancing the\nprecision and stability of flexible robotic manipulators."}
{"id": "2506.08706", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.08706", "abs": "https://arxiv.org/abs/2506.08706", "authors": ["Tomasz Winiarski", "Jan Kaniuka", "Daniel Giełdowski", "Jakub Ostrysz", "Krystian Radlak", "Dmytro Kushnir"], "title": "ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel", "comment": "19 pages", "summary": "As robotic systems grow increasingly complex, heterogeneous, and\nsafety-critical, the need for structured development methodologies becomes\nparamount. Although frameworks like the Robot Operating System (ROS) and\nModel-Based Systems Engineering (MBSE) offer foundational tools, they often\nlack integration when used together. This paper addresses that gap by aligning\nthe widely recognized V-model development paradigm with the MeROS metamodel\nSysML-based modeling language tailored for ROS-based systems.\n  We propose a domain-specific methodology that bridges ROS-centric modelling\nwith systems engineering practices. Our approach formalises the structure,\nbehaviour, and validation processes of robotic systems using MeROS, while\nextending it with a generalized, adaptable V-model compatible with both ROS and\nROS 2. Rather than prescribing a fixed procedure, the approach supports\nproject-specific flexibility and reuse, offering guidance across all stages of\ndevelopment.\n  The approach is validated through a comprehensive case study on HeROS, a\nheterogeneous multi-robot platform comprising manipulators, mobile units, and\ndynamic test environments. This example illustrates how the MeROS-compatible\nV-model enhances traceability and system consistency while remaining accessible\nand extensible for future adaptation. The work contributes a structured,\ntool-agnostic foundation for developers and researchers seeking to apply MBSE\npractices in ROS-based projects."}
{"id": "2506.08708", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving."}
{"id": "2506.08756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08756", "abs": "https://arxiv.org/abs/2506.08756", "authors": ["Octavio Arriaga", "Rebecca Adam", "Melvin Laux", "Lisa Gutzeit", "Marco Ragni", "Jan Peters", "Frank Kirchner"], "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning", "comment": null, "summary": "Real-world robotic applications, from autonomous exploration to assistive\ntechnologies, require adaptive, interpretable, and data-efficient learning\nparadigms. While deep learning architectures and foundation models have driven\nsignificant advances in diverse robotic applications, they remain limited in\ntheir ability to operate efficiently and reliably in unknown and dynamic\nenvironments. In this position paper, we critically assess these limitations\nand introduce a conceptual framework for combining data-driven learning with\ndeliberate, structured reasoning. Specifically, we propose leveraging\ndifferentiable physics for efficient world modeling, Bayesian inference for\nuncertainty-aware decision-making, and meta-learning for rapid adaptation to\nnew tasks. By embedding physical symbolic reasoning within neural models,\nrobots could generalize beyond their training data, reason about novel\nsituations, and continuously expand their knowledge. We argue that such hybrid\nneuro-symbolic architectures are essential for the next generation of\nautonomous systems, and to this end, we provide a research roadmap to guide and\naccelerate their development."}
{"id": "2506.08795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08795", "abs": "https://arxiv.org/abs/2506.08795", "authors": ["Kaijie Shi", "Wanglong Lu", "Hanli Zhao", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning", "comment": null, "summary": "Limb loss affects millions globally, impairing physical function and reducing\nquality of life. Most traditional surface electromyographic (sEMG) and\nsemi-autonomous methods require users to generate myoelectric signals for each\ncontrol, imposing physically and mentally taxing demands. This study aims to\ndevelop a fully autonomous control system that enables a prosthetic hand to\nautomatically grasp and release objects of various shapes using only a camera\nattached to the wrist. By placing the hand near an object, the system will\nautomatically execute grasping actions with a proper grip force in response to\nthe hand's movements and the environment. To release the object being grasped,\njust naturally place the object close to the table and the system will\nautomatically open the hand. Such a system would provide individuals with limb\nloss with a very easy-to-use prosthetic control interface and greatly reduce\nmental effort while using. To achieve this goal, we developed a teleoperation\nsystem to collect human demonstration data for training the prosthetic hand\ncontrol model using imitation learning, which mimics the prosthetic hand\nactions from human. Through training the model using only a few objects' data\nfrom one single participant, we have shown that the imitation learning\nalgorithm can achieve high success rates, generalizing to more individuals and\nunseen objects with a variation of weights. The demonstrations are available at\n\\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}"}
{"id": "2506.08822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08822", "abs": "https://arxiv.org/abs/2506.08822", "authors": ["Yifei Su", "Ning Liu", "Dong Chen", "Zhen Zhao", "Kun Wu", "Meng Li", "Zhiyuan Xu", "Zhengping Che", "Jian Tang"], "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency", "comment": null, "summary": "Generative modeling-based visuomotor policies have been widely adopted in\nrobotic manipulation attributed to their ability to model multimodal action\ndistributions. However, the high inference cost of multi-step sampling limits\ntheir applicability in real-time robotic systems. To address this issue,\nexisting approaches accelerate the sampling process in generative\nmodeling-based visuomotor policies by adapting acceleration techniques\noriginally developed for image generation. Despite this progress, a major\ndistinction remains: image generation typically involves producing independent\nsamples without temporal dependencies, whereas robotic manipulation involves\ngenerating time-series action trajectories that require continuity and temporal\ncoherence. To effectively exploit temporal information in robotic manipulation,\nwe propose FreqPolicy, a novel approach that first imposes frequency\nconsistency constraints on flow-based visuomotor policies. Our work enables the\naction model to capture temporal structure effectively while supporting\nefficient, high-quality one-step action generation. We introduce a frequency\nconsistency constraint that enforces alignment of frequency-domain action\nfeatures across different timesteps along the flow, thereby promoting\nconvergence of one-step action generation toward the target distribution. In\naddition, we design an adaptive consistency loss to capture structural temporal\nvariations inherent in robotic manipulation tasks. We assess FreqPolicy on 53\ntasks across 3 simulation benchmarks, proving its superiority over existing\none-step action generators. We further integrate FreqPolicy into the\nvision-language-action (VLA) model and achieve acceleration without performance\ndegradation on the 40 tasks of Libero. Besides, we show efficiency and\neffectiveness in real-world robotic scenarios with an inference frequency\n93.5Hz. The code will be publicly available."}
{"id": "2506.08840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08840", "abs": "https://arxiv.org/abs/2506.08840", "authors": ["Dewei Wang", "Xinmiao Wang", "Xinzhe Liu", "Jiyuan Shi", "Yingnan Zhao", "Chenjia Bai", "Xuelong Li"], "title": "MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains", "comment": "9 pages, 5 figures", "summary": "Humanoid robots have demonstrated robust locomotion capabilities using\nReinforcement Learning (RL)-based approaches. Further, to obtain human-like\nbehaviors, existing methods integrate human motion-tracking or motion prior in\nthe RL framework. However, these methods are limited in flat terrains with\nproprioception only, restricting their abilities to traverse challenging\nterrains with human-like gaits. In this work, we propose a novel framework\nusing a mixture of latent residual experts with multi-discriminators to train\nan RL policy, which is capable of traversing complex terrains in controllable\nlifelike gaits with exteroception. Our two-stage training pipeline first\nteaches the policy to traverse complex terrains using a depth camera, and then\nenables gait-commanded switching between human-like gait patterns. We also\ndesign gait rewards to adjust human-like behaviors like robot base height.\nSimulation and real-world experiments demonstrate that our framework exhibits\nexceptional performance in traversing complex terrains, and achieves seamless\ntransitions between multiple human-like gait patterns."}
{"id": "2506.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08851", "abs": "https://arxiv.org/abs/2506.08851", "authors": ["Sepehr Samavi", "Garvish Bhutani", "Florian Shkurti", "Angela P. Schoellig"], "title": "Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics\n  (non-archival)", "summary": "Safe and efficient navigation in crowded environments remains a critical\nchallenge for robots that provide a variety of service tasks such as food\ndelivery or autonomous wheelchair mobility. Classical robot crowd navigation\nmethods decouple human motion prediction from robot motion planning, which\nneglects the closed-loop interactions between humans and robots. This lack of a\nmodel for human reactions to the robot plan (e.g. moving out of the way) can\ncause the robot to get stuck. Our proposed Safe and Interactive Crowd\nNavigation (SICNav) method is a bilevel Model Predictive Control (MPC)\nframework that combines prediction and planning into one optimization problem,\nexplicitly modeling interactions among agents. In this paper, we present a\nsystems overview of the crowd navigation platform we use to deploy SICNav in\npreviously unseen indoor and outdoor environments. We provide a preliminary\nanalysis of the system's operation over the course of nearly 7 km of autonomous\nnavigation over two hours in both indoor and outdoor environments."}
{"id": "2506.08856", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08856", "abs": "https://arxiv.org/abs/2506.08856", "authors": ["Jonathan P. King", "Harnoor Ahluwalia", "Michael Zhang", "Nancy S. Pollard"], "title": "Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation", "comment": "Submitted to IEEE Conference on Humanoid Robots", "summary": "This work presents a fast anytime algorithm for computing globally optimal\nindependent contact regions (ICRs). ICRs are regions such that one contact\nwithin each region enables a valid grasp. Locations of ICRs can provide\nguidance for grasp and manipulation planning, learning, and policy transfer.\nHowever, ICRs for modern applications have been little explored, in part due to\nthe expense of computing them, as they have a search space exponential in the\nnumber of contacts. We present a divide and conquer algorithm based on\nincremental n-dimensional Delaunay triangulation that produces results with\nbounded suboptimality in times sufficient for real-time planning. This paper\npresents the base algorithm for grasps where contacts lie within a plane. Our\nexperiments show substantial benefits over competing grasp quality metrics and\nspeedups of 100X and more for competing approaches to computing ICRs. We\nexplore robustness of a policy guided by ICRs and outline a path to general 3D\nimplementation. Code will be released on publication to facilitate further\ndevelopment and applications."}
{"id": "2506.08868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08868", "abs": "https://arxiv.org/abs/2506.08868", "authors": ["Marco Ruggia"], "title": "MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation", "comment": "12 pages, 12 figures, preprint", "summary": "MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone\nthat is fully actuated, meaning it can control its orientation independently of\nits position. MOMAV is also highly symmetrical, making its flight efficiency\nlargely unaffected by its current orientation. These characteristics are\nachieved by a novel drone design where six rotor arms align with the vertices\nof an octahedron, and where each arm can actively rotate along its long axis.\nVarious standout features of MOMAV are presented: The high flight efficiency\ncompared to arm configuration of other fully-actuated drones, the design of an\noriginal rotating arm assembly featuring slip-rings used to enable continuous\narm rotation, and a novel control allocation algorithm based on sequential\nquadratic programming (SQP) used to calculate throttle and arm-angle setpoints\nin flight. Flight tests have shown that MOMAV is able to achieve remarkably low\nmean position/orientation errors of 6.6mm, 2.1{\\deg} ({\\sigma}: 3.0mm,\n1.0{\\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\\deg} ({\\sigma}:\n8.6mm, 2.0{\\deg}) when sweeping orientation setpoints."}
{"id": "2506.08890", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08890", "abs": "https://arxiv.org/abs/2506.08890", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.08931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08931", "abs": "https://arxiv.org/abs/2506.08931", "authors": ["Yixuan Li", "Yutang Lin", "Jieming Cui", "Tengyu Liu", "Wei Liang", "Yixin Zhu", "Siyuan Huang"], "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks", "comment": "18 pages, 13 figures", "summary": "Humanoid teleoperation plays a vital role in demonstrating and collecting\ndata for complex humanoid-scene interactions. However, current teleoperation\nsystems face critical limitations: they decouple upper- and lower-body control\nto maintain stability, restricting natural coordination, and operate open-loop\nwithout real-time position feedback, leading to accumulated drift. The\nfundamental challenge is achieving precise, coordinated whole-body\nteleoperation over extended durations while maintaining accurate global\npositioning. Here we show that an MoE-based teleoperation system, CLONE, with\nclosed-loop error correction enables unprecedented whole-body teleoperation\nfidelity, maintaining minimal positional drift over long-range trajectories\nusing only head and hand tracking from an MR headset. Unlike previous methods\nthat either sacrifice coordination for stability or suffer from unbounded\ndrift, CLONE learns diverse motion skills while preventing tracking error\naccumulation through real-time feedback, enabling complex coordinated movements\nsuch as ``picking up objects from the ground.'' These results establish a new\nmilestone for whole-body humanoid teleoperation for long-horizon humanoid-scene\ninteraction tasks."}
{"id": "2506.08039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08039", "abs": "https://arxiv.org/abs/2506.08039", "authors": ["Ray Wai Man Kong"], "title": "AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production", "comment": "12 pages, 9 Figures", "summary": "Efficiency, speed, and precision are essential in modern manufacturing. AI\nMaglev Conveyor system, combining magnetic levitation (maglev) technology with\nartificial intelligence (AI), revolutionizes automated production processes.\nThis system reduces maintenance costs and downtime by eliminating friction,\nenhancing operational efficiency. It transports goods swiftly with minimal\nenergy consumption, optimizing resource use and supporting sustainability. AI\nintegration enables real-time monitoring and adaptive control, allowing\nbusinesses to respond to production demand fluctuations and streamline supply\nchain operations.\n  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse\nproduct types and sizes for flexible manufacturing without extensive\nreconfiguration. AI algorithms optimize routing, reduce cycle times, and\nimprove throughput, creating an agile production line adaptable to market\nchanges.\n  This applied research paper introduces the Maglev Conveyor system, featuring\nan electromagnetic controller and multiple movers to enhance automation. It\noffers cost savings as an alternative to setups using six-axis robots or linear\nmotors, with precise adjustments for robotic arm loading. Operating at high\nspeeds minimizes treatment time for delicate components while maintaining\nprecision. Its adaptable design accommodates various materials, facilitating\nintegration of processing stations alongside electronic product assembly.\nPositioned between linear-axis and robotic systems in cost, the Maglev Conveyor\nis ideal for flat parts requiring minimal travel, transforming production\nefficiency across industries. It explores its technical advantages,\nflexibility, cost reductions, and overall benefits."}
{"id": "2506.08045", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08045", "abs": "https://arxiv.org/abs/2506.08045", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs", "comment": "40 pages, 6 Figures", "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence,\nintegrating perception, decision-making, memory, and collaborative planning to\noperate adaptively in complex, real-world environments. Driven by recent\nadvances in Agentic AI, these systems surpass traditional UAVs by exhibiting\ngoal-driven behavior, contextual reasoning, and interactive autonomy. We\nprovide a comprehensive foundation for understanding the architectural\ncomponents and enabling technologies that distinguish Agentic UAVs from\ntraditional autonomous UAVs. Furthermore, a detailed comparative analysis\nhighlights advancements in autonomy with AI agents, learning, and mission\nflexibility. This study explores seven high-impact application domains\nprecision agriculture, construction & mining, disaster response, environmental\nmonitoring, infrastructure inspection, logistics, security, and wildlife\nconservation, illustrating the broad societal value of agentic aerial\nintelligence. Furthermore, we identify key challenges in technical constraints,\nregulatory limitations, and data-model reliability, and we present emerging\nsolutions across hardware innovation, learning architectures, and human-AI\ninteraction. Finally, a future roadmap is proposed, outlining pathways toward\nself-evolving aerial ecosystems, system-level collaboration, and sustainable,\nequitable deployments. This survey establishes a foundational framework for the\nfuture development, deployment, and governance of agentic aerial systems\n(Agentic UAVs) across diverse societal and industrial domains."}
{"id": "2506.08061", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08061", "abs": "https://arxiv.org/abs/2506.08061", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments."}
{"id": "2506.08149", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08149", "abs": "https://arxiv.org/abs/2506.08149", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "comment": null, "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}."}
{"id": "2506.08291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08291", "abs": "https://arxiv.org/abs/2506.08291", "authors": ["Won Kyung Do", "Matthew Strong", "Aiden Swann", "Boshu Lei", "Monroe Kennedy III"], "title": "TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation", "comment": null, "summary": "Advanced dexterous manipulation involving multiple simultaneous contacts\nacross different surfaces, like pinching coins from ground or manipulating\nintertwined objects, remains challenging for robotic systems. Such tasks exceed\nthe capabilities of vision and proprioception alone, requiring high-resolution\ntactile sensing with calibrated physical metrics. Raw optical tactile sensor\nimages, while information-rich, lack interpretability and cross-sensor\ntransferability, limiting their real-world utility. TensorTouch addresses this\nchallenge by integrating finite element analysis with deep learning to extract\ncomprehensive contact information from optical tactile sensors, including\nstress tensors, deformation fields, and force distributions at pixel-level\nresolution. The TensorTouch framework achieves sub-millimeter position accuracy\nand precise force estimation while supporting large sensor deformations crucial\nfor manipulating soft objects. Experimental validation demonstrates 90% success\nin selectively grasping one of two strings based on detected motion, enabling\nnew contact-rich manipulation capabilities previously inaccessible to robotic\nsystems."}
{"id": "2506.08296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08296", "abs": "https://arxiv.org/abs/2506.08296", "authors": ["Hongjun Wu", "Heng Zhang", "Pengsong Zhang", "Jin Wang", "Cong Wang"], "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation", "comment": "31 pages,5 figures", "summary": "Recent advances in multimodal vision-language-action (VLA) models have\nrevolutionized traditional robot learning, enabling systems to interpret\nvision, language, and action in unified frameworks for complex task planning.\nHowever, mastering complex manipulation tasks remains an open challenge,\nconstrained by limitations in persistent contextual memory, multi-agent\ncoordination under uncertainty, and dynamic long-horizon planning across\nvariable sequences. To address this challenge, we propose \\textbf{HiBerNAC}, a\n\\textbf{Hi}erarchical \\textbf{B}rain-\\textbf{e}mulated \\textbf{r}obotic\n\\textbf{N}eural \\textbf{A}gent \\textbf{C}ollective, inspired by breakthroughs\nin neuroscience, particularly in neural circuit mechanisms and hierarchical\ndecision-making. Our framework combines: (1) multimodal VLA planning and\nreasoning with (2) neuro-inspired reflection and multi-agent mechanisms,\nspecifically designed for complex robotic manipulation tasks. By leveraging\nneuro-inspired functional modules with decentralized multi-agent collaboration,\nour approach enables robust and enhanced real-time execution of complex\nmanipulation tasks. In addition, the agentic system exhibits scalable\ncollective intelligence via dynamic agent specialization, adapting its\ncoordination strategy to variable task horizons and complexity. Through\nextensive experiments on complex manipulation tasks compared with\nstate-of-the-art VLA models, we demonstrate that \\textbf{HiBerNAC} reduces\naverage long-horizon task completion time by 23\\%, and achieves non-zero\nsuccess rates (12\\textendash 31\\%) on multi-path tasks where prior\nstate-of-the-art VLA models consistently fail. These results provide indicative\nevidence for bridging biological cognition and robotic learning mechanisms."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Neşet Ünver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Taşkın Padır"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08416", "abs": "https://arxiv.org/abs/2506.08416", "authors": ["Bolin Li", "Linwei Sun", "Xuecong Huang", "Yuzhi Jiang", "Lijun Zhu"], "title": "Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots", "comment": null, "summary": "This paper presents a periodic bipedal gait learning method using reward\ncomposition, integrated with a real-time gait planner for humanoid robots.\nFirst, we introduce a novel gait planner that incorporates dynamics to design\nthe desired joint trajectory. In the gait design process, the 3D robot model is\ndecoupled into two 2D models, which are then approximated as hybrid inverted\npendulums (H-LIP) for trajectory planning. The gait planner operates in\nparallel in real time within the robot's learning environment. Second, based on\nthis gait planner, we design three effective reward functions within a\nreinforcement learning framework, forming a reward composition to achieve\nperiodic bipedal gait. This reward composition reduces the robot's learning\ntime and enhances locomotion performance. Finally, a gait design example and\nperformance comparison are presented to demonstrate the effectiveness of the\nproposed method."}
{"id": "2506.08434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08434", "abs": "https://arxiv.org/abs/2506.08434", "authors": ["Rui Zhao", "Xingjian Zhang", "Yuhong Cao", "Yizhuo Wang", "Guillaume Sartoretti"], "title": "Attention-based Learning for 3D Informative Path Planning", "comment": null, "summary": "In this work, we propose an attention-based deep reinforcement learning\napproach to address the adaptive informative path planning (IPP) problem in 3D\nspace, where an aerial robot equipped with a downward-facing sensor must\ndynamically adjust its 3D position to balance sensing footprint and accuracy,\nand finally obtain a high-quality belief of an underlying field of interest\nover a given domain (e.g., presence of specific plants, hazardous gas,\ngeological structures, etc.). In adaptive IPP tasks, the agent is tasked with\nmaximizing information collected under time/distance constraints, continuously\nadapting its path based on newly acquired sensor data. To this end, we leverage\nattention mechanisms for their strong ability to capture global spatial\ndependencies across large action spaces, allowing the agent to learn an\nimplicit estimation of environmental transitions. Our model builds a contextual\nbelief representation over the entire domain, guiding sequential movement\ndecisions that optimize both short- and long-term search objectives.\nComparative evaluations against state-of-the-art planners demonstrate that our\napproach significantly reduces environmental uncertainty within constrained\nbudgets, thus allowing the agent to effectively balance exploration and\nexploitation. We further show our model generalizes well to environments of\nvarying sizes, highlighting its potential for many real-world applications."}
{"id": "2506.08440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08440", "abs": "https://arxiv.org/abs/2506.08440", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO"}
{"id": "2506.08459", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08459", "abs": "https://arxiv.org/abs/2506.08459", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "comment": null, "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections."}
{"id": "2506.08578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08578", "abs": "https://arxiv.org/abs/2506.08578", "authors": ["Boyang Chen", "Xizhe Zang", "Chao Song", "Yue Zhang", "Xuehe Zhang", "Jie Zhao"], "title": "Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot", "comment": null, "summary": "The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design\ninspired by the rollover shape of the human foot, significantly enhances the\nenergy efficiency of the robot walking gait. However, due to the tilt of the\nsupporting leg, the error of the contact model are amplified, making robot\nstate estimation more challenging. Therefore, this paper focuses on the noise\nanalysis and state estimation for robot walking with the ESVC foot. First,\nthrough physical robot experiments, we investigate the effect of the ESVC foot\non robot measurement noise and process noise. and a noise-time regression model\nusing sliding window strategy is developed. Then, a hierarchical adaptive state\nestimator for biped robots with the ESVC foot is proposed. The state estimator\nconsists of two stages: pre-estimation and post-estimation. In the\npre-estimation stage, a data fusion-based estimation is employed to process the\nsensory data. During post-estimation, the acceleration of center of mass is\nfirst estimated, and then the noise covariance matrices are adjusted based on\nthe regression model. Following that, an EKF(Extended Kalman Filter) based\napproach is applied to estimate the centroid state during robot walking.\nPhysical experiments demonstrate that the proposed adaptive state estimator for\nbiped robot walking with the ESVC foot not only provides higher precision than\nboth EKF and Adaptive EKF, but also converges faster under varying noise\nconditions."}
{"id": "2506.08639", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08639", "abs": "https://arxiv.org/abs/2506.08639", "authors": ["Amir Hossein Barjini", "Seyed Adel Alizadeh Kolagar", "Sadeq Yaqubi", "Jouni Mattila"], "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators", "comment": null, "summary": "This article presents a motion planning and control framework for flexible\nrobotic manipulators, integrating deep reinforcement learning (DRL) with a\nnonlinear partial differential equation (PDE) controller. Unlike conventional\napproaches that focus solely on control, we demonstrate that the desired\ntrajectory significantly influences endpoint vibrations. To address this, a DRL\nmotion planner, trained using the soft actor-critic (SAC) algorithm, generates\noptimized trajectories that inherently minimize vibrations. The PDE nonlinear\ncontroller then computes the required torques to track the planned trajectory\nwhile ensuring closed-loop stability using Lyapunov analysis. The proposed\nmethodology is validated through both simulations and real-world experiments,\ndemonstrating superior vibration suppression and tracking accuracy compared to\ntraditional methods. The results underscore the potential of combining\nlearning-based motion planning with model-based control for enhancing the\nprecision and stability of flexible robotic manipulators."}
{"id": "2506.08706", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.08706", "abs": "https://arxiv.org/abs/2506.08706", "authors": ["Tomasz Winiarski", "Jan Kaniuka", "Daniel Giełdowski", "Jakub Ostrysz", "Krystian Radlak", "Dmytro Kushnir"], "title": "ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel", "comment": "19 pages", "summary": "As robotic systems grow increasingly complex, heterogeneous, and\nsafety-critical, the need for structured development methodologies becomes\nparamount. Although frameworks like the Robot Operating System (ROS) and\nModel-Based Systems Engineering (MBSE) offer foundational tools, they often\nlack integration when used together. This paper addresses that gap by aligning\nthe widely recognized V-model development paradigm with the MeROS metamodel\nSysML-based modeling language tailored for ROS-based systems.\n  We propose a domain-specific methodology that bridges ROS-centric modelling\nwith systems engineering practices. Our approach formalises the structure,\nbehaviour, and validation processes of robotic systems using MeROS, while\nextending it with a generalized, adaptable V-model compatible with both ROS and\nROS 2. Rather than prescribing a fixed procedure, the approach supports\nproject-specific flexibility and reuse, offering guidance across all stages of\ndevelopment.\n  The approach is validated through a comprehensive case study on HeROS, a\nheterogeneous multi-robot platform comprising manipulators, mobile units, and\ndynamic test environments. This example illustrates how the MeROS-compatible\nV-model enhances traceability and system consistency while remaining accessible\nand extensible for future adaptation. The work contributes a structured,\ntool-agnostic foundation for developers and researchers seeking to apply MBSE\npractices in ROS-based projects."}
{"id": "2506.08708", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving."}
{"id": "2506.08756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08756", "abs": "https://arxiv.org/abs/2506.08756", "authors": ["Octavio Arriaga", "Rebecca Adam", "Melvin Laux", "Lisa Gutzeit", "Marco Ragni", "Jan Peters", "Frank Kirchner"], "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning", "comment": null, "summary": "Real-world robotic applications, from autonomous exploration to assistive\ntechnologies, require adaptive, interpretable, and data-efficient learning\nparadigms. While deep learning architectures and foundation models have driven\nsignificant advances in diverse robotic applications, they remain limited in\ntheir ability to operate efficiently and reliably in unknown and dynamic\nenvironments. In this position paper, we critically assess these limitations\nand introduce a conceptual framework for combining data-driven learning with\ndeliberate, structured reasoning. Specifically, we propose leveraging\ndifferentiable physics for efficient world modeling, Bayesian inference for\nuncertainty-aware decision-making, and meta-learning for rapid adaptation to\nnew tasks. By embedding physical symbolic reasoning within neural models,\nrobots could generalize beyond their training data, reason about novel\nsituations, and continuously expand their knowledge. We argue that such hybrid\nneuro-symbolic architectures are essential for the next generation of\nautonomous systems, and to this end, we provide a research roadmap to guide and\naccelerate their development."}
{"id": "2506.08795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08795", "abs": "https://arxiv.org/abs/2506.08795", "authors": ["Kaijie Shi", "Wanglong Lu", "Hanli Zhao", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning", "comment": null, "summary": "Limb loss affects millions globally, impairing physical function and reducing\nquality of life. Most traditional surface electromyographic (sEMG) and\nsemi-autonomous methods require users to generate myoelectric signals for each\ncontrol, imposing physically and mentally taxing demands. This study aims to\ndevelop a fully autonomous control system that enables a prosthetic hand to\nautomatically grasp and release objects of various shapes using only a camera\nattached to the wrist. By placing the hand near an object, the system will\nautomatically execute grasping actions with a proper grip force in response to\nthe hand's movements and the environment. To release the object being grasped,\njust naturally place the object close to the table and the system will\nautomatically open the hand. Such a system would provide individuals with limb\nloss with a very easy-to-use prosthetic control interface and greatly reduce\nmental effort while using. To achieve this goal, we developed a teleoperation\nsystem to collect human demonstration data for training the prosthetic hand\ncontrol model using imitation learning, which mimics the prosthetic hand\nactions from human. Through training the model using only a few objects' data\nfrom one single participant, we have shown that the imitation learning\nalgorithm can achieve high success rates, generalizing to more individuals and\nunseen objects with a variation of weights. The demonstrations are available at\n\\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}"}
{"id": "2506.08822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08822", "abs": "https://arxiv.org/abs/2506.08822", "authors": ["Yifei Su", "Ning Liu", "Dong Chen", "Zhen Zhao", "Kun Wu", "Meng Li", "Zhiyuan Xu", "Zhengping Che", "Jian Tang"], "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency", "comment": null, "summary": "Generative modeling-based visuomotor policies have been widely adopted in\nrobotic manipulation attributed to their ability to model multimodal action\ndistributions. However, the high inference cost of multi-step sampling limits\ntheir applicability in real-time robotic systems. To address this issue,\nexisting approaches accelerate the sampling process in generative\nmodeling-based visuomotor policies by adapting acceleration techniques\noriginally developed for image generation. Despite this progress, a major\ndistinction remains: image generation typically involves producing independent\nsamples without temporal dependencies, whereas robotic manipulation involves\ngenerating time-series action trajectories that require continuity and temporal\ncoherence. To effectively exploit temporal information in robotic manipulation,\nwe propose FreqPolicy, a novel approach that first imposes frequency\nconsistency constraints on flow-based visuomotor policies. Our work enables the\naction model to capture temporal structure effectively while supporting\nefficient, high-quality one-step action generation. We introduce a frequency\nconsistency constraint that enforces alignment of frequency-domain action\nfeatures across different timesteps along the flow, thereby promoting\nconvergence of one-step action generation toward the target distribution. In\naddition, we design an adaptive consistency loss to capture structural temporal\nvariations inherent in robotic manipulation tasks. We assess FreqPolicy on 53\ntasks across 3 simulation benchmarks, proving its superiority over existing\none-step action generators. We further integrate FreqPolicy into the\nvision-language-action (VLA) model and achieve acceleration without performance\ndegradation on the 40 tasks of Libero. Besides, we show efficiency and\neffectiveness in real-world robotic scenarios with an inference frequency\n93.5Hz. The code will be publicly available."}
{"id": "2506.08840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08840", "abs": "https://arxiv.org/abs/2506.08840", "authors": ["Dewei Wang", "Xinmiao Wang", "Xinzhe Liu", "Jiyuan Shi", "Yingnan Zhao", "Chenjia Bai", "Xuelong Li"], "title": "MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains", "comment": "9 pages, 5 figures", "summary": "Humanoid robots have demonstrated robust locomotion capabilities using\nReinforcement Learning (RL)-based approaches. Further, to obtain human-like\nbehaviors, existing methods integrate human motion-tracking or motion prior in\nthe RL framework. However, these methods are limited in flat terrains with\nproprioception only, restricting their abilities to traverse challenging\nterrains with human-like gaits. In this work, we propose a novel framework\nusing a mixture of latent residual experts with multi-discriminators to train\nan RL policy, which is capable of traversing complex terrains in controllable\nlifelike gaits with exteroception. Our two-stage training pipeline first\nteaches the policy to traverse complex terrains using a depth camera, and then\nenables gait-commanded switching between human-like gait patterns. We also\ndesign gait rewards to adjust human-like behaviors like robot base height.\nSimulation and real-world experiments demonstrate that our framework exhibits\nexceptional performance in traversing complex terrains, and achieves seamless\ntransitions between multiple human-like gait patterns."}
{"id": "2506.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08851", "abs": "https://arxiv.org/abs/2506.08851", "authors": ["Sepehr Samavi", "Garvish Bhutani", "Florian Shkurti", "Angela P. Schoellig"], "title": "Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics\n  (non-archival)", "summary": "Safe and efficient navigation in crowded environments remains a critical\nchallenge for robots that provide a variety of service tasks such as food\ndelivery or autonomous wheelchair mobility. Classical robot crowd navigation\nmethods decouple human motion prediction from robot motion planning, which\nneglects the closed-loop interactions between humans and robots. This lack of a\nmodel for human reactions to the robot plan (e.g. moving out of the way) can\ncause the robot to get stuck. Our proposed Safe and Interactive Crowd\nNavigation (SICNav) method is a bilevel Model Predictive Control (MPC)\nframework that combines prediction and planning into one optimization problem,\nexplicitly modeling interactions among agents. In this paper, we present a\nsystems overview of the crowd navigation platform we use to deploy SICNav in\npreviously unseen indoor and outdoor environments. We provide a preliminary\nanalysis of the system's operation over the course of nearly 7 km of autonomous\nnavigation over two hours in both indoor and outdoor environments."}
{"id": "2506.08856", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08856", "abs": "https://arxiv.org/abs/2506.08856", "authors": ["Jonathan P. King", "Harnoor Ahluwalia", "Michael Zhang", "Nancy S. Pollard"], "title": "Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation", "comment": "Submitted to IEEE Conference on Humanoid Robots", "summary": "This work presents a fast anytime algorithm for computing globally optimal\nindependent contact regions (ICRs). ICRs are regions such that one contact\nwithin each region enables a valid grasp. Locations of ICRs can provide\nguidance for grasp and manipulation planning, learning, and policy transfer.\nHowever, ICRs for modern applications have been little explored, in part due to\nthe expense of computing them, as they have a search space exponential in the\nnumber of contacts. We present a divide and conquer algorithm based on\nincremental n-dimensional Delaunay triangulation that produces results with\nbounded suboptimality in times sufficient for real-time planning. This paper\npresents the base algorithm for grasps where contacts lie within a plane. Our\nexperiments show substantial benefits over competing grasp quality metrics and\nspeedups of 100X and more for competing approaches to computing ICRs. We\nexplore robustness of a policy guided by ICRs and outline a path to general 3D\nimplementation. Code will be released on publication to facilitate further\ndevelopment and applications."}
{"id": "2506.08868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08868", "abs": "https://arxiv.org/abs/2506.08868", "authors": ["Marco Ruggia"], "title": "MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation", "comment": "12 pages, 12 figures, preprint", "summary": "MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone\nthat is fully actuated, meaning it can control its orientation independently of\nits position. MOMAV is also highly symmetrical, making its flight efficiency\nlargely unaffected by its current orientation. These characteristics are\nachieved by a novel drone design where six rotor arms align with the vertices\nof an octahedron, and where each arm can actively rotate along its long axis.\nVarious standout features of MOMAV are presented: The high flight efficiency\ncompared to arm configuration of other fully-actuated drones, the design of an\noriginal rotating arm assembly featuring slip-rings used to enable continuous\narm rotation, and a novel control allocation algorithm based on sequential\nquadratic programming (SQP) used to calculate throttle and arm-angle setpoints\nin flight. Flight tests have shown that MOMAV is able to achieve remarkably low\nmean position/orientation errors of 6.6mm, 2.1{\\deg} ({\\sigma}: 3.0mm,\n1.0{\\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\\deg} ({\\sigma}:\n8.6mm, 2.0{\\deg}) when sweeping orientation setpoints."}
{"id": "2506.08890", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08890", "abs": "https://arxiv.org/abs/2506.08890", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.08931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08931", "abs": "https://arxiv.org/abs/2506.08931", "authors": ["Yixuan Li", "Yutang Lin", "Jieming Cui", "Tengyu Liu", "Wei Liang", "Yixin Zhu", "Siyuan Huang"], "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks", "comment": "18 pages, 13 figures", "summary": "Humanoid teleoperation plays a vital role in demonstrating and collecting\ndata for complex humanoid-scene interactions. However, current teleoperation\nsystems face critical limitations: they decouple upper- and lower-body control\nto maintain stability, restricting natural coordination, and operate open-loop\nwithout real-time position feedback, leading to accumulated drift. The\nfundamental challenge is achieving precise, coordinated whole-body\nteleoperation over extended durations while maintaining accurate global\npositioning. Here we show that an MoE-based teleoperation system, CLONE, with\nclosed-loop error correction enables unprecedented whole-body teleoperation\nfidelity, maintaining minimal positional drift over long-range trajectories\nusing only head and hand tracking from an MR headset. Unlike previous methods\nthat either sacrifice coordination for stability or suffer from unbounded\ndrift, CLONE learns diverse motion skills while preventing tracking error\naccumulation through real-time feedback, enabling complex coordinated movements\nsuch as ``picking up objects from the ground.'' These results establish a new\nmilestone for whole-body humanoid teleoperation for long-horizon humanoid-scene\ninteraction tasks."}
{"id": "2506.08061", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08061", "abs": "https://arxiv.org/abs/2506.08061", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Neşet Ünver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Taşkın Padır"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08039", "abs": "https://arxiv.org/abs/2506.08039", "authors": ["Ray Wai Man Kong"], "title": "AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production", "comment": "12 pages, 9 Figures", "summary": "Efficiency, speed, and precision are essential in modern manufacturing. AI\nMaglev Conveyor system, combining magnetic levitation (maglev) technology with\nartificial intelligence (AI), revolutionizes automated production processes.\nThis system reduces maintenance costs and downtime by eliminating friction,\nenhancing operational efficiency. It transports goods swiftly with minimal\nenergy consumption, optimizing resource use and supporting sustainability. AI\nintegration enables real-time monitoring and adaptive control, allowing\nbusinesses to respond to production demand fluctuations and streamline supply\nchain operations.\n  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse\nproduct types and sizes for flexible manufacturing without extensive\nreconfiguration. AI algorithms optimize routing, reduce cycle times, and\nimprove throughput, creating an agile production line adaptable to market\nchanges.\n  This applied research paper introduces the Maglev Conveyor system, featuring\nan electromagnetic controller and multiple movers to enhance automation. It\noffers cost savings as an alternative to setups using six-axis robots or linear\nmotors, with precise adjustments for robotic arm loading. Operating at high\nspeeds minimizes treatment time for delicate components while maintaining\nprecision. Its adaptable design accommodates various materials, facilitating\nintegration of processing stations alongside electronic product assembly.\nPositioned between linear-axis and robotic systems in cost, the Maglev Conveyor\nis ideal for flat parts requiring minimal travel, transforming production\nefficiency across industries. It explores its technical advantages,\nflexibility, cost reductions, and overall benefits."}
{"id": "2506.08045", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08045", "abs": "https://arxiv.org/abs/2506.08045", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs", "comment": "40 pages, 6 Figures", "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence,\nintegrating perception, decision-making, memory, and collaborative planning to\noperate adaptively in complex, real-world environments. Driven by recent\nadvances in Agentic AI, these systems surpass traditional UAVs by exhibiting\ngoal-driven behavior, contextual reasoning, and interactive autonomy. We\nprovide a comprehensive foundation for understanding the architectural\ncomponents and enabling technologies that distinguish Agentic UAVs from\ntraditional autonomous UAVs. Furthermore, a detailed comparative analysis\nhighlights advancements in autonomy with AI agents, learning, and mission\nflexibility. This study explores seven high-impact application domains\nprecision agriculture, construction & mining, disaster response, environmental\nmonitoring, infrastructure inspection, logistics, security, and wildlife\nconservation, illustrating the broad societal value of agentic aerial\nintelligence. Furthermore, we identify key challenges in technical constraints,\nregulatory limitations, and data-model reliability, and we present emerging\nsolutions across hardware innovation, learning architectures, and human-AI\ninteraction. Finally, a future roadmap is proposed, outlining pathways toward\nself-evolving aerial ecosystems, system-level collaboration, and sustainable,\nequitable deployments. This survey establishes a foundational framework for the\nfuture development, deployment, and governance of agentic aerial systems\n(Agentic UAVs) across diverse societal and industrial domains."}
{"id": "2506.08061", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08061", "abs": "https://arxiv.org/abs/2506.08061", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments."}
{"id": "2506.08149", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08149", "abs": "https://arxiv.org/abs/2506.08149", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "comment": null, "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}."}
{"id": "2506.08291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08291", "abs": "https://arxiv.org/abs/2506.08291", "authors": ["Won Kyung Do", "Matthew Strong", "Aiden Swann", "Boshu Lei", "Monroe Kennedy III"], "title": "TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation", "comment": null, "summary": "Advanced dexterous manipulation involving multiple simultaneous contacts\nacross different surfaces, like pinching coins from ground or manipulating\nintertwined objects, remains challenging for robotic systems. Such tasks exceed\nthe capabilities of vision and proprioception alone, requiring high-resolution\ntactile sensing with calibrated physical metrics. Raw optical tactile sensor\nimages, while information-rich, lack interpretability and cross-sensor\ntransferability, limiting their real-world utility. TensorTouch addresses this\nchallenge by integrating finite element analysis with deep learning to extract\ncomprehensive contact information from optical tactile sensors, including\nstress tensors, deformation fields, and force distributions at pixel-level\nresolution. The TensorTouch framework achieves sub-millimeter position accuracy\nand precise force estimation while supporting large sensor deformations crucial\nfor manipulating soft objects. Experimental validation demonstrates 90% success\nin selectively grasping one of two strings based on detected motion, enabling\nnew contact-rich manipulation capabilities previously inaccessible to robotic\nsystems."}
{"id": "2506.08296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08296", "abs": "https://arxiv.org/abs/2506.08296", "authors": ["Hongjun Wu", "Heng Zhang", "Pengsong Zhang", "Jin Wang", "Cong Wang"], "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation", "comment": "31 pages,5 figures", "summary": "Recent advances in multimodal vision-language-action (VLA) models have\nrevolutionized traditional robot learning, enabling systems to interpret\nvision, language, and action in unified frameworks for complex task planning.\nHowever, mastering complex manipulation tasks remains an open challenge,\nconstrained by limitations in persistent contextual memory, multi-agent\ncoordination under uncertainty, and dynamic long-horizon planning across\nvariable sequences. To address this challenge, we propose \\textbf{HiBerNAC}, a\n\\textbf{Hi}erarchical \\textbf{B}rain-\\textbf{e}mulated \\textbf{r}obotic\n\\textbf{N}eural \\textbf{A}gent \\textbf{C}ollective, inspired by breakthroughs\nin neuroscience, particularly in neural circuit mechanisms and hierarchical\ndecision-making. Our framework combines: (1) multimodal VLA planning and\nreasoning with (2) neuro-inspired reflection and multi-agent mechanisms,\nspecifically designed for complex robotic manipulation tasks. By leveraging\nneuro-inspired functional modules with decentralized multi-agent collaboration,\nour approach enables robust and enhanced real-time execution of complex\nmanipulation tasks. In addition, the agentic system exhibits scalable\ncollective intelligence via dynamic agent specialization, adapting its\ncoordination strategy to variable task horizons and complexity. Through\nextensive experiments on complex manipulation tasks compared with\nstate-of-the-art VLA models, we demonstrate that \\textbf{HiBerNAC} reduces\naverage long-horizon task completion time by 23\\%, and achieves non-zero\nsuccess rates (12\\textendash 31\\%) on multi-path tasks where prior\nstate-of-the-art VLA models consistently fail. These results provide indicative\nevidence for bridging biological cognition and robotic learning mechanisms."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Neşet Ünver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Taşkın Padır"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08416", "abs": "https://arxiv.org/abs/2506.08416", "authors": ["Bolin Li", "Linwei Sun", "Xuecong Huang", "Yuzhi Jiang", "Lijun Zhu"], "title": "Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots", "comment": null, "summary": "This paper presents a periodic bipedal gait learning method using reward\ncomposition, integrated with a real-time gait planner for humanoid robots.\nFirst, we introduce a novel gait planner that incorporates dynamics to design\nthe desired joint trajectory. In the gait design process, the 3D robot model is\ndecoupled into two 2D models, which are then approximated as hybrid inverted\npendulums (H-LIP) for trajectory planning. The gait planner operates in\nparallel in real time within the robot's learning environment. Second, based on\nthis gait planner, we design three effective reward functions within a\nreinforcement learning framework, forming a reward composition to achieve\nperiodic bipedal gait. This reward composition reduces the robot's learning\ntime and enhances locomotion performance. Finally, a gait design example and\nperformance comparison are presented to demonstrate the effectiveness of the\nproposed method."}
{"id": "2506.08434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08434", "abs": "https://arxiv.org/abs/2506.08434", "authors": ["Rui Zhao", "Xingjian Zhang", "Yuhong Cao", "Yizhuo Wang", "Guillaume Sartoretti"], "title": "Attention-based Learning for 3D Informative Path Planning", "comment": null, "summary": "In this work, we propose an attention-based deep reinforcement learning\napproach to address the adaptive informative path planning (IPP) problem in 3D\nspace, where an aerial robot equipped with a downward-facing sensor must\ndynamically adjust its 3D position to balance sensing footprint and accuracy,\nand finally obtain a high-quality belief of an underlying field of interest\nover a given domain (e.g., presence of specific plants, hazardous gas,\ngeological structures, etc.). In adaptive IPP tasks, the agent is tasked with\nmaximizing information collected under time/distance constraints, continuously\nadapting its path based on newly acquired sensor data. To this end, we leverage\nattention mechanisms for their strong ability to capture global spatial\ndependencies across large action spaces, allowing the agent to learn an\nimplicit estimation of environmental transitions. Our model builds a contextual\nbelief representation over the entire domain, guiding sequential movement\ndecisions that optimize both short- and long-term search objectives.\nComparative evaluations against state-of-the-art planners demonstrate that our\napproach significantly reduces environmental uncertainty within constrained\nbudgets, thus allowing the agent to effectively balance exploration and\nexploitation. We further show our model generalizes well to environments of\nvarying sizes, highlighting its potential for many real-world applications."}
{"id": "2506.08440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08440", "abs": "https://arxiv.org/abs/2506.08440", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO"}
{"id": "2506.08459", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08459", "abs": "https://arxiv.org/abs/2506.08459", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "comment": null, "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections."}
{"id": "2506.08578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08578", "abs": "https://arxiv.org/abs/2506.08578", "authors": ["Boyang Chen", "Xizhe Zang", "Chao Song", "Yue Zhang", "Xuehe Zhang", "Jie Zhao"], "title": "Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot", "comment": null, "summary": "The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design\ninspired by the rollover shape of the human foot, significantly enhances the\nenergy efficiency of the robot walking gait. However, due to the tilt of the\nsupporting leg, the error of the contact model are amplified, making robot\nstate estimation more challenging. Therefore, this paper focuses on the noise\nanalysis and state estimation for robot walking with the ESVC foot. First,\nthrough physical robot experiments, we investigate the effect of the ESVC foot\non robot measurement noise and process noise. and a noise-time regression model\nusing sliding window strategy is developed. Then, a hierarchical adaptive state\nestimator for biped robots with the ESVC foot is proposed. The state estimator\nconsists of two stages: pre-estimation and post-estimation. In the\npre-estimation stage, a data fusion-based estimation is employed to process the\nsensory data. During post-estimation, the acceleration of center of mass is\nfirst estimated, and then the noise covariance matrices are adjusted based on\nthe regression model. Following that, an EKF(Extended Kalman Filter) based\napproach is applied to estimate the centroid state during robot walking.\nPhysical experiments demonstrate that the proposed adaptive state estimator for\nbiped robot walking with the ESVC foot not only provides higher precision than\nboth EKF and Adaptive EKF, but also converges faster under varying noise\nconditions."}
{"id": "2506.08639", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08639", "abs": "https://arxiv.org/abs/2506.08639", "authors": ["Amir Hossein Barjini", "Seyed Adel Alizadeh Kolagar", "Sadeq Yaqubi", "Jouni Mattila"], "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators", "comment": null, "summary": "This article presents a motion planning and control framework for flexible\nrobotic manipulators, integrating deep reinforcement learning (DRL) with a\nnonlinear partial differential equation (PDE) controller. Unlike conventional\napproaches that focus solely on control, we demonstrate that the desired\ntrajectory significantly influences endpoint vibrations. To address this, a DRL\nmotion planner, trained using the soft actor-critic (SAC) algorithm, generates\noptimized trajectories that inherently minimize vibrations. The PDE nonlinear\ncontroller then computes the required torques to track the planned trajectory\nwhile ensuring closed-loop stability using Lyapunov analysis. The proposed\nmethodology is validated through both simulations and real-world experiments,\ndemonstrating superior vibration suppression and tracking accuracy compared to\ntraditional methods. The results underscore the potential of combining\nlearning-based motion planning with model-based control for enhancing the\nprecision and stability of flexible robotic manipulators."}
{"id": "2506.08706", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.08706", "abs": "https://arxiv.org/abs/2506.08706", "authors": ["Tomasz Winiarski", "Jan Kaniuka", "Daniel Giełdowski", "Jakub Ostrysz", "Krystian Radlak", "Dmytro Kushnir"], "title": "ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel", "comment": "19 pages", "summary": "As robotic systems grow increasingly complex, heterogeneous, and\nsafety-critical, the need for structured development methodologies becomes\nparamount. Although frameworks like the Robot Operating System (ROS) and\nModel-Based Systems Engineering (MBSE) offer foundational tools, they often\nlack integration when used together. This paper addresses that gap by aligning\nthe widely recognized V-model development paradigm with the MeROS metamodel\nSysML-based modeling language tailored for ROS-based systems.\n  We propose a domain-specific methodology that bridges ROS-centric modelling\nwith systems engineering practices. Our approach formalises the structure,\nbehaviour, and validation processes of robotic systems using MeROS, while\nextending it with a generalized, adaptable V-model compatible with both ROS and\nROS 2. Rather than prescribing a fixed procedure, the approach supports\nproject-specific flexibility and reuse, offering guidance across all stages of\ndevelopment.\n  The approach is validated through a comprehensive case study on HeROS, a\nheterogeneous multi-robot platform comprising manipulators, mobile units, and\ndynamic test environments. This example illustrates how the MeROS-compatible\nV-model enhances traceability and system consistency while remaining accessible\nand extensible for future adaptation. The work contributes a structured,\ntool-agnostic foundation for developers and researchers seeking to apply MBSE\npractices in ROS-based projects."}
{"id": "2506.08708", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving."}
{"id": "2506.08756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08756", "abs": "https://arxiv.org/abs/2506.08756", "authors": ["Octavio Arriaga", "Rebecca Adam", "Melvin Laux", "Lisa Gutzeit", "Marco Ragni", "Jan Peters", "Frank Kirchner"], "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning", "comment": null, "summary": "Real-world robotic applications, from autonomous exploration to assistive\ntechnologies, require adaptive, interpretable, and data-efficient learning\nparadigms. While deep learning architectures and foundation models have driven\nsignificant advances in diverse robotic applications, they remain limited in\ntheir ability to operate efficiently and reliably in unknown and dynamic\nenvironments. In this position paper, we critically assess these limitations\nand introduce a conceptual framework for combining data-driven learning with\ndeliberate, structured reasoning. Specifically, we propose leveraging\ndifferentiable physics for efficient world modeling, Bayesian inference for\nuncertainty-aware decision-making, and meta-learning for rapid adaptation to\nnew tasks. By embedding physical symbolic reasoning within neural models,\nrobots could generalize beyond their training data, reason about novel\nsituations, and continuously expand their knowledge. We argue that such hybrid\nneuro-symbolic architectures are essential for the next generation of\nautonomous systems, and to this end, we provide a research roadmap to guide and\naccelerate their development."}
{"id": "2506.08795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08795", "abs": "https://arxiv.org/abs/2506.08795", "authors": ["Kaijie Shi", "Wanglong Lu", "Hanli Zhao", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning", "comment": null, "summary": "Limb loss affects millions globally, impairing physical function and reducing\nquality of life. Most traditional surface electromyographic (sEMG) and\nsemi-autonomous methods require users to generate myoelectric signals for each\ncontrol, imposing physically and mentally taxing demands. This study aims to\ndevelop a fully autonomous control system that enables a prosthetic hand to\nautomatically grasp and release objects of various shapes using only a camera\nattached to the wrist. By placing the hand near an object, the system will\nautomatically execute grasping actions with a proper grip force in response to\nthe hand's movements and the environment. To release the object being grasped,\njust naturally place the object close to the table and the system will\nautomatically open the hand. Such a system would provide individuals with limb\nloss with a very easy-to-use prosthetic control interface and greatly reduce\nmental effort while using. To achieve this goal, we developed a teleoperation\nsystem to collect human demonstration data for training the prosthetic hand\ncontrol model using imitation learning, which mimics the prosthetic hand\nactions from human. Through training the model using only a few objects' data\nfrom one single participant, we have shown that the imitation learning\nalgorithm can achieve high success rates, generalizing to more individuals and\nunseen objects with a variation of weights. The demonstrations are available at\n\\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}"}
{"id": "2506.08822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08822", "abs": "https://arxiv.org/abs/2506.08822", "authors": ["Yifei Su", "Ning Liu", "Dong Chen", "Zhen Zhao", "Kun Wu", "Meng Li", "Zhiyuan Xu", "Zhengping Che", "Jian Tang"], "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency", "comment": null, "summary": "Generative modeling-based visuomotor policies have been widely adopted in\nrobotic manipulation attributed to their ability to model multimodal action\ndistributions. However, the high inference cost of multi-step sampling limits\ntheir applicability in real-time robotic systems. To address this issue,\nexisting approaches accelerate the sampling process in generative\nmodeling-based visuomotor policies by adapting acceleration techniques\noriginally developed for image generation. Despite this progress, a major\ndistinction remains: image generation typically involves producing independent\nsamples without temporal dependencies, whereas robotic manipulation involves\ngenerating time-series action trajectories that require continuity and temporal\ncoherence. To effectively exploit temporal information in robotic manipulation,\nwe propose FreqPolicy, a novel approach that first imposes frequency\nconsistency constraints on flow-based visuomotor policies. Our work enables the\naction model to capture temporal structure effectively while supporting\nefficient, high-quality one-step action generation. We introduce a frequency\nconsistency constraint that enforces alignment of frequency-domain action\nfeatures across different timesteps along the flow, thereby promoting\nconvergence of one-step action generation toward the target distribution. In\naddition, we design an adaptive consistency loss to capture structural temporal\nvariations inherent in robotic manipulation tasks. We assess FreqPolicy on 53\ntasks across 3 simulation benchmarks, proving its superiority over existing\none-step action generators. We further integrate FreqPolicy into the\nvision-language-action (VLA) model and achieve acceleration without performance\ndegradation on the 40 tasks of Libero. Besides, we show efficiency and\neffectiveness in real-world robotic scenarios with an inference frequency\n93.5Hz. The code will be publicly available."}
{"id": "2506.08840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08840", "abs": "https://arxiv.org/abs/2506.08840", "authors": ["Dewei Wang", "Xinmiao Wang", "Xinzhe Liu", "Jiyuan Shi", "Yingnan Zhao", "Chenjia Bai", "Xuelong Li"], "title": "MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains", "comment": "9 pages, 5 figures", "summary": "Humanoid robots have demonstrated robust locomotion capabilities using\nReinforcement Learning (RL)-based approaches. Further, to obtain human-like\nbehaviors, existing methods integrate human motion-tracking or motion prior in\nthe RL framework. However, these methods are limited in flat terrains with\nproprioception only, restricting their abilities to traverse challenging\nterrains with human-like gaits. In this work, we propose a novel framework\nusing a mixture of latent residual experts with multi-discriminators to train\nan RL policy, which is capable of traversing complex terrains in controllable\nlifelike gaits with exteroception. Our two-stage training pipeline first\nteaches the policy to traverse complex terrains using a depth camera, and then\nenables gait-commanded switching between human-like gait patterns. We also\ndesign gait rewards to adjust human-like behaviors like robot base height.\nSimulation and real-world experiments demonstrate that our framework exhibits\nexceptional performance in traversing complex terrains, and achieves seamless\ntransitions between multiple human-like gait patterns."}
{"id": "2506.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08851", "abs": "https://arxiv.org/abs/2506.08851", "authors": ["Sepehr Samavi", "Garvish Bhutani", "Florian Shkurti", "Angela P. Schoellig"], "title": "Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics\n  (non-archival)", "summary": "Safe and efficient navigation in crowded environments remains a critical\nchallenge for robots that provide a variety of service tasks such as food\ndelivery or autonomous wheelchair mobility. Classical robot crowd navigation\nmethods decouple human motion prediction from robot motion planning, which\nneglects the closed-loop interactions between humans and robots. This lack of a\nmodel for human reactions to the robot plan (e.g. moving out of the way) can\ncause the robot to get stuck. Our proposed Safe and Interactive Crowd\nNavigation (SICNav) method is a bilevel Model Predictive Control (MPC)\nframework that combines prediction and planning into one optimization problem,\nexplicitly modeling interactions among agents. In this paper, we present a\nsystems overview of the crowd navigation platform we use to deploy SICNav in\npreviously unseen indoor and outdoor environments. We provide a preliminary\nanalysis of the system's operation over the course of nearly 7 km of autonomous\nnavigation over two hours in both indoor and outdoor environments."}
{"id": "2506.08856", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08856", "abs": "https://arxiv.org/abs/2506.08856", "authors": ["Jonathan P. King", "Harnoor Ahluwalia", "Michael Zhang", "Nancy S. Pollard"], "title": "Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation", "comment": "Submitted to IEEE Conference on Humanoid Robots", "summary": "This work presents a fast anytime algorithm for computing globally optimal\nindependent contact regions (ICRs). ICRs are regions such that one contact\nwithin each region enables a valid grasp. Locations of ICRs can provide\nguidance for grasp and manipulation planning, learning, and policy transfer.\nHowever, ICRs for modern applications have been little explored, in part due to\nthe expense of computing them, as they have a search space exponential in the\nnumber of contacts. We present a divide and conquer algorithm based on\nincremental n-dimensional Delaunay triangulation that produces results with\nbounded suboptimality in times sufficient for real-time planning. This paper\npresents the base algorithm for grasps where contacts lie within a plane. Our\nexperiments show substantial benefits over competing grasp quality metrics and\nspeedups of 100X and more for competing approaches to computing ICRs. We\nexplore robustness of a policy guided by ICRs and outline a path to general 3D\nimplementation. Code will be released on publication to facilitate further\ndevelopment and applications."}
{"id": "2506.08868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08868", "abs": "https://arxiv.org/abs/2506.08868", "authors": ["Marco Ruggia"], "title": "MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation", "comment": "12 pages, 12 figures, preprint", "summary": "MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone\nthat is fully actuated, meaning it can control its orientation independently of\nits position. MOMAV is also highly symmetrical, making its flight efficiency\nlargely unaffected by its current orientation. These characteristics are\nachieved by a novel drone design where six rotor arms align with the vertices\nof an octahedron, and where each arm can actively rotate along its long axis.\nVarious standout features of MOMAV are presented: The high flight efficiency\ncompared to arm configuration of other fully-actuated drones, the design of an\noriginal rotating arm assembly featuring slip-rings used to enable continuous\narm rotation, and a novel control allocation algorithm based on sequential\nquadratic programming (SQP) used to calculate throttle and arm-angle setpoints\nin flight. Flight tests have shown that MOMAV is able to achieve remarkably low\nmean position/orientation errors of 6.6mm, 2.1{\\deg} ({\\sigma}: 3.0mm,\n1.0{\\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\\deg} ({\\sigma}:\n8.6mm, 2.0{\\deg}) when sweeping orientation setpoints."}
{"id": "2506.08890", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08890", "abs": "https://arxiv.org/abs/2506.08890", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.08931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08931", "abs": "https://arxiv.org/abs/2506.08931", "authors": ["Yixuan Li", "Yutang Lin", "Jieming Cui", "Tengyu Liu", "Wei Liang", "Yixin Zhu", "Siyuan Huang"], "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks", "comment": "18 pages, 13 figures", "summary": "Humanoid teleoperation plays a vital role in demonstrating and collecting\ndata for complex humanoid-scene interactions. However, current teleoperation\nsystems face critical limitations: they decouple upper- and lower-body control\nto maintain stability, restricting natural coordination, and operate open-loop\nwithout real-time position feedback, leading to accumulated drift. The\nfundamental challenge is achieving precise, coordinated whole-body\nteleoperation over extended durations while maintaining accurate global\npositioning. Here we show that an MoE-based teleoperation system, CLONE, with\nclosed-loop error correction enables unprecedented whole-body teleoperation\nfidelity, maintaining minimal positional drift over long-range trajectories\nusing only head and hand tracking from an MR headset. Unlike previous methods\nthat either sacrifice coordination for stability or suffer from unbounded\ndrift, CLONE learns diverse motion skills while preventing tracking error\naccumulation through real-time feedback, enabling complex coordinated movements\nsuch as ``picking up objects from the ground.'' These results establish a new\nmilestone for whole-body humanoid teleoperation for long-horizon humanoid-scene\ninteraction tasks."}
