{"id": "2507.01111", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01111", "abs": "https://arxiv.org/abs/2507.01111", "authors": ["Haosen Xing", "Haoran Ma", "Sijin Zhang", "Hartmut Geyer"], "title": "Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios", "comment": null, "summary": "Current control strategies for powered lower limb prostheses often lack\nawareness of the environment and the user's intended interactions with it. This\nlimitation becomes particularly apparent in complex terrains. Obstacle\nnegotiation, a critical scenario exemplifying such challenges, requires both\nreal-time perception of obstacle geometry and responsiveness to user intention\nabout when and where to step over or onto, to dynamically adjust swing\ntrajectories. We propose a novel control strategy that fuses environmental\nawareness and human cooperativeness: an on-board depth camera detects obstacles\nahead of swing phase, prompting an elevated early-swing trajectory to ensure\nclearance, while late-swing control defers to natural biomechanical cues from\nthe user. This approach enables intuitive stepping strategies without requiring\nunnatural movement patterns. Experiments with three non-amputee participants\ndemonstrated 100 percent success across more than 150 step-overs and 30\nstep-ons with randomly placed obstacles of varying heights (4-16 cm) and\ndistances (15-70 cm). By effectively addressing obstacle navigation -- a\ngateway challenge for complex terrain mobility -- our system demonstrates\nadaptability to both environmental constraints and user intentions, with\npromising applications across diverse locomotion scenarios.", "AI": {"tldr": "提出了一种融合环境感知和用户意图的新型控制策略，用于下肢假肢的障碍物导航，实验证明其高效性。", "motivation": "现有假肢控制策略缺乏对环境及用户意图的感知，尤其在复杂地形中表现不足。", "method": "利用机载深度摄像头检测障碍物，动态调整摆动轨迹，结合用户生物力学信号实现自然步态。", "result": "实验显示，在150多次跨越和30多次踏上障碍物的测试中，成功率达100%。", "conclusion": "该系统有效解决了障碍物导航问题，展示了在复杂地形中的适应性和应用潜力。"}}
{"id": "2507.01125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01125", "abs": "https://arxiv.org/abs/2507.01125", "authors": ["Keiko Nagami", "Timothy Chen", "Javier Yu", "Ola Shorinwa", "Maximilian Adang", "Carlyn Dougherty", "Eric Cristofalo", "Mac Schwager"], "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting", "comment": "9 pages, 4 figures", "summary": "We present VISTA (Viewpoint-based Image selection with Semantic Task\nAwareness), an active exploration method for robots to plan informative\ntrajectories that improve 3D map quality in areas most relevant for task\ncompletion. Given an open-vocabulary search instruction (e.g., \"find a\nperson\"), VISTA enables a robot to explore its environment to search for the\nobject of interest, while simultaneously building a real-time semantic 3D\nGaussian Splatting reconstruction of the scene. The robot navigates its\nenvironment by planning receding-horizon trajectories that prioritize semantic\nsimilarity to the query and exploration of unseen regions of the environment.\nTo evaluate trajectories, VISTA introduces a novel, efficient\nviewpoint-semantic coverage metric that quantifies both the geometric view\ndiversity and task relevance in the 3D scene. On static datasets, our coverage\nmetric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in\ncomputation speed and reconstruction quality. In quadrotor hardware\nexperiments, VISTA achieves 6x higher success rates in challenging maps,\ncompared to baseline methods, while matching baseline performance in less\nchallenging maps. Lastly, we show that VISTA is platform-agnostic by deploying\nit on a quadrotor drone and a Spot quadruped robot. Open-source code will be\nreleased upon acceptance of the paper.", "AI": {"tldr": "VISTA是一种主动探索方法，帮助机器人规划信息丰富的轨迹，提升任务相关区域的3D地图质量。", "motivation": "解决机器人在开放词汇搜索任务中高效探索环境并构建语义3D地图的需求。", "method": "通过规划优先考虑语义相似性和未探索区域的轨迹，结合新颖的视点-语义覆盖度量评估。", "result": "在静态数据集和硬件实验中表现优于现有方法，计算速度和重建质量更优。", "conclusion": "VISTA具有平台无关性，适用于多种机器人平台，显著提升任务成功率。"}}
{"id": "2507.01143", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01143", "abs": "https://arxiv.org/abs/2507.01143", "authors": ["Reza Jalayer", "Masoud Jalayer", "Amirali Baniasadi"], "title": "A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods", "comment": "35 pages", "summary": "Sound source localization (SSL) adds a spatial dimension to auditory\nperception, allowing a system to pinpoint the origin of speech, machinery\nnoise, warning tones, or other acoustic events, capabilities that facilitate\nrobot navigation, human-machine dialogue, and condition monitoring. While\nexisting surveys provide valuable historical context, they typically address\ngeneral audio applications and do not fully account for robotic constraints or\nthe latest advancements in deep learning. This review addresses these gaps by\noffering a robotics-focused synthesis, emphasizing recent progress in deep\nlearning methodologies. We start by reviewing classical methods such as Time\nDifference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and\nsubspace analysis. Subsequently, we delve into modern machine learning (ML) and\ndeep learning (DL) approaches, discussing traditional ML and neural networks\n(NNs), convolutional neural networks (CNNs), convolutional recurrent neural\nnetworks (CRNNs), and emerging attention-based architectures. The data and\ntraining strategy that are the two cornerstones of DL-based SSL are explored.\nStudies are further categorized by robot types and application domains to\nfacilitate researchers in identifying relevant work for their specific\ncontexts. Finally, we highlight the current challenges in SSL works in general,\nregarding environmental robustness, sound source multiplicity, and specific\nimplementation constraints in robotics, as well as data and learning strategies\nin DL-based SSL. Also, we sketch promising directions to offer an actionable\nroadmap toward robust, adaptable, efficient, and explainable DL-based SSL for\nnext-generation robots.", "AI": {"tldr": "本文综述了机器人领域中基于深度学习的声源定位技术，填补了现有综述的不足，并探讨了未来研究方向。", "motivation": "现有综述多关注通用音频应用，未充分考虑机器人领域的限制和深度学习的最新进展，本文旨在填补这一空白。", "method": "回顾了经典方法（如TDOA、波束成形等）和现代深度学习方法（如CNN、CRNN等），并探讨了数据和训练策略。", "result": "总结了机器人声源定位的当前挑战，包括环境鲁棒性、多声源问题等，并提出了未来研究方向。", "conclusion": "提出了实现下一代机器人稳健、高效、可解释的深度学习声源定位的行动路线。"}}
{"id": "2507.01152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01152", "abs": "https://arxiv.org/abs/2507.01152", "authors": ["Yunke Ao", "Masoud Moghani", "Mayank Mittal", "Manish Prajapat", "Luohong Wu", "Frederic Giraud", "Fabio Carrillo", "Andreas Krause", "Philipp Fürnstahl"], "title": "SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound", "comment": "21 pages, 15 figures", "summary": "Ultrasound (US) is a widely used medical imaging modality due to its\nreal-time capabilities, non-invasive nature, and cost-effectiveness. Robotic\nultrasound can further enhance its utility by reducing operator dependence and\nimproving access to complex anatomical regions. For this, while deep\nreinforcement learning (DRL) and imitation learning (IL) have shown potential\nfor autonomous navigation, their use in complex surgical tasks such as anatomy\nreconstruction and surgical guidance remains limited -- largely due to the lack\nof realistic and efficient simulation environments tailored to these tasks. We\nintroduce SonoGym, a scalable simulation platform for complex robotic\nultrasound tasks that enables parallel simulation across tens to hundreds of\nenvironments. Our framework supports realistic and real-time simulation of US\ndata from CT-derived 3D models of the anatomy through both a physics-based and\na generative modeling approach. Sonogym enables the training of DRL and recent\nIL agents (vision transformers and diffusion policies) for relevant tasks in\nrobotic orthopedic surgery by integrating common robotic platforms and\northopedic end effectors. We further incorporate submodular DRL -- a recent\nmethod that handles history-dependent rewards -- for anatomy reconstruction and\nsafe reinforcement learning for surgery. Our results demonstrate successful\npolicy learning across a range of scenarios, while also highlighting the\nlimitations of current methods in clinically relevant environments. We believe\nour simulation can facilitate research in robot learning approaches for such\nchallenging robotic surgery applications. Dataset, codes, and videos are\npublicly available at https://sonogym.github.io/.", "AI": {"tldr": "SonoGym是一个用于复杂机器人超声任务的模拟平台，支持并行模拟，结合深度强化学习和模仿学习，用于骨科手术中的自主导航和任务训练。", "motivation": "当前深度强化学习和模仿学习在复杂手术任务中的应用受限，主要因为缺乏针对这些任务的真实高效模拟环境。", "method": "提出SonoGym平台，支持基于物理和生成模型的实时超声数据模拟，集成机器人平台和骨科工具，用于训练深度强化学习和模仿学习策略。", "result": "实验展示了在多种场景下策略学习的成功，同时揭示了当前方法在临床相关环境中的局限性。", "conclusion": "SonoGym有助于推动机器人学习在复杂手术应用中的研究，相关数据和代码已公开。"}}
{"id": "2507.01426", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01426", "abs": "https://arxiv.org/abs/2507.01426", "authors": ["Ratnangshu Das", "Pushpak Jagtap"], "title": "Approximation-free Control of Unknown Euler-Lagrangian Systems under Input Constraints", "comment": null, "summary": "In this paper, we present a novel funnel-based tracking control algorithm for\nrobotic systems with unknown dynamics and prescribed input constraints. The\nEuler-Lagrange formulation, a common modeling approach for robotic systems, has\nbeen adopted in this study to address the trade-off between performance and\nactuator safety. We establish feasibility conditions that ensure tracking\nerrors evolve within predefined funnel bounds while maintaining bounded control\nefforts, a crucial consideration for robots with limited actuation\ncapabilities. We propose two approximation-free control strategies for\nscenarios where these conditions are violated: one actively corrects the error,\nand the other stops further deviation. Finally, we demonstrate the robust\nperformance and safety of the approach through simulations and experimental\nvalidations. This work represents a significant advancement in funnel-based\ncontrol, enhancing its applicability to real-world robotics systems with input\nconstraints.", "AI": {"tldr": "提出了一种基于漏斗的跟踪控制算法，用于未知动态和输入约束的机器人系统，通过仿真和实验验证了其鲁棒性和安全性。", "motivation": "解决机器人系统在未知动态和输入约束下的性能与执行器安全之间的权衡问题。", "method": "采用欧拉-拉格朗日建模，提出两种无近似控制策略，确保跟踪误差在预设漏斗边界内演化。", "result": "通过仿真和实验验证了算法的鲁棒性能和安全性。", "conclusion": "该研究显著提升了漏斗控制在具有输入约束的实际机器人系统中的适用性。"}}
{"id": "2507.01181", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01181", "abs": "https://arxiv.org/abs/2507.01181", "authors": ["Vinicius M. Gonçalves", "Shiqing Wei", "Eduardo Malacarne S. de Souza", "Krishnamurthy Prashanth", "Anthony Tzes", "Farshad Khorrami"], "title": "A Differentiable Distance Metric for Robotics Through Generalized Alternating Projection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In many robotics applications, it is necessary to compute not only the\ndistance between the robot and the environment, but also its derivative - for\nexample, when using control barrier functions. However, since the traditional\nEuclidean distance is not differentiable, there is a need for alternative\ndistance metrics that possess this property. Recently, a metric with guaranteed\ndifferentiability was proposed [1]. This approach has some important drawbacks,\nwhich we address in this paper. We provide much simpler and practical\nexpressions for the smooth projection for general convex polytopes.\nAdditionally, as opposed to [1], we ensure that the distance vanishes as the\nobjects overlap. We show the efficacy of the approach in experimental results.\nOur proposed distance metric is publicly available through the Python-based\nsimulation package UAIBot.", "AI": {"tldr": "本文提出了一种更简单且实用的平滑投影方法，用于计算凸多面体的可微距离度量，解决了现有方法的不足，并通过实验验证了其有效性。", "motivation": "传统欧几里得距离不可微，而现有可微距离度量方法存在重要缺陷，本文旨在解决这些问题。", "method": "提出了一种新的平滑投影方法，适用于一般凸多面体，并确保距离在物体重叠时消失。", "result": "实验结果表明，该方法有效且优于现有方法。", "conclusion": "本文提出的可微距离度量方法更简单实用，已通过Python仿真包UAIBot公开。"}}
{"id": "2507.01198", "categories": ["cs.RO", "cs.AI", "cs.CG"], "pdf": "https://arxiv.org/pdf/2507.01198", "abs": "https://arxiv.org/abs/2507.01198", "authors": ["Benjamin Kraljusic", "Zlatan Ajanovic", "Nermin Covic", "Bakir Lacevic"], "title": "Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives", "comment": "6 pages, 3 figures, submitted to a conference", "summary": "This work proposes a motion planning algorithm for robotic manipulators that\ncombines sampling-based and search-based planning methods. The core\ncontribution of the proposed approach is the usage of burs of free\nconfiguration space (C-space) as adaptive motion primitives within the graph\nsearch algorithm. Due to their feature to adaptively expand in free C-space,\nburs enable more efficient exploration of the configuration space compared to\nfixed-sized motion primitives, significantly reducing the time to find a valid\npath and the number of required expansions. The algorithm is implemented within\nthe existing SMPL (Search-Based Motion Planning Library) library and evaluated\nthrough a series of different scenarios involving manipulators with varying\nnumber of degrees-of-freedom (DoF) and environment complexity. Results\ndemonstrate that the bur-based approach outperforms fixed-primitive planning in\ncomplex scenarios, particularly for high DoF manipulators, while achieving\ncomparable performance in simpler scenarios.", "AI": {"tldr": "提出了一种结合采样和搜索的运动规划算法，利用自由配置空间的burs作为自适应运动基元，显著提高路径规划效率。", "motivation": "传统固定尺寸运动基元在复杂场景中效率低，需要更高效的自适应方法。", "method": "在SMPL库中实现burs作为自适应运动基元，结合图搜索算法。", "result": "在复杂场景中，尤其高自由度机械臂，性能优于固定基元方法。", "conclusion": "burs自适应基元显著提升规划效率，适用于复杂和高自由度场景。"}}
{"id": "2507.01206", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01206", "abs": "https://arxiv.org/abs/2507.01206", "authors": ["Kathy Zhuang", "Zixun Huang", "Yukun Song", "Rui Li", "Yinuo Zhou", "Allen Y. Yang"], "title": "2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration", "comment": null, "summary": "As modern computing advances, new interaction paradigms have emerged,\nparticularly in Augmented Reality (AR), which overlays virtual interfaces onto\nphysical objects. This evolution poses challenges in machine perception,\nespecially for tasks like 3D object pose estimation in complex, dynamic\nenvironments. Our project addresses critical issues in human-robot interaction\nwithin mobile AR, focusing on non-intrusive, spatially aware interfaces. We\npresent URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024\nSUITS challenge, targeting future spaceflight needs such as the Artemis\nmissions. URSA integrates three core technologies: a head-mounted AR device\n(e.g., HoloLens) for intuitive visual feedback, voice control powered by large\nlanguage models for hands-free interaction, and robot tracking algorithms that\nenable accurate 3D localization in dynamic settings. To enhance precision, we\nleverage digital twin localization technologies, using datasets like\nDTTD-Mobile and specialized hardware such as the ZED2 camera for real-world\ntracking under noise and occlusion. Our system enables real-time robot control\nand monitoring via an AR interface, even in the absence of ground-truth\nsensors--vital for hazardous or remote operations. Key contributions include:\n(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based\ndataset tailored for non-rigid robotic bodies; (3) a Local Mission Control\nConsole (LMCC) for mission visualization; (4) a transformer-based 6DoF pose\nestimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)\nend-to-end integration for astronaut mission support. This work advances\ndigital twin applications in robotics, offering scalable solutions for both\naerospace and industrial domains.", "AI": {"tldr": "URSA是一个基于LLM的AR系统，用于NASA的SUITS挑战，结合AR设备、语音控制和机器人跟踪技术，支持实时机器人控制和任务可视化。", "motivation": "解决移动AR中的人机交互问题，特别是在复杂动态环境中的3D物体姿态估计，为未来的太空任务（如Artemis）提供支持。", "method": "整合头戴式AR设备、LLM驱动的语音控制、机器人跟踪算法和数字孪生定位技术，使用DTTD-Mobile数据集和ZED2相机。", "result": "开发了非侵入式AR界面、专用数据集、任务可视化控制台、优化的6DoF姿态估计器和端到端集成系统。", "conclusion": "URSA推动了数字孪生在机器人领域的应用，为航空航天和工业领域提供了可扩展的解决方案。"}}
{"id": "2507.01243", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01243", "abs": "https://arxiv.org/abs/2507.01243", "authors": ["Ziang Zheng", "Guojian Zhan", "Shiqi Liu", "Yao Lyu", "Tao Zhang", "Shengbo Eben Li"], "title": "Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion", "comment": null, "summary": "Reinforcement learning (RL) has shown great potential in enabling quadruped\nrobots to perform agile locomotion. However, directly training policies to\nsimultaneously handle dual extreme challenges, i.e., extreme underactuation and\nextreme terrains, as in monopedal hopping tasks, remains highly challenging due\nto unstable early-stage interactions and unreliable reward feedback. To address\nthis, we propose JumpER (jump-start reinforcement learning via self-evolving\npriors), an RL training framework that structures policy learning into multiple\nstages of increasing complexity. By dynamically generating self-evolving priors\nthrough iterative bootstrapping of previously learned policies, JumpER\nprogressively refines and enhances guidance, thereby stabilizing exploration\nand policy optimization without relying on external expert priors or\nhandcrafted reward shaping. Specifically, when integrated with a structured\nthree-stage curriculum that incrementally evolves action modality, observation\nspace, and task objective, JumpER enables quadruped robots to achieve robust\nmonopedal hopping on unpredictable terrains for the first time. Remarkably, the\nresulting policy effectively handles challenging scenarios that traditional\nmethods struggle to conquer, including wide gaps up to 60 cm, irregularly\nspaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.\nJumpER thus provides a principled and scalable approach for addressing\nlocomotion tasks under the dual challenges of extreme underactuation and\nextreme terrains.", "AI": {"tldr": "JumpER是一种通过自演化先验分阶段训练强化学习策略的框架，成功解决了四足机器人在极端欠驱动和极端地形下的单足跳跃任务。", "motivation": "传统方法在同时应对极端欠驱动和极端地形时，由于早期交互不稳定和奖励反馈不可靠，难以直接训练有效的策略。", "method": "JumpER通过多阶段策略学习，动态生成自演化先验，逐步优化策略，无需外部专家先验或手工奖励设计。", "result": "JumpER使四足机器人首次在不可预测地形上实现稳健的单足跳跃，并成功应对传统方法难以处理的挑战性场景。", "conclusion": "JumpER为极端欠驱动和极端地形下的运动任务提供了一种原则性和可扩展的解决方案。"}}
{"id": "2507.01264", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01264", "abs": "https://arxiv.org/abs/2507.01264", "authors": ["Yongjie Fu", "Ruijian Zha", "Pei Tian", "Xuan Di"], "title": "LLM-based Realistic Safety-Critical Driving Video Generation", "comment": null, "summary": "Designing diverse and safety-critical driving scenarios is essential for\nevaluating autonomous driving systems. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) for few-shot code\ngeneration to automatically synthesize driving scenarios within the CARLA\nsimulator, which has flexibility in scenario scripting, efficient code-based\ncontrol of traffic participants, and enforcement of realistic physical\ndynamics. Given a few example prompts and code samples, the LLM generates\nsafety-critical scenario scripts that specify the behavior and placement of\ntraffic participants, with a particular focus on collision events. To bridge\nthe gap between simulation and real-world appearance, we integrate a video\ngeneration pipeline using Cosmos-Transfer1 with ControlNet, which converts\nrendered scenes into realistic driving videos. Our approach enables\ncontrollable scenario generation and facilitates the creation of rare but\ncritical edge cases, such as pedestrian crossings under occlusion or sudden\nvehicle cut-ins. Experimental results demonstrate the effectiveness of our\nmethod in generating a wide range of realistic, diverse, and safety-critical\nscenarios, offering a promising tool for simulation-based testing of autonomous\nvehicles.", "AI": {"tldr": "提出了一种基于大语言模型（LLM）的框架，用于在CARLA模拟器中自动生成多样化和安全关键的驾驶场景，并结合视频生成技术提升场景的真实性。", "motivation": "评估自动驾驶系统需要多样化和安全关键的驾驶场景，但手动设计这些场景耗时且难以覆盖所有边缘情况。", "method": "利用LLM进行少样本代码生成，自动合成驾驶场景脚本，并通过视频生成技术（Cosmos-Transfer1与ControlNet）提升场景的真实性。", "result": "实验表明，该方法能高效生成多样、真实且安全关键的驾驶场景，包括罕见的边缘情况。", "conclusion": "该方法为自动驾驶系统的仿真测试提供了高效且可控的场景生成工具。"}}
{"id": "2507.01284", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01284", "abs": "https://arxiv.org/abs/2507.01284", "authors": ["Cristian Gariboldi", "Hayato Tokida", "Ken Kinjo", "Yuki Asada", "Alexander Carballo"], "title": "VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process", "comment": "2025 IEEE 28th International Conference on Intelligent Transportation\n  Systems (ITSC)", "summary": "Recent advancements in open-source Visual Language Models (VLMs) such as\nLLaVA, Qwen-VL, and Llama have catalyzed extensive research on their\nintegration with diverse systems. The internet-scale general knowledge\nencapsulated within these models presents significant opportunities for\nenhancing autonomous driving perception, prediction, and planning capabilities.\nIn this paper we propose VLAD, a vision-language autonomous driving model,\nwhich integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end\nsystem. We implement a specialized fine-tuning approach using custom\nquestion-answer datasets designed specifically to improve the spatial reasoning\ncapabilities of the model. The enhanced VLM generates high-level navigational\ncommands that VAD subsequently processes to guide vehicle operation.\nAdditionally, our system produces interpretable natural language explanations\nof driving decisions, thereby increasing transparency and trustworthiness of\nthe traditionally black-box end-to-end architecture. Comprehensive evaluation\non the real-world nuScenes dataset demonstrates that our integrated system\nreduces average collision rates by 31.82% compared to baseline methodologies,\nestablishing a new benchmark for VLM-augmented autonomous driving systems.", "AI": {"tldr": "VLAD是一个结合视觉语言模型（VLM）和端到端自动驾驶系统（VAD）的模型，通过定制问答数据集微调VLM，提升空间推理能力，生成导航指令并解释驾驶决策，显著降低碰撞率。", "motivation": "利用开源视觉语言模型（如LLaVA、Qwen-VL等）的通用知识，提升自动驾驶的感知、预测和规划能力。", "method": "提出VLAD模型，通过定制问答数据集微调VLM，增强其空间推理能力，生成导航指令并与VAD系统集成，同时提供自然语言解释。", "result": "在nuScenes数据集上测试，VLAD将平均碰撞率降低31.82%，优于基线方法。", "conclusion": "VLAD为视觉语言模型增强的自动驾驶系统设定了新基准，提高了透明度和可信度。"}}
{"id": "2507.01308", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01308", "abs": "https://arxiv.org/abs/2507.01308", "authors": ["Muhammad Atta ur Rahman", "Dooseop Choi", "KyoungWook Min"], "title": "LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction", "comment": "Accepted at the 17th IEEE International Conference on Advanced\n  Computational Intelligence (ICACI 2025)", "summary": "Accurate motion forecasting is critical for safe and efficient autonomous\ndriving, enabling vehicles to predict future trajectories and make informed\ndecisions in complex traffic scenarios. Most of the current designs of motion\nprediction models are based on the major representation of lane centerlines,\nwhich limits their capability to capture critical road environments and traffic\nrules and constraints. In this work, we propose an enhanced motion forecasting\nmodel informed by multiple vector map elements, including lane boundaries and\nroad edges, that facilitates a richer and more complete representation of\ndriving environments. An effective feature fusion strategy is developed to\nmerge information in different vector map components, where the model learns\nholistic information on road structures and their interactions with agents.\nSince encoding more information about the road environment increases memory\nusage and is computationally expensive, we developed an effective pruning\nmechanism that filters the most relevant map connections to the target agent,\nensuring computational efficiency while maintaining essential spatial and\nsemantic relationships for accurate trajectory prediction. Overcoming the\nlimitations of lane centerline-based models, our method provides a more\ninformative and efficient representation of the driving environment and\nadvances the state of the art for autonomous vehicle motion forecasting. We\nverify our approach with extensive experiments on the Argoverse 2 motion\nforecasting dataset, where our method maintains competitiveness on AV2 while\nachieving improved performance.\n  Index Terms-Autonomous driving, trajectory prediction, vector map elements,\nroad topology, connection pruning, Argoverse 2.", "AI": {"tldr": "论文提出了一种基于多向量地图元素的运动预测模型，通过融合车道边界和道路边缘等信息，提升自动驾驶中轨迹预测的准确性和效率。", "motivation": "当前基于车道中心线的运动预测模型无法充分捕捉道路环境和交通规则，限制了预测能力。", "method": "开发了一种特征融合策略和修剪机制，结合多向量地图元素，并过滤无关连接以提高计算效率。", "result": "在Argoverse 2数据集上验证了方法的竞争力，性能有所提升。", "conclusion": "该方法克服了车道中心线模型的局限性，提供了更高效和全面的驾驶环境表示，推动了自动驾驶运动预测的技术进步。"}}
{"id": "2507.01424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01424", "abs": "https://arxiv.org/abs/2507.01424", "authors": ["Zhenyang Liu", "Yongchong Gu", "Sixiao Zheng", "Xiangyang Xue", "Yanwei Fu"], "title": "TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control", "comment": null, "summary": "Recent advancements in vision-language models (VLMs) for common-sense\nreasoning have led to the development of vision-language-action (VLA) models,\nenabling robots to perform generalized manipulation. Although existing\nautoregressive VLA methods design a specific architecture like dual-system to\nleverage large-scale pretrained knowledge, they tend to capture static\ninformation, often neglecting the dynamic aspects vital for embodied tasks. To\nthis end, we propose TriVLA, a unified Vision-Language-Action model with a\ntriple-system architecture for general robot control. The vision-language\nmodule (System 2) interprets the environment through vision and language\ninstructions. The dynamics perception module (System 3) inherently produces\nvisual representations that encompass both current static information and\npredicted future dynamics, thereby providing valuable guidance for policy\nlearning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained\nvideo foundation model on robot datasets along with internet human manipulation\ndata. The subsequent policy learning module (System 1) generates fluid motor\nactions in real time. Experimental evaluation demonstrates that TriVLA operates\nat approximately 36 Hz and surpasses state-of-the-art imitation learning\nbaselines on standard simulation benchmarks as well as challenging real-world\nmanipulation tasks.", "AI": {"tldr": "TriVLA是一种统一的三系统架构视觉-语言-动作模型，用于通用机器人控制，通过动态感知模块提升性能。", "motivation": "现有自回归VLA方法常忽略动态信息，而动态信息对机器人任务至关重要。", "method": "TriVLA采用三系统架构：视觉-语言模块、动态感知模块和策略学习模块，结合预训练模型和机器人数据。", "result": "TriVLA在36Hz下运行，性能优于现有模仿学习方法。", "conclusion": "TriVLA通过动态感知模块显著提升了机器人控制的性能。"}}
{"id": "2507.01462", "categories": ["cs.RO", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.01462", "abs": "https://arxiv.org/abs/2507.01462", "authors": ["Eneko Osaba", "Estibaliz Garrote", "Pablo Miranda-Rodriguez", "Alessia Ciacco", "Itziar Cabanes", "Aitziber Mancisidor"], "title": "Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0", "comment": "2 pages, 1 figure, paper accepted for presentation at the IEEE\n  International Conference on Quantum Computing and Engineering (QCE)", "summary": "This work explores the application of hybrid quantum-classical algorithms to\noptimize robotic inspection trajectories derived from Computer-Aided Design\n(CAD) models in industrial settings. By modeling the task as a 3D variant of\nthe Traveling Salesman Problem, incorporating incomplete graphs and open-route\nconstraints, this study evaluates the performance of two D-Wave-based solvers\nagainst classical methods such as GUROBI and Google OR-Tools. Results across\nfive real-world cases demonstrate competitive solution quality with\nsignificantly reduced computation times, highlighting the potential of quantum\napproaches in automation under Industry 4.0.", "AI": {"tldr": "研究探讨混合量子-经典算法在工业环境中优化机器人检测轨迹的应用，与传统方法相比，量子方法在计算时间上显著减少。", "motivation": "探索量子计算在工业自动化中的潜力，特别是在优化复杂任务如机器人检测轨迹时。", "method": "将任务建模为3D旅行商问题，使用D-Wave量子求解器与传统方法（GUROBI、Google OR-Tools）对比。", "result": "在五个实际案例中，量子方法在解质量和计算时间上表现竞争力。", "conclusion": "量子方法在工业4.0自动化中具有潜力，尤其在需要快速优化的场景。"}}
{"id": "2507.01485", "categories": ["cs.RO", "cs.AI", "cs.MA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01485", "abs": "https://arxiv.org/abs/2507.01485", "authors": ["Yibo Qiu", "Zan Huang", "Zhiyu Wang", "Handi Liu", "Yiling Qiao", "Yifeng Hu", "Shu'ang Sun", "Hangke Peng", "Ronald X Xu", "Mingzhai Sun"], "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments", "comment": null, "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.", "AI": {"tldr": "BioMARS是一个结合大型语言模型（LLMs）、视觉语言模型（VLMs）和模块化机器人的智能平台，用于自主设计、规划和执行生物实验，性能优于人工操作。", "motivation": "当前LLMs和VLMs在生物研究中的应用受限于协议设计的刚性、动态实验室条件的适应性不足、错误处理能力有限以及操作复杂性高。", "method": "BioMARS采用分层架构：Biologist Agent通过检索增强生成合成协议；Technician Agent将其转化为可执行的机器人伪代码；Inspector Agent通过多模态感知和异常检测确保程序完整性。", "result": "系统在细胞传代和培养任务中表现优于或等同于人工操作，并在视网膜色素上皮细胞分化中优于传统策略。", "conclusion": "BioMARS展示了通用AI驱动的实验室自动化的可行性，以及语言推理在生物研究中的变革作用。"}}
{"id": "2507.01550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01550", "abs": "https://arxiv.org/abs/2507.01550", "authors": ["Johannes Kohl", "Georg Muck", "Georg Jäger", "Sebastian Zug"], "title": "Dynamic System Model Generation for Online Fault Detection and Diagnosis of Robotic Systems", "comment": "Accepted for publication in Ada User Journal", "summary": "With the rapid development of more complex robots, Fault Detection and\nDiagnosis (FDD) becomes increasingly harder. Especially the need for\npredetermined models and historic data is problematic because they do not\nencompass the dynamic and fast-changing nature of such systems. To this end, we\npropose a concept that actively generates a dynamic system model at runtime and\nutilizes it to locate root causes. The goal is to be applicable to all kinds of\nrobotic systems that share a similar software design. Additionally, it should\nexhibit minimal overhead and enhance independence from expert attention.", "AI": {"tldr": "提出了一种动态生成系统模型的方法，用于实时定位机器人系统中的故障根因，适用于多种机器人系统，且减少对专家干预的依赖。", "motivation": "随着机器人系统的复杂性增加，传统故障检测与诊断方法因依赖预定义模型和历史数据而难以应对动态变化。", "method": "动态生成系统模型并在运行时利用该模型定位故障根因。", "result": "方法适用于多种机器人系统，且具有低开销和减少专家依赖的特点。", "conclusion": "该方法为复杂机器人系统的故障诊断提供了一种灵活且高效的解决方案。"}}
{"id": "2507.01561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01561", "abs": "https://arxiv.org/abs/2507.01561", "authors": ["Huijiang Wang", "Holger Kunz", "Timon Adler", "Fumiya Iida"], "title": "Self-Closing Suction Grippers for Industrial Grasping via Form-Flexible Design", "comment": "This manuscript has been submitted for potential consideration at\n  IEEE publication venues", "summary": "Shape-morphing robots have shown benefits in industrial grasping. We propose\nform-flexible grippers for adaptive grasping. The design is based on the hybrid\njamming and suction mechanism, which deforms to handle objects that vary\nsignificantly in size from the aperture, including both larger and smaller\nparts. Compared with traditional grippers, the gripper achieves self-closing to\nform an airtight seal. Under a vacuum, a wide range of grasping is realized\nthrough the passive morphing mechanism at the interface that harmonizes\npressure and flow rate. This hybrid gripper showcases the capability to\nsecurely grasp an egg, as small as 54.5% of its aperture, while achieving a\nmaximum load-to-mass ratio of 94.3.", "AI": {"tldr": "提出了一种基于混合堵塞和吸力机制的形变柔性夹爪，用于自适应抓取，能够处理尺寸与夹爪开口差异较大的物体。", "motivation": "传统夹爪在抓取尺寸差异大的物体时表现不佳，需要一种能自适应形变的解决方案。", "method": "采用混合堵塞和吸力机制，通过被动形变界面协调压力和流速，实现自适应抓取。", "result": "夹爪能够安全抓取尺寸仅为开口54.5%的鸡蛋，最大负载质量比达到94.3。", "conclusion": "混合形变夹爪在自适应抓取方面表现出色，适用于工业场景。"}}
{"id": "2507.01697", "categories": ["cs.RO", "math.OC", "00A69, 93C85, 14H55", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.01697", "abs": "https://arxiv.org/abs/2507.01697", "authors": ["Yu Zhang", "Qi Zhou", "Xiao-Song Yang"], "title": "An RRT* algorithm based on Riemannian metric model for optimal path planning", "comment": "27 pages", "summary": "This paper presents a Riemannian metric-based model to solve the optimal path\nplanning problem on two-dimensional smooth submanifolds in high-dimensional\nspace. Our model is based on constructing a new Riemannian metric on a\ntwo-dimensional projection plane, which is induced by the high-dimensional\nEuclidean metric on two-dimensional smooth submanifold and reflects the\nenvironmental information of the robot. The optimal path planning problem in\nhigh-dimensional space is therefore transformed into a geometric problem on the\ntwo-dimensional plane with new Riemannian metric. Based on the new Riemannian\nmetric, we proposed an incremental algorithm RRT*-R on the projection plane.\nThe experimental results show that the proposed algorithm is suitable for\nscenarios with uneven fields in multiple dimensions. The proposed algorithm can\nhelp the robot to effectively avoid areas with drastic changes in height,\nground resistance and other environmental factors. More importantly, the RRT*-R\nalgorithm shows better smoothness and optimization properties compared with the\noriginal RRT* algorithm using Euclidean distance in high-dimensional workspace.\nThe length of the entire path by RRT*-R is a good approximation of the\ntheoretical minimum geodesic distance on projection plane.", "AI": {"tldr": "提出了一种基于黎曼度量的模型，用于解决高维空间中二维光滑子流形上的最优路径规划问题。通过构造新的黎曼度量，将高维问题转化为二维平面上的几何问题，并提出了增量算法RRT*-R。实验表明该算法在多维不均匀场景中表现优异。", "motivation": "解决高维空间中二维光滑子流形上的最优路径规划问题，同时考虑环境因素（如高度变化、地面阻力等）对机器人路径的影响。", "method": "构造新的黎曼度量，将高维问题转化为二维平面上的几何问题，并提出增量算法RRT*-R。", "result": "RRT*-R算法在多维不均匀场景中表现优异，路径平滑且优化性能优于原始RRT*算法，路径长度接近理论最小测地距离。", "conclusion": "RRT*-R算法在高维路径规划中具有优越性，能有效应对复杂环境因素，路径更平滑且接近最优。"}}
{"id": "2507.01705", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01705", "abs": "https://arxiv.org/abs/2507.01705", "authors": ["Marc-Philip Ecker", "Bernhard Bischof", "Minh Nhat Vu", "Christoph Fröhlich", "Tobias Glück", "Wolfgang Kemmetmüller"], "title": "Efficient Collision Detection for Long and Slender Robotic Links in Euclidean Distance Fields: Application to a Forestry Crane", "comment": "Accepted at IROS 2025", "summary": "Collision-free motion planning in complex outdoor environments relies heavily\non perceiving the surroundings through exteroceptive sensors. A widely used\napproach represents the environment as a voxelized Euclidean distance field,\nwhere robots are typically approximated by spheres. However, for large-scale\nmanipulators such as forestry cranes, which feature long and slender links,\nthis conventional spherical approximation becomes inefficient and inaccurate.\nThis work presents a novel collision detection algorithm specifically designed\nto exploit the elongated structure of such manipulators, significantly\nenhancing the computational efficiency of motion planning algorithms. Unlike\ntraditional sphere decomposition methods, our approach not only improves\ncomputational efficiency but also naturally eliminates the need to fine-tune\nthe approximation accuracy as an additional parameter. We validate the\nalgorithm's effectiveness using real-world LiDAR data from a forestry crane\napplication, as well as simulated environment data.", "AI": {"tldr": "提出了一种针对细长机械臂的新型碰撞检测算法，显著提高了运动规划的计算效率。", "motivation": "传统方法将机器人近似为球体，但对于细长机械臂（如林业起重机）效率低且不准确。", "method": "开发了一种专门利用机械臂细长结构的碰撞检测算法，无需调整近似精度参数。", "result": "在真实LiDAR数据和模拟环境中验证了算法的有效性。", "conclusion": "新算法显著提升了计算效率，适用于细长机械臂的运动规划。"}}
{"id": "2507.01723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01723", "abs": "https://arxiv.org/abs/2507.01723", "authors": ["Xupeng Zhu", "Fan Wang", "Robin Walters", "Jane Shi"], "title": "SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space", "comment": "Accepted at ICML 2025", "summary": "Diffusion Policies are effective at learning closed-loop manipulation\npolicies from human demonstrations but generalize poorly to novel arrangements\nof objects in 3D space, hurting real-world performance. To address this issue,\nwe propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion\npolicy that adapts trajectories according to 3D transformations of the scene.\nSuch equivariance is achieved by embedding the states, actions, and the\ndenoising process in spherical Fourier space. Additionally, we employ novel\nspherical FiLM layers to condition the action denoising process equivariantly\non the scene embeddings. Lastly, we propose a spherical denoising temporal\nU-net that achieves spatiotemporal equivariance with computational efficiency.\nIn the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization\nacross transformed 3D scenes. SDP demonstrates a large performance improvement\nover strong baselines in 20 simulation tasks and 5 physical robot tasks\nincluding single-arm and bi-manual embodiments. Code is available at\nhttps://github.com/amazon-science/Spherical_Diffusion_Policy.", "AI": {"tldr": "提出了一种SE(3)等变扩散策略（SDP），通过在球面傅里叶空间中嵌入状态、动作和去噪过程，实现对3D场景变换的鲁棒泛化。", "motivation": "扩散策略在3D空间中物体新排列时泛化能力差，影响实际性能。", "method": "采用球面傅里叶空间嵌入和球面FiLM层，提出球面去噪时间U-net，实现时空等变性和计算效率。", "result": "在20个仿真任务和5个物理机器人任务中表现优于基线。", "conclusion": "SDP通过SE(3)等变性实现了对3D场景变换的鲁棒泛化。"}}
{"id": "2507.01753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01753", "abs": "https://arxiv.org/abs/2507.01753", "authors": ["Yash Kulkarni", "Susheela Sharma", "Omid Rezayof", "Siddhartha Kapuria", "Jordan P. Amadio", "Mohsen Khadem", "Maryam Tilton", "Farshid Alambeigi"], "title": "Augmented Bridge Spinal Fixation: A New Concept for Addressing Pedicle Screw Pullout via a Steerable Drilling Robot and Flexible Pedicle Screws", "comment": null, "summary": "To address the screw loosening and pullout limitations of rigid pedicle\nscrews in spinal fixation procedures, and to leverage our recently developed\nConcentric Tube Steerable Drilling Robot (CT-SDR) and Flexible Pedicle Screw\n(FPS), in this paper, we introduce the concept of Augmented Bridge Spinal\nFixation (AB-SF). In this concept, two connecting J-shape tunnels are first\ndrilled through pedicles of vertebra using the CT-SDR. Next, two FPSs are\npassed through this tunnel and bone cement is then injected through the\ncannulated region of the FPS to form an augmented bridge between two pedicles\nand reinforce strength of the fixated spine. To experimentally analyze and\nstudy the feasibility of AB-SF technique, we first used our robotic system\n(i.e., a CT-SDR integrated with a robotic arm) to create two different fixation\nscenarios in which two J-shape tunnels, forming a bridge, were drilled at\ndifferent depth of a vertebral phantom. Next, we implanted two FPSs within the\ndrilled tunnels and then successfully simulated the bone cement augmentation\nprocess.", "AI": {"tldr": "提出了一种名为AB-SF的新型脊柱固定技术，通过柔性螺钉和机器人辅助钻孔解决传统刚性螺钉的松动和拔出问题。", "motivation": "传统刚性椎弓根螺钉在脊柱固定中存在松动和拔出的局限性，需要更可靠的解决方案。", "method": "使用CT-SDR机器人钻孔J形隧道，植入柔性螺钉并注入骨水泥形成增强桥接结构。", "result": "实验成功模拟了骨水泥增强过程，验证了AB-SF技术的可行性。", "conclusion": "AB-SF技术有望提升脊柱固定的强度和可靠性。"}}
{"id": "2507.01779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01779", "abs": "https://arxiv.org/abs/2507.01779", "authors": ["Daniyal Maroufi", "Xinyuan Huang", "Yash Kulkarni", "Omid Rezayof", "Susheela Sharma", "Vaibhav Goggela", "Jordan P. Amadio", "Mohsen Khadem", "Farshid Alambeigi"], "title": "S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures", "comment": null, "summary": "In this paper, we introduce S3D: A Spatial Steerable Surgical Drilling\nFramework for Robotic Spinal Fixation Procedures. S3D is designed to enable\nrealistic steerable drilling while accounting for the anatomical constraints\nassociated with vertebral access in spinal fixation (SF) procedures. To achieve\nthis, we first enhanced our previously designed concentric tube Steerable\nDrilling Robot (CT-SDR) to facilitate steerable drilling across all vertebral\nlevels of the spinal column. Additionally, we propose a four-Phase calibration,\nregistration, and navigation procedure to perform realistic SF procedures on a\nspine holder phantom by integrating the CT-SDR with a seven-degree-of-freedom\nrobotic manipulator. The functionality of this framework is validated through\nplanar and out-of-plane steerable drilling experiments in vertebral phantoms.", "AI": {"tldr": "本文介绍了S3D框架，一种用于机器人脊柱固定手术的空间可操控钻孔系统，旨在解决脊柱固定手术中的解剖限制问题。", "motivation": "脊柱固定手术中，钻孔需要适应复杂的解剖结构，现有技术难以满足需求。S3D旨在提供一种可操控的钻孔解决方案。", "method": "通过改进同心管可操控钻孔机器人（CT-SDR），并结合四阶段校准、配准和导航流程，实现脊柱各节段的钻孔操作。", "result": "在脊柱模型上进行了平面和非平面钻孔实验，验证了框架的功能性。", "conclusion": "S3D框架能够有效支持脊柱固定手术中的可操控钻孔需求。"}}
{"id": "2507.01811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01811", "abs": "https://arxiv.org/abs/2507.01811", "authors": ["Yash Kulkarni", "Susheela Sharma", "Sarah Go", "Jordan P. Amadio", "Mohsen Khadem", "Farshid Alambeigi"], "title": "Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures", "comment": null, "summary": "Current pelvic fixation techniques rely on rigid drilling tools, which\ninherently constrain the placement of rigid medical screws in the complex\nanatomy of pelvis. These constraints prevent medical screws from following\nanatomically optimal pathways and force clinicians to fixate screws in linear\ntrajectories. This suboptimal approach, combined with the unnatural placement\nof the excessively long screws, lead to complications such as screw\nmisplacement, extended surgery times, and increased radiation exposure due to\nrepeated X-ray images taken ensure to safety of procedure. To address these\nchallenges, in this paper, we present the design and development of a unique 4\ndegree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic\nCT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling\ntrajectories that follow the natural curvatures of the pelvic anatomy. The\nperformance of the pelvic CT-SDR was thoroughly evaluated through several\nS-shape drilling experiments in simulated bone phantoms.", "AI": {"tldr": "提出了一种新型4自由度骨盆同心管可操纵钻孔机器人（pelvic CT-SDR），用于解决传统刚性钻孔工具在骨盆复杂解剖结构中的局限性。", "motivation": "传统刚性钻孔工具限制了螺钉的放置路径，导致螺钉错位、手术时间延长和辐射暴露增加等问题。", "method": "设计并开发了一种4自由度的骨盆同心管可操纵钻孔机器人，能够实现S形钻孔轨迹。", "result": "在模拟骨模型上进行了多次S形钻孔实验，验证了其性能。", "conclusion": "该机器人能够适应骨盆的自然曲率，有望改善手术效果并减少并发症。"}}
{"id": "2507.01843", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01843", "abs": "https://arxiv.org/abs/2507.01843", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics", "comment": "Preprint of a manuscript submitted for peer review", "summary": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems.", "AI": {"tldr": "MoIRA是一个模块化的MoE框架，通过外部文本路由器协调专家模型，提供零-shot路由选项，并在机器人任务中表现优异。", "motivation": "解决传统MoE架构中路由机制无法定制和需要额外训练的问题，提出更灵活、低开销的模块化方案。", "method": "MoIRA采用嵌入相似性和提示驱动的语言模型推理作为零-shot路由选项，结合低秩适配器进行低开销推理。", "result": "在GR1 Humanoid和LIBERO基准测试中，MoIRA优于通用模型，并与其他MoE方案竞争。", "conclusion": "MoIRA展示了模块化部署的可行性，为未来多专家机器人系统提供了可扩展的基础。"}}
{"id": "2507.01857", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01857", "abs": "https://arxiv.org/abs/2507.01857", "authors": ["Yuhao Lin", "Yi-Lin Wei", "Haoran Liao", "Mu Lin", "Chengyi Xing", "Hao Li", "Dandan Zhang", "Mark Cutkosky", "Wei-Shi Zheng"], "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types", "comment": "Project Page: https://isee-laboratory.github.io/TypeTele", "summary": "Dexterous teleoperation plays a crucial role in robotic manipulation for\nreal-world data collection and remote robot control. Previous dexterous\nteleoperation mostly relies on hand retargeting to closely mimic human hand\npostures. However, these approaches may fail to fully leverage the inherent\ndexterity of dexterous hands, which can execute unique actions through their\nstructural advantages compared to human hands. To address this limitation, we\npropose TypeTele, a type-guided dexterous teleoperation system, which enables\ndexterous hands to perform actions that are not constrained by human motion\npatterns. This is achieved by introducing dexterous manipulation types into the\nteleoperation system, allowing operators to employ appropriate types to\ncomplete specific tasks. To support this system, we build an extensible\ndexterous manipulation type library to cover comprehensive dexterous postures\nused in manipulation tasks. During teleoperation, we employ a MLLM\n(Multi-modality Large Language Model)-assisted type retrieval module to\nidentify the most suitable manipulation type based on the specific task and\noperator commands. Extensive experiments of real-world teleoperation and\nimitation learning demonstrate that the incorporation of manipulation types\nsignificantly takes full advantage of the dexterous robot's ability to perform\ndiverse and complex tasks with higher success rates.", "AI": {"tldr": "TypeTele是一种类型引导的灵巧遥操作系统，通过引入灵巧操作类型库和MLLM辅助检索模块，使灵巧手能够执行不受人类动作模式限制的任务。", "motivation": "传统灵巧遥操作依赖手部重定向模仿人类手部姿态，未能充分利用灵巧手的结构优势。", "method": "提出TypeTele系统，构建灵巧操作类型库，并利用MLLM辅助检索模块选择适合的操作类型。", "result": "实验表明，TypeTele显著提升了灵巧机器人执行复杂任务的多样性和成功率。", "conclusion": "TypeTele通过类型引导的遥操作，充分发挥了灵巧手的潜力，适用于多样化任务。"}}
{"id": "2507.01925", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01925", "abs": "https://arxiv.org/abs/2507.01925", "authors": ["Yifan Zhong", "Fengshuo Bai", "Shaofei Cai", "Xuchuan Huang", "Zhang Chen", "Xiaowei Zhang", "Yuanfei Wang", "Shaoyang Guo", "Tianrui Guan", "Ka Nam Lui", "Zhiquan Qi", "Yitao Liang", "Yuanpei Chen", "Yaodong Yang"], "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective", "comment": "70 pages, 5 figures", "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof \\textit{action tokens} that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.", "AI": {"tldr": "该论文提出了一种统一框架，将当前视觉-语言-动作（VLA）模型归类为基于动作令牌的生成过程，并分析了不同动作令牌类型的优缺点，为未来研究提供指导。", "motivation": "当前VLA模型缺乏对动作令牌的全面理解，阻碍了有效开发和未来方向探索。", "method": "通过系统综述和分析，将VLA模型归类为基于动作令牌的框架，并比较不同令牌类型的优劣。", "result": "总结了动作令牌的八种类型，并指出了每种类型的局限性和改进空间。", "conclusion": "论文为VLA模型的未来发展提供了系统化的展望，并提出了有潜力的研究方向。"}}
{"id": "2507.01930", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01930", "abs": "https://arxiv.org/abs/2507.01930", "authors": ["Wenhao Wang", "Yanyan Li", "Long Jiao", "Jiawei Yuan"], "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations", "comment": "10 pages", "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.", "AI": {"tldr": "提出了一种基于LLM的闭环控制框架，通过两个LLM模块（代码生成器和评估器）实现可靠的无人机操作，并通过自然语言轨迹描述和模拟优化提升性能。", "motivation": "LLM在无人机操作中存在逻辑推理和复杂决策的挑战，影响可靠性，因此需要一种更可靠的LLM驱动框架。", "method": "采用闭环控制框架，包含代码生成器和评估器，将数值状态转换为自然语言描述，并通过模拟优化避免物理风险。", "result": "实验表明，该框架在任务复杂度增加时，成功率和完整性显著优于基线方法。", "conclusion": "该框架通过反馈和优化，显著提升了LLM驱动的无人机操作的可靠性。"}}
{"id": "2507.01961", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01961", "abs": "https://arxiv.org/abs/2507.01961", "authors": ["Sixiang Chen", "Jiaming Liu", "Siyuan Qian", "Han Jiang", "Lily Li", "Renrui Zhang", "Zhuoyang Liu", "Chenyang Gu", "Chengkai Hou", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation", "comment": null, "summary": "Recently, mobile manipulation has attracted increasing attention for enabling\nlanguage-conditioned robotic control in household tasks. However, existing\nmethods still face challenges in coordinating mobile base and manipulator,\nprimarily due to two limitations. On the one hand, they fail to explicitly\nmodel the influence of the mobile base on manipulator control, which easily\nleads to error accumulation under high degrees of freedom. On the other hand,\nthey treat the entire mobile manipulation process with the same visual\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\nmultimodal perception requirements at different stages during mobile\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\nconditioning mechanism that guides the model to first extract base motion\nrepresentations, which are then used as context prior for predicting whole-body\nactions. This enables whole-body control that accounts for the potential impact\nof the mobile base's motion. Second, to meet the perception requirements at\ndifferent stages of mobile manipulation, we design a perception-aware\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\nbetween various 2D visual images and 3D point clouds, yielding visual features\ntailored to the current perceptual needs. This allows the model to, for\nexample, adaptively rely more on 2D inputs when semantic information is crucial\nfor action prediction, while placing greater emphasis on 3D geometric\ninformation when precise spatial understanding is required. We validate AC-DiT\nthrough extensive experiments on both simulated and real-world mobile\nmanipulation tasks.", "AI": {"tldr": "提出了一种自适应协调扩散变换器（AC-DiT），通过显式建模移动底座对机械臂的影响和动态调整多模态感知权重，提升移动操作任务中的协调能力。", "motivation": "现有方法在移动底座与机械臂协调方面存在不足，未能显式建模移动底座对机械臂的影响，且忽视了不同阶段的多模态感知需求。", "method": "引入移动底座到机械臂的条件机制和感知感知的多模态条件策略，动态调整2D和3D视觉输入的融合权重。", "result": "在仿真和真实世界的移动操作任务中验证了AC-DiT的有效性。", "conclusion": "AC-DiT通过显式协调和动态感知策略，显著提升了移动底座与机械臂的协调能力。"}}
