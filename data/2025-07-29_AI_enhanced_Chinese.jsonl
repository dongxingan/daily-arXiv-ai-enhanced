{"id": "2507.19642", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19642", "abs": "https://arxiv.org/abs/2507.19642", "authors": ["Ahmad Suleman", "Misha Urooj Khan", "Zeeshan Kaleem", "Ali H. Alenezi", "Iqra Shabbir Sinem Coleri", "Chau Yuen"], "title": "Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods", "comment": null, "summary": "Autonomous parking (AP) represents a critical yet complex subset of\nintelligent vehicle automation, characterized by tight spatial constraints,\nfrequent close-range obstacle interactions, and stringent safety margins.\nHowever, conventional rule-based and model-predictive methods often lack the\nadaptability and generalization needed to handle the nonlinear and\nenvironment-dependent complexities of AP. To address these limitations, we\npropose a reward-augmented learning framework for AP (RARLAP), that mitigates\nthe inherent complexities of continuous-domain control by leveraging structured\nreward design to induce smooth and adaptable policy behavior, trained entirely\nwithin a high-fidelity Unity-based custom 3D simulation environment. We\nsystematically design and assess three structured reward strategies: goal-only\nreward (GOR), dense proximity reward (DPR), and milestone-augmented reward\n(MAR), each integrated with both on-policy and off-policy optimization\nparadigms. Empirical evaluations demonstrate that the on-policy MAR achieves a\n91\\% success rate, yielding smoother trajectories and more robust behavior,\nwhile GOR and DPR fail to guide effective learning. Convergence and trajectory\nanalyses demonstrate that the proposed framework enhances policy adaptability,\naccelerates training, and improves safety in continuous control. Overall,\nRARLAP establishes that reward augmentation effectively addresses complex\nautonomous parking challenges, enabling scalable and efficient policy\noptimization with both on- and off-policy methods. To support reproducibility,\nthe code accompanying this paper is publicly available.", "AI": {"tldr": "论文提出了一种奖励增强学习框架（RARLAP），用于解决自主停车（AP）中的复杂控制问题，通过结构化奖励设计提升策略的适应性和安全性。", "motivation": "传统基于规则和模型预测的方法在自主停车中缺乏适应性和泛化能力，无法处理非线性及环境依赖的复杂性。", "method": "提出RARLAP框架，利用三种结构化奖励策略（GOR、DPR、MAR）结合策略优化方法，在高保真Unity仿真环境中训练。", "result": "实验表明，MAR策略在91%的成功率下表现最佳，轨迹更平滑且行为更鲁棒，而GOR和DPR效果不佳。", "conclusion": "RARLAP通过奖励增强有效解决了自主停车的复杂控制问题，支持可扩展且高效的策略优化。"}}
{"id": "2507.19652", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19652", "abs": "https://arxiv.org/abs/2507.19652", "authors": ["Mattia Risiglione", "Abdelrahman Abdalla", "Victor Barasuol", "Kim Tien Ly", "Ioannis Havoutis", "Claudio Semini"], "title": "RAKOMO: Reachability-Aware K-Order Markov Path Optimization for Quadrupedal Loco-Manipulation", "comment": null, "summary": "Legged manipulators, such as quadrupeds equipped with robotic arms, require\nmotion planning techniques that account for their complex kinematic constraints\nin order to perform manipulation tasks both safely and effectively. However,\ntrajectory optimization methods often face challenges due to the hybrid\ndynamics introduced by contact discontinuities, and tend to neglect leg\nlimitations during planning for computational reasons. In this work, we propose\nRAKOMO, a path optimization technique that integrates the strengths of K-Order\nMarkov Optimization (KOMO) with a kinematically-aware criterion based on the\nreachable region defined as reachability margin. We leverage a neural-network\nto predict the margin and optimize it by incorporating it in the standard KOMO\nformulation. This approach enables rapid convergence of gradient-based motion\nplanning -- commonly tailored for continuous systems -- while adapting it\neffectively to legged manipulators, successfully executing loco-manipulation\ntasks. We benchmark RAKOMO against a baseline KOMO approach through a set of\nsimulations for pick-and-place tasks with the HyQReal quadruped robot equipped\nwith a Kinova Gen3 robotic arm.", "AI": {"tldr": "RAKOMO是一种结合K-Order Markov Optimization（KOMO）和基于可达性边界的运动规划技术，用于解决腿式机械臂的复杂运动规划问题。", "motivation": "腿式机械臂（如带机械臂的四足机器人）的运动规划需考虑复杂的运动学约束和接触不连续性，现有方法常因计算原因忽略腿部限制。", "method": "RAKOMO将KOMO与基于神经网络预测的可达性边界结合，优化运动规划。", "result": "在仿真实验中，RAKOMO比基线KOMO方法更有效地完成拾取任务。", "conclusion": "RAKOMO成功适应腿式机械臂的运动规划，实现了高效的任务执行。"}}
{"id": "2507.19742", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19742", "abs": "https://arxiv.org/abs/2507.19742", "authors": ["Yanbin Li", "Canran Xiao", "Hongyang He", "Shenghai Yuan", "Zong Ke", "Jiajie Yu", "Zixiong Qin", "Zhiguo Zhang", "Wenzheng Chi", "Wei Zhang"], "title": "DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning", "comment": "10 pages,9 figures", "summary": "Particle filter-based 2D-SLAM is widely used in indoor localization tasks due\nto its efficiency. However, indoor environments such as long straight corridors\ncan cause severe degeneracy problems in SLAM. In this paper, we use Proximal\nPolicy Optimization (PPO) to train an adaptive degeneracy optimization agent\n(DOA) to address degeneracy problem. We propose a systematic methodology to\naddress three critical challenges in traditional supervised learning\nframeworks: (1) data acquisition bottlenecks in degenerate dataset, (2)\ninherent quality deterioration of training samples, and (3) ambiguity in\nannotation protocol design. We design a specialized reward function to guide\nthe agent in developing perception capabilities for degenerate environments.\nUsing the output degeneracy factor as a reference weight, the agent can\ndynamically adjust the contribution of different sensors to pose optimization.\nSpecifically, the observation distribution is shifted towards the motion model\ndistribution, with the step size determined by a linear interpolation formula\nrelated to the degeneracy factor. In addition, we employ a transfer learning\nmodule to endow the agent with generalization capabilities across different\nenvironments and address the inefficiency of training in degenerate\nenvironments. Finally, we conduct ablation studies to demonstrate the\nrationality of our model design and the role of transfer learning. We also\ncompare the proposed DOA with SOTA methods to prove its superior degeneracy\ndetection and optimization capabilities across various environments.", "AI": {"tldr": "提出了一种基于PPO的自适应退化优化代理（DOA），用于解决2D-SLAM在长直走廊等退化环境中的问题，通过奖励函数和迁移学习提升性能。", "motivation": "室内环境（如长直走廊）会导致SLAM严重退化，传统监督学习方法面临数据获取瓶颈、样本质量下降和标注协议模糊等挑战。", "method": "使用PPO训练DOA，设计奖励函数引导代理感知退化环境，动态调整传感器贡献，并通过迁移学习提升泛化能力。", "result": "DOA在退化检测和优化能力上优于SOTA方法，并通过消融实验验证了模型设计的合理性。", "conclusion": "提出的DOA能有效解决SLAM退化问题，具有跨环境的泛化能力，为类似问题提供了系统解决方案。"}}
{"id": "2507.19831", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19831", "abs": "https://arxiv.org/abs/2507.19831", "authors": ["Zaar Khizar", "Johann Laconte", "Roland Lenain", "Romuald Aufrere"], "title": "Feeling the Force: A Nuanced Physics-based Traversability Sensor for Navigation in Unstructured Vegetation", "comment": null, "summary": "In many applications, robots are increasingly deployed in unstructured and\nnatural environments where they encounter various types of vegetation.\nVegetation presents unique challenges as a traversable obstacle, where the\nmechanical properties of the plants can influence whether a robot can safely\ncollide with and overcome the obstacle. A more nuanced approach is required to\nassess the safety and traversability of these obstacles, as collisions can\nsometimes be safe and necessary for navigating through dense or unavoidable\nvegetation. This paper introduces a novel sensor designed to directly measure\nthe applied forces exerted by vegetation on a robot: by directly capturing the\npush-back forces, our sensor provides a detailed understanding of the\ninteractions between the robot and its surroundings. We demonstrate the\nsensor's effectiveness through experimental validations, showcasing its ability\nto measure subtle force variations. This force-based approach provides a\nquantifiable metric that can inform navigation decisions and serve as a\nfoundation for developing future learning algorithms.", "AI": {"tldr": "论文提出了一种新型传感器，用于直接测量植被对机器人施加的力，以评估其安全性和可穿越性。", "motivation": "机器人在自然环境中常遇到植被障碍，传统方法难以量化其安全性，需更精确的力测量方法。", "method": "设计了一种传感器，直接捕捉植被对机器人的反作用力，并通过实验验证其有效性。", "result": "传感器能精确测量细微的力变化，为导航决策提供量化依据。", "conclusion": "该传感器为机器人导航和未来学习算法的发展提供了重要基础。"}}
{"id": "2507.19555", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19555", "abs": "https://arxiv.org/abs/2507.19555", "authors": ["Rajat Khanda", "Mohammad Baqar", "Sambuddha Chakrabarti", "Satyasaran Changdar"], "title": "Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning", "comment": "13 pages, 2 figures", "summary": "Group Relative Policy Optimization (GRPO) has shown promise in discrete\naction spaces by eliminating value function dependencies through group-based\nadvantage estimation. However, its application to continuous control remains\nunexplored, limiting its utility in robotics where continuous actions are\nessential. This paper presents a theoretical framework extending GRPO to\ncontinuous control environments, addressing challenges in high-dimensional\naction spaces, sparse rewards, and temporal dynamics. Our approach introduces\ntrajectory-based policy clustering, state-aware advantage estimation, and\nregularized policy updates designed for robotic applications. We provide\ntheoretical analysis of convergence properties and computational complexity,\nestablishing a foundation for future empirical validation in robotic systems\nincluding locomotion and manipulation tasks.", "AI": {"tldr": "GRPO扩展到连续控制环境，提出轨迹聚类、状态感知优势估计和正则化策略更新，适用于机器人任务。", "motivation": "GRPO在离散动作空间表现良好，但连续控制应用尚未探索，限制了其在机器人领域的实用性。", "method": "引入轨迹聚类、状态感知优势估计和正则化策略更新，解决高维动作空间、稀疏奖励和时序动态问题。", "result": "提供理论分析，证明收敛性和计算复杂度，为未来机器人任务验证奠定基础。", "conclusion": "GRPO扩展至连续控制的理论框架为机器人应用提供了新方向。"}}
{"id": "2507.19860", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19860", "abs": "https://arxiv.org/abs/2507.19860", "authors": ["Haoze Dong", "Meng Guo", "Chengyi He", "Zhongkui Li"], "title": "Homotopy-aware Multi-agent Navigation via Distributed Model Predictive Control", "comment": null, "summary": "Multi-agent trajectory planning requires ensuring both safety and efficiency,\nyet deadlocks remain a significant challenge, especially in obstacle-dense\nenvironments. Such deadlocks frequently occur when multiple agents attempt to\ntraverse the same long and narrow corridor simultaneously. To address this, we\npropose a novel distributed trajectory planning framework that bridges the gap\nbetween global path and local trajectory cooperation. At the global level, a\nhomotopy-aware optimal path planning algorithm is proposed, which fully\nleverages the topological structure of the environment. A reference path is\nchosen from distinct homotopy classes by considering both its spatial and\ntemporal properties, leading to improved coordination among agents globally. At\nthe local level, a model predictive control-based trajectory optimization\nmethod is used to generate dynamically feasible and collision-free\ntrajectories. Additionally, an online replanning strategy ensures its\nadaptability to dynamic environments. Simulations and experiments validate the\neffectiveness of our approach in mitigating deadlocks. Ablation studies\ndemonstrate that by incorporating time-aware homotopic properties into the\nunderlying global paths, our method can significantly reduce deadlocks and\nimprove the average success rate from 4%-13% to over 90% in randomly generated\ndense scenarios.", "AI": {"tldr": "提出了一种分布式轨迹规划框架，通过全局路径和局部轨迹协作解决多智能体在密集环境中的死锁问题。", "motivation": "多智能体轨迹规划在密集环境中常出现死锁问题，尤其是在狭窄走廊中。", "method": "全局层面使用同伦感知的最优路径规划算法，局部层面采用模型预测控制优化轨迹，并结合在线重规划策略。", "result": "实验表明，该方法显著减少死锁，成功率从4%-13%提升至90%以上。", "conclusion": "通过全局路径的时间感知同伦属性和局部优化，有效解决了多智能体轨迹规划中的死锁问题。"}}
{"id": "2507.19947", "categories": ["cs.RO", "cs.CL", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.19947", "abs": "https://arxiv.org/abs/2507.19947", "authors": ["Supawich Sitdhipol", "Waritwong Sukprasongdee", "Ekapol Chuangsuwanich", "Rina Tse"], "title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations", "comment": "Accepted to the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC)", "summary": "Fusing information from human observations can help robots overcome sensing\nlimitations in collaborative tasks. However, an uncertainty-aware fusion\nframework requires a grounded likelihood representing the uncertainty of human\ninputs. This paper presents a Feature Pyramid Likelihood Grounding Network\n(FP-LGN) that grounds spatial language by learning relevant map image features\nand their relationships with spatial relation semantics. The model is trained\nas a probability estimator to capture aleatoric uncertainty in human language\nusing three-stage curriculum learning. Results showed that FP-LGN matched\nexpert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated\ngreater robustness with lower standard deviation. Collaborative sensing results\ndemonstrated that the grounded likelihood successfully enabled\nuncertainty-aware fusion of heterogeneous human language observations and robot\nsensor measurements, achieving significant improvements in human-robot\ncollaborative task performance.", "AI": {"tldr": "FP-LGN模型通过三阶段课程学习，将人类空间语言与地图特征关联，实现不确定性感知的信息融合，提升人机协作任务性能。", "motivation": "解决机器人感知局限，通过融合人类观察信息，实现不确定性感知的协作任务。", "method": "提出FP-LGN模型，学习地图特征与空间语言关系，通过三阶段课程学习估计概率。", "result": "FP-LGN在NLL上匹配专家规则，鲁棒性更强，显著提升人机协作任务性能。", "conclusion": "FP-LGN成功实现不确定性感知的信息融合，为人机协作任务提供有效解决方案。"}}
{"id": "2507.19647", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19647", "abs": "https://arxiv.org/abs/2507.19647", "authors": ["Amin Banayeeanzade", "Fatemeh Bahrani", "Yutai Zhou", "Erdem Bıyık"], "title": "GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning", "comment": "IROS 2025 camera-ready version. First two authors contributed equally", "summary": "Imitation Learning (IL) is a widely adopted approach which enables agents to\nlearn from human expert demonstrations by framing the task as a supervised\nlearning problem. However, IL often suffers from causal confusion, where agents\nmisinterpret spurious correlations as causal relationships, leading to poor\nperformance in testing environments with distribution shift. To address this\nissue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a\nnovel method that leverages the human gaze data gathered during the data\ncollection phase to guide the representation learning in IL. GABRIL utilizes a\nregularization loss which encourages the model to focus on causally relevant\nfeatures identified through expert gaze and consequently mitigates the effects\nof confounding variables. We validate our approach in Atari environments and\nthe Bench2Drive benchmark in CARLA by collecting human gaze datasets and\napplying our method in both domains. Experimental results show that the\nimprovement of GABRIL over behavior cloning is around 179% more than the same\nnumber for other baselines in the Atari and 76% in the CARLA setup. Finally, we\nshow that our method provides extra explainability when compared to regular IL\nagents.", "AI": {"tldr": "GABRIL利用人类注视数据改进模仿学习，通过正则化损失减少因果混淆，提升性能。", "motivation": "模仿学习常因因果混淆导致性能下降，需解决这一问题。", "method": "引入GABRIL，利用人类注视数据指导表示学习，通过正则化损失聚焦因果相关特征。", "result": "在Atari和CARLA中，GABRIL性能分别提升179%和76%。", "conclusion": "GABRIL有效减少因果混淆，并提供更强的可解释性。"}}
{"id": "2507.20509", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.20509", "abs": "https://arxiv.org/abs/2507.20509", "authors": ["Zhongchao Zhou", "Yuxi Lu", "Yaonan Zhu", "Yifan Zhao", "Bin He", "Liang He", "Wenwen Yu", "Yusuke Iwasawa"], "title": "LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models", "comment": null, "summary": "With rapid advances in code generation, reasoning, and problem-solving, Large\nLanguage Models (LLMs) are increasingly applied in robotics. Most existing work\nfocuses on high-level tasks such as task decomposition. A few studies have\nexplored the use of LLMs in feedback controller design; however, these efforts\nare restricted to overly simplified systems, fixed-structure gain tuning, and\nlack real-world validation. To further investigate LLMs in automatic control,\nthis work targets a key subfield: adaptive control. Inspired by the framework\nof model reference adaptive control (MRAC), we propose an LLM-guided adaptive\ncompensator framework that avoids designing controllers from scratch. Instead,\nthe LLMs are prompted using the discrepancies between an unknown system and a\nreference system to design a compensator that aligns the response of the\nunknown system with that of the reference, thereby achieving adaptivity.\nExperiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided\nadaptive controller, indirect adaptive control, learning-based adaptive\ncontrol, and MRAC, on soft and humanoid robots in both simulated and real-world\nenvironments. Results show that the LLM-guided adaptive compensator outperforms\ntraditional adaptive controllers and significantly reduces reasoning complexity\ncompared to the LLM-guided adaptive controller. The Lyapunov-based analysis and\nreasoning-path inspection demonstrate that the LLM-guided adaptive compensator\nenables a more structured design process by transforming mathematical\nderivation into a reasoning task, while exhibiting strong generalizability,\nadaptability, and robustness. This study opens a new direction for applying\nLLMs in the field of automatic control, offering greater deployability and\npracticality compared to vision-language models.", "AI": {"tldr": "本文提出了一种基于大语言模型（LLM）的自适应补偿器框架，用于机器人控制，避免了从头设计控制器，并通过实验验证其优于传统方法。", "motivation": "现有研究多关注LLM在高层任务中的应用，而在反馈控制器设计中仅涉及简化系统，缺乏实际验证。本文旨在探索LLM在自适应控制中的潜力。", "method": "受模型参考自适应控制（MRAC）启发，提出LLM引导的自适应补偿器框架，利用未知系统与参考系统的差异设计补偿器。", "result": "实验表明，LLM引导的自适应补偿器优于传统方法，显著降低推理复杂度，并展现出强泛化性、适应性和鲁棒性。", "conclusion": "本研究为LLM在自动控制领域的应用开辟了新方向，具有更高的实用性和部署潜力。"}}
{"id": "2507.19701", "categories": ["cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19701", "abs": "https://arxiv.org/abs/2507.19701", "authors": ["Haichuan Li", "Tomi Westerlund"], "title": "PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction", "comment": null, "summary": "Accurate prediction of future agent trajectories is a critical challenge for\nensuring safe and efficient autonomous navigation, particularly in complex\nurban environments characterized by multiple plausible future scenarios. In\nthis paper, we present a novel hybrid approach that integrates learning-based\nwith physics-based constraints to address the multi-modality inherent in\ntrajectory prediction. Our method employs a variational Bayesian mixture model\nto effectively capture the diverse range of potential future behaviors, moving\nbeyond traditional unimodal assumptions. Unlike prior approaches that\npredominantly treat trajectory prediction as a data-driven regression task, our\nframework incorporates physical realism through sector-specific boundary\nconditions and Model Predictive Control (MPC)-based smoothing. These\nconstraints ensure that predicted trajectories are not only data-consistent but\nalso physically plausible, adhering to kinematic and dynamic principles.\nFurthermore, our method produces interpretable and diverse trajectory\npredictions, enabling enhanced downstream decision-making and planning in\nautonomous driving systems. We evaluate our approach on two benchmark datasets,\ndemonstrating superior performance compared to existing methods. Comprehensive\nablation studies validate the contributions of each component and highlight\ntheir synergistic impact on prediction accuracy and reliability. By balancing\ndata-driven insights with physics-informed constraints, our approach offers a\nrobust and scalable solution for navigating the uncertainties of real-world\nurban environments.", "AI": {"tldr": "提出一种结合学习与物理约束的混合方法，用于多模态轨迹预测，提升自动驾驶的准确性和可靠性。", "motivation": "解决复杂城市环境中多模态轨迹预测的挑战，确保自动驾驶的安全性和高效性。", "method": "使用变分贝叶斯混合模型结合物理约束（如边界条件和MPC平滑），生成数据一致且物理可行的轨迹。", "result": "在两个基准数据集上表现优于现有方法，验证了各组件对预测准确性和可靠性的贡献。", "conclusion": "通过平衡数据驱动与物理约束，提供了一种稳健且可扩展的解决方案，适用于现实城市环境的不确定性。"}}
{"id": "2507.19760", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19760", "abs": "https://arxiv.org/abs/2507.19760", "authors": ["Alberto Confente", "Takanori Jin", "Taisuke Kobayashi", "Julio Rogelio Guadarrama-Olvera", "Gordon Cheng"], "title": "Skin-Machine Interface with Multimodal Contact Motion Classifier", "comment": "8 pages, 8 figures (accepted in Humanoids2025)", "summary": "This paper proposes a novel framework for utilizing skin sensors as a new\noperation interface of complex robots. The skin sensors employed in this study\npossess the capability to quantify multimodal tactile information at multiple\ncontact points. The time-series data generated from these sensors is\nanticipated to facilitate the classification of diverse contact motions\nexhibited by an operator. By mapping the classification results with robot\nmotion primitives, a diverse range of robot motions can be generated by\naltering the manner in which the skin sensors are interacted with. In this\npaper, we focus on a learning-based contact motion classifier employing\nrecurrent neural networks. This classifier is a pivotal factor in the success\nof this framework. Furthermore, we elucidate the requisite conditions for\nsoftware-hardware designs. Firstly, multimodal sensing and its comprehensive\nencoding significantly contribute to the enhancement of classification accuracy\nand learning stability. Utilizing all modalities simultaneously as inputs to\nthe classifier proves to be an effective approach. Secondly, it is essential to\nmount the skin sensors on a flexible and compliant support to enable the\nactivation of three-axis accelerometers. These accelerometers are capable of\nmeasuring horizontal tactile information, thereby enhancing the correlation\nwith other modalities. Furthermore, they serve to absorb the noises generated\nby the robot's movements during deployment. Through these discoveries, the\naccuracy of the developed classifier surpassed 95 %, enabling the dual-arm\nmobile manipulator to execute a diverse range of tasks via the Skin-Machine\nInterface. https://youtu.be/UjUXT4Z4BC8", "AI": {"tldr": "提出了一种利用皮肤传感器作为复杂机器人操作界面的新框架，通过多模态触觉信息分类实现多样化机器人动作生成。", "motivation": "探索皮肤传感器作为机器人操作界面的潜力，通过多模态触觉信息提升交互的多样性和准确性。", "method": "采用基于循环神经网络的接触动作分类器，结合多模态传感和柔性支撑设计，优化分类性能。", "result": "分类器准确率超过95%，成功应用于双臂移动机械臂的多样化任务执行。", "conclusion": "该框架通过多模态传感和柔性设计显著提升了机器人操作的灵活性和准确性，为未来人机交互提供了新思路。"}}
{"id": "2507.19817", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19817", "abs": "https://arxiv.org/abs/2507.19817", "authors": ["Ziyin Xiong", "Yinghan Chen", "Puhao Li", "Yixin Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation", "comment": "Accepted to IROS 2025, oral presentation. Project page link:\n  https://ziyin-xiong.github.io/ag2x2.github.io/", "summary": "Bimanual manipulation, fundamental to human daily activities, remains a\nchallenging task due to its inherent complexity of coordinated control. Recent\nadvances have enabled zero-shot learning of single-arm manipulation skills\nthrough agent-agnostic visual representations derived from human videos;\nhowever, these methods overlook crucial agent-specific information necessary\nfor bimanual coordination, such as end-effector positions. We propose Ag2x2, a\ncomputational framework for bimanual manipulation through coordination-aware\nvisual representations that jointly encode object states and hand motion\npatterns while maintaining agent-agnosticism. Extensive experiments demonstrate\nthat Ag2x2 achieves a 73.5% success rate across 13 diverse bimanual tasks from\nBi-DexHands and PerAct2, including challenging scenarios with deformable\nobjects like ropes. This performance outperforms baseline methods and even\nsurpasses the success rate of policies trained with expert-engineered rewards.\nFurthermore, we show that representations learned through Ag2x2 can be\neffectively leveraged for imitation learning, establishing a scalable pipeline\nfor skill acquisition without expert supervision. By maintaining robust\nperformance across diverse tasks without human demonstrations or engineered\nrewards, Ag2x2 represents a step toward scalable learning of complex bimanual\nrobotic skills.", "AI": {"tldr": "Ag2x2是一个通过协调感知视觉表示实现双手机器人操作的计算框架，无需专家监督即可学习复杂技能。", "motivation": "双手机器人操作因协调控制的复杂性而具有挑战性，现有方法忽略了关键的代理特定信息。", "method": "Ag2x2通过联合编码对象状态和手部运动模式，同时保持代理无关性。", "result": "在13项任务中达到73.5%的成功率，优于基线方法和专家设计的奖励策略。", "conclusion": "Ag2x2为复杂双手机器人技能的可扩展学习提供了有效途径。"}}
{"id": "2507.19829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19829", "abs": "https://arxiv.org/abs/2507.19829", "authors": ["Chuan Cao", "Xiaoning Wang", "Wenqian Xi", "Han Zhang", "Weidong Chen", "Jingchuan Wang"], "title": "A 4D Radar Camera Extrinsic Calibration Tool Based on 3D Uncertainty Perspective N Points", "comment": null, "summary": "4D imaging radar is a type of low-cost millimeter-wave radar(costing merely\n10-20$\\%$ of lidar systems) capable of providing range, azimuth, elevation, and\nDoppler velocity information. Accurate extrinsic calibration between\nmillimeter-wave radar and camera systems is critical for robust multimodal\nperception in robotics, yet remains challenging due to inherent sensor noise\ncharacteristics and complex error propagation. This paper presents a systematic\ncalibration framework to address critical challenges through a spatial 3d\nuncertainty-aware PnP algorithm (3DUPnP) that explicitly models spherical\ncoordinate noise propagation in radar measurements, then compensating for\nnon-zero error expectations during coordinate transformations. Finally,\nexperimental validation demonstrates significant performance improvements over\nstate-of-the-art CPnP baseline, including improved consistency in simulations\nand enhanced precision in physical experiments. This study provides a robust\ncalibration solution for robotic systems equipped with millimeter-wave radar\nand cameras, tailored specifically for autonomous driving and robotic\nperception applications.", "AI": {"tldr": "本文提出了一种针对毫米波雷达与相机系统的外参校准框架，通过3DUPnP算法显式建模雷达测量中的球坐标噪声传播，并在坐标变换中补偿非零误差期望，实验验证了其优于现有方法的性能。", "motivation": "毫米波雷达与相机系统的准确外参校准对机器人多模态感知至关重要，但由于传感器噪声特性和复杂误差传播，校准仍具挑战性。", "method": "提出了一种空间3D不确定性感知的PnP算法（3DUPnP），显式建模雷达测量中的球坐标噪声传播，并在坐标变换中补偿非零误差期望。", "result": "实验验证表明，该方法在仿真中具有更好的一致性，在物理实验中提高了精度，显著优于现有CPnP基线。", "conclusion": "该研究为配备毫米波雷达和相机的机器人系统提供了一种鲁棒的校准解决方案，特别适用于自动驾驶和机器人感知应用。"}}
{"id": "2507.19851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19851", "abs": "https://arxiv.org/abs/2507.19851", "authors": ["Ye Wang", "Haodong Jing", "Yang Liao", "Yongqiang Ma", "Nanning Zheng"], "title": "PlaneHEC: Efficient Hand-Eye Calibration for Multi-view Robotic Arm via Any Point Cloud Plane Detection", "comment": "Accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA)", "summary": "Hand-eye calibration is an important task in vision-guided robotic systems\nand is crucial for determining the transformation matrix between the camera\ncoordinate system and the robot end-effector. Existing methods, for multi-view\nrobotic systems, usually rely on accurate geometric models or manual\nassistance, generalize poorly, and can be very complicated and inefficient.\nTherefore, in this study, we propose PlaneHEC, a generalized hand-eye\ncalibration method that does not require complex models and can be accomplished\nusing only depth cameras, which achieves the optimal and fastest calibration\nresults using arbitrary planar surfaces like walls and tables. PlaneHEC\nintroduces hand-eye calibration equations based on planar constraints, which\nmakes it strongly interpretable and generalizable. PlaneHEC also uses a\ncomprehensive solution that starts with a closed-form solution and improves it\nwithiterative optimization, which greatly improves accuracy. We comprehensively\nevaluated the performance of PlaneHEC in both simulated and real-world\nenvironments and compared the results with other point-cloud-based calibration\nmethods, proving its superiority. Our approach achieves universal and fast\ncalibration with an innovative design of computational models, providing a\nstrong contribution to the development of multi-agent systems and embodied\nintelligence.", "AI": {"tldr": "PlaneHEC是一种无需复杂模型、仅需深度相机的手眼标定方法，利用任意平面表面实现快速最优标定。", "motivation": "现有方法依赖精确几何模型或人工辅助，泛化性差且复杂低效，需改进。", "method": "基于平面约束设计标定方程，结合闭式解和迭代优化提高精度。", "result": "在仿真和真实环境中表现优异，优于其他点云标定方法。", "conclusion": "PlaneHEC为多智能体系统和具身智能发展提供重要贡献。"}}
{"id": "2507.19854", "categories": ["cs.RO", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.19854", "abs": "https://arxiv.org/abs/2507.19854", "authors": ["Anjali R. Menon", "Rohit K. Sharma", "Priya Singh", "Chengyu Wang", "Aurora M. Ferreira", "Mateja Novak"], "title": "Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models", "comment": "13 pages, 7 figures", "summary": "The integration of Large Language Models (LLMs) into robotics has unlocked\nunprecedented capabilities in high-level task planning. However, most current\nsystems operate in an open-loop fashion, where LLMs act as one-shot planners,\nrendering them brittle and unable to adapt to unforeseen circumstances in\ndynamic physical environments. To overcome this limitation, this paper\nintroduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that\nenables an embodied agent to autonomously learn and refine its policies through\ncontinuous interaction. Our framework establishes a closed-loop cycle where an\nLLM first \"thinks\" by decomposing high-level commands into actionable plans.\nThe robot then \"acts\" by executing these plans while gathering rich, multimodal\nsensory feedback. Critically, the \"learn\" module processes this feedback to\nfacilitate LLM-driven self-reflection, allowing the agent to perform causal\nanalysis on its failures and generate corrective strategies. These insights are\nstored in an experiential memory to guide future planning cycles. We\ndemonstrate through extensive experiments in both simulation and the real world\nthat our T-A-L agent significantly outperforms baseline methods, including\nopen-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our\nframework achieves over a 97% success rate on complex, long-horizon tasks,\nconverges to a stable policy in an average of just 9 trials, and exhibits\nremarkable generalization to unseen tasks. This work presents a significant\nstep towards developing more robust, adaptive, and truly autonomous robotic\nagents.", "AI": {"tldr": "论文提出了一种名为“Think, Act, Learn”（T-A-L）的闭环框架，通过LLM驱动的自我反思和持续学习，显著提升了机器人在动态环境中的适应能力和任务成功率。", "motivation": "当前基于LLM的机器人系统多为开环规划，无法适应动态环境中的突发情况，限制了其鲁棒性和适应性。", "method": "T-A-L框架通过“思考-行动-学习”的闭环循环，利用LLM分解任务、执行计划并收集反馈，通过自我反思和因果分析优化策略。", "result": "实验表明，T-A-L框架在复杂任务中成功率超过97%，仅需9次试验即可收敛，且能泛化到未见任务。", "conclusion": "T-A-L框架为开发更鲁棒、自适应和真正自主的机器人系统迈出了重要一步。"}}
{"id": "2507.19883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19883", "abs": "https://arxiv.org/abs/2507.19883", "authors": ["Ahmed Abouelazm", "Mohammad Mahmoud", "Conrad Walter", "Oleksandr Shchetsura", "Erne Hussong", "Helen Gremmelmaier", "J. Marius Zöllner"], "title": "Bridging Simulation and Usability: A User-Friendly Framework for Scenario Generation in CARLA", "comment": "Paper is accepted in IEEE International Automated Vehicle Validation\n  Conference (IAVVC 2025)", "summary": "Autonomous driving promises safer roads, reduced congestion, and improved\nmobility, yet validating these systems across diverse conditions remains a\nmajor challenge. Real-world testing is expensive, time-consuming, and sometimes\nunsafe, making large-scale validation impractical. In contrast, simulation\nenvironments offer a scalable and cost-effective alternative for rigorous\nverification and validation. A critical component of the validation process is\nscenario generation, which involves designing and configuring traffic scenarios\nto evaluate autonomous systems' responses to various events and uncertainties.\nHowever, existing scenario generation tools often require programming\nknowledge, limiting accessibility for non-technical users. To address this\nlimitation, we present an interactive, no-code framework for scenario\ngeneration. Our framework features a graphical interface that enables users to\ncreate, modify, save, load, and execute scenarios without needing coding\nexpertise or detailed simulation knowledge. Unlike script-based tools such as\nScenic or ScenarioRunner, our approach lowers the barrier to entry and supports\na broader user base. Central to our framework is a graph-based scenario\nrepresentation that facilitates structured management, supports both manual and\nautomated generation, and enables integration with deep learning-based scenario\nand behavior generation methods. In automated mode, the framework can randomly\nsample parameters such as actor types, behaviors, and environmental conditions,\nallowing the generation of diverse and realistic test datasets. By simplifying\nthe scenario generation process, this framework supports more efficient testing\nworkflows and increases the accessibility of simulation-based validation for\nresearchers, engineers, and policymakers.", "AI": {"tldr": "提出了一种无需编程的交互式场景生成框架，用于自动驾驶系统的验证，降低技术门槛并支持更广泛的用户群体。", "motivation": "自动驾驶系统验证需要大规模场景测试，但现有工具依赖编程知识，限制了非技术用户的使用。", "method": "开发了一个图形化界面框架，支持手动和自动生成场景，采用基于图的场景表示方法，并与深度学习方法集成。", "result": "框架能够生成多样化和真实的测试数据集，简化了测试流程，提高了验证的可访问性。", "conclusion": "该框架为自动驾驶验证提供了高效、易用的工具，适用于研究人员、工程师和政策制定者。"}}
{"id": "2507.19914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19914", "abs": "https://arxiv.org/abs/2507.19914", "authors": ["Akram Khairi", "Hussain Sajwani", "Abdallah Mohammad Alkilany", "Laith AbuAssi", "Mohamad Halwani", "Islam Mohamed Zaid", "Ahmed Awadalla", "Dewald Swart", "Abdulla Ayyad", "Yahya Zweiri"], "title": "High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements", "comment": "14 pages, 11 figures", "summary": "Inspecting large-scale industrial surfaces like aircraft fuselages for\nquality control requires capturing their precise 3D surface geometry at high\nresolution. Vision-based tactile sensors (VBTSs) offer high local resolution\nbut require slow 'press-and-lift' measurements stitched for large areas.\nApproaches with sliding or roller/belt VBTS designs provide measurements\ncontinuity. However, they face significant challenges respectively: sliding\nstruggles with friction/wear and both approaches are speed-limited by\nconventional camera frame rates and motion blur, making large-area scanning\ntime consuming. Thus, a rapid, continuous, high-resolution method is needed. We\nintroduce a novel tactile sensor integrating a neuromorphic camera in a rolling\nmechanism to achieve this. Leveraging its high temporal resolution and\nrobustness to motion blur, our system uses a modified event-based multi-view\nstereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning\nspeeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11\ntimes faster than prior continuous tactile sensing methods. A multi-reference\nBayesian fusion strategy enhances accuracy (reducing MAE by 25.2\\% compared to\nEMVS) and mitigates curvature errors. We also validate high-speed feature\nrecognition via Braille reading 2.6 times faster than previous approaches.", "AI": {"tldr": "提出了一种新型触觉传感器，结合神经形态相机和滚动机制，实现快速、连续、高分辨率的3D表面扫描，显著提升速度和精度。", "motivation": "现有触觉传感器在大面积扫描时速度慢且受限于相机帧率和运动模糊，需要一种更高效的方法。", "method": "采用神经形态相机和滚动机制，结合事件驱动的多视角立体视觉方法进行3D重建，并使用贝叶斯融合策略提升精度。", "result": "扫描速度达0.5 m/s，平均绝对误差低于100微米，比现有方法快11倍；特征识别速度提升2.6倍。", "conclusion": "该方法显著提高了大面积工业表面扫描的效率和精度，具有实际应用潜力。"}}
{"id": "2507.19975", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19975", "abs": "https://arxiv.org/abs/2507.19975", "authors": ["Aude Billard", "Alin Albu-Schaeffer", "Michael Beetz", "Wolfram Burgard", "Peter Corke", "Matei Ciocarlie", "Ravinder Dahiya", "Danica Kragic", "Ken Goldberg", "Yukie Nagai", "Davide Scaramuzza"], "title": "A roadmap for AI in robotics", "comment": null, "summary": "AI technologies, including deep learning, large-language models have gone\nfrom one breakthrough to the other. As a result, we are witnessing growing\nexcitement in robotics at the prospect of leveraging the potential of AI to\ntackle some of the outstanding barriers to the full deployment of robots in our\ndaily lives. However, action and sensing in the physical world pose greater and\ndifferent challenges than analysing data in isolation. As the development and\napplication of AI in robotic products advances, it is important to reflect on\nwhich technologies, among the vast array of network architectures and learning\nmodels now available in the AI field, are most likely to be successfully\napplied to robots; how they can be adapted to specific robot designs, tasks,\nenvironments; which challenges must be overcome. This article offers an\nassessment of what AI for robotics has achieved since the 1990s and proposes a\nshort- and medium-term research roadmap listing challenges and promises. These\nrange from keeping up-to-date large datasets, representatives of a diversity of\ntasks robots may have to perform, and of environments they may encounter, to\ndesigning AI algorithms tailored specifically to robotics problems but generic\nenough to apply to a wide range of applications and transfer easily to a\nvariety of robotic platforms. For robots to collaborate effectively with\nhumans, they must predict human behavior without relying on bias-based\nprofiling. Explainability and transparency in AI-driven robot control are not\noptional but essential for building trust, preventing misuse, and attributing\nresponsibility in accidents. We close on what we view as the primary long-term\nchallenges, that is, to design robots capable of lifelong learning, while\nguaranteeing safe deployment and usage, and sustainable computational costs.", "AI": {"tldr": "论文探讨了AI在机器人领域的应用潜力、挑战及未来研究方向，强调了数据、算法设计、人机协作和安全性等问题。", "motivation": "随着AI技术的快速发展，如何将其成功应用于机器人领域以解决实际物理世界中的挑战成为研究重点。", "method": "评估了自1990年代以来AI在机器人领域的成就，并提出了短期和中期的研究路线图，包括数据集更新、算法设计和人机协作等。", "result": "指出了AI在机器人应用中的关键挑战，如数据多样性、算法通用性、透明性和安全性。", "conclusion": "未来的长期挑战包括设计能够终身学习、安全部署且计算成本可持续的机器人。"}}
{"id": "2507.19983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19983", "abs": "https://arxiv.org/abs/2507.19983", "authors": ["Yuhong Deng", "Chao Tang", "Cunjun Yu", "Linfeng Li", "David Hsu"], "title": "CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints", "comment": null, "summary": "Clothes manipulation, such as folding or hanging, is a critical capability\nfor home service robots. Despite recent advances, most existing methods remain\nlimited to specific tasks and clothes types, due to the complex,\nhigh-dimensional geometry of clothes. This paper presents CLothes mAnipulation\nwith Semantic keyPoints (CLASP), which aims at general-purpose clothes\nmanipulation over different clothes types, T-shirts, shorts, skirts, long\ndresses, ... , as well as different tasks, folding, flattening, hanging, ... .\nThe core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right\nshoulder'', etc. -- a sparse spatial-semantic representation that is salient\nfor both perception and action. Semantic keypoints of clothes can be reliably\nextracted from RGB-D images and provide an effective intermediate\nrepresentation of clothes manipulation policies. CLASP uses semantic keypoints\nto bridge high-level task planning and low-level action execution. At the high\nlevel, it exploits vision language models (VLMs) to predict task plans over the\nsemantic keypoints. At the low level, it executes the plans with the help of a\nsimple pre-built manipulation skill library. Extensive simulation experiments\nshow that CLASP outperforms state-of-the-art baseline methods on multiple tasks\nacross diverse clothes types, demonstrating strong performance and\ngeneralization. Further experiments with a Franka dual-arm system on four\ndistinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's\nperformance on a real robot.", "AI": {"tldr": "CLASP提出了一种基于语义关键点的通用衣物操作方法，通过稀疏的空间语义表示连接感知与动作，实现了对不同衣物类型和任务的高效处理。", "motivation": "现有衣物操作方法受限于特定任务和衣物类型，无法应对复杂的高维几何结构。", "method": "利用语义关键点（如“左袖”、“右肩”）作为中间表示，结合视觉语言模型（VLMs）进行任务规划，并通过预构建的技能库执行动作。", "result": "在仿真和真实机器人实验中，CLASP在多种任务和衣物类型上表现优于现有方法，展示了强大的性能和泛化能力。", "conclusion": "CLASP通过语义关键点实现了通用衣物操作，为家庭服务机器人提供了更灵活和高效的解决方案。"}}
{"id": "2507.19999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19999", "abs": "https://arxiv.org/abs/2507.19999", "authors": ["Laura Treers", "Daniel Soto", "Joonha Hwang", "Michael A. D. Goodisman", "Daniel I. Goldman"], "title": "Robot Excavation and Manipulation of Geometrically Cohesive Granular Media", "comment": null, "summary": "Construction throughout history typically assumes that its blueprints and\nbuilding blocks are pre-determined. However, recent work suggests that\nalternative approaches can enable new paradigms for structure formation.\nAleatory architectures, or those which rely on the properties of their granular\nbuilding blocks rather than pre-planned design or computation, have thus far\nrelied on human intervention for their creation. We imagine that robotic swarms\ncould be valuable to create such aleatory structures by manipulating and\nforming structures from entangled granular materials. To discover principles by\nwhich robotic systems can effectively manipulate soft matter, we develop a\nrobophysical model for interaction with geometrically cohesive granular media\ncomposed of u-shape particles. This robotic platform uses environmental signals\nto autonomously coordinate excavation, transport, and deposition of material.\nWe test the effect of substrate initial conditions by characterizing robot\nperformance in two different material compaction states and observe as much as\na 75% change in transported mass depending on initial substrate compressive\nloading. These discrepancies suggest the functional role that material\nproperties such as packing and cohesion/entanglement play in excavation and\nconstruction. To better understand these material properties, we develop an\napparatus for tensile testing of the geometrically cohesive substrates, which\nreveals how entangled material strength responds strongly to initial\ncompressive loading. These results explain the variation observed in robotic\nperformance and point to future directions for better understanding robotic\ninteraction mechanics with entangled materials.", "AI": {"tldr": "论文探讨了机器人群体如何通过操纵纠缠颗粒材料构建随机结构，并研究了材料特性对机器人性能的影响。", "motivation": "传统建筑依赖预先设计的蓝图和材料，而随机结构（aleatory architectures）依赖材料本身的特性。研究旨在探索机器人如何自主构建此类结构。", "method": "开发了一个机器人物理模型，用于与几何粘性颗粒材料（U形颗粒）交互，测试了不同初始压缩状态下的机器人性能。", "result": "机器人性能受材料初始压缩状态显著影响，运输质量变化高达75%。拉伸测试揭示了材料强度与初始压缩加载的关系。", "conclusion": "材料特性（如堆积和纠缠）对机器人操作至关重要，研究为未来理解机器人与纠缠材料的交互机制提供了方向。"}}
{"id": "2507.20002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20002", "abs": "https://arxiv.org/abs/2507.20002", "authors": ["Peiyao Hou", "Danning Sun", "Meng Wang", "Yuzhe Huang", "Zeyu Zhang", "Hangxin Liu", "Wanlin Li", "Ziyuan Jiao"], "title": "SuperMag: Vision-based Tactile Data Guided High-resolution Tactile Shape Reconstruction for Magnetic Tactile Sensors", "comment": "7 pages, 7 figures; accepted by IROS 2025", "summary": "Magnetic-based tactile sensors (MBTS) combine the advantages of compact\ndesign and high-frequency operation but suffer from limited spatial resolution\ndue to their sparse taxel arrays. This paper proposes SuperMag, a tactile shape\nreconstruction method that addresses this limitation by leveraging\nhigh-resolution vision-based tactile sensor (VBTS) data to supervise MBTS\nsuper-resolution. Co-designed, open-source VBTS and MBTS with identical contact\nmodules enable synchronized data collection of high-resolution shapes and\nmagnetic signals via a symmetric calibration setup. We frame tactile shape\nreconstruction as a conditional generative problem, employing a conditional\nvariational auto-encoder to infer high-resolution shapes from low-resolution\nMBTS inputs. The MBTS achieves a sampling frequency of 125 Hz, whereas the\nshape reconstruction sustains an inference time within 2.5 ms. This\ncross-modality synergy advances tactile perception of the MBTS, potentially\nunlocking its new capabilities in high-precision robotic tasks.", "AI": {"tldr": "SuperMag利用高分辨率视觉触觉传感器数据监督磁触觉传感器的超分辨率重建，提升其空间分辨率。", "motivation": "磁触觉传感器（MBTS）设计紧凑且高频操作，但稀疏的阵列限制了其空间分辨率。", "method": "通过条件变分自编码器从低分辨率MBTS输入推断高分辨率形状，利用同步数据收集的VBTS和MBTS数据。", "result": "MBTS采样频率达125Hz，形状重建推理时间低于2.5ms。", "conclusion": "跨模态协同提升了MBTS的触觉感知能力，有望用于高精度机器人任务。"}}
{"id": "2507.20021", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20021", "abs": "https://arxiv.org/abs/2507.20021", "authors": ["Matin Aghaei", "Mohammad Ali Alomrani", "Yingxue Zhang", "Mahdi Biparva"], "title": "When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation", "comment": null, "summary": "Large language models (LLMs) are often credited with recent leaps in\nObjectGoal Navigation, yet the extent to which they improve planning remains\nunclear. We revisit this question on the HM3D-v1 validation split. First, we\nstrip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary\nGLEE detector and Intuition saliency map, and replace them with a simple\nDistance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises\nSuccess from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000\nvalidation episodes, outperforming all previous training-free baselines.\nSecond, we add a lightweight language prior (SHF); on a 200-episode subset this\nyields a further +2% Success and +0.9% SPL while shortening paths by five steps\non average. Qualitative trajectories confirm the trend: InstructNav back-tracks\nand times-out, DWFE reaches the goal after a few islands, and SHF follows an\nalmost straight route. Our results indicate that frontier geometry, not\nemergent LLM reasoning, drives most reported gains, and suggest that\nmetric-aware prompts or offline semantic graphs are necessary before\nattributing navigation success to \"LLM intelligence.\"", "AI": {"tldr": "论文探讨了大型语言模型（LLM）在目标导航中的作用，发现几何启发式方法比LLM更能提升性能。", "motivation": "研究LLM在目标导航中的实际贡献，验证其是否真正提升了规划能力。", "method": "通过简化InstructNav系统，使用几何启发式方法（DWFE）和轻量级语言先验（SHF）进行实验。", "result": "DWFE将成功率从58.0%提升至61.1%，SPL从20.9%提升至36.0%；SHF进一步提升了性能。", "conclusion": "几何启发式方法是性能提升的主要驱动力，LLM的作用有限，需进一步优化才能发挥其潜力。"}}
{"id": "2507.20034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20034", "abs": "https://arxiv.org/abs/2507.20034", "authors": ["Aviad Golan", "Gregory Zin", "Zahra Ahmed", "Emily Bates", "Toby Bell", "Pol Francesch Huc", "Samuel Y. W. Low", "Juergen Bosse", "Simone D'Amico"], "title": "Digital and Robotic Twinning for Validation of Proximity Operations and Formation Flying", "comment": "23 pages, 12 figures. 2025 Astrodynamics Specialist Conference", "summary": "In spacecraft Rendezvous, Proximity Operations (RPO), and Formation Flying\n(FF), the Guidance Navigation and Control (GNC) system is safety-critical and\nmust meet strict performance requirements. However, validating such systems is\nchallenging due to the complexity of the space environment, necessitating a\nverification and validation (V&V) process that bridges simulation and\nreal-world behavior. The key contribution of this paper is a unified,\nend-to-end digital and robotic twinning framework that enables software- and\nhardware-in-the-loop testing for multi-modal GNC systems. The robotic twin\nincludes three testbeds at Stanford's Space Rendezvous Laboratory (SLAB): the\nGNSS and Radiofrequency Autonomous Navigation Testbed for Distributed Space\nSystems (GRAND) to validate RF-based navigation techniques, and the Testbed for\nRendezvous and Optical Navigation (TRON) and Optical Stimulator (OS) to\nvalidate vision-based methods. The test article for this work is an integrated\nmulti-modal GNC software stack for RPO and FF developed at SLAB. This paper\nintroduces the hybrid framework and summarizes calibration and error\ncharacterization for the robotic twin. Then, the GNC stack's performance and\nrobustness is characterized using the integrated digital and robotic twinning\npipeline for a full-range RPO mission scenario in Low-Earth Orbit (LEO). The\nresults shown in the paper demonstrate consistency between digital and robotic\ntwins, validating the hybrid twinning pipeline as a reliable framework for\nrealistic assessment and verification of GNC systems.", "AI": {"tldr": "本文提出了一种统一的数字和机器人孪生框架，用于多模态GNC系统的验证，展示了其在低地球轨道RPO任务中的性能与鲁棒性。", "motivation": "由于太空环境的复杂性，验证GNC系统具有挑战性，需要一种连接仿真与真实行为的V&V流程。", "method": "开发了数字和机器人孪生框架，结合软件和硬件在环测试，使用三个测试台（GRAND、TRON、OS）验证多模态GNC系统。", "result": "数字与机器人孪生结果一致，验证了该框架在GNC系统评估中的可靠性。", "conclusion": "混合孪生框架为GNC系统的现实评估和验证提供了可靠方法。"}}
{"id": "2507.20049", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20049", "abs": "https://arxiv.org/abs/2507.20049", "authors": ["Frederico Belmonte Klein", "Zhaoyuan Wan", "Huawei Wang", "Ruoli Wang"], "title": "A real-time full-chain wearable sensor-based musculoskeletal simulation: an OpenSim-ROS Integration", "comment": "11 pages, 10 figures", "summary": "Musculoskeletal modeling and simulations enable the accurate description and\nanalysis of the movement of biological systems with applications such as\nrehabilitation assessment, prosthesis, and exoskeleton design. However, the\nwidespread usage of these techniques is limited by costly sensors,\nlaboratory-based setups, computationally demanding processes, and the use of\ndiverse software tools that often lack seamless integration. In this work, we\naddress these limitations by proposing an integrated, real-time framework for\nmusculoskeletal modeling and simulations that leverages OpenSimRT, the robotics\noperating system (ROS), and wearable sensors. As a proof-of-concept, we\ndemonstrate that this framework can reasonably well describe inverse kinematics\nof both lower and upper body using either inertial measurement units or\nfiducial markers. Additionally, we show that it can effectively estimate\ninverse dynamics of the ankle joint and muscle activations of major lower limb\nmuscles during daily activities, including walking, squatting and sit to stand,\nstand to sit when combined with pressure insoles. We believe this work lays the\ngroundwork for further studies with more complex real-time and wearable\nsensor-based human movement analysis systems and holds potential to advance\ntechnologies in rehabilitation, robotics and exoskeleton designs.", "AI": {"tldr": "提出了一种基于OpenSimRT、ROS和可穿戴传感器的实时集成框架，用于肌肉骨骼建模和仿真，解决了现有技术的高成本、复杂性和集成问题。", "motivation": "现有肌肉骨骼建模技术因高成本传感器、实验室设置、计算复杂性和软件集成问题而难以广泛应用。", "method": "结合OpenSimRT、ROS和可穿戴传感器，开发实时框架，验证其在下肢和上肢逆运动学及踝关节逆动力学和肌肉激活估计中的应用。", "result": "框架能够准确描述逆运动学，并有效估计踝关节动力学和下肢肌肉激活，适用于日常活动。", "conclusion": "该框架为复杂实时和可穿戴传感器的人体运动分析系统奠定了基础，对康复、机器人和外骨骼设计有潜在推动作用。"}}
{"id": "2507.20217", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20217", "abs": "https://arxiv.org/abs/2507.20217", "authors": ["Wei Cui", "Haoyu Wang", "Wenkang Qin", "Yijie Guo", "Gang Han", "Wen Zhao", "Jiahang Cao", "Zhang Zhang", "Jiaru Zhong", "Jingkai Sun", "Pihai Sun", "Shuai Shi", "Botuo Jiang", "Jiahao Ma", "Jiaxu Wang", "Hao Cheng", "Zhichao Liu", "Yang Wang", "Zheng Zhu", "Guan Huang", "Jian Tang", "Qiang Zhang"], "title": "Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots", "comment": "Tech Report", "summary": "Humanoid robot technology is advancing rapidly, with manufacturers\nintroducing diverse heterogeneous visual perception modules tailored to\nspecific scenarios. Among various perception paradigms, occupancy-based\nrepresentation has become widely recognized as particularly suitable for\nhumanoid robots, as it provides both rich semantic and 3D geometric information\nessential for comprehensive environmental understanding. In this work, we\npresent Humanoid Occupancy, a generalized multimodal occupancy perception\nsystem that integrates hardware and software components, data acquisition\ndevices, and a dedicated annotation pipeline. Our framework employs advanced\nmulti-modal fusion techniques to generate grid-based occupancy outputs encoding\nboth occupancy status and semantic labels, thereby enabling holistic\nenvironmental understanding for downstream tasks such as task planning and\nnavigation. To address the unique challenges of humanoid robots, we overcome\nissues such as kinematic interference and occlusion, and establish an effective\nsensor layout strategy. Furthermore, we have developed the first panoramic\noccupancy dataset specifically for humanoid robots, offering a valuable\nbenchmark and resource for future research and development in this domain. The\nnetwork architecture incorporates multi-modal feature fusion and temporal\ninformation integration to ensure robust perception. Overall, Humanoid\nOccupancy delivers effective environmental perception for humanoid robots and\nestablishes a technical foundation for standardizing universal visual modules,\npaving the way for the widespread deployment of humanoid robots in complex\nreal-world scenarios.", "AI": {"tldr": "Humanoid Occupancy是一个多模态占用感知系统，结合硬件和软件组件，为仿人机器人提供全面的环境理解。", "motivation": "仿人机器人需要丰富的语义和3D几何信息以实现环境理解，而现有的占用表示方法被认为是最适合的。", "method": "采用多模态融合技术和网格占用输出，解决运动干扰和遮挡问题，并开发首个全景占用数据集。", "result": "系统实现了鲁棒的环境感知，为标准化视觉模块奠定了基础。", "conclusion": "Humanoid Occupancy为仿人机器人在复杂现实场景中的广泛应用铺平了道路。"}}
{"id": "2507.20282", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20282", "abs": "https://arxiv.org/abs/2507.20282", "authors": ["Yifan Zhang", "Dianye Huang", "Nassir Navab", "Zhongliang Jiang"], "title": "Tactile-Guided Robotic Ultrasound: Mapping Preplanned Scan Paths for Intercostal Imaging", "comment": "Accepted by IROS2025, video link: https://youtu.be/SBwpFVzEhAg", "summary": "Medical ultrasound (US) imaging is widely used in clinical examinations due\nto its portability, real-time capability, and radiation-free nature. To address\ninter- and intra-operator variability, robotic ultrasound systems have gained\nincreasing attention. However, their application in challenging intercostal\nimaging remains limited due to the lack of an effective scan path generation\nmethod within the constrained acoustic window. To overcome this challenge, we\nexplore the potential of tactile cues for characterizing subcutaneous rib\nstructures as an alternative signal for ultrasound segmentation-free bone\nsurface point cloud extraction. Compared to 2D US images, 1D tactile-related\nsignals offer higher processing efficiency and are less susceptible to acoustic\nnoise and artifacts. By leveraging robotic tracking data, a sparse tactile\npoint cloud is generated through a few scans along the rib, mimicking human\npalpation. To robustly map the scanning trajectory into the intercostal space,\nthe sparse tactile bone location point cloud is first interpolated to form a\ndenser representation. This refined point cloud is then registered to an\nimage-based dense bone surface point cloud, enabling accurate scan path mapping\nfor individual patients. Additionally, to ensure full coverage of the object of\ninterest, we introduce an automated tilt angle adjustment method to visualize\nstructures beneath the bone. To validate the proposed method, we conducted\ncomprehensive experiments on four distinct phantoms. The final scanning\nwaypoint mapping achieved Mean Nearest Neighbor Distance (MNND) and Hausdorff\ndistance (HD) errors of 3.41 mm and 3.65 mm, respectively, while the\nreconstructed object beneath the bone had errors of 0.69 mm and 2.2 mm compared\nto the CT ground truth.", "AI": {"tldr": "提出了一种利用触觉信号生成超声扫描路径的方法，以解决肋间成像中扫描路径生成的挑战。", "motivation": "解决机器人超声系统在肋间成像中因缺乏有效扫描路径生成方法而受限的问题。", "method": "利用触觉信号提取肋骨表面点云，并通过插值和配准生成扫描路径，同时引入自动倾斜角调整方法。", "result": "扫描路径映射的平均最近邻距离和Hausdorff距离误差分别为3.41 mm和3.65 mm，重建对象的误差为0.69 mm和2.2 mm。", "conclusion": "该方法在肋间超声成像中有效，提高了扫描路径的准确性和覆盖范围。"}}
{"id": "2507.20293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20293", "abs": "https://arxiv.org/abs/2507.20293", "authors": ["Stepan Dergachev", "Konstantin Yakovlev"], "title": "Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance With Model Predictive Path Integral", "comment": "This is a pre-print of the paper accepted to IROS2025. It contains 8\n  pages, 4 figures and 1 table. The supplementary video available at\n  https://youtu.be/_D4zDYJ4KCk", "summary": "Decentralized multi-agent navigation under uncertainty is a complex task that\narises in numerous robotic applications. It requires collision avoidance\nstrategies that account for both kinematic constraints, sensing and action\nexecution noise. In this paper, we propose a novel approach that integrates the\nModel Predictive Path Integral (MPPI) with a probabilistic adaptation of\nOptimal Reciprocal Collision Avoidance. Our method ensures safe and efficient\nmulti-agent navigation by incorporating probabilistic safety constraints\ndirectly into the MPPI sampling process via a Second-Order Cone Programming\nformulation. This approach enables agents to operate independently using local\nnoisy observations while maintaining safety guarantees. We validate our\nalgorithm through extensive simulations with differential-drive robots and\nbenchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.\nResults demonstrate that our approach outperforms them while achieving high\nsuccess rates, even in densely populated environments. Additionally, validation\nin the Gazebo simulator confirms its practical applicability to robotic\nplatforms.", "AI": {"tldr": "提出了一种结合MPPI和概率ORCA的新方法，用于多智能体导航，确保安全性和高效性。", "motivation": "解决多智能体导航中的不确定性和碰撞避免问题，适应运动约束和噪声。", "method": "将MPPI与概率ORCA结合，通过二阶锥规划引入概率安全约束。", "result": "在密集环境中表现优于ORCA-DD和B-UAVC，成功率高，Gazebo验证实用性强。", "conclusion": "该方法在多智能体导航中安全高效，适用于实际机器人平台。"}}
{"id": "2507.20370", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20370", "abs": "https://arxiv.org/abs/2507.20370", "authors": ["Michele Grimaldi", "Carlo Cernicchiaro", "Sebastian Realpe Rua", "Alaaeddine El-Masri-El-Chaarani", "Markus Buchholz", "Loizos Michael", "Pere Ridao Rodriguez", "Ignacio Carlucho", "Yvan R. Petillot"], "title": "Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation", "comment": null, "summary": "Robotic platforms have become essential for marine operations by providing\nregular and continuous access to offshore assets, such as underwater\ninfrastructure inspection, environmental monitoring, and resource exploration.\nHowever, the complex and dynamic nature of underwater environments,\ncharacterized by limited visibility, unpredictable currents, and communication\nconstraints, presents significant challenges that demand advanced autonomy\nwhile ensuring operator trust and oversight. Central to addressing these\nchallenges are knowledge representation and reasoning techniques, particularly\nknowledge graphs and retrieval-augmented generation (RAG) systems, that enable\nrobots to efficiently structure, retrieve, and interpret complex environmental\ndata. These capabilities empower robotic agents to reason, adapt, and respond\neffectively to changing conditions. The primary goal of this work is to\ndemonstrate both multi-agent autonomy and shared autonomy, where multiple\nrobotic agents operate independently while remaining connected to a human\nsupervisor. We show how a RAG-powered large language model, augmented with\nknowledge graph data and domain taxonomy, enables autonomous multi-agent\ndecision-making and facilitates seamless human-robot interaction, resulting in\n100\\% mission validation and behavior completeness. Finally, ablation studies\nreveal that without structured knowledge from the graph and/or taxonomy, the\nLLM is prone to hallucinations, which can compromise decision quality.", "AI": {"tldr": "论文探讨了在复杂水下环境中，通过知识图谱和检索增强生成（RAG）系统提升机器人自主性和人机协作能力，实现100%任务验证和行为完整性。", "motivation": "水下环境的复杂性和动态性（如有限能见度、不可预测的洋流和通信限制）对机器人自主性提出了高要求，同时需确保操作者的信任和监督。", "method": "采用知识图谱和RAG系统，结合大型语言模型（LLM），支持多机器人自主决策和人机交互。", "result": "实验表明，该方法实现了100%的任务验证和行为完整性，而缺乏结构化知识会导致LLM产生幻觉，影响决策质量。", "conclusion": "知识图谱和RAG系统是提升水下机器人自主性和人机协作的关键技术。"}}
{"id": "2507.20382", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20382", "abs": "https://arxiv.org/abs/2507.20382", "authors": ["Yuyou Zhang", "Radu Corcodel", "Ding Zhao"], "title": "Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning", "comment": "Humanoids 2025", "summary": "Loco-manipulation of quadrupedal robots has broadened robotic applications,\nbut using legs as manipulators often compromises locomotion, while mounting\narms complicates the system. To mitigate this issue, we introduce bipedalism\nfor quadrupedal robots, thus freeing the front legs for versatile interactions\nwith the environment. We propose a risk-adaptive distributional Reinforcement\nLearning (RL) framework designed for quadrupedal robots walking on their hind\nlegs, balancing worst-case conservativeness with optimal performance in this\ninherently unstable task. During training, the adaptive risk preference is\ndynamically adjusted based on the uncertainty of the return, measured by the\ncoefficient of variation of the estimated return distribution. Extensive\nexperiments in simulation show our method's superior performance over\nbaselines. Real-world deployment on a Unitree Go2 robot further demonstrates\nthe versatility of our policy, enabling tasks like cart pushing, obstacle\nprobing, and payload transport, while showcasing robustness against challenging\ndynamics and external disturbances.", "AI": {"tldr": "提出了一种基于风险适应的分布强化学习框架，使四足机器人能够用后腿行走，释放前腿进行环境交互。", "motivation": "四足机器人的腿用作操纵器会影响运动能力，而加装手臂会增加系统复杂性。通过引入双足行走，释放前腿以实现多功能交互。", "method": "采用风险适应的分布强化学习框架，动态调整风险偏好，基于回报分布的不确定性（变异系数）。", "result": "仿真实验显示优于基线方法，实际部署展示了多功能性（如推车、探测障碍、运输负载）和鲁棒性。", "conclusion": "该方法有效解决了四足机器人双足行走的不稳定性问题，实现了多功能交互。"}}
{"id": "2507.20427", "categories": ["cs.RO", "J.2; I.2; I.6"], "pdf": "https://arxiv.org/pdf/2507.20427", "abs": "https://arxiv.org/abs/2507.20427", "authors": ["Mattia Piccinini", "Aniello Mungiello", "Georg Jank", "Gastone Pietro Rosati Papini", "Francesco Biral", "Johannes Betz"], "title": "Model-Structured Neural Networks to Control the Steering Dynamics of Autonomous Race Cars", "comment": "Accepted at the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC)", "summary": "Autonomous racing has gained increasing attention in recent years, as a safe\nenvironment to accelerate the development of motion planning and control\nmethods for autonomous driving. Deep learning models, predominantly based on\nneural networks (NNs), have demonstrated significant potential in modeling the\nvehicle dynamics and in performing various tasks in autonomous driving.\nHowever, their black-box nature is critical in the context of autonomous\nracing, where safety and robustness demand a thorough understanding of the\ndecision-making algorithms. To address this challenge, this paper proposes\nMS-NN-steer, a new Model-Structured Neural Network for vehicle steering\ncontrol, integrating the prior knowledge of the nonlinear vehicle dynamics into\nthe neural architecture. The proposed controller is validated using real-world\ndata from the Abu Dhabi Autonomous Racing League (A2RL) competition, with\nfull-scale autonomous race cars. In comparison with general-purpose NNs,\nMS-NN-steer is shown to achieve better accuracy and generalization with small\ntraining datasets, while being less sensitive to the weights' initialization.\nAlso, MS-NN-steer outperforms the steering controller used by the A2RL winning\nteam. Our implementation is available open-source in a GitHub repository.", "AI": {"tldr": "本文提出了一种名为MS-NN-steer的新型模型结构神经网络，用于车辆转向控制，通过整合非线性车辆动力学的先验知识，提高了自动驾驶赛车运动规划的准确性和泛化能力。", "motivation": "自动驾驶赛车作为加速运动规划和控制方法开发的平台，其安全性和鲁棒性要求对决策算法有深入理解。然而，传统神经网络的“黑盒”特性难以满足这一需求。", "method": "提出MS-NN-steer，将非线性车辆动力学的先验知识融入神经网络架构中，并通过阿布扎比自动驾驶赛车联赛（A2RL）的真实数据进行验证。", "result": "MS-NN-steer在小规模训练数据集上表现出更高的准确性和泛化能力，对权重初始化不敏感，且性能优于A2RL冠军团队的转向控制器。", "conclusion": "MS-NN-steer为自动驾驶赛车提供了一种更安全、更可靠的转向控制解决方案，其开源实现有助于进一步研究和应用。"}}
{"id": "2507.20445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20445", "abs": "https://arxiv.org/abs/2507.20445", "authors": ["Tianyu Li", "Hengbo Ma", "Sehoon Ha", "Kwonjoon Lee"], "title": "Learning Physical Interaction Skills from Human Demonstrations", "comment": null, "summary": "Learning physical interaction skills, such as dancing, handshaking, or\nsparring, remains a fundamental challenge for agents operating in human\nenvironments, particularly when the agent's morphology differs significantly\nfrom that of the demonstrator. Existing approaches often rely on handcrafted\nobjectives or morphological similarity, limiting their capacity for\ngeneralization. Here, we introduce a framework that enables agents with diverse\nembodiments to learn wholebbody interaction behaviors directly from human\ndemonstrations. The framework extracts a compact, transferable representation\nof interaction dynamics, called the Embedded Interaction Graph (EIG), which\ncaptures key spatiotemporal relationships between the interacting agents. This\ngraph is then used as an imitation objective to train control policies in\nphysics-based simulations, allowing the agent to generate motions that are both\nsemantically meaningful and physically feasible. We demonstrate BuddyImitation\non multiple agents, such as humans, quadrupedal robots with manipulators, or\nmobile manipulators and various interaction scenarios, including sparring,\nhandshaking, rock-paper-scissors, or dancing. Our results demonstrate a\npromising path toward coordinated behaviors across morphologically distinct\ncharacters via cross embodiment interaction learning.", "AI": {"tldr": "提出了一种名为BuddyImitation的框架，通过Embedded Interaction Graph（EIG）从人类演示中学习全身交互行为，适用于形态各异的智能体。", "motivation": "解决智能体在形态与演示者差异显著时学习物理交互技能的挑战，避免依赖手工目标或形态相似性。", "method": "提取交互动态的紧凑表示EIG，作为模仿目标训练控制策略，生成语义明确且物理可行的动作。", "result": "在多种智能体和交互场景（如格斗、握手、跳舞）中验证了框架的有效性。", "conclusion": "为形态差异显著的智能体间的协调行为提供了一条可行路径。"}}
{"id": "2507.20516", "categories": ["cs.RO", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.20516", "abs": "https://arxiv.org/abs/2507.20516", "authors": ["Xiaofeng Jin", "Ningbo Bu", "Shijie Wang", "Jianfei Ge", "Jiangjian Xiao", "Matteo Matteucci"], "title": "Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping", "comment": "9 pages,7 figures, 6 tables", "summary": "This paper introduces a large-scale, high-precision LiDAR-Inertial Odometry\n(LIO) dataset, aiming to address the insufficient validation of LIO systems in\ncomplex real-world scenarios in existing research. The dataset covers four\ndiverse real-world environments spanning 60,000 to 750,000 square meters,\ncollected using a custom backpack-mounted platform equipped with multi-beam\nLiDAR, an industrial-grade IMU, and RTK-GNSS modules. The dataset includes long\ntrajectories, complex scenes, and high-precision ground truth, generated by\nfusing SLAM-based optimization with RTK-GNSS anchoring, and validated for\ntrajectory accuracy through the integration of oblique photogrammetry and\nRTK-GNSS. This dataset provides a comprehensive benchmark for evaluating the\ngeneralization ability of LIO systems in practical high-precision mapping\nscenarios.", "AI": {"tldr": "该论文介绍了一个大规模、高精度的LiDAR-惯性里程计（LIO）数据集，旨在解决现有研究中LIO系统在复杂现实场景中验证不足的问题。", "motivation": "现有研究对LIO系统在复杂现实场景中的验证不足，因此需要提供更全面的数据集来评估其性能。", "method": "使用定制背包平台，配备多光束LiDAR、工业级IMU和RTK-GNSS模块，在四种不同环境中收集数据，并通过SLAM优化与RTK-GNSS锚定生成高精度地面真实数据。", "result": "数据集覆盖60,000至750,000平方米的复杂场景，提供长轨迹和高精度地面真实数据，验证了轨迹精度。", "conclusion": "该数据集为评估LIO系统在实际高精度测绘场景中的泛化能力提供了全面基准。"}}
{"id": "2507.20538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20538", "abs": "https://arxiv.org/abs/2507.20538", "authors": ["Gilhwan Kang", "Hogyun Kim", "Byunghee Choi", "Seokhwan Jeong", "Young-Sik Shin", "Younggun Cho"], "title": "Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments", "comment": "18 pages, 14 figures", "summary": "The unification of disparate maps is crucial for enabling scalable robot\noperation across multiple sessions and collaborative multi-robot scenarios.\nHowever, achieving a unified map robust to sensor modalities and dynamic\nenvironments remains a challenging problem. Variations in LiDAR types and\ndynamic elements lead to differences in point cloud distribution and scene\nconsistency, hindering reliable descriptor generation and loop closure\ndetection essential for accurate map alignment. To address these challenges,\nthis paper presents Uni-Mapper, a dynamic-aware 3D point cloud map merging\nframework for multi-modal LiDAR systems. It comprises dynamic object removal,\ndynamic-aware loop closure, and multi-modal LiDAR map merging modules. A\nvoxel-wise free space hash map is built in a coarse-to-fine manner to identify\nand reject dynamic objects via temporal occupancy inconsistencies. The removal\nmodule is integrated with a LiDAR global descriptor, which encodes preserved\nstatic local features to ensure robust place recognition in dynamic\nenvironments. In the final stage, multiple pose graph optimizations are\nconducted for both intra-session and inter-map loop closures. We adopt a\ncentralized anchor-node strategy to mitigate intra-session drift errors during\nmap merging. In the final stage, centralized anchor-node-based pose graph\noptimization is performed to address intra- and inter-map loop closures for\nglobally consistent map merging. Our framework is evaluated on diverse\nreal-world datasets with dynamic objects and heterogeneous LiDARs, showing\nsuperior performance in loop detection across sensor modalities, robust mapping\nin dynamic environments, and accurate multi-map alignment over existing\nmethods. Project Page: https://sparolab.github.io/research/uni_mapper.", "AI": {"tldr": "Uni-Mapper是一个动态感知的3D点云地图合并框架，用于多模态LiDAR系统，解决了动态环境和传感器差异带来的地图统一问题。", "motivation": "统一不同地图对于多会话和多机器人协作至关重要，但动态环境和传感器差异导致点云分布和场景一致性差异，阻碍了可靠的地图对齐。", "method": "Uni-Mapper包括动态物体移除、动态感知闭环和多模态LiDAR地图合并模块，采用体素自由空间哈希图和全局描述符，并通过集中锚节点策略优化位姿图。", "result": "在真实动态环境和异构LiDAR数据集上，Uni-Mapper在闭环检测、动态环境建图和多地图对齐方面优于现有方法。", "conclusion": "Uni-Mapper为动态环境和多模态LiDAR系统的地图统一提供了高效解决方案。"}}
{"id": "2507.20589", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20589", "abs": "https://arxiv.org/abs/2507.20589", "authors": ["Francisco J. Soler Mora", "Adrián Peidró Vidal", "Marc Fabregat-Jaén", "Luis Payá Castelló", "Óscar Reinoso García"], "title": "Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation", "comment": null, "summary": "Reticular structures form the backbone of major infrastructure like bridges,\npylons, and airports, but their inspection and maintenance are costly and\nhazardous, often requiring human intervention. While prior research has focused\non fault detection via images or robotic platform design, the autonomous\nnavigation of robots within these structures is less explored. This study\naddresses that gap by proposing methods to detect navigable surfaces in truss\nstructures, enhancing the autonomy of climbing robots. The paper introduces\nseveral approaches for binary segmentation of navigable surfaces versus\nbackground from 3D point clouds of metallic trusses. These methods fall into\ntwo categories: analytical algorithms and deep learning models. The analytical\napproach features a custom algorithm that segments structures by analyzing the\neigendecomposition of planar patches in the point cloud. In parallel, advanced\ndeep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3\nare trained and evaluated for the same task. Comparative analysis shows that\nthe analytical algorithm offers easier parameter tuning and performance\ncomparable to deep learning models, which, while more computationally\nintensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves\na Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the\npromise of both analytical and deep learning methods for improving autonomous\nnavigation in complex truss environments. The results highlight the trade-offs\nbetween computational efficiency and segmentation performance, providing\nvaluable guidance for future research and practical applications in autonomous\ninfrastructure inspection and maintenance.", "AI": {"tldr": "论文提出两种方法（分析算法和深度学习模型）用于桁架结构中可导航表面的分割，比较了它们的性能与计算效率。", "motivation": "桁架结构的检查和维护成本高且危险，现有研究多关注故障检测或机器人设计，而自主导航研究较少。", "method": "采用分析算法和深度学习模型（如PointNet、PointTransformerV3）对3D点云进行可导航表面分割。", "result": "分析算法参数调整简单且性能接近深度学习模型，而深度学习模型（如PointTransformerV3）在分割精度上更优（mIoU达97%）。", "conclusion": "研究表明两种方法均能提升桁架环境中的自主导航能力，为未来研究和实际应用提供了权衡指导。"}}
{"id": "2507.20622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20622", "abs": "https://arxiv.org/abs/2507.20622", "authors": ["Guangyan Chen", "Meiling Wang", "Te Cui", "Yao Mu", "Haoyang Lu", "Zicai Peng", "Mengxiao Hu", "Tianxing Zhou", "Mengyin Fu", "Yi Yang", "Yufeng Yue"], "title": "FMimic: Foundation Models are Fine-grained Action Learners from Human Videos", "comment": "accepted to International Journal of Robotics Research(IJRR)", "summary": "Visual imitation learning (VIL) provides an efficient and intuitive strategy\nfor robotic systems to acquire novel skills. Recent advancements in foundation\nmodels, particularly Vision Language Models (VLMs), have demonstrated\nremarkable capabilities in visual and linguistic reasoning for VIL tasks.\nDespite this progress, existing approaches primarily utilize these models for\nlearning high-level plans from human demonstrations, relying on pre-defined\nmotion primitives for executing physical interactions, which remains a major\nbottleneck for robotic systems. In this work, we present FMimic, a novel\nparadigm that harnesses foundation models to directly learn generalizable\nskills at even fine-grained action levels, using only a limited number of human\nvideos. Extensive experiments demonstrate that our FMimic delivers strong\nperformance with a single human video, and significantly outperforms all other\nmethods with five videos. Furthermore, our method exhibits significant\nimprovements of over 39% and 29% in RLBench multi-task experiments and\nreal-world manipulation tasks, respectively, and exceeds baselines by more than\n34% in high-precision tasks and 47% in long-horizon tasks.", "AI": {"tldr": "FMimic利用基础模型直接从少量人类视频中学习可泛化的精细动作技能，显著优于现有方法。", "motivation": "现有方法依赖预定义动作基元执行物理交互，限制了机器人系统的灵活性。", "method": "提出FMimic，利用基础模型直接从人类视频中学习精细动作技能。", "result": "FMimic在单视频和五视频条件下均表现优异，在RLBench多任务和真实世界任务中分别提升39%和29%，在高精度和长时任务中分别超过基线34%和47%。", "conclusion": "FMimic为视觉模仿学习提供了一种高效且泛化能力强的解决方案。"}}
{"id": "2507.20784", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20784", "abs": "https://arxiv.org/abs/2507.20784", "authors": ["Mohamed Sorour", "Mohamed Heshmat", "Khaled Elgeneidy", "Pål Johan From"], "title": "A Strawberry Harvesting Tool with Minimal Footprint", "comment": null, "summary": "In this paper, a novel prototype for harvesting table-top grown strawberries\nis presented, that is minimalist in its footprint interacting with the fruit.\nIn our methodology, a smooth trapper manipulates the stem into a precise groove\nlocation at which a distant laser beam is focused. The tool reaches\ntemperatures as high as 188{\\deg} Celsius and as such killing germs and\npreventing the spread of local plant diseases. The burnt stem wound preserves\nwater content and in turn the fruit shelf life. Cycle and cut times achieved\nare 5.56 and 2.88 seconds respectively in successful in-door harvesting\ndemonstration. Extensive experiments are performed to optimize the laser spot\ndiameter and lateral speed against the cutting time.", "AI": {"tldr": "提出了一种新型草莓采摘原型，通过激光切割茎部，提高采摘效率并延长果实保鲜期。", "motivation": "解决传统草莓采摘方法效率低、易传播病害的问题。", "method": "使用平滑捕捉器将茎部引导至精确位置，通过高温激光切割茎部，杀菌并防止病害传播。", "result": "实验显示切割时间为2.88秒，采摘周期为5.56秒，有效延长了草莓的保鲜期。", "conclusion": "该方法高效、卫生，适用于室内草莓采摘，具有实际应用潜力。"}}
{"id": "2507.20800", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20800", "abs": "https://arxiv.org/abs/2507.20800", "authors": ["Vinil Polepalli"], "title": "LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations", "comment": null, "summary": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes.", "AI": {"tldr": "LanternNet是一种新型自主机器人系统，用于检测和抑制斑点灯笼蝇（SLF），相比传统方法更高效、经济且环保。", "motivation": "斑点灯笼蝇对农业和生态系统造成严重威胁，现有控制方法效率低且环境危害大。", "method": "LanternNet采用中心-辐条式机器人系统，利用YOLOv8模型识别SLF，并通过三种机器人执行灭虫、环境监测和导航任务。", "result": "实地测试显示SLF数量显著减少（p < 0.01），树木健康指标改善，且成本更低、扩展性更强。", "conclusion": "LanternNet展示了机器人与AI结合在入侵物种管理中的潜力，具有广泛生态应用前景。"}}
{"id": "2507.20832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20832", "abs": "https://arxiv.org/abs/2507.20832", "authors": ["Mihai Pomarlan", "Stefano De Giorgis", "Rachel Ringe", "Maria M. Hedblom", "Nikolaos Tsiogkas"], "title": "Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics", "comment": "This article is published online with Open Access by IOS Press and\n  distributed under the terms of the Creative Commons Attribution\n  Non-Commercial License 4.0 (CC BY-NC 4.0)", "summary": "Situationally-aware artificial agents operating with competence in natural\nenvironments face several challenges: spatial awareness, object affordance\ndetection, dynamic changes and unpredictability. A critical challenge is the\nagent's ability to identify and monitor environmental elements pertinent to its\nobjectives. Our research introduces a neurosymbolic modular architecture for\nreactive robotics. Our system combines a neural component performing object\nrecognition over the environment and image processing techniques such as\noptical flow, with symbolic representation and reasoning. The reasoning system\nis grounded in the embodied cognition paradigm, via integrating image schematic\nknowledge in an ontological structure. The ontology is operatively used to\ncreate queries for the perception system, decide on actions, and infer\nentities' capabilities derived from perceptual data. The combination of\nreasoning and image processing allows the agent to focus its perception for\nnormal operation as well as discover new concepts for parts of objects involved\nin particular interactions. The discovered concepts allow the robot to\nautonomously acquire training data and adjust its subsymbolic perception to\nrecognize the parts, as well as making planning for more complex tasks feasible\nby focusing search on those relevant object parts. We demonstrate our approach\nin a simulated world, in which an agent learns to recognize parts of objects\ninvolved in support relations. While the agent has no concept of handle\ninitially, by observing examples of supported objects hanging from a hook it\nlearns to recognize the parts involved in establishing support and becomes able\nto plan the establishment/destruction of the support relation. This underscores\nthe agent's capability to expand its knowledge through observation in a\nsystematic way, and illustrates the potential of combining deep reasoning\n[...].", "AI": {"tldr": "论文提出了一种神经符号模块化架构，用于反应式机器人，结合神经组件和符号推理，使机器人能够识别环境元素并自主扩展知识。", "motivation": "解决人工代理在自然环境中操作时面临的挑战，如空间感知、对象功能检测和动态变化，特别是如何识别和监控与目标相关的环境元素。", "method": "结合神经网络（对象识别和图像处理）与符号表示和推理，通过本体结构整合图像模式知识，用于感知查询、动作决策和推断实体能力。", "result": "在模拟环境中，代理能够学习识别对象部分（如支撑关系中的部分），并自主扩展知识，例如识别把手并规划支撑关系的建立/破坏。", "conclusion": "展示了神经符号结合方法的潜力，使代理能够通过观察系统性地扩展知识，为复杂任务规划提供支持。"}}
{"id": "2507.20850", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20850", "abs": "https://arxiv.org/abs/2507.20850", "authors": ["Meiting Dang", "Yanping Wu", "Yafei Wang", "Dezong Zhao", "David Flynn", "Chongfeng Wei"], "title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments", "comment": "14 pages, 5 figures", "summary": "Recent advances in autonomous vehicle (AV) behavior planning have shown\nimpressive social interaction capabilities when interacting with other road\nusers. However, achieving human-like prediction and decision-making in\ninteractions with vulnerable road users remains a key challenge in complex\nmulti-agent interactive environments. Existing research focuses primarily on\ncrowd navigation for small mobile robots, which cannot be directly applied to\nAVs due to inherent differences in their decision-making strategies and dynamic\nboundaries. Moreover, pedestrians in these multi-agent simulations follow fixed\nbehavior patterns that cannot dynamically respond to AV actions. To overcome\nthese limitations, this paper proposes a novel framework for modeling\ninteractions between the AV and multiple pedestrians. In this framework, a\ncognitive process modeling approach inspired by the Free Energy Principle is\nintegrated into both the AV and pedestrian models to simulate more realistic\ninteraction dynamics. Specifically, the proposed pedestrian Cognitive-Risk\nSocial Force Model adjusts goal-directed and repulsive forces using a fused\nmeasure of cognitive uncertainty and physical risk to produce human-like\ntrajectories. Meanwhile, the AV leverages this fused risk to construct a\ndynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a\nSoft Actor-Critic architecture, allowing it to make more reasonable and\ninformed decisions. Simulation results indicate that our proposed framework\neffectively improves safety, efficiency, and smoothness of AV navigation\ncompared to the state-of-the-art method.", "AI": {"tldr": "提出了一种新框架，结合认知过程建模和风险感知，改进自动驾驶车辆与行人的交互。", "motivation": "解决自动驾驶车辆在复杂多代理环境中与弱势道路使用者（如行人）交互时预测和决策的挑战。", "method": "采用基于自由能原理的认知过程建模，结合认知风险社会力模型和图卷积网络，优化交互动态。", "result": "仿真结果表明，框架在安全性、效率和导航流畅性上优于现有方法。", "conclusion": "新框架为自动驾驶车辆与行人的交互提供了更真实和高效的解决方案。"}}
{"id": "2507.20861", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20861", "abs": "https://arxiv.org/abs/2507.20861", "authors": ["Marco Faroni", "Carlo Odesco", "Andrea Zanchettin", "Paolo Rocco"], "title": "Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling", "comment": "Accepted at IEEE/RSJ IROS 2025", "summary": "Physics-based simulations and learning-based models are vital for complex\nrobotics tasks like deformable object manipulation and liquid handling.\nHowever, these models often struggle with accuracy due to epistemic uncertainty\nor the sim-to-real gap. For instance, accurately pouring liquid from one\ncontainer to another poses challenges, particularly when models are trained on\nlimited demonstrations and may perform poorly in novel situations. This paper\nproposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed\nto mitigate these inaccuracies. By incorporating estimates of model\nuncertainty, the proposed MCTS strategy biases the search towards actions with\nlower predicted uncertainty. This approach enhances the reliability of planning\nunder uncertain conditions. Applied to a liquid pouring task, our method\ndemonstrates improved success rates even with models trained on minimal data,\noutperforming traditional methods and showcasing its potential for robust\ndecision-making in robotics.", "AI": {"tldr": "论文提出了一种基于不确定性感知的蒙特卡洛树搜索算法，用于提升复杂机器人任务（如液体倾倒）的可靠性。", "motivation": "物理仿真和学习模型在复杂机器人任务中常因认知不确定性或仿真与现实的差距而表现不佳，尤其是在数据有限的情况下。", "method": "通过结合模型不确定性估计，改进蒙特卡洛树搜索（MCTS），使其偏向预测不确定性较低的动作。", "result": "在液体倾倒任务中，该方法在数据有限的情况下仍表现出更高的成功率，优于传统方法。", "conclusion": "该方法为机器人决策提供了更鲁棒的解决方案，尤其在不确定性高的任务中。"}}
{"id": "2507.20870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20870", "abs": "https://arxiv.org/abs/2507.20870", "authors": ["Elena Merlo", "Marta Lagomarsino", "Arash Ajoudani"], "title": "A Human-in-the-loop Approach to Robot Action Replanning through LLM Common-Sense Reasoning", "comment": null, "summary": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness.", "AI": {"tldr": "论文提出了一种结合自然语言输入和大语言模型（LLM）的方法，用于改进基于单次RGB视频生成的机器人执行计划，以提高其适应性和鲁棒性。", "motivation": "为了推广机器人技术的应用，需要为非专家提供更易用的编程工具。单纯依赖视觉输入的观察学习在可扩展性和错误缓解方面效率不足。", "method": "通过人类参与的闭环方法，利用自然语言输入和LLM的常识推理，调整基于视觉生成的计划，避免潜在错误并根据指令适应任务。", "result": "实验证明该方法能直观有效地纠正视觉生成的错误并调整计划，无需额外演示。交互式计划优化和幻觉修正提升了系统鲁棒性。", "conclusion": "该方法为非专家提供了一种直观且高效的机器人编程工具，通过结合自然语言和LLM增强了系统的适应性和鲁棒性。"}}
{"id": "2507.20892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20892", "abs": "https://arxiv.org/abs/2507.20892", "authors": ["Sergey Bakulin", "Timur Akhtyamov", "Denis Fatykhov", "German Devchich", "Gonzalo Ferrer"], "title": "PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs", "comment": null, "summary": "This work proposes a novel hybrid approach for vision-only navigation of\nmobile robots, which combines advances of both deep learning approaches and\nclassical model-based planning algorithms. Today, purely data-driven end-to-end\nmodels are dominant solutions to this problem. Despite advantages such as\nflexibility and adaptability, the requirement of a large amount of training\ndata and limited interpretability are the main bottlenecks for their practical\napplications. To address these limitations, we propose a hierarchical system\nthat utilizes recent advances in model predictive control, traversability\nestimation, visual place recognition, and pose estimation, employing\ntopological graphs as a representation of the target environment. Using such a\ncombination, we provide a scalable system with a higher level of\ninterpretability compared to end-to-end approaches. Extensive real-world\nexperiments show the efficiency of the proposed method.", "AI": {"tldr": "提出了一种结合深度学习和经典模型规划算法的混合方法，用于移动机器人的视觉导航，解决了纯数据驱动方法的训练数据需求和可解释性问题。", "motivation": "纯数据驱动的端到端模型虽然灵活，但需要大量训练数据且可解释性差，限制了实际应用。", "method": "采用分层系统，结合模型预测控制、可通行性估计、视觉地点识别和位姿估计，使用拓扑图表示目标环境。", "result": "实验证明该方法高效且比端到端方法更具可解释性。", "conclusion": "提出的混合方法在视觉导航中具有实际应用潜力。"}}
