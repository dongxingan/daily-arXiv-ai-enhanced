{"id": "2507.18808", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18808", "abs": "https://arxiv.org/abs/2507.18808", "authors": ["Miguel Saavedra-Ruiz", "Samer B. Nashed", "Charlie Gauthier", "Liam Paull"], "title": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025) Code available at\n  https://github.com/montrealrobotics/perpetua-code. Webpage and additional\n  videos at https://montrealrobotics.ca/perpetua/", "summary": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations.", "AI": {"tldr": "Perpetua是一种用于建模半静态特征动态的方法，能够整合先验知识、跟踪多假设并预测未来特征状态。", "motivation": "复杂动态环境中，机器人系统需要处理环境变化，而现有方法无法有效预测动态特征的未来状态。", "method": "通过链式混合“持久性”和“涌现性”滤波器，在贝叶斯框架中建模特征消失或重现的概率。", "result": "实验表明，Perpetua在模拟和真实数据中比类似方法更准确，且具有在线适应性和鲁棒性。", "conclusion": "Perpetua是一种高效、可扩展且通用的方法，适用于动态环境中的特征状态预测。"}}
{"id": "2507.18819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18819", "abs": "https://arxiv.org/abs/2507.18819", "authors": ["Trent Weiss", "Madhur Behl"], "title": "Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Overtaking in high-speed autonomous racing demands precise, real-time\nestimation of collision risk; particularly in wheel-to-wheel scenarios where\nsafety margins are minimal. Existing methods for collision risk estimation\neither rely on simplified geometric approximations, like bounding circles, or\nperform Monte Carlo sampling which leads to overly conservative motion planning\nbehavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)\nalgorithm, a principled two-stage integration method that estimates collision\nrisk by combining Gauss-Legendre with a non-homogeneous Poisson process over\ntime. GLR produces accurate risk estimates that account for vehicle geometry\nand trajectory uncertainty. In experiments across 446 overtaking scenarios in a\nhigh-fidelity Formula One racing simulation, GLR outperforms five\nstate-of-the-art baselines achieving an average error reduction of 77% and\nsurpassing the next-best method by 52%, all while running at 1000 Hz. The\nframework is general and applicable to broader motion planning contexts beyond\nautonomous racing.", "AI": {"tldr": "论文提出了一种名为Gauss-Legendre Rectangle (GLR)的算法，用于高速自动驾驶赛车中的碰撞风险估计，显著优于现有方法。", "motivation": "现有碰撞风险估计方法在高速赛车场景中过于保守或简化，无法满足精确性和实时性需求。", "method": "GLR算法结合Gauss-Legendre积分和非齐次泊松过程，分两阶段估计碰撞风险，考虑车辆几何和轨迹不确定性。", "result": "在446个超车场景实验中，GLR平均误差减少77%，优于五种现有方法，且运行频率达1000 Hz。", "conclusion": "GLR算法在高速赛车场景中表现出色，且具有通用性，可应用于更广泛的运动规划领域。"}}
{"id": "2507.18820", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18820", "abs": "https://arxiv.org/abs/2507.18820", "authors": ["Rachel Ringe", "Robin Nolte", "Nima Zargham", "Robert Porzel", "Rainer Malaka"], "title": "MetaMorph -- A Metamodelling Approach For Robot Morphology", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Robot appearance crucially shapes Human-Robot Interaction (HRI) but is\ntypically described via broad categories like anthropomorphic, zoomorphic, or\ntechnical. More precise approaches focus almost exclusively on anthropomorphic\nfeatures, which fail to classify robots across all types, limiting the ability\nto draw meaningful connections between robot design and its effect on\ninteraction. In response, we present MetaMorph, a comprehensive framework for\nclassifying robot morphology. Using a metamodeling approach, MetaMorph was\nsynthesized from 222 robots in the IEEE Robots Guide, offering a structured\nmethod for comparing visual features. This model allows researchers to assess\nthe visual distances between robot models and explore optimal design traits\ntailored to different tasks and contexts.", "AI": {"tldr": "MetaMorph框架通过元建模方法，从222个机器人中提取形态特征，提供了一种全面的机器人形态分类方法，弥补了现有分类方法的不足。", "motivation": "现有机器人外观分类方法过于宽泛或局限于拟人化特征，无法全面分类所有类型的机器人，限制了设计与交互效果的研究。", "method": "采用元建模方法，基于IEEE Robots Guide中的222个机器人数据，构建了MetaMorph框架，用于系统比较机器人视觉特征。", "result": "MetaMorph能够量化机器人模型之间的视觉差异，并探索适合不同任务和场景的设计特征。", "conclusion": "MetaMorph为机器人形态分类提供了更全面的工具，有助于优化机器人设计与交互效果的研究。"}}
{"id": "2507.18847", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18847", "abs": "https://arxiv.org/abs/2507.18847", "authors": ["Pinhao Song", "Yutong Hu", "Pengteng Li", "Renaud Detry"], "title": "Equivariant Volumetric Grasping", "comment": "19 pages", "summary": "We propose a new volumetric grasp model that is equivariant to rotations\naround the vertical axis, leading to a significant improvement in sample\nefficiency. Our model employs a tri-plane volumetric feature representation --\ni.e., the projection of 3D features onto three canonical planes. We introduce a\nnovel tri-plane feature design in which features on the horizontal plane are\nequivariant to 90{\\deg} rotations, while the sum of features from the other two\nplanes remains invariant to the same transformations. This design is enabled by\na new deformable steerable convolution, which combines the adaptability of\ndeformable convolutions with the rotational equivariance of steerable ones.\nThis allows the receptive field to adapt to local object geometry while\npreserving equivariance properties. We further develop equivariant adaptations\nof two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,\nwe derive a new equivariant formulation of IGD's deformable attention mechanism\nand propose an equivariant generative model of grasp orientations based on flow\nmatching. We provide a detailed analytical justification of the proposed\nequivariance properties and validate our approach through extensive simulated\nand real-world experiments. Our results demonstrate that the proposed\nprojection-based design significantly reduces both computational and memory\ncosts. Moreover, the equivariant grasp models built on top of our tri-plane\nfeatures consistently outperform their non-equivariant counterparts, achieving\nhigher performance with only a modest computational overhead. Video and code\ncan be viewed in: https://mousecpn.github.io/evg-page/", "AI": {"tldr": "提出了一种新的体积抓取模型，具有垂直轴旋转等变性，显著提高了样本效率。采用三平面体积特征表示，并引入新的特征设计，结合可变形和可转向卷积，保持等变性同时适应局部几何。实验验证了其高效性和优越性能。", "motivation": "提高抓取模型的样本效率和计算效率，同时保持对旋转的等变性。", "method": "使用三平面体积特征表示，结合可变形可转向卷积，开发等变性抓取规划器（GIGA和IGD）。", "result": "显著降低计算和内存成本，等变性模型性能优于非等变模型。", "conclusion": "提出的投影设计和等变性模型在效率和性能上均有显著优势。"}}
{"id": "2507.19100", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19100", "abs": "https://arxiv.org/abs/2507.19100", "authors": ["Taewon Kang", "Ji-Wook Kwon", "Il Bae", "Jin Hyo Kim"], "title": "Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations", "comment": null, "summary": "Localization of mobile robots is crucial for deploying robots in real-world\napplications such as search and rescue missions. This work aims to develop an\naccurate localization system applicable to swarm robots equipped only with\nlow-cost monocular vision sensors and visual markers. The system is designed to\noperate in fully open spaces, without landmarks or support from positioning\ninfrastructures. To achieve this, we propose a localization method based on\nequilateral triangular formations. By leveraging the geometric properties of\nequilateral triangles, the accurate two-dimensional position of each\nparticipating robot is estimated using one-dimensional lateral distance\ninformation between robots, which can be reliably and accurately obtained with\na low-cost monocular vision sensor. Experimental and simulation results\ndemonstrate that, as travel time increases, the positioning error of the\nproposed method becomes significantly smaller than that of a conventional\ndead-reckoning system, another low-cost localization approach applicable to\nopen environments.", "AI": {"tldr": "提出了一种基于等边三角形构型的低成本单目视觉传感器定位方法，适用于开放空间中的群机器人定位。", "motivation": "移动机器人在现实应用（如搜救任务）中的定位至关重要，尤其是在缺乏基础设施支持的环境中。", "method": "利用等边三角形的几何特性，通过单目视觉传感器获取的一维横向距离信息估计机器人的二维位置。", "result": "实验和仿真表明，随着运行时间增加，该方法定位误差显著低于传统航位推算系统。", "conclusion": "该方法为低成本群机器人在开放空间中的定位提供了有效解决方案。"}}
{"id": "2507.18886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18886", "abs": "https://arxiv.org/abs/2507.18886", "authors": ["Zheng Yang", "Kuan Xu", "Shenghai Yuan", "Lihua Xie"], "title": "A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras", "comment": null, "summary": "In this paper, we introduce a novel approach for efficiently estimating the\n6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method\nthat capitalizes on overlapping planar elements. Conventional RGB-D visual\nodometry(RGBD-VO) often relies on iterative optimization solvers to estimate\npose and involves a process of feature extraction and matching. This results in\nsignificant computational burden and time delays. To address this, our\ninnovative method for RGBD-VO separates the estimation of rotation and\ntranslation. Initially, we exploit the overlaid planar characteristics within\nthe scene to calculate the rotation matrix. Following this, we utilize a kernel\ncross-correlator (KCC) to ascertain the translation. By sidestepping the\nresource-intensive iterative optimization and feature extraction and alignment\nprocedures, our methodology offers improved computational efficacy, achieving a\nperformance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on\nfeature points, our technique exhibits enhanced performance in low-texture\ndegenerative environments compared to state-of-the-art methods.", "AI": {"tldr": "提出了一种基于重叠平面元素的非迭代解耦方法，用于高效估计6自由度机器人位姿，避免了传统RGB-D视觉里程计的计算负担。", "motivation": "传统RGB-D视觉里程计依赖迭代优化和特征提取，计算量大且耗时，难以满足实时性需求。", "method": "通过分离旋转和平移估计，利用场景中的重叠平面特征计算旋转矩阵，再使用核互相关器确定平移。", "result": "方法在低端i5 CPU上达到71Hz的性能，在低纹理退化环境中优于现有技术。", "conclusion": "该方法显著提高了计算效率，适用于实时机器人位姿估计。"}}
{"id": "2507.18947", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18947", "abs": "https://arxiv.org/abs/2507.18947", "authors": ["Asad Ali Shahid", "Angelo Moroncelli", "Drazen Brscic", "Takayuki Kanda", "Loris Roveda"], "title": "GEAR: Gaze-Enabled Human-Robot Collaborative Assembly", "comment": "Accepted for publication at 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "Recent progress in robot autonomy and safety has significantly improved\nhuman-robot interactions, enabling robots to work alongside humans on various\ntasks. However, complex assembly tasks still present significant challenges due\nto inherent task variability and the need for precise operations. This work\nexplores deploying robots in an assistive role for such tasks, where the robot\nassists by fetching parts while the skilled worker provides high-level guidance\nand performs the assembly. We introduce GEAR, a gaze-enabled system designed to\nenhance human-robot collaboration by allowing robots to respond to the user's\ngaze. We evaluate GEAR against a touch-based interface where users interact\nwith the robot through a touchscreen. The experimental study involved 30\nparticipants working on two distinct assembly scenarios of varying complexity.\nResults demonstrated that GEAR enabled participants to accomplish the assembly\nwith reduced physical demand and effort compared to the touchscreen interface,\nespecially for complex tasks, maintaining great performance, and receiving\nobjects effectively. Participants also reported enhanced user experience while\nperforming assembly tasks. Project page: sites.google.com/view/gear-hri", "AI": {"tldr": "GEAR是一种基于视线追踪的系统，旨在通过机器人对用户视线的响应来增强人机协作。与触摸屏界面相比，GEAR在复杂任务中减少了用户的体力需求和努力，提升了用户体验。", "motivation": "复杂装配任务由于任务多变性和精确操作需求仍具挑战性，研究探索机器人辅助角色以优化人机协作。", "method": "引入GEAR系统，通过视线追踪技术让机器人响应用户视线，并与触摸屏界面进行对比实验。", "result": "实验显示，GEAR在复杂任务中显著减少用户体力需求，保持高效性能，并提升用户体验。", "conclusion": "GEAR系统在复杂装配任务中有效优化人机协作，减少用户负担，提升效率与体验。"}}
{"id": "2507.18979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18979", "abs": "https://arxiv.org/abs/2507.18979", "authors": ["Deokjin Lee", "Junho Song", "Alireza Karimi", "Sehoon Oh"], "title": "Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots", "comment": null, "summary": "Motion control of flexible joint robots (FJR) is challenged by inherent\nflexibility and configuration-dependent variations in system dynamics. While\ndisturbance observers (DOB) can enhance system robustness, their performance is\noften limited by the elasticity of the joints and the variations in system\nparameters, which leads to a conservative design of the DOB. This paper\npresents a novel frequency response function (FRF)-based optimization method\naimed at improving DOB performance, even in the presence of flexibility and\nsystem variability. The proposed method maximizes control bandwidth and\neffectively suppresses vibrations, thus enhancing overall system performance.\nClosed-loop stability is rigorously proven using the Nyquist stability\ncriterion. Experimental validation on a FJR demonstrates that the proposed\napproach significantly improves robustness and motion performance, even under\nconditions of joint flexibility and system variation.", "AI": {"tldr": "提出了一种基于频率响应函数（FRF）的优化方法，用于提升柔性关节机器人（FJR）的干扰观测器（DOB）性能。", "motivation": "柔性关节机器人（FJR）的动态特性受关节弹性和系统参数变化影响，传统DOB设计保守且性能受限。", "method": "采用FRF优化方法，最大化控制带宽并抑制振动，同时通过Nyquist稳定性准则确保闭环稳定性。", "result": "实验验证表明，该方法显著提升了系统鲁棒性和运动性能，尤其在关节弹性和系统变化条件下。", "conclusion": "FRF优化方法有效解决了DOB在FJR中的性能限制，为柔性关节控制提供了新思路。"}}
{"id": "2507.19079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19079", "abs": "https://arxiv.org/abs/2507.19079", "authors": ["Feng Zhu", "Zihang Zhang", "Kangcheng Teng", "Abduhelil Yakup", "Xiaohong Zhang"], "title": "SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research", "comment": null, "summary": "High-precision navigation and positioning systems are critical for\napplications in autonomous vehicles and mobile mapping, where robust and\ncontinuous localization is essential. To test and enhance the performance of\nalgorithms, some research institutions and companies have successively\nconstructed and publicly released datasets. However, existing datasets still\nsuffer from limitations in sensor diversity and environmental coverage. To\naddress these shortcomings and advance development in related fields, the\nSmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset\nhas been developed. This dataset integrates data from multiple sensors,\nincluding Global Navigation Satellite Systems (GNSS), Inertial Measurement\nUnits (IMU), optical cameras, and LiDAR, to provide a rich and versatile\nresource for research in multi-sensor fusion and high-precision navigation. The\ndataset construction process is thoroughly documented, encompassing sensor\nconfigurations, coordinate system definitions, and calibration procedures for\nboth cameras and LiDAR. A standardized framework for data collection and\nprocessing ensures consistency and scalability, enabling large-scale analysis.\nValidation using state-of-the-art Simultaneous Localization and Mapping (SLAM)\nalgorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's\napplicability for advanced navigation research. Covering a wide range of\nreal-world scenarios, including urban areas, campuses, tunnels, and suburban\nenvironments, the dataset offers a valuable tool for advancing navigation\ntechnologies and addressing challenges in complex environments. By providing a\npublicly accessible, high-quality dataset, this work aims to bridge gaps in\nsensor diversity, data accessibility, and environmental representation,\nfostering further innovation in the field.", "AI": {"tldr": "该论文介绍了SmartPNT多源集成导航、定位和姿态数据集，旨在解决现有数据集在传感器多样性和环境覆盖方面的不足，为多传感器融合和高精度导航研究提供丰富资源。", "motivation": "现有数据集在传感器多样性和环境覆盖方面存在局限，阻碍了高精度导航和定位系统的研究进展。", "method": "数据集整合了GNSS、IMU、光学相机和LiDAR等多传感器数据，并详细记录了传感器配置、坐标系定义和校准过程。采用标准化框架确保数据一致性和可扩展性。", "result": "通过VINS-Mono和LIO-SAM等SLAM算法验证，数据集适用于复杂环境下的高级导航研究。", "conclusion": "该数据集填补了传感器多样性、数据可访问性和环境代表性方面的空白，推动了导航技术的创新。"}}
{"id": "2507.19082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19082", "abs": "https://arxiv.org/abs/2507.19082", "authors": ["Rachel Ringe", "Leandra Thiele", "Mihai Pomarlan", "Nima Zargham", "Robin Nolte", "Lars Hurrelbrink", "Rainer Malaka"], "title": "Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This study explores which factors of the visual design of a robot may\ninfluence how humans would place it in a collaborative cooking scenario and how\nthese features may influence task delegation. Human participants were placed in\na Virtual Reality (VR) environment and asked to set up a kitchen for cooking\nalongside a robot companion while considering the robot's morphology. We\ncollected multimodal data for the arrangements created by the participants,\ntranscripts of their think-aloud as they were performing the task, and\ntranscripts of their answers to structured post-task questionnaires. Based on\nanalyzing this data, we formulate several hypotheses: humans prefer to\ncollaborate with biomorphic robots; human beliefs about the sensory\ncapabilities of robots are less influenced by the morphology of the robot than\nbeliefs about action capabilities; and humans will implement fewer avoidance\nstrategies when sharing space with gracile robots. We intend to verify these\nhypotheses in follow-up studies.", "AI": {"tldr": "研究探讨机器人视觉设计如何影响人类在协作烹饪场景中的布局及任务分配，发现人类偏爱生物形态机器人，且对机器人感官能力的信念受形态影响较小。", "motivation": "探索机器人视觉设计对人类协作行为和任务分配的影响，以优化人机协作体验。", "method": "在VR环境中，参与者与不同形态的机器人协作布置厨房，收集布局数据、口头反馈及问卷回答。", "result": "人类更倾向与生物形态机器人协作，对机器人感官能力的信念受形态影响较小，且对纤细机器人采取更少的避让策略。", "conclusion": "研究提出假设需后续验证，为机器人设计提供潜在优化方向。"}}
{"id": "2507.19146", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19146", "abs": "https://arxiv.org/abs/2507.19146", "authors": ["Ahmed Abouelazm", "Johannes Ratz", "Philip Schörner", "J. Marius Zöllner"], "title": "Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL", "comment": "Paper accepted in IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Autonomous driving faces challenges in navigating complex real-world traffic,\nrequiring safe handling of both common and critical scenarios. Reinforcement\nlearning (RL), a prominent method in end-to-end driving, enables agents to\nlearn through trial and error in simulation. However, RL training often relies\non rule-based traffic scenarios, limiting generalization. Additionally, current\nscenario generation methods focus heavily on critical scenarios, neglecting a\nbalance with routine driving behaviors. Curriculum learning, which\nprogressively trains agents on increasingly complex tasks, is a promising\napproach to improving the robustness and coverage of RL driving policies.\nHowever, existing research mainly emphasizes manually designed curricula,\nfocusing on scenery and actor placement rather than traffic behavior dynamics.\nThis work introduces a novel student-teacher framework for automatic curriculum\nlearning. The teacher, a graph-based multi-agent RL component, adaptively\ngenerates traffic behaviors across diverse difficulty levels. An adaptive\nmechanism adjusts task difficulty based on student performance, ensuring\nexposure to behaviors ranging from common to critical. The student, though\nexchangeable, is realized as a deep RL agent with partial observability,\nreflecting real-world perception constraints. Results demonstrate the teacher's\nability to generate diverse traffic behaviors. The student, trained with\nautomatic curricula, outperformed agents trained on rule-based traffic,\nachieving higher rewards and exhibiting balanced, assertive driving.", "AI": {"tldr": "论文提出了一种基于学生-教师框架的自动课程学习方法，用于提升自动驾驶强化学习策略的鲁棒性和覆盖范围。", "motivation": "自动驾驶在复杂交通中面临挑战，现有强化学习方法依赖规则化场景生成，缺乏对常规与关键场景的平衡处理。", "method": "采用图基多智能体强化学习教师模型，自适应生成不同难度交通行为，结合学生模型（深度RL）进行训练。", "result": "教师模型能生成多样化交通行为，学生模型在自动课程训练下表现优于基于规则的方法，驾驶更平衡和自信。", "conclusion": "自动课程学习框架有效提升了自动驾驶策略的泛化能力和性能。"}}
{"id": "2507.19151", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.19151", "abs": "https://arxiv.org/abs/2507.19151", "authors": ["Michael Amir", "Guang Yang", "Zhan Gao", "Keisuke Okumura", "Heedo Woo", "Amanda Prorok"], "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination", "comment": null, "summary": "Constraint-based optimization is a cornerstone of robotics, enabling the\ndesign of controllers that reliably encode task and safety requirements such as\ncollision avoidance or formation adherence. However, handcrafted constraints\ncan fail in multi-agent settings that demand complex coordination. We introduce\nReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid\nframework that merges the reliability of optimization-based controllers with\nthe adaptability of multi-agent reinforcement learning. Rather than discarding\nexpert controllers, ReCoDe improves them by learning additional, dynamic\nconstraints that capture subtler behaviors, for example, by constraining agent\nmovements to prevent congestion in cluttered scenarios. Through local\ncommunication, agents collectively constrain their allowed actions to\ncoordinate more effectively under changing conditions. In this work, we focus\non applications of ReCoDe to multi-agent navigation tasks requiring intricate,\ncontext-based movements and consensus, where we show that it outperforms purely\nhandcrafted controllers, other hybrid approaches, and standard MARL baselines.\nWe give empirical (real robot) and theoretical evidence that retaining a\nuser-defined controller, even when it is imperfect, is more efficient than\nlearning from scratch, especially because ReCoDe can dynamically change the\ndegree to which it relies on this controller.", "AI": {"tldr": "ReCoDe是一个结合优化控制器和多智能体强化学习的混合框架，通过学习动态约束提升专家控制器的性能，适用于复杂协调任务。", "motivation": "手工设计的约束在多智能体环境中可能失效，需要更灵活的方法来适应复杂协调需求。", "method": "ReCoDe通过局部通信学习动态约束，改进专家控制器，实现更有效的协调。", "result": "在需要复杂导航和共识的任务中，ReCoDe优于手工控制器、其他混合方法和标准MARL基线。", "conclusion": "保留用户定义的控制器并结合动态约束学习，比从头学习更高效，且适应性更强。"}}
{"id": "2507.19196", "categories": ["cs.RO", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19196", "abs": "https://arxiv.org/abs/2507.19196", "authors": ["Ruben Janssens", "Tony Belpaeme"], "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "comment": "Submitted to the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices.", "AI": {"tldr": "论文探讨了如何利用视觉语言模型提升社交机器人的多模态交互能力，以弥补当前大型语言模型在社交对话中的不足。", "motivation": "当前大型语言模型虽能支持开放域对话，但缺乏多模态交互能力，限制了社交机器人的表现。", "method": "提出利用视觉语言模型处理广泛的视觉信息，并探讨了其适应社交机器人场景的方法与技术挑战。", "result": "视觉语言模型能够为自主社交机器人提供足够通用的多模态交互能力。", "conclusion": "视觉语言模型是提升社交机器人多模态交互能力的有前景方向，但仍需解决技术挑战和评估问题。"}}
{"id": "2507.19242", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19242", "abs": "https://arxiv.org/abs/2507.19242", "authors": ["Kang Xiangli", "Yage He", "Xianwu Gong", "Zehan Liu", "Yuru Bai"], "title": "Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation", "comment": null, "summary": "This study presents a grasping method for objects with uneven mass\ndistribution by leveraging diffusion models to localize the center of gravity\n(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to\npostural instability, where existing keypoint-based or affordance-driven\nmethods exhibit limitations. We constructed a dataset of 790 images featuring\nunevenly distributed objects with keypoint annotations for CoG localization. A\nvision-driven framework based on foundation models was developed to achieve\nCoG-aware grasping. Experimental evaluations across real-world scenarios\ndemonstrate that our method achieves a 49\\% higher success rate compared to\nconventional keypoint-based approaches and an 11\\% improvement over\nstate-of-the-art affordance-driven methods. The system exhibits strong\ngeneralization with a 76\\% CoG localization accuracy on unseen objects,\nproviding a novel solution for precise and stable grasping tasks.", "AI": {"tldr": "该研究提出了一种利用扩散模型定位未知物体重心的抓取方法，解决了现有方法在质量分布不均物体上的局限性。", "motivation": "在机器人抓取中，重心偏差常导致姿态不稳定，而现有的关键点或功能驱动方法存在不足。", "method": "构建了790张质量分布不均物体的图像数据集，并开发了基于基础模型的视觉驱动框架以实现重心感知抓取。", "result": "实验表明，该方法比传统关键点方法成功率提高49%，比最新功能驱动方法提高11%，且在未见物体上重心定位准确率达76%。", "conclusion": "该方法为精确稳定的抓取任务提供了新颖解决方案。"}}
{"id": "2507.19335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19335", "abs": "https://arxiv.org/abs/2507.19335", "authors": ["Ilaria Consoli", "Claudio Mattutino", "Cristina Gena", "Berardina de Carolis", "Giuseppe Palestra"], "title": "How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version", "comment": null, "summary": "This paper presents an empirical study investigating how individuals across\ndifferent age groups, children, young and older adults, interpret emotional\nbody language expressed by the humanoid robot NAO. The aim is to offer insights\ninto how users perceive and respond to emotional cues from robotic agents,\nthrough an empirical evaluation of the robot's effectiveness in conveying\nemotions to different groups of users. By analyzing data collected from elderly\nparticipants and comparing these findings with previously gathered data from\nyoung adults and children, the study highlights similarities and differences\nbetween the groups, with younger and older users more similar but different\nfrom young adults.", "AI": {"tldr": "研究不同年龄段（儿童、年轻人和老年人）如何理解人形机器人NAO表达的情感肢体语言，揭示用户对机器人情感线索的感知差异。", "motivation": "探讨用户如何感知和响应机器人情感线索，评估机器人在不同年龄段用户中传达情感的效果。", "method": "通过分析老年参与者数据，并与年轻人和儿童数据对比，研究各年龄组的异同。", "result": "年轻和老年用户相似，但与年轻人存在差异。", "conclusion": "研究为机器人情感表达设计提供了跨年龄段的参考。"}}
