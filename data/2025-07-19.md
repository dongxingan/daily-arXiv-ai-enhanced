<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 本文提出了一种针对LiDAR仿真的新视角合成方法，通过显式建模滚动快门、激光功率变化和强度衰减等传感器特性，实现了比现有技术更准确的LiDAR仿真效果。


<details>
  <summary>Details</summary>
Motivation: 现有的新视角合成方法在LiDAR仿真和大规模3D场景重建中虽然有所应用，但对LiDAR特定效应的处理仍然不够充分。现有解决方案主要关注更快的渲染或处理动态场景，但缺乏对LiDAR传感器特有特性的精确建模。

Method: 通过显式建模LiDAR传感器的关键特性来改进新视角合成方法，包括：1）滚动快门效应建模；2）激光功率变化建模；3）强度衰减建模。这些传感器特性的精确建模使得LiDAR仿真更加真实和准确。

Result: 通过与最先进方法的定量和定性比较验证了方法的有效性，消融研究突出了每个传感器模型组件的重要性。该方法展现出先进的重仿真能力，能够在相机视角下生成高分辨率LiDAR扫描数据。

Conclusion: 该方法通过显式建模LiDAR传感器特性，显著提高了LiDAR仿真的准确性，相比现有技术取得了更好的效果。方法具备强大的重仿真能力，可以生成高质量的LiDAR数据，为LiDAR仿真领域提供了新的解决方案。

Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.

</details>


### [2] [FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making](https://arxiv.org/abs/2507.12496)
*Yucen Wang,Rui Yu,Shenghua Wan,Le Gan,De-Chuan Zhan*

Main category: cs.RO

TL;DR: FOUNDER框架结合基础模型（FMs）和世界模型（WMs）的优势，通过无奖励方式在具身环境中实现开放任务解决。


<details>
  <summary>Details</summary>
Motivation: 利用FMs的通用知识和WMs的动态建模能力，解决复杂观测或领域差异下的任务泛化问题。

Method: 学习一个映射函数，将FM表示嵌入WM状态空间，通过想象学习目标条件策略，并利用预测的时间距离作为奖励信号。

Result: 在多任务离线视觉控制基准测试中表现优异，尤其在复杂观测或领域差异场景中。

Conclusion: FOUNDER框架有效整合FMs和WMs，提升了任务泛化能力，并通过实验验证了奖励函数的准确性。

Abstract: Foundation Models (FMs) and World Models (WMs) offer complementary strengths
in task generalization at different levels. In this work, we propose FOUNDER, a
framework that integrates the generalizable knowledge embedded in FMs with the
dynamic modeling capabilities of WMs to enable open-ended task solving in
embodied environments in a reward-free manner. We learn a mapping function that
grounds FM representations in the WM state space, effectively inferring the
agent's physical states in the world simulator from external observations. This
mapping enables the learning of a goal-conditioned policy through imagination
during behavior learning, with the mapped task serving as the goal state. Our
method leverages the predicted temporal distance to the goal state as an
informative reward signal. FOUNDER demonstrates superior performance on various
multi-task offline visual control benchmarks, excelling in capturing the
deep-level semantics of tasks specified by text or videos, particularly in
scenarios involving complex observations or domain gaps where prior methods
struggle. The consistency of our learned reward function with the ground-truth
reward is also empirically validated. Our project website is
https://sites.google.com/view/founder-rl.

</details>


### [3] [ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving](https://arxiv.org/abs/2507.12499)
*Yuhang Lu,Jiadong Tu,Yuexin Ma,Xinge Zhu*

Main category: cs.RO

TL;DR: ReAL-AD框架通过结合人类认知模型和视觉语言模型，提升自动驾驶的决策层次性和可解释性，显著提高规划准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法依赖固定稀疏轨迹监督，无法模拟人类驾驶的分层推理过程。

Method: 提出ReAL-AD框架，包括战略推理注入器、战术推理整合器和分层轨迹解码器，结合视觉语言模型增强情境感知和结构化推理。

Result: 实验表明，该框架将规划准确性和安全性提升30%以上。

Conclusion: ReAL-AD使自动驾驶更符合人类分层推理，更具可解释性和适应性。

Abstract: End-to-end autonomous driving has emerged as a promising approach to unify
perception, prediction, and planning within a single framework, reducing
information loss and improving adaptability. However, existing methods often
rely on fixed and sparse trajectory supervision, limiting their ability to
capture the hierarchical reasoning process that human drivers naturally employ.
To bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning
framework that structures decision-making in autonomous driving based on the
three-tier human cognitive model: Driving Strategy, Driving Decision, and
Driving Operation, where Vision-Language Models (VLMs) are incorporated to
enhance situational awareness and structured reasoning across these levels.
Specifically, we introduce: (1) the Strategic Reasoning Injector, which
formulates high-level driving strategies by interpreting complex traffic
contexts from VLM-generated insights; (2) the Tactical Reasoning Integrator,
which refines strategic intent into interpretable tactical choices such as lane
changes, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory
Decoder, which progressively translates tactical decisions into precise control
actions for smooth and human-like trajectory execution. Extensive evaluations
show that integrating our framework improves planning accuracy and safety by
over 30%, making end-to-end autonomous driving more interpretable and aligned
with human-like hierarchical reasoning. The project page can be found at:
\href{https://4dvlab.github.io/project_page/realad}{\texttt{4dvlab.github.io/project\_page/realad}}

</details>


### [4] [VLMgineer: Vision Language Models as Robotic Toolsmiths](https://arxiv.org/abs/2507.12644)
*George Jiayuan Gao,Tianyu Li,Junyao Shi,Yihan Li,Zizhe Zhang,Nadia Figueroa,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: VLMgineer利用视觉语言模型和进化搜索共同设计工具和操作策略，显著提升任务解决效率和创新性。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型是否能通过工具设计和使用的智能来提升物理世界的理解和操作能力。

Method: 结合视觉语言模型的代码生成能力和进化搜索，迭代设计工具和操作策略。

Result: VLMgineer在多样化任务中表现优异，优于人工设计和现有工具。

Conclusion: VLMgineer为自动化工具发明提供了新方向，未来将公开基准和代码。

Abstract: Tool design and use reflect the ability to understand and manipulate the
physical world through creativity, planning, and foresight. As such, these
capabilities are often regarded as measurable indicators of intelligence across
biological species. While much of today's research on robotic intelligence
focuses on generating better controllers, inventing smarter tools offers a
complementary form of physical intelligence: shifting the onus of
problem-solving onto the tool's design. Given the vast and impressive
common-sense, reasoning, and creative capabilities of today's foundation
models, we investigate whether these models can provide useful priors to
automatically design and effectively wield such tools? We present VLMgineer, a
framework that harnesses the code generation abilities of vision language
models (VLMs) together with evolutionary search to iteratively co-design
physical tools and the action plans that operate them to perform a task. We
evaluate VLMgineer on a diverse new benchmark of everyday manipulation
scenarios that demand creative tool design and use. Across this suite,
VLMgineer consistently discovers tools and policies that solve tasks more
effectively and innovatively, transforming challenging robotics problems into
straightforward executions. It also outperforms VLM-generated designs from
human specifications and existing human-crafted tools for everyday tasks. To
facilitate future research on automated tool invention, we will release our
benchmark and code.

</details>


### [5] [DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning](https://arxiv.org/abs/2507.12855)
*Rahel Rickenbach,Bruce Lee,René Zurbrügg,Carmen Amo Alonso,Melanie N. Zeilinger*

Main category: cs.RO

TL;DR: 论文提出DEMONSTRATE方法，通过任务描述的嵌入表示和逆最优控制工具，减少对LLMs生成复杂优化问题的依赖，并利用多任务学习确保任务相似性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在控制系统中依赖任务示例设计及无法评估幻觉的问题。

Method: 利用逆最优控制工具和多任务学习，以任务演示替代上下文提示示例。

Result: 通过仿真和硬件实验验证了方法的有效性，减少了工程设计的依赖。

Conclusion: DEMONSTRATE方法显著降低了对LLMs和工程设计的依赖，并能在任务执行前评估幻觉。

Abstract: The integration of large language models (LLMs) with control systems has
demonstrated significant potential in various settings, such as task completion
with a robotic manipulator. A main reason for this success is the ability of
LLMs to perform in-context learning, which, however, strongly relies on the
design of task examples, closely related to the target tasks. Consequently,
employing LLMs to formulate optimal control problems often requires task
examples that contain explicit mathematical expressions, designed by trained
engineers. Furthermore, there is often no principled way to evaluate for
hallucination before task execution. To address these challenges, we propose
DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex
optimization problem generations, and instead only relies on the embedding
representations of task descriptions. To do this, we leverage tools from
inverse optimal control to replace in-context prompt examples with task
demonstrations, as well as the concept of multitask learning, which ensures
target and example task similarity by construction. Given the fact that
hardware demonstrations can easily be collected using teleoperation or guidance
of the robot, our approach significantly reduces the reliance on engineering
expertise for designing in-context examples. Furthermore, the enforced
multitask structure enables learning from few demonstrations and assessment of
hallucinations prior to task execution. We demonstrate the effectiveness of our
method through simulation and hardware experiments involving a robotic arm
tasked with tabletop manipulation.

</details>


### [6] [MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil Moisture Mapping at Scale](https://arxiv.org/abs/2507.12716)
*Nathaniel Rose,Hannah Chuang,Manuel A Andrade-Rodriguez,Rishi Parashar,Dani Or,Parikshit Maini*

Main category: cs.RO

TL;DR: 本文提出了一种自主移动机器人MoistureMapper，用于土壤湿度传感，通过自适应采样策略优化测量效率。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度测量方法成本高且不适合大规模应用，需要一种高效、低成本的解决方案。

Method: 设计并部署了配备TDR传感器和直接推进钻探机制的机器人，结合高斯过程建模的自适应采样策略。

Result: 自适应采样策略在计算模拟和实地部署中表现优异，减少30%的移动距离和5%的重建湿度图方差。

Conclusion: MoistureMapper及其自适应采样策略为大规模土壤湿度测量提供了高效、低成本的方法。

Abstract: Soil moisture is a quantity of interest in many application areas including
agriculture and climate modeling. Existing methods are not suitable for scale
applications due to large deployment costs in high-resolution sensing
applications such as for variable irrigation. In this work, we design, build
and field deploy an autonomous mobile robot, MoistureMapper, for soil moisture
sensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and
a direct push drill mechanism for deploying the sensor to measure volumetric
water content in the soil. Additionally, we implement and evaluate multiple
adaptive sampling strategies based on a Gaussian Process based modeling to
build a spatial mapping of moisture distribution in the soil. We present
results from large scale computational simulations and proof-of-concept
deployment on the field. The adaptive sampling approach outperforms a greedy
benchmark approach and results in up to 30\% reduction in travel distance and
5\% reduction in variance in the reconstructed moisture maps. Link to video
showing field experiments: https://youtu.be/S4bJ4tRzObg

</details>


### [7] [ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning](https://arxiv.org/abs/2507.13088)
*Rahel Rickenbach,Alan A. Lahoud,Erik Schaffernicht,Melanie N. Zeilinger,Johannes A. Stork*

Main category: cs.RO

TL;DR: ZipMPC通过为短时域MPC学习压缩且上下文相关的成本函数，模仿长时域MPC行为，提升性能并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决模型预测控制（MPC）在实时系统（如机器人）中因计算负担而限制应用的问题，尤其是短预测时域对控制性能和成本函数设计的负面影响。

Method: 利用可微分MPC和神经网络，通过MPC优化传播模仿损失的梯度，学习压缩且上下文相关的成本函数。

Result: 在仿真和真实世界的自动驾驶赛车实验中，ZipMPC表现优于其他方法，尤其在优化长期目标、计算效率、约束满足和泛化能力方面。

Conclusion: ZipMPC在保持短时域MPC计算效率的同时，实现了接近长时域MPC的性能，并在未见过的环境中表现出良好的泛化能力。

Abstract: The computational burden of model predictive control (MPC) limits its
application on real-time systems, such as robots, and often requires the use of
short prediction horizons. This not only affects the control performance, but
also increases the difficulty of designing MPC cost functions that reflect the
desired long-term objective. This paper proposes ZipMPC, a method that imitates
a long-horizon MPC behaviour by learning a compressed and context-dependent
cost function for a short-horizon MPC. It improves performance over alternative
methods, such as approximate explicit MPC and automatic cost parameter tuning,
in particular in terms of i) optimizing the long term objective; ii)
maintaining computational costs comparable to a short-horizon MPC; iii)
ensuring constraint satisfaction; and iv) generalizing control behaviour to
environments not observed during training. For this purpose, ZipMPC leverages
the concept of differentiable MPC with neural networks to propagate gradients
of the imitation loss through the MPC optimization. We validate our proposed
method in simulation and real-world experiments on autonomous racing. ZipMPC
consistently completes laps faster than selected baselines, achieving lap times
close to the long-horizon MPC baseline. In challenging scenarios where the
short-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In
particular, these performance gains are also observed on tracks unseen during
training.

</details>


### [8] [Learning to Predict Mobile Robot Stability in Off-Road Environments](https://arxiv.org/abs/2507.12731)
*Nathaniel Rose,Arif Ahmed,Emanuel Gutierrez-Cornejo,Parikshit Maini*

Main category: cs.RO

TL;DR: 提出了一种基于学习的方法（IMUnet），直接从本体感知数据估计机器人稳定性，无需地形模型或力传感。同时开发了基于视觉的ArUco跟踪方法，用于量化稳定性（C3分数）。


<details>
  <summary>Details</summary>
Motivation: 越野环境中机器人导航的挑战在于动态和崎岖地形，传统物理稳定性指标难以准确测量。

Method: 使用轻量级神经网络IMUnet从本体感知数据推断稳定性，并开发C3分数作为训练信号。

Result: 在多地形和速度数据上验证了方法的泛化能力，展示了IMU和速度输入估计稳定性的潜力。

Conclusion: 该方法适用于农业和太空应用中的移动操作任务，并为感知和规划提供了监督机制。

Abstract: Navigating in off-road environments for wheeled mobile robots is challenging
due to dynamic and rugged terrain. Traditional physics-based stability metrics,
such as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require
knowledge of contact forces, terrain geometry, and the robot's precise
center-of-mass that are difficult to measure accurately in real-world field
conditions. In this work, we propose a learning-based approach to estimate
robot platform stability directly from proprioceptive data using a lightweight
neural network, IMUnet. Our method enables data-driven inference of robot
stability without requiring an explicit terrain model or force sensing.
  We also develop a novel vision-based ArUco tracking method to compute a
scalar score to quantify robot platform stability called C3 score. The score
captures image-space perturbations over time as a proxy for physical
instability and is used as a training signal for the neural network based
model. As a pilot study, we evaluate our approach on data collected across
multiple terrain types and speeds and demonstrate generalization to previously
unseen conditions. These initial results highlight the potential of using IMU
and robot velocity as inputs to estimate platform stability. The proposed
method finds application in gating robot tasks such as precision actuation and
sensing, especially for mobile manipulation tasks in agricultural and space
applications. Our learning method also provides a supervision mechanism for
perception based traversability estimation and planning.

</details>


### [9] [ASC-SW: Atrous strip convolution network with sliding windows for visual-assisted map navigation](https://arxiv.org/abs/2507.12744)
*Cheng Liu,Fan Zhu,Yaoyu Zhuang Zhinan Chen Jiefeng Tang*

Main category: cs.RO

TL;DR: 提出了一种名为ASC-SW的视觉辅助导航框架，结合轻量级视觉神经网络和深度相机，用于移动机器人导航，解决了传统LiDAR无法检测地面障碍物的问题。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR传感器无法检测地面障碍物（如电线），限制了移动机器人的导航能力。

Method: 采用轻量级分割模型ASCnet，结合MobileNetV2和ASCSPP模块，高效提取DLO特征；引入滑动窗口（SW）后处理模块提升识别精度。

Result: 在自建数据集上达到75.3%的Miou，边缘设备上推理速度为9.3 FPS，优于现有DLO检测模型。

Conclusion: ASC-SW框架在推理速度和分割性能间取得平衡，成功应用于实际机器人平台。

Abstract: With the rapid development of lightweight visual neural network
architectures, traditional high-performance vision models have undergone
significant compression, greatly improving their computational efficiency and
energy consumption ratio. This makes them feasible for deployment on
resource-constrained edge computing devices. We propose a visual-assisted
navigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW),
which leverages a depth camera and a lightweight visual neural network to
assist map-based mobile robot navigation. This framework compensates for the
inability of traditional light detection and range (LiDAR) sensors to detect
ground-level obstacles such as ground-level wires. We introduce a lightweight
and efficient segmentation model, Atrous Strip Convolution Network (ASCnet),
for detecting deformable linear objects (DLOs). MobileNetV2 is used as the
backbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP)
is designed to extract DLO features more effectively. Atrous Strip Convolution
is integrated into ASCSPP to accurately identify the linear structure of DLOs
with low computational cost. Additionally, a Sliding Window (SW)
post-processing module is proposed to denoise the output in complex
environments, improving recognition accuracy. Our method strikes a balance
between inference speed and segmentation performance. It achieves a mean
Intersection over Union (Miou) score of 75.3% on a self-built dataset and
reaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall,
our approach outperforms existing DLO detection models and has been
successfully validated on a physical robotic platform.

</details>


### [10] [Refining Motion for Peak Performance: Identifying Optimal Gait Parameters for Energy-Efficient Quadrupedal Bounding](https://arxiv.org/abs/2507.12751)
*Yasser G. Alqaham,Jing Cheng,Zhenyu Gan*

Main category: cs.RO

TL;DR: 研究探索步态参数对四足机器人能量效率的影响，发现优化步态参数可显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管机械设计和驱动改进已有研究，但步态参数对能量效率的影响尚未充分探索。

Method: 通过建模Unitree A1机器人并开发独立调整步态参数的控制器，在Gazebo中进行模拟和实验验证。

Result: 优化步态参数可显著减少能量消耗，提升四足运动的整体效率。

Conclusion: 研究为四足机器人提供了节能控制策略，可直接应用于商业平台。

Abstract: Energy efficiency is a critical factor in the performance and autonomy of
quadrupedal robots. While previous research has focused on mechanical design
and actuation improvements, the impact of gait parameters on energetics has
been less explored. In this paper, we hypothesize that gait parameters,
specifically duty factor, phase shift, and stride duration, are key
determinants of energy consumption in quadrupedal locomotion. To test this
hypothesis, we modeled the Unitree A1 quadrupedal robot and developed a
locomotion controller capable of independently adjusting these gait parameters.
Simulations of bounding gaits were conducted in Gazebo across a range of gait
parameters at three different speeds: low, medium, and high. Experimental tests
were also performed to validate the simulation results. The findings
demonstrate that optimizing gait parameters can lead to significant reductions
in energy consumption, enhancing the overall efficiency of quadrupedal
locomotion. This work contributes to the advancement of energy-efficient
control strategies for legged robots, offering insights directly applicable to
commercially available platforms.

</details>


### [11] [osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning](https://arxiv.org/abs/2507.12753)
*Fujing Xie,Sören Schwertfeger,Hermann Blum*

Main category: cs.RO

TL;DR: 该论文提出了一种针对动态或未映射对象的导航系统，结合环境地图与LLM语义先验，提高了检索成功率。


<details>
  <summary>Details</summary>
Motivation: 现有高细节对象地图易过时，无法应对对象移动或未映射的情况，需开发更灵活的系统。

Method: 开发了结合环境地图和LLM语义先验的导航系统，采用主动在线方法定位对象。

Result: 在静态和动态对象查询中，系统检索成功率更高，路径更短。

Conclusion: 系统在动态环境中表现优越，为对象导航提供了更高效的解决方案。

Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features, achieving a high level of detail and
guiding robots to find objects specified by open-vocabulary language queries.
While the issue of scalability for such approaches has received some attention,
another fundamental problem is that high-detail object mapping quickly becomes
outdated, as objects get moved around a lot. In this work, we develop a mapping
and navigation system for object-goal navigation that, from the ground up,
considers the possibilities that a queried object can have moved, or may not be
mapped at all. Instead of striving for high-fidelity mapping detail, we
consider that the main purpose of a map is to provide environment grounding and
context, which we combine with the semantic priors of LLMs to reason about
object locations and deploy an active, online approach to navigate to the
objects. Through simulated and real-world experiments we find that our approach
tends to have higher retrieval success at shorter path lengths for static
objects and by far outperforms prior approaches in cases of dynamic or unmapped
object queries. We provide our code and dataset at:
https://anonymous.4open.science/r/osmAG-LLM.

</details>


### [12] [FFI-VTR: Lightweight and Robust Visual Teach and Repeat Navigation based on Feature Flow Indicator and Probabilistic Motion Planning](https://arxiv.org/abs/2507.12800)
*Jikai Wang,Yunqi Cheng,Zonghai Chen*

Main category: cs.RO

TL;DR: 提出了一种轻量且鲁棒的视觉重复导航方法，无需精确定位和密集重建模块。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在视觉重复导航中效率与鲁棒性平衡的挑战。

Method: 引入特征流并建立其与机器人运动的定性映射，将地图表示为关键帧图，导航建模为特征流最小化问题，开发概率运动规划。

Result: 实验证明该方法轻量、鲁棒且优于基线。

Conclusion: 该方法在无需精确定位的情况下实现了高效且鲁棒的导航。

Abstract: Though visual and repeat navigation is a convenient solution for mobile robot
self-navigation, achieving balance between efficiency and robustness in task
environment still remains challenges. In this paper, we propose a novel visual
and repeat robotic autonomous navigation method that requires no accurate
localization and dense reconstruction modules, which makes our system featured
by lightweight and robustness. Firstly, feature flow is introduced and we
develop a qualitative mapping between feature flow and robot's motion, in which
feature flow is defined as pixel location bias between matched features. Based
on the mapping model, the map outputted by the teaching phase is represented as
a keyframe graph, in which the feature flow on the edge encodes the relative
motion between adjacent keyframes. Secondly, the visual repeating navigation is
essentially modeled as a feature flow minimization problem between current
observation and the map keyframe. To drive the robot to consistently reduce the
feature flow between current frame and map keyframes without accurate
localization, a probabilistic motion planning is developed based on our
qualitative feature flow-motion mapping indicator. Extensive experiments using
our mobile platform demonstrates that our proposed method is lightweight,
robust, and superior to baselines. The source code has been made public at
https://github.com/wangjks/FFI-VTR to benefit the community.

</details>


### [13] [Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering](https://arxiv.org/abs/2507.12846)
*Muhammad Fadhil Ginting,Dong-Ki Kim,Xiangyun Meng,Andrzej Reinke,Bandi Jai Krishna,Navid Kayhani,Oriana Peltzer,David D. Fan,Amirreza Shaban,Sung-Kyun Kim,Mykel J. Kochenderfer,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: 本文提出了一种长期主动具身问答（LA-EQA）任务，通过结构化记忆系统和基于信息价值的停止标准，显著提升了机器人在复杂环境中的问答能力。


<details>
  <summary>Details</summary>
Motivation: 随着机器人操作时间的延长，需要解决其如何有效积累和利用环境知识的问题，传统EQA方法因上下文限制和缺乏持久记忆而表现不佳。

Method: 提出了一种基于场景图的世界实例结构化记忆系统，结合推理和规划算法，以及基于信息价值的停止标准。

Result: 在真实世界实验和新基准测试中，该方法显著优于现有基线，提高了答案准确性和探索效率。

Conclusion: 结构化记忆系统和信息价值平衡策略为长期机器人任务提供了有效解决方案。

Abstract: As robots become increasingly capable of operating over extended periods --
spanning days, weeks, and even months -- they are expected to accumulate
knowledge of their environments and leverage this experience to assist humans
more effectively. This paper studies the problem of Long-term Active Embodied
Question Answering (LA-EQA), a new task in which a robot must both recall past
experiences and actively explore its environment to answer complex,
temporally-grounded questions. Unlike traditional EQA settings, which typically
focus either on understanding the present environment alone or on recalling a
single past observation, LA-EQA challenges an agent to reason over past,
present, and possible future states, deciding when to explore, when to consult
its memory, and when to stop gathering observations and provide a final answer.
Standard EQA approaches based on large models struggle in this setting due to
limited context windows, absence of persistent memory, and an inability to
combine memory recall with active exploration. To address this, we propose a
structured memory system for robots, inspired by the mind palace method from
cognitive science. Our method encodes episodic experiences as scene-graph-based
world instances, forming a reasoning and planning algorithm that enables
targeted memory retrieval and guided navigation. To balance the
exploration-recall trade-off, we introduce value-of-information-based stopping
criteria that determines when the agent has gathered sufficient information. We
evaluate our method on real-world experiments and introduce a new benchmark
that spans popular simulation environments and actual industrial sites. Our
approach significantly outperforms state-of-the-art baselines, yielding
substantial gains in both answer accuracy and exploration efficiency.

</details>


### [14] [LaViPlan : Language-Guided Visual Path Planning with RLVR](https://arxiv.org/abs/2507.12911)
*Hayeon Oh*

Main category: cs.RO

TL;DR: 论文提出LaViPlan框架，利用强化学习与可验证奖励（RLVR）优化视觉语言模型（VLM），解决其在自动驾驶中视觉-语言-动作不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的分布外（OOD）场景可能导致危险行为，现有VLM虽能识别OOD场景，但其高级决策与低级轨迹预测存在不匹配。

Method: 提出LaViPlan框架，结合强化学习和规划导向的指标优化VLM。

Result: 实验表明，该方法提升了OOD条件下的情境感知和决策能力，缓解了不匹配问题。

Conclusion: LaViPlan为自动驾驶中的VLM提供了一种有前景的后训练范式。

Abstract: Out-of-distribution (OOD) scenarios in autonomous driving refer to situations
that deviate from the training domain, often leading to unexpected and
potentially hazardous behavior from planners that lack prior exposure to such
cases. Recently, Vision-Language Models (VLMs) have been introduced into
autonomous driving research for their promising generalization capabilities in
OOD settings. Early studies demonstrated that VLMs could recognize OOD
scenarios and generate user-level decisions such as "go straight" or "turn
right." However, a new challenge has emerged due to the misalignment between
the VLM's high-level decisions or visual reasoning expressed in language, and
the low-level predicted trajectories interpreted as actions. In this paper, we
propose LaViPlan, a framework that leverages Reinforcement Learning with
Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.
This approach addresses the vision-language-action misalignment observed in
existing VLMs fine-tuned via supervised learning, which can recognize driving
scenarios but often produce context-unaware decisions. Experimental results
demonstrate that our method improves situational awareness and decision-making
under OOD conditions, highlighting its potential to mitigate the misalignment
issue. This work introduces a promising post-training paradigm for VLM agents
in the context of autonomous driving.

</details>


### [15] [MoCap2GT: A High-Precision Ground Truth Estimator for SLAM Benchmarking Based on Motion Capture and IMU Fusion](https://arxiv.org/abs/2507.12920)
*Zichao Shu,Shitao Bei,Jicheng Dai,Lijun Li,Zetao Chen*

Main category: cs.RO

TL;DR: MoCap2GT是一种联合优化方法，结合MoCap数据和IMU测量，生成高精度地面真实轨迹，用于SLAM算法的精确评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoCap的地面真实轨迹存在时空校准误差和固有抖动问题，限制了SLAM算法的全面评估。

Method: 提出MoCap2GT，包括鲁棒状态初始化器、高阶B样条姿态参数化和退化感知测量拒绝策略。

Result: 实验表明，MoCap2GT优于现有方法，显著提升了SLAM评估的精确性。

Conclusion: MoCap2GT为SLAM基准测试提供了高精度的地面真实轨迹，代码已开源。

Abstract: Marker-based optical motion capture (MoCap) systems are widely used to
provide ground truth (GT) trajectories for benchmarking SLAM algorithms.
However, the accuracy of MoCap-based GT trajectories is mainly affected by two
factors: spatiotemporal calibration errors between the MoCap system and the
device under test (DUT), and inherent MoCap jitter. Consequently, existing
benchmarks focus primarily on absolute translation error, as accurate
assessment of rotation and inter-frame errors remains challenging, hindering
thorough SLAM evaluation. This paper proposes MoCap2GT, a joint optimization
approach that integrates MoCap data and inertial measurement unit (IMU)
measurements from the DUT for generating high-precision GT trajectories.
MoCap2GT includes a robust state initializer to ensure global convergence,
introduces a higher-order B-spline pose parameterization on the SE(3) manifold
with variable time offset to effectively model MoCap factors, and employs a
degeneracy-aware measurement rejection strategy to enhance estimation accuracy.
Experimental results demonstrate that MoCap2GT outperforms existing methods and
significantly contributes to precise SLAM benchmarking. The source code is
available at https://anonymous.4open.science/r/mocap2gt (temporarily hosted
anonymously for double-blind review).

</details>


### [16] [Non-differentiable Reward Optimization for Diffusion-based Autonomous Motion Planning](https://arxiv.org/abs/2507.12977)
*Giwon Lee,Daehee Park,Jaewoo Jeong,Kuk-Jin Yoon*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的扩散运动规划模型训练方案，通过非可微目标优化提升安全性和有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在运动规划中表现优异，但无法直接优化非可微目标（如安全性和有效性），需改进训练方法。

Method: 采用强化学习训练扩散模型，引入奖励加权动态阈值算法优化非可微目标。

Result: 在行人数据集（CrowdNav, ETH-UCY）上取得最优性能。

Conclusion: 该方法为安全有效的运动规划提供了新思路，性能优于传统方法。

Abstract: Safe and effective motion planning is crucial for autonomous robots.
Diffusion models excel at capturing complex agent interactions, a fundamental
aspect of decision-making in dynamic environments. Recent studies have
successfully applied diffusion models to motion planning, demonstrating their
competence in handling complex scenarios and accurately predicting multi-modal
future trajectories. Despite their effectiveness, diffusion models have
limitations in training objectives, as they approximate data distributions
rather than explicitly capturing the underlying decision-making dynamics.
However, the crux of motion planning lies in non-differentiable downstream
objectives, such as safety (collision avoidance) and effectiveness
(goal-reaching), which conventional learning algorithms cannot directly
optimize. In this paper, we propose a reinforcement learning-based training
scheme for diffusion motion planning models, enabling them to effectively learn
non-differentiable objectives that explicitly measure safety and effectiveness.
Specifically, we introduce a reward-weighted dynamic thresholding algorithm to
shape a dense reward signal, facilitating more effective training and
outperforming models trained with differentiable objectives. State-of-the-art
performance on pedestrian datasets (CrowdNav, ETH-UCY) compared to various
baselines demonstrates the versatility of our approach for safe and effective
motion planning.

</details>


### [17] [Robustness Requirement Coverage using a Situation Coverage Approach for Vision-based AI Systems](https://arxiv.org/abs/2507.12986)
*Sepeedeh Shahbeigi,Nawshin Mannan Proma,Victoria Hodge,Richard Hawkins,Boda Li,Valentina Donzella*

Main category: cs.RO

TL;DR: 提出了一种新框架，结合相机噪声因素识别与情境覆盖分析，为基于AI的感知系统制定鲁棒性安全需求。


<details>
  <summary>Details</summary>
Motivation: AI感知系统在传感器性能退化时可能失效，现有方法难以覆盖所有退化场景，需系统性方法确保安全。

Method: 整合相机噪声因素识别与情境覆盖分析，结合专家意见和操作设计域规范，扩展退化模型。

Result: 初步实现了噪声因素分析与情境覆盖的整合，支持鲁棒性需求的系统性制定与完整性评估。

Conclusion: 该框架为相机退化场景下的AI感知系统提供了鲁棒性安全需求的新方法。

Abstract: AI-based robots and vehicles are expected to operate safely in complex and
dynamic environments, even in the presence of component degradation. In such
systems, perception relies on sensors such as cameras to capture environmental
data, which is then processed by AI models to support decision-making. However,
degradation in sensor performance directly impacts input data quality and can
impair AI inference. Specifying safety requirements for all possible sensor
degradation scenarios leads to unmanageable complexity and inevitable gaps. In
this position paper, we present a novel framework that integrates camera noise
factor identification with situation coverage analysis to systematically elicit
robustness-related safety requirements for AI-based perception systems. We
focus specifically on camera degradation in the automotive domain. Building on
an existing framework for identifying degradation modes, we propose involving
domain, sensor, and safety experts, and incorporating Operational Design Domain
specifications to extend the degradation model by incorporating noise factors
relevant to AI performance. Situation coverage analysis is then applied to
identify representative operational contexts. This work marks an initial step
toward integrating noise factor analysis and situational coverage to support
principled formulation and completeness assessment of robustness requirements
for camera-based AI perception.

</details>


### [18] [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/abs/2507.13019)
*Liuyi Wang,Xinyuan Xia,Hui Zhao,Hanqing Wang,Tai Wang,Yilun Chen,Chengju Liu,Qijun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: VLN-PE是一个物理现实的视觉与语言导航平台，支持多种机器人类型，揭示了当前模型在物理部署中的性能下降问题，并提供了改进路径。


<details>
  <summary>Details</summary>
Motivation: 当前视觉与语言导航（VLN）方法在物理机器人部署中存在理想化假设，无法反映实际挑战，VLN-PE旨在填补这一差距。

Method: VLN-PE平台支持人形、四足和轮式机器人，评估了多种导航方法，包括分类模型、扩散模型和基于大型语言模型的路径规划。

Result: 结果显示性能显著下降，原因包括有限的观察空间、光照变化和物理挑战（如碰撞和跌倒），同时暴露了腿式机器人在复杂环境中的局限性。

Conclusion: VLN-PE为改进跨具身适应性提供了新途径，并希望激发社区重新思考VLN的局限性，推动更鲁棒的模型发展。

Abstract: Recent Vision-and-Language Navigation (VLN) advancements are promising, but
their idealized assumptions about robot movement and control fail to reflect
physically embodied deployment challenges. To bridge this gap, we introduce
VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and
wheeled robots. For the first time, we systematically evaluate several
ego-centric VLN methods in physical robotic settings across different technical
pipelines, including classification models for single-step discrete action
prediction, a diffusion model for dense waypoint prediction, and a train-free,
map-based large language model (LLM) integrated with path planning. Our results
reveal significant performance degradation due to limited robot observation
space, environmental lighting variations, and physical challenges like
collisions and falls. This also exposes locomotion constraints for legged
robots in complex environments. VLN-PE is highly extensible, allowing seamless
integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN
evaluation. Despite the weak generalization of current models in physical
deployment, VLN-PE provides a new pathway for improving cross-embodiment's
overall adaptability. We hope our findings and tools inspire the community to
rethink VLN limitations and advance robust, practical VLN models. The code is
available at https://crystalsixone.github.io/vln_pe.github.io/.

</details>


### [19] [What Can Robots Teach Us About Trust and Reliance? An interdisciplinary dialogue between Social Sciences and Social Robotics](https://arxiv.org/abs/2507.13041)
*Julien Wacquez,Elisabetta Zibetti,Joffrey Becker,Lorenzo Aloe,Fabio Amadio,Salvatore Anzalone,Lola Cañamero,Serena Ivaldi*

Main category: cs.RO

TL;DR: 论文探讨了机器人信任问题，提出需要跨学科方法结合社会科学与机器人学，以建立更全面的信任框架。


<details>
  <summary>Details</summary>
Motivation: 随着机器人融入日常生活，信任问题日益重要，但现有研究碎片化且缺乏与社会学的对话。

Method: 结合社会科学与社会机器人学的见解，探讨信任的形成、测试与表现。

Result: 提出跨学科对话的必要性，为理解人机交互中的信任奠定基础。

Conclusion: 需要更跨学科的框架来应对人机交互中信任的复杂性。

Abstract: As robots find their way into more and more aspects of everyday life,
questions around trust are becoming increasingly important. What does it mean
to trust a robot? And how should we think about trust in relationships that
involve both humans and non-human agents? While the field of Human-Robot
Interaction (HRI) has made trust a central topic, the concept is often
approached in fragmented ways. At the same time, established work in sociology,
where trust has long been a key theme, is rarely brought into conversation with
developments in robotics. This article argues that we need a more
interdisciplinary approach. By drawing on insights from both social sciences
and social robotics, we explore how trust is shaped, tested and made visible.
Our goal is to open up a dialogue between disciplines and help build a more
grounded and adaptable framework for understanding trust in the evolving world
of human-robot interaction.

</details>


### [20] [Efficient Online Learning and Adaptive Planning for Robotic Information Gathering Based on Streaming Data](https://arxiv.org/abs/2507.13053)
*Sanjeev Ramkumar Sudha,Joel Jose,Erlend M. Coates*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯过程（GP）的高效自适应信息规划方法，用于映射连续标量场，解决了现有方法在大数据集上实时性能不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器人信息采集（RIG）方法通常假设环境已知，而真实环境可能是未知或时变的，自适应规划成为研究热点。高斯过程回归在RIG中广泛应用，但其在大数据集上的实时性能不足。

Method: 提出了一种基于流式稀疏高斯过程的自适应信息规划方法，通过仿真实验和真实数据集验证其性能。

Result: 实验结果表明，该方法在保持与基线相似映射精度的同时，显著降低了长时间任务的计算复杂度。

Conclusion: 该方法为未知或时变环境中的高效信息采集提供了可行的解决方案。

Abstract: Robotic information gathering (RIG) techniques refer to methods where mobile
robots are used to acquire data about the physical environment with a suite of
sensors. Informative planning is an important part of RIG where the goal is to
find sequences of actions or paths that maximize efficiency or the quality of
information collected. Many existing solutions solve this problem by assuming
that the environment is known in advance. However, real environments could be
unknown or time-varying, and adaptive informative planning remains an active
area of research. Adaptive planning and incremental online mapping are required
for mapping initially unknown or varying spatial fields. Gaussian process (GP)
regression is a widely used technique in RIG for mapping continuous spatial
fields. However, it falls short in many applications as its real-time
performance does not scale well to large datasets. To address these challenges,
this paper proposes an efficient adaptive informative planning approach for
mapping continuous scalar fields with GPs with streaming sparse GPs. Simulation
experiments are performed with a synthetic dataset and compared against
existing benchmarks. Finally, it is also verified with a real-world dataset to
further validate the efficacy of the proposed method. Results show that our
method achieves similar mapping accuracy to the baselines while reducing
computational complexity for longer missions.

</details>


### [21] [GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training](https://arxiv.org/abs/2507.13097)
*Adithyavairavan Murali,Balakumar Sundaralingam,Yu-Wei Chao,Wentao Yuan,Jun Yamada,Mark Carlson,Fabio Ramos,Stan Birchfield,Dieter Fox,Clemens Eppner*

Main category: cs.RO

TL;DR: GraspGen是一个基于扩散变换器的抓取生成框架，通过高效判别器评分和过滤抓取，在模拟和真实机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管基于学习的6自由度抓取方法取得进展，但仍难以泛化到不同机器人和实际场景。

Method: 采用扩散变换器架构和高效判别器，提出新的判别器训练方法，并发布包含5300万次抓取的新模拟数据集。

Result: 在模拟和FetchBench基准测试中优于现有方法，在真实机器人任务中表现良好。

Conclusion: GraspGen在抓取生成和泛化能力上取得显著进展，为实际应用提供了有效解决方案。

Abstract: Grasping is a fundamental robot skill, yet despite significant research
advancements, learning-based 6-DOF grasping approaches are still not turnkey
and struggle to generalize across different embodiments and in-the-wild
settings. We build upon the recent success on modeling the object-centric grasp
generation process as an iterative diffusion process. Our proposed framework,
GraspGen, consists of a DiffusionTransformer architecture that enhances grasp
generation, paired with an efficient discriminator to score and filter sampled
grasps. We introduce a novel and performant on-generator training recipe for
the discriminator. To scale GraspGen to both objects and grippers, we release a
new simulated dataset consisting of over 53 million grasps. We demonstrate that
GraspGen outperforms prior methods in simulations with singulated objects
across different grippers, achieves state-of-the-art performance on the
FetchBench grasping benchmark, and performs well on a real robot with noisy
visual observations.

</details>


### [22] [Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback](https://arxiv.org/abs/2507.13171)
*Suzie Kim,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.RO

TL;DR: 论文提出了一种基于隐式人类反馈（RLIHF）的强化学习框架，利用脑电图（EEG）信号提供连续反馈，无需用户显式干预，解决了稀疏奖励条件下的策略学习问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在稀疏奖励条件下效果不佳，且依赖复杂的手工设计奖励函数。RLHF虽引入人类反馈，但显式反馈机制（如按钮或偏好标签）会干扰自然交互并增加用户认知负担。

Method: 提出RLIHF框架，通过非侵入式EEG信号（如错误相关电位ErrPs）提供隐式反馈，使用预训练解码器将EEG信号转化为概率奖励组件，结合稀疏外部奖励进行策略学习。

Result: 在MuJoCo模拟环境中，使用Kinova Gen2机械臂进行复杂抓取任务，结果显示基于EEG反馈训练的智能体性能与密集手工设计奖励相当。

Conclusion: 研究表明，隐式神经反馈在交互式机器人中具有潜力，可实现可扩展且与人类对齐的强化学习。

Abstract: Conventional reinforcement learning (RL) ap proaches often struggle to learn
effective policies under sparse reward conditions, necessitating the manual
design of complex, task-specific reward functions. To address this limitation,
rein forcement learning from human feedback (RLHF) has emerged as a promising
strategy that complements hand-crafted rewards with human-derived evaluation
signals. However, most existing RLHF methods depend on explicit feedback
mechanisms such as button presses or preference labels, which disrupt the
natural interaction process and impose a substantial cognitive load on the
user. We propose a novel reinforcement learning from implicit human feedback
(RLIHF) framework that utilizes non-invasive electroencephalography (EEG)
signals, specifically error-related potentials (ErrPs), to provide continuous,
implicit feedback without requiring explicit user intervention. The proposed
method adopts a pre-trained decoder to transform raw EEG signals into
probabilistic reward components, en abling effective policy learning even in
the presence of sparse external rewards. We evaluate our approach in a
simulation environment built on the MuJoCo physics engine, using a Kinova Gen2
robotic arm to perform a complex pick-and-place task that requires avoiding
obstacles while manipulating target objects. The results show that agents
trained with decoded EEG feedback achieve performance comparable to those
trained with dense, manually designed rewards. These findings validate the
potential of using implicit neural feedback for scalable and human-aligned
reinforcement learning in interactive robotics.

</details>


### [23] [Few-shot transfer of tool-use skills using human demonstrations with proximity and tactile sensing](https://arxiv.org/abs/2507.13200)
*Marina Y. Aoyama,Sethu Vijayakumar,Tetsuya Narita*

Main category: cs.RO

TL;DR: 提出了一种基于多模态感知的小样本工具使用技能迁移框架，通过仿真预训练和现实世界微调，解决机器人工具操作中的复杂接触问题。


<details>
  <summary>Details</summary>
Motivation: 人类擅长工具操作，但机器人学习这些技能面临挑战，尤其是复杂接触关系的识别和模拟到现实的差距。

Method: 提出框架包括仿真预训练基础策略以捕捉工具使用中的常见接触状态，并通过现实世界的人类演示微调以弥合领域差距。

Result: 验证了框架在Franka Emika机械臂上能通过少量演示教授表面跟随任务，并成功迁移工具-环境接触关系识别能力。

Conclusion: 结合接近和触觉传感器能增强接触状态和环境几何的识别，机器人通过迁移学习获得新工具使用技能。

Abstract: Tools extend the manipulation abilities of robots, much like they do for
humans. Despite human expertise in tool manipulation, teaching robots these
skills faces challenges. The complexity arises from the interplay of two
simultaneous points of contact: one between the robot and the tool, and another
between the tool and the environment. Tactile and proximity sensors play a
crucial role in identifying these complex contacts. However, learning tool
manipulation using these sensors remains challenging due to limited real-world
data and the large sim-to-real gap. To address this, we propose a few-shot
tool-use skill transfer framework using multimodal sensing. The framework
involves pre-training the base policy to capture contact states common in
tool-use skills in simulation and fine-tuning it with human demonstrations
collected in the real-world target domain to bridge the domain gap. We validate
that this framework enables teaching surface-following tasks using tools with
diverse physical and geometric properties with a small number of demonstrations
on the Franka Emika robot arm. Our analysis suggests that the robot acquires
new tool-use skills by transferring the ability to recognise tool-environment
contact relationships from pre-trained to fine-tuned policies. Additionally,
combining proximity and tactile sensors enhances the identification of contact
states and environmental geometry.

</details>


### [24] [Signal Temporal Logic Compliant Co-design of Planning and Control](https://arxiv.org/abs/2507.13225)
*Manas Sashank Juvvi,Tushar Dilip Kurne,Vaishnavi J,Shishir Kolathaya,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 提出了一种结合轨迹规划与控制的协同设计策略，用于处理自主机器人中基于STL的任务。


<details>
  <summary>Details</summary>
Motivation: 解决自主机器人在复杂环境中执行STL任务时的轨迹规划与控制问题。

Method: 分两阶段：1) 学习时空运动基元以封装机器人约束；2) 基于这些基元构建符合STL的运动计划。使用强化学习生成控制策略库，并通过采样方法实现STL合规的运动规划。

Result: 在差速驱动和四足机器人上验证了方法的有效性，适用于多种环境和STL规范。

Conclusion: 提出的无模型方法能生成符合STL的可行运动计划，适用于多种机器人平台。

Abstract: This work presents a novel co-design strategy that integrates trajectory
planning and control to handle STL-based tasks in autonomous robots. The method
consists of two phases: $(i)$ learning spatio-temporal motion primitives to
encapsulate the inherent robot-specific constraints and $(ii)$ constructing an
STL-compliant motion plan from these primitives. Initially, we employ
reinforcement learning to construct a library of control policies that perform
trajectories described by the motion primitives. Then, we map motion primitives
to spatio-temporal characteristics. Subsequently, we present a sampling-based
STL-compliant motion planning strategy tailored to meet the STL specification.
The proposed model-free approach, which generates feasible STL-compliant motion
plans across various environments, is validated on differential-drive and
quadruped robots across various STL specifications. Demonstration videos are
available at https://tinyurl.com/m6zp7rsm.

</details>


### [25] [Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour](https://arxiv.org/abs/2507.13277)
*Emma M. A. Harrison*

Main category: cs.RO

TL;DR: 研究比较了三种强化学习算法在四足机器人导航和避障中的表现，发现PPO算法最优，为辅助机器人提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 探索四足机器人在医疗和辅助领域的潜力，特别是作为导盲犬的替代方案。

Method: 使用自定义环境比较PPO、DQN和Q-learning算法，评估传感器输入、碰撞频率等指标。

Result: PPO在导航和避障任务中表现最佳，优于DQN和Q-learning。

Conclusion: 研究为AI驱动的四足机器人辅助应用提供了可行性支持，推动了医疗机器人领域的发展。

Abstract: Robots are increasingly integrated across industries, particularly in
healthcare. However, many valuable applications for quadrupedal robots remain
overlooked. This research explores the effectiveness of three reinforcement
learning algorithms in training a simulated quadruped robot for autonomous
navigation and obstacle avoidance. The goal is to develop a robotic guide dog
simulation capable of path following and obstacle avoidance, with long-term
potential for real-world assistance to guide dogs and visually impaired
individuals. It also seeks to expand research into medical 'pets', including
robotic guide and alert dogs.
  A comparative analysis of thirteen related research papers shaped key
evaluation criteria, including collision detection, pathfinding algorithms,
sensor usage, robot type, and simulation platforms. The study focuses on sensor
inputs, collision frequency, reward signals, and learning progression to
determine which algorithm best supports robotic navigation in complex
environments.
  Custom-made environments were used to ensure fair evaluation of all three
algorithms under controlled conditions, allowing consistent data collection.
Results show that Proximal Policy Optimization (PPO) outperformed Deep
Q-Network (DQN) and Q-learning across all metrics, particularly in average and
median steps to goal per episode.
  By analysing these results, this study contributes to robotic navigation, AI
and medical robotics, offering insights into the feasibility of AI-driven
quadruped mobility and its role in assistive robotics.

</details>


### [26] [Latent Policy Steering with Embodiment-Agnostic Pretrained World Models](https://arxiv.org/abs/2507.13340)
*Yiqi Wang,Mrinal Verghese,Jeff Schneider*

Main category: cs.RO

TL;DR: 通过模仿学习视觉运动策略效果显著，但依赖大量训练数据。本文提出利用现有低成本数据（如公开机器人数据集和人类玩物体数据）减少数据收集，结合光流作为动作表示和潜在策略搜索方法，显著提升小数据量下的策略性能。


<details>
  <summary>Details</summary>
Motivation: 减少学习视觉运动策略时对昂贵真实世界数据收集的依赖，利用现有或低成本的多体现数据。

Method: 使用光流作为体现无关的动作表示训练世界模型，结合潜在策略搜索（LPS）优化行为克隆策略。

Result: 在真实实验中，小数据量下策略性能显著提升（30次演示相对提升50%，50次演示提升20%）。

Conclusion: 结合多体现数据预训练的世界模型和潜在策略搜索方法，能有效减少数据需求并提升策略性能。

Abstract: Learning visuomotor policies via imitation has proven effective across a wide
range of robotic domains. However, the performance of these policies is heavily
dependent on the number of training demonstrations, which requires expensive
data collection in the real world. In this work, we aim to reduce data
collection efforts when learning visuomotor robot policies by leveraging
existing or cost-effective data from a wide range of embodiments, such as
public robot datasets and the datasets of humans playing with objects (human
data from play). Our approach leverages two key insights. First, we use optic
flow as an embodiment-agnostic action representation to train a World Model
(WM) across multi-embodiment datasets, and finetune it on a small amount of
robot data from the target embodiment. Second, we develop a method, Latent
Policy Steering (LPS), to improve the output of a behavior-cloned policy by
searching in the latent space of the WM for better action sequences. In real
world experiments, we observe significant improvements in the performance of
policies trained with a small amount of data (over 50% relative improvement
with 30 demonstrations and over 20% relative improvement with 50
demonstrations) by combining the policy with a WM pretrained on two thousand
episodes sampled from the existing Open X-embodiment dataset across different
robots or a cost-effective human dataset from play.

</details>
