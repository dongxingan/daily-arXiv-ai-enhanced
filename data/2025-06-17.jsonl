{"id": "2506.12082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12082", "abs": "https://arxiv.org/abs/2506.12082", "authors": ["Harith S. Gallage", "Bailey F. De Sousa", "Benjamin I. Chesnik", "Chaikel G. Brownstein", "Anson Paul", "Ronghuai Qi"], "title": "Design and Development of a Robotic Transcatheter Delivery System for Aortic Valve Replacement", "comment": "1 page with 2 figures. This abstract has been accepted by the 2025\n  International Conference on Robotics and Automation (ICRA) Workshop on\n  Robot-Assisted Endovascular Interventions", "summary": "Minimally invasive transcatheter approaches are increasingly adopted for\naortic stenosis treatment, where optimal commissural and coronary alignment is\nimportant. Achieving precise alignment remains clinically challenging, even\nwith contemporary robotic transcatheter aortic valve replacement (TAVR)\ndevices, as this task is still performed manually. This paper proposes the\ndevelopment of a robotic transcatheter delivery system featuring an\nomnidirectional bending joint and an actuation system designed to enhance\npositional accuracy and precision in TAVR procedures. The preliminary\nexperimental results validate the functionality of this novel robotic system."}
{"id": "2506.12089", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12089", "abs": "https://arxiv.org/abs/2506.12089", "authors": ["Razan Ghzouli", "Atieh Hanna", "Endre Erös", "Rebekka Wohlrab"], "title": "Using Behavior Trees in Risk Assessment", "comment": "8 pages, 5 figures", "summary": "Cyber-physical production systems increasingly involve collaborative robotic\nmissions, requiring more demand for robust and safe missions. Industries rely\non risk assessments to identify potential failures and implement measures to\nmitigate their risks. Although it is recommended to conduct risk assessments\nearly in the design of robotic missions, the state of practice in the industry\nis different. Safety experts often struggle to completely understand robotics\nmissions at the early design stages of projects and to ensure that the output\nof risk assessments is adequately considered during implementation.\n  This paper presents a design science study that conceived a model-based\napproach for early risk assessment in a development-centric way. Our approach\nsupports risk assessment activities by using the behavior-tree model. We\nevaluated the approach together with five practitioners from four companies.\nOur findings highlight the potential of the behavior-tree model in supporting\nearly identification, visualisation, and bridging the gap between code\nimplementation and risk assessments' outputs. This approach is the first\nattempt to use the behavior-tree model to support risk assessment; thus, the\nfindings highlight the need for further development."}
{"id": "2506.12095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12095", "abs": "https://arxiv.org/abs/2506.12095", "authors": ["Khang Nguyen", "An T. Le", "Jan Peters", "Minh Nhat Vu"], "title": "DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion", "comment": null, "summary": "Achieving robust robot learning for humanoid locomotion is a fundamental\nchallenge in model-based reinforcement learning (MBRL), where environmental\nstochasticity and randomness can hinder efficient exploration and learning\nstability. The environmental, so-called aleatoric, uncertainty can be amplified\nin high-dimensional action spaces with complex contact dynamics, and further\nentangled with epistemic uncertainty in the models during learning phases. In\nthis work, we propose DoublyAware, an uncertainty-aware extension of Temporal\nDifference Model Predictive Control (TD-MPC) that explicitly decomposes\nuncertainty into two disjoint interpretable components, i.e., planning and\npolicy uncertainties. To handle the planning uncertainty, DoublyAware employs\nconformal prediction to filter candidate trajectories using quantile-calibrated\nrisk bounds, ensuring statistical consistency and robustness against stochastic\ndynamics. Meanwhile, policy rollouts are leveraged as structured informative\npriors to support the learning phase with Group-Relative Policy Constraint\n(GRPC) optimizers that impose a group-based adaptive trust-region in the latent\naction space. This principled combination enables the robot agent to prioritize\nhigh-confidence, high-reward behavior while maintaining effective, targeted\nexploration under uncertainty. Evaluated on the HumanoidBench locomotion suite\nwith the Unitree 26-DoF H1-2 humanoid, DoublyAware demonstrates improved sample\nefficiency, accelerated convergence, and enhanced motion feasibility compared\nto RL baselines. Our simulation results emphasize the significance of\nstructured uncertainty modeling for data-efficient and reliable decision-making\nin TD-MPC-based humanoid locomotion learning."}
{"id": "2506.12184", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12184", "abs": "https://arxiv.org/abs/2506.12184", "authors": ["Stanley Lewis", "Vishal Chandra", "Tom Gao", "Odest Chadwicke Jenkins"], "title": "SPLATART: Articulated Gaussian Splatting with Estimated Object Structure", "comment": "7 pages, Accepted to the 2025 RSS Workshop on Gaussian\n  Representations for Robot Autonomy. Contact: Stanley Lewis, stanlew@umich.edu", "summary": "Representing articulated objects remains a difficult problem within the field\nof robotics. Objects such as pliers, clamps, or cabinets require\nrepresentations that capture not only geometry and color information, but also\npart seperation, connectivity, and joint parametrization. Furthermore, learning\nthese representations becomes even more difficult with each additional degree\nof freedom. Complex articulated objects such as robot arms may have seven or\nmore degrees of freedom, and the depth of their kinematic tree may be notably\ngreater than the tools, drawers, and cabinets that are the typical subjects of\narticulated object research. To address these concerns, we introduce SPLATART -\na pipeline for learning Gaussian splat representations of articulated objects\nfrom posed images, of which a subset contains image space part segmentations.\nSPLATART disentangles the part separation task from the articulation estimation\ntask, allowing for post-facto determination of joint estimation and\nrepresentation of articulated objects with deeper kinematic trees than\npreviously exhibited. In this work, we present data on the SPLATART pipeline as\napplied to the syntheic Paris dataset objects, and qualitative results on a\nreal-world object under spare segmentation supervision. We additionally present\non articulated serial chain manipulators to demonstrate usage on deeper\nkinematic tree structures."}
{"id": "2506.12273", "categories": ["cs.RO", "cs.SY", "eess.SY", "93B30 (Primary), 93B35 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.12273", "abs": "https://arxiv.org/abs/2506.12273", "authors": ["Rongfei Li", "Francis Assadian"], "title": "Role of Uncertainty in Model Development and Control Design for a Manufacturing Process", "comment": "35 pages, 26 figures, Book Chapter. Published in: Role of Uncertainty\n  in Model Development and Control Design for a Manufacturing Process,\n  IntechOpen, 2022. For published version, see this http URL:\n  https://doi.org/10.5772/intechopen.104780", "summary": "The use of robotic technology has drastically increased in manufacturing in\nthe 21st century. But by utilizing their sensory cues, humans still outperform\nmachines, especially in the micro scale manufacturing, which requires\nhigh-precision robot manipulators. These sensory cues naturally compensate for\nhigh level of uncertainties that exist in the manufacturing environment.\nUncertainties in performing manufacturing tasks may come from measurement\nnoise, model inaccuracy, joint compliance (e.g., elasticity) etc. Although\nadvanced metrology sensors and high-precision microprocessors, which are\nutilized in nowadays robots, have compensated for many structural and dynamic\nerrors in robot positioning, but a well-designed control algorithm still works\nas a comparable and cheaper alternative to reduce uncertainties in automated\nmanufacturing. Our work illustrates that a multi-robot control system can\nreduce various uncertainties to a great amount."}
{"id": "2506.12239", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12239", "abs": "https://arxiv.org/abs/2506.12239", "authors": ["Jayjun Lee", "Nima Fazeli"], "title": "ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation", "comment": "Accepted to RSS 2025 | Project page:\n  https://jayjunlee.github.io/vitascope/", "summary": "Mastering dexterous, contact-rich object manipulation demands precise\nestimation of both in-hand object poses and external contact\nlocations$\\unicode{x2013}$tasks particularly challenging due to partial and\nnoisy observations. We present ViTaSCOPE: Visuo-Tactile Simultaneous Contact\nand Object Pose Estimation, an object-centric neural implicit representation\nthat fuses vision and high-resolution tactile feedback. By representing objects\nas signed distance fields and distributed tactile feedback as neural shear\nfields, ViTaSCOPE accurately localizes objects and registers extrinsic contacts\nonto their 3D geometry as contact fields. Our method enables seamless reasoning\nover complementary visuo-tactile cues by leveraging simulation for scalable\ntraining and zero-shot transfers to the real-world by bridging the sim-to-real\ngap. We evaluate our method through comprehensive simulated and real-world\nexperiments, demonstrating its capabilities in dexterous manipulation\nscenarios."}
{"id": "2506.12314", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.12314", "abs": "https://arxiv.org/abs/2506.12314", "authors": ["Xiaoshuai Ma", "Haoxiang Qi", "Qingqing Li", "Haochen Xu", "Xuechao Chen", "Junyao Gao", "Zhangguo Yu", "Qiang Huang"], "title": "Explosive Output to Enhance Jumping Ability: A Variable Reduction Ratio Design Paradigm for Humanoid Robots Knee Joint", "comment": null, "summary": "Enhancing the explosive power output of the knee joints is critical for\nimproving the agility and obstacle-crossing capabilities of humanoid robots.\nHowever, a mismatch between the knee-to-center-of-mass (CoM) transmission ratio\nand jumping demands, coupled with motor performance degradation at high speeds,\nrestricts the duration of high-power output and limits jump performance. To\naddress these problems, this paper introduces a novel knee joint design\nparadigm employing a dynamically decreasing reduction ratio for explosive\noutput during jump. Analysis of motor output characteristics and knee\nkinematics during jumping inspired a coupling strategy in which the reduction\nratio gradually decreases as the joint extends. A high initial ratio rapidly\nincreases torque at jump initiation, while its gradual reduction minimizes\nmotor speed increments and power losses, thereby maintaining sustained\nhigh-power output. A compact and efficient linear actuator-driven guide-rod\nmechanism realizes this coupling strategy, supported by parameter optimization\nguided by explosive jump control strategies. Experimental validation\ndemonstrated a 63 cm vertical jump on a single-joint platform (a theoretical\nimprovement of 28.1\\% over the optimal fixed-ratio joints). Integrated into a\nhumanoid robot, the proposed design enabled a 1.1 m long jump, a 0.5 m vertical\njump, and a 0.5 m box jump."}
{"id": "2506.12248", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12248", "abs": "https://arxiv.org/abs/2506.12248", "authors": ["Jennifer Grannen", "Siddharth Karamcheti", "Blake Wulfe", "Dorsa Sadigh"], "title": "ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration", "comment": "Accepted by IEEE Robotics and Automation Letters 2025", "summary": "Collaborative robots must quickly adapt to their partner's intent and\npreferences to proactively identify helpful actions. This is especially true in\nsituated settings where human partners can continually teach robots new\nhigh-level behaviors, visual concepts, and physical skills (e.g., through\ndemonstration), growing the robot's capabilities as the human-robot pair work\ntogether to accomplish diverse tasks. In this work, we argue that robots should\nbe able to infer their partner's goals from early interactions and use this\ninformation to proactively plan behaviors ahead of explicit instructions from\nthe user. Building from the strong commonsense priors and steerability of large\nlanguage models, we introduce ProVox (\"Proactive Voice\"), a novel framework\nthat enables robots to efficiently personalize and adapt to individual\ncollaborators. We design a meta-prompting protocol that empowers users to\ncommunicate their distinct preferences, intent, and expected robot behaviors\nahead of starting a physical interaction. ProVox then uses the personalized\nprompt to condition a proactive language model task planner that anticipates a\nuser's intent from the current interaction context and robot capabilities to\nsuggest helpful actions; in doing so, we alleviate user burden, minimizing the\namount of time partners spend explicitly instructing and supervising the robot.\nWe evaluate ProVox through user studies grounded in household manipulation\ntasks (e.g., assembling lunch bags) that measure the efficiency of the\ncollaboration, as well as features such as perceived helpfulness, ease of use,\nand reliability. Our analysis suggests that both meta-prompting and proactivity\nare critical, resulting in 38.7% faster task completion times and 31.9% less\nuser burden relative to non-active baselines. Supplementary material, code, and\nvideos can be found at https://provox-2025.github.io."}
{"id": "2506.13019", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13019", "abs": "https://arxiv.org/abs/2506.13019", "authors": ["Jiachen Li", "Jian Chu", "Feiyang Zhao", "Shihao Li", "Wei Li", "Dongmei Chen"], "title": "Constrained Optimal Planning to Minimize Battery Degradation of Autonomous Mobile Robots", "comment": null, "summary": "This paper proposes an optimization framework that addresses both cycling\ndegradation and calendar aging of batteries for autonomous mobile robot (AMR)\nto minimize battery degradation while ensuring task completion. A rectangle\nmethod of piecewise linear approximation is employed to linearize the bilinear\noptimization problem. We conduct a case study to validate the efficiency of the\nproposed framework in achieving an optimal path planning for AMRs while\nreducing battery aging."}
{"id": "2506.12261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12261", "abs": "https://arxiv.org/abs/2506.12261", "authors": ["Sreevishakh Vasudevan", "Som Sagar", "Ransalu Senanayake"], "title": "Strategic Vantage Selection for Learning Viewpoint-Agnostic Manipulation Policies", "comment": null, "summary": "Vision-based manipulation has shown remarkable success, achieving promising\nperformance across a range of tasks. However, these manipulation policies often\nfail to generalize beyond their training viewpoints, which is a persistent\nchallenge in achieving perspective-agnostic manipulation, especially in\nsettings where the camera is expected to move at runtime. Although collecting\ndata from many angles seems a natural solution, such a naive approach is both\nresource-intensive and degrades manipulation policy performance due to\nexcessive and unstructured visual diversity. This paper proposes Vantage, a\nframework that systematically identifies and integrates data from optimal\nperspectives to train robust, viewpoint-agnostic policies. By formulating\nviewpoint selection as a continuous optimization problem, we iteratively\nfine-tune policies on a few vantage points. Since we leverage Bayesian\noptimization to efficiently navigate the infinite space of potential camera\nconfigurations, we are able to balance exploration of novel views and\nexploitation of high-performing ones, thereby ensuring data collection from a\nminimal number of effective viewpoints. We empirically evaluate this framework\non diverse standard manipulation tasks using multiple policy learning methods,\ndemonstrating that fine-tuning with data from strategic camera placements\nyields substantial performance gains, achieving average improvements of up to\n46.19% when compared to fixed, random, or heuristic-based strategies."}
{"id": "2506.13149", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13149", "abs": "https://arxiv.org/abs/2506.13149", "authors": ["Jaehong Oh"], "title": "Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots", "comment": null, "summary": "This paper presents SEGO (Semantic Graph Ontology), a cognitive mapping\narchitecture designed to integrate geometric perception, semantic reasoning,\nand explanation generation into a unified framework for human-centric\ncollaborative robotics. SEGO constructs dynamic cognitive scene graphs that\nrepresent not only the spatial configuration of the environment but also the\nsemantic relations and ontological consistency among detected objects. The\narchitecture seamlessly combines SLAM-based localization, deep-learning-based\nobject detection and tracking, and ontology-driven reasoning to enable\nreal-time, semantically coherent mapping."}
{"id": "2506.12273", "categories": ["cs.RO", "cs.SY", "eess.SY", "93B30 (Primary), 93B35 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.12273", "abs": "https://arxiv.org/abs/2506.12273", "authors": ["Rongfei Li", "Francis Assadian"], "title": "Role of Uncertainty in Model Development and Control Design for a Manufacturing Process", "comment": "35 pages, 26 figures, Book Chapter. Published in: Role of Uncertainty\n  in Model Development and Control Design for a Manufacturing Process,\n  IntechOpen, 2022. For published version, see this http URL:\n  https://doi.org/10.5772/intechopen.104780", "summary": "The use of robotic technology has drastically increased in manufacturing in\nthe 21st century. But by utilizing their sensory cues, humans still outperform\nmachines, especially in the micro scale manufacturing, which requires\nhigh-precision robot manipulators. These sensory cues naturally compensate for\nhigh level of uncertainties that exist in the manufacturing environment.\nUncertainties in performing manufacturing tasks may come from measurement\nnoise, model inaccuracy, joint compliance (e.g., elasticity) etc. Although\nadvanced metrology sensors and high-precision microprocessors, which are\nutilized in nowadays robots, have compensated for many structural and dynamic\nerrors in robot positioning, but a well-designed control algorithm still works\nas a comparable and cheaper alternative to reduce uncertainties in automated\nmanufacturing. Our work illustrates that a multi-robot control system can\nreduce various uncertainties to a great amount."}
{"id": "2506.13421", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13421", "abs": "https://arxiv.org/abs/2506.13421", "authors": ["Dongliang Zheng", "Yebin Wang", "Stefano Di Cairano", "Panagiotis Tsiotras"], "title": "Delayed Expansion AGT: Kinodynamic Planning with Application to Tractor-Trailer Parking", "comment": null, "summary": "Kinodynamic planning of articulated vehicles in cluttered environments faces\nadditional challenges arising from high-dimensional state space and complex\nsystem dynamics. Built upon [1],[2], this work proposes the DE-AGT algorithm\nthat grows a tree using pre-computed motion primitives (MPs) and A* heuristics.\nThe first feature of DE-AGT is a delayed expansion of MPs. In particular, the\nMPs are divided into different modes, which are ranked online. With the MP\nclassification and prioritization, DE-AGT expands the most promising mode of\nMPs first, which eliminates unnecessary computation and finds solutions faster.\nTo obtain the cost-to-go heuristic for nonholonomic articulated vehicles, we\nrely on supervised learning and train neural networks for fast and accurate\ncost-to-go prediction. The learned heuristic is used for online mode ranking\nand node selection. Another feature of DE-AGT is the improved goal-reaching.\nExactly reaching a goal state usually requires a constant connection checking\nwith the goal by solving steering problems -- non-trivial and time-consuming\nfor articulated vehicles. The proposed termination scheme overcomes this\nchallenge by tightly integrating a light-weight trajectory tracking controller\nwith the search process. DE-AGT is implemented for autonomous parking of a\ngeneral car-like tractor with 3-trailer. Simulation results show an average of\n10x acceleration compared to a previous method."}
{"id": "2506.12312", "categories": ["cs.RO", "cs.CL", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2506.12312", "abs": "https://arxiv.org/abs/2506.12312", "authors": ["Kan Hatakeyama-Sato", "Toshihiko Nishida", "Kenta Kitamura", "Yoshitaka Ushiku", "Koichi Takahashi", "Yuta Nabae", "Teruaki Hayakawa"], "title": "Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research", "comment": null, "summary": "This review explores the potential of foundation models to advance laboratory\nautomation in the materials and chemical sciences. It emphasizes the dual roles\nof these models: cognitive functions for experimental planning and data\nanalysis, and physical functions for hardware operations. While traditional\nlaboratory automation has relied heavily on specialized, rigid systems,\nfoundation models offer adaptability through their general-purpose intelligence\nand multimodal capabilities. Recent advancements have demonstrated the\nfeasibility of using large language models (LLMs) and multimodal robotic\nsystems to handle complex and dynamic laboratory tasks. However, significant\nchallenges remain, including precision manipulation of hardware, integration of\nmultimodal data, and ensuring operational safety. This paper outlines a roadmap\nhighlighting future directions, advocating for close interdisciplinary\ncollaboration, benchmark establishment, and strategic human-AI integration to\nrealize fully autonomous experimental laboratories."}
{"id": "2506.13498", "categories": ["cs.RO", "cs.HC", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13498", "abs": "https://arxiv.org/abs/2506.13498", "authors": ["Toshiaki Tsuji", "Yasuhiro Kato", "Gokhan Solak", "Heng Zhang", "Tadej Petrič", "Francesco Nori", "Arash Ajoudani"], "title": "A Survey on Imitation Learning for Contact-Rich Tasks in Robotics", "comment": "47pages, 1 figures", "summary": "This paper comprehensively surveys research trends in imitation learning for\ncontact-rich robotic tasks. Contact-rich tasks, which require complex physical\ninteractions with the environment, represent a central challenge in robotics\ndue to their nonlinear dynamics and sensitivity to small positional deviations.\nThe paper examines demonstration collection methodologies, including teaching\nmethods and sensory modalities crucial for capturing subtle interaction\ndynamics. We then analyze imitation learning approaches, highlighting their\napplications to contact-rich manipulation. Recent advances in multimodal\nlearning and foundation models have significantly enhanced performance in\ncomplex contact tasks across industrial, household, and healthcare domains.\nThrough systematic organization of current research and identification of\nchallenges, this survey provides a foundation for future advancements in\ncontact-rich robotic manipulation."}
{"id": "2506.12314", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.12314", "abs": "https://arxiv.org/abs/2506.12314", "authors": ["Xiaoshuai Ma", "Haoxiang Qi", "Qingqing Li", "Haochen Xu", "Xuechao Chen", "Junyao Gao", "Zhangguo Yu", "Qiang Huang"], "title": "Explosive Output to Enhance Jumping Ability: A Variable Reduction Ratio Design Paradigm for Humanoid Robots Knee Joint", "comment": null, "summary": "Enhancing the explosive power output of the knee joints is critical for\nimproving the agility and obstacle-crossing capabilities of humanoid robots.\nHowever, a mismatch between the knee-to-center-of-mass (CoM) transmission ratio\nand jumping demands, coupled with motor performance degradation at high speeds,\nrestricts the duration of high-power output and limits jump performance. To\naddress these problems, this paper introduces a novel knee joint design\nparadigm employing a dynamically decreasing reduction ratio for explosive\noutput during jump. Analysis of motor output characteristics and knee\nkinematics during jumping inspired a coupling strategy in which the reduction\nratio gradually decreases as the joint extends. A high initial ratio rapidly\nincreases torque at jump initiation, while its gradual reduction minimizes\nmotor speed increments and power losses, thereby maintaining sustained\nhigh-power output. A compact and efficient linear actuator-driven guide-rod\nmechanism realizes this coupling strategy, supported by parameter optimization\nguided by explosive jump control strategies. Experimental validation\ndemonstrated a 63 cm vertical jump on a single-joint platform (a theoretical\nimprovement of 28.1\\% over the optimal fixed-ratio joints). Integrated into a\nhumanoid robot, the proposed design enabled a 1.1 m long jump, a 0.5 m vertical\njump, and a 0.5 m box jump."}
{"id": "2506.12374", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.10; I.4.8; H.5.2"], "pdf": "https://arxiv.org/pdf/2506.12374", "abs": "https://arxiv.org/abs/2506.12374", "authors": ["Wenbo Li", "Shiyi Wang", "Yiteng Chen", "Huiping Zhuang", "Qingyao Wu"], "title": "AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making", "comment": "submitted to NeurIPS 2025", "summary": "Vision-Language Models (VLMs) encode knowledge and reasoning capabilities for\nrobotic manipulation within high-dimensional representation spaces. However,\ncurrent approaches often project them into compressed intermediate\nrepresentations, discarding important task-specific information such as\nfine-grained spatial or semantic details. To address this, we propose\nAntiGrounding, a new framework that reverses the instruction grounding process.\nIt lifts candidate actions directly into the VLM representation space, renders\ntrajectories from multiple views, and uses structured visual question answering\nfor instruction-based decision making. This enables zero-shot synthesis of\noptimal closed-loop robot trajectories for new tasks. We also propose an\noffline policy refinement module that leverages past experience to enhance\nlong-term performance. Experiments in both simulation and real-world\nenvironments show that our method outperforms baselines across diverse robotic\nmanipulation tasks."}
{"id": "2506.12507", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12507", "abs": "https://arxiv.org/abs/2506.12507", "authors": ["Pablo Gonzalez-Oliveras", "Olov Engwall", "Ali Reza Majlesi"], "title": "Sense and Sensibility: What makes a social robot convincing to high-school students?", "comment": "14 pages; 8 figures; 3 tables; RSS 2025 (Robotics: Science & Systems)", "summary": "This study with 40 high-school students demonstrates the high influence of a\nsocial educational robot on students' decision-making for a set of eight\ntrue-false questions on electric circuits, for which the theory had been\ncovered in the students' courses. The robot argued for the correct answer on\nsix questions and the wrong on two, and 75% of the students were persuaded by\nthe robot to perform beyond their expected capacity, positively when the robot\nwas correct and negatively when it was wrong. Students with more experience of\nusing large language models were even more likely to be influenced by the\nrobot's stance -- in particular for the two easiest questions on which the\nrobot was wrong -- suggesting that familiarity with AI can increase\nsusceptibility to misinformation by AI.\n  We further examined how three different levels of portrayed robot certainty,\ndisplayed using semantics, prosody and facial signals, affected how the\nstudents aligned with the robot's answer on specific questions and how\nconvincing they perceived the robot to be on these questions. The students\naligned with the robot's answers in 94.4% of the cases when the robot was\nportrayed as Certain, 82.6% when it was Neutral and 71.4% when it was\nUncertain. The alignment was thus high for all conditions, highlighting\nstudents' general susceptibility to accept the robot's stance, but alignment in\nthe Uncertain condition was significantly lower than in the Certain. Post-test\nquestionnaire answers further show that students found the robot most\nconvincing when it was portrayed as Certain. These findings highlight the need\nfor educational robots to adjust their display of certainty based on the\nreliability of the information they convey, to promote students' critical\nthinking and reduce undue influence."}
{"id": "2506.12525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12525", "abs": "https://arxiv.org/abs/2506.12525", "authors": ["Peng Wang", "Minh Huy Pham", "Zhihao Guo", "Wei Zhou"], "title": "A Spatial Relationship Aware Dataset for Robotics", "comment": "7 pages; 7 figures, 1 table", "summary": "Robotic task planning in real-world environments requires not only object\nrecognition but also a nuanced understanding of spatial relationships between\nobjects. We present a spatial-relationship-aware dataset of nearly 1,000\nrobot-acquired indoor images, annotated with object attributes, positions, and\ndetailed spatial relationships. Captured using a Boston Dynamics Spot robot and\nlabelled with a custom annotation tool, the dataset reflects complex scenarios\nwith similar or identical objects and intricate spatial arrangements. We\nbenchmark six state-of-the-art scene-graph generation models on this dataset,\nanalysing their inference speed and relational accuracy. Our results highlight\nsignificant differences in model performance and demonstrate that integrating\nexplicit spatial relationships into foundation models, such as ChatGPT 4o,\nsubstantially improves their ability to generate executable, spatially-aware\nplans for robotics. The dataset and annotation tool are publicly available at\nhttps://github.com/PengPaulWang/SpatialAwareRobotDataset, supporting further\nresearch in spatial reasoning for robotics."}
{"id": "2506.12536", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12536", "abs": "https://arxiv.org/abs/2506.12536", "authors": ["Farida Mohsen", "Ali Safa"], "title": "Deep Fusion of Ultra-Low-Resolution Thermal Camera and Gyroscope Data for Lighting-Robust and Compute-Efficient Rotational Odometry", "comment": null, "summary": "Accurate rotational odometry is crucial for autonomous robotic systems,\nparticularly for small, power-constrained platforms such as drones and mobile\nrobots. This study introduces thermal-gyro fusion, a novel sensor fusion\napproach that integrates ultra-low-resolution thermal imaging with gyroscope\nreadings for rotational odometry. Unlike RGB cameras, thermal imaging is\ninvariant to lighting conditions and, when fused with gyroscopic data,\nmitigates drift which is a common limitation of inertial sensors. We first\ndevelop a multimodal data acquisition system to collect synchronized thermal\nand gyroscope data, along with rotational speed labels, across diverse\nenvironments. Subsequently, we design and train a lightweight Convolutional\nNeural Network (CNN) that fuses both modalities for rotational speed\nestimation. Our analysis demonstrates that thermal-gyro fusion enables a\nsignificant reduction in thermal camera resolution without significantly\ncompromising accuracy, thereby improving computational efficiency and memory\nutilization. These advantages make our approach well-suited for real-time\ndeployment in resource-constrained robotic systems. Finally, to facilitate\nfurther research, we publicly release our dataset as supplementary material."}
{"id": "2506.12676", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12676", "abs": "https://arxiv.org/abs/2506.12676", "authors": ["Yingyi Kuang", "Luis J. Manso", "George Vogiatzis"], "title": "Goal-based Self-Adaptive Generative Adversarial Imitation Learning (Goal-SAGAIL) for Multi-goal Robotic Manipulation Tasks", "comment": "6 pages, 5 figures", "summary": "Reinforcement learning for multi-goal robot manipulation tasks poses\nsignificant challenges due to the diversity and complexity of the goal space.\nTechniques such as Hindsight Experience Replay (HER) have been introduced to\nimprove learning efficiency for such tasks. More recently, researchers have\ncombined HER with advanced imitation learning methods such as Generative\nAdversarial Imitation Learning (GAIL) to integrate demonstration data and\naccelerate training speed. However, demonstration data often fails to provide\nenough coverage for the goal space, especially when acquired from human\nteleoperation. This biases the learning-from-demonstration process toward\nmastering easier sub-tasks instead of tackling the more challenging ones. In\nthis work, we present Goal-based Self-Adaptive Generative Adversarial Imitation\nLearning (Goal-SAGAIL), a novel framework specifically designed for multi-goal\nrobot manipulation tasks. By integrating self-adaptive learning principles with\ngoal-conditioned GAIL, our approach enhances imitation learning efficiency,\neven when limited, suboptimal demonstrations are available. Experimental\nresults validate that our method significantly improves learning efficiency\nacross various multi-goal manipulation scenarios -- including complex in-hand\nmanipulation tasks -- using suboptimal demonstrations provided by both\nsimulation and human experts."}
{"id": "2506.12678", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12678", "abs": "https://arxiv.org/abs/2506.12678", "authors": ["Pranay Gupta", "Henny Admoni", "Andrea Bajcsy"], "title": "Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence", "comment": "15 pages, 11 figures", "summary": "End-to-end visuomotor policies trained using behavior cloning have shown a\nremarkable ability to generate complex, multi-modal low-level robot behaviors.\nHowever, at deployment time, these policies still struggle to act reliably when\nfaced with out-of-distribution (OOD) visuals induced by objects, backgrounds,\nor environment changes. Prior works in interactive imitation learning solicit\ncorrective expert demonstrations under the OOD conditions -- but this can be\ncostly and inefficient. We observe that task success under OOD conditions does\nnot always warrant novel robot behaviors. In-distribution (ID) behaviors can\ndirectly be transferred to OOD conditions that share functional similarities\nwith ID conditions. For example, behaviors trained to interact with\nin-distribution (ID) pens can apply to interacting with a visually-OOD pencil.\nThe key challenge lies in disambiguating which ID observations functionally\ncorrespond to the OOD observation for the task at hand. We propose that an\nexpert can provide this OOD-to-ID functional correspondence. Thus, instead of\ncollecting new demonstrations and re-training at every OOD encounter, our\nmethod: (1) detects the need for feedback by first checking if current\nobservations are OOD and then identifying whether the most similar training\nobservations show divergent behaviors, (2) solicits functional correspondence\nfeedback to disambiguate between those behaviors, and (3) intervenes on the OOD\nobservations with the functionally corresponding ID observations to perform\ndeployment-time generalization. We validate our method across diverse\nreal-world robotic manipulation tasks with a Franka Panda robotic manipulator.\nOur results show that test-time functional correspondences can improve the\ngeneralization of a vision-based diffusion policy to OOD objects and\nenvironment conditions with low feedback."}
{"id": "2506.12710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12710", "abs": "https://arxiv.org/abs/2506.12710", "authors": ["Yuqi Ping", "Tianhao Liang", "Huahao Ding", "Guangyu Lei", "Junwei Wu", "Xuan Zou", "Kuan Shi", "Rui Shao", "Chiya Zhang", "Weizheng Zhang", "Weijie Yuan", "Tingting Zhang"], "title": "Multimodal Large Language Models-Enabled UAV Swarm: Towards Efficient and Intelligent Autonomous Aerial Systems", "comment": "8 pages, 5 figures,submitted to IEEE wcm", "summary": "Recent breakthroughs in multimodal large language models (MLLMs) have endowed\nAI systems with unified perception, reasoning and natural-language interaction\nacross text, image and video streams. Meanwhile, Unmanned Aerial Vehicle (UAV)\nswarms are increasingly deployed in dynamic, safety-critical missions that\ndemand rapid situational understanding and autonomous adaptation. This paper\nexplores potential solutions for integrating MLLMs with UAV swarms to enhance\nthe intelligence and adaptability across diverse tasks. Specifically, we first\noutline the fundamental architectures and functions of UAVs and MLLMs. Then, we\nanalyze how MLLMs can enhance the UAV system performance in terms of target\ndetection, autonomous navigation, and multi-agent coordination, while exploring\nsolutions for integrating MLLMs into UAV systems. Next, we propose a practical\ncase study focused on the forest fire fighting. To fully reveal the\ncapabilities of the proposed framework, human-machine interaction, swarm task\nplanning, fire assessment, and task execution are investigated. Finally, we\ndiscuss the challenges and future research directions for the MLLMs-enabled UAV\nswarm. An experiment illustration video could be found online at\nhttps://youtu.be/zwnB9ZSa5A4."}
{"id": "2506.12742", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12742", "abs": "https://arxiv.org/abs/2506.12742", "authors": ["Yuchen Liu", "Alexiy Buynitsky", "Ruiqi Ni", "Ahmed H. Qureshi"], "title": "Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments", "comment": null, "summary": "Physics-informed Neural Motion Planners (PiNMPs) provide a data-efficient\nframework for solving the Eikonal Partial Differential Equation (PDE) and\nrepresenting the cost-to-go function for motion planning. However, their\nscalability remains limited by spectral bias and the complex loss landscape of\nPDE-driven training. Domain decomposition mitigates these issues by dividing\nthe environment into smaller subdomains, but existing methods enforce\ncontinuity only at individual spatial points. While effective for function\napproximation, these methods fail to capture the spatial connectivity required\nfor motion planning, where the cost-to-go function depends on both the start\nand goal coordinates rather than a single query point. We propose Finite Basis\nNeural Time Fields (FB-NTFields), a novel neural field representation for\nscalable cost-to-go estimation. Instead of enforcing continuity in output\nspace, FB-NTFields construct a latent space representation, computing the\ncost-to-go as a distance between the latent embeddings of start and goal\ncoordinates. This enables global spatial coherence while integrating domain\ndecomposition, ensuring efficient large-scale motion planning. We validate\nFB-NTFields in complex synthetic and real-world scenarios, demonstrating\nsubstantial improvements over existing PiNMPs. Finally, we deploy our method on\na Unitree B1 quadruped robot, successfully navigating indoor environments. The\nsupplementary videos can be found at https://youtu.be/OpRuCbLNOwM."}
{"id": "2506.12762", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12762", "abs": "https://arxiv.org/abs/2506.12762", "authors": ["Adrian Rubio-Solis", "Luciano Nava-Balanzar", "Tomas Salgado-Jimenez"], "title": "On-board Sonar Data Classification for Path Following in Underwater Vehicles using Fast Interval Type-2 Fuzzy Extreme Learning Machine", "comment": null, "summary": "In autonomous underwater missions, the successful completion of predefined\npaths mainly depends on the ability of underwater vehicles to recognise their\nsurroundings. In this study, we apply the concept of Fast Interval Type-2 Fuzzy\nExtreme Learning Machine (FIT2-FELM) to train a Takagi-Sugeno-Kang IT2 Fuzzy\nInference System (TSK IT2-FIS) for on-board sonar data classification using an\nunderwater vehicle called BlueROV2. The TSK IT2-FIS is integrated into a\nHierarchical Navigation Strategy (HNS) as the main navigation engine to infer\nlocal motions and provide the BlueROV2 with full autonomy to follow an\nobstacle-free trajectory in a water container of 2.5m x 2.5m x 3.5m. Compared\nto traditional navigation architectures, using the proposed method, we observe\na robust path following behaviour in the presence of uncertainty and noise. We\nfound that the proposed approach provides the BlueROV with a more complete\nsensory picture about its surroundings while real-time navigation planning is\nperformed by the concurrent execution of two or more tasks."}
{"id": "2506.12769", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12769", "abs": "https://arxiv.org/abs/2506.12769", "authors": ["Junpeng Yue", "Zepeng Wang", "Yuxuan Wang", "Weishuai Zeng", "Jiangxing Wang", "Xinrun Xu", "Yu Zhang", "Sipeng Zheng", "Ziluo Ding", "Zongqing Lu"], "title": "RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control", "comment": null, "summary": "This paper focuses on a critical challenge in robotics: translating\ntext-driven human motions into executable actions for humanoid robots, enabling\nefficient and cost-effective learning of new behaviors. While existing\ntext-to-motion generation methods achieve semantic alignment between language\nand motion, they often produce kinematically or physically infeasible motions\nunsuitable for real-world deployment. To bridge this sim-to-real gap, we\npropose Reinforcement Learning from Physical Feedback (RLPF), a novel framework\nthat integrates physics-aware motion evaluation with text-conditioned motion\ngeneration. RLPF employs a motion tracking policy to assess feasibility in a\nphysics simulator, generating rewards for fine-tuning the motion generator.\nFurthermore, RLPF introduces an alignment verification module to preserve\nsemantic fidelity to text instructions. This joint optimization ensures both\nphysical plausibility and instruction alignment. Extensive experiments show\nthat RLPF greatly outperforms baseline methods in generating physically\nfeasible motions while maintaining semantic correspondence with text\ninstruction, enabling successful deployment on real humanoid robots."}
{"id": "2506.12779", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12779", "abs": "https://arxiv.org/abs/2506.12779", "authors": ["Yuxuan Wang", "Ming Yang", "Weishuai Zeng", "Yu Zhang", "Xinrun Xu", "Haobin Jiang", "Ziluo Ding", "Zongqing Lu"], "title": "From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots", "comment": null, "summary": "Achieving general agile whole-body control on humanoid robots remains a major\nchallenge due to diverse motion demands and data conflicts. While existing\nframeworks excel in training single motion-specific policies, they struggle to\ngeneralize across highly varied behaviors due to conflicting control\nrequirements and mismatched data distributions. In this work, we propose\nBumbleBee (BB), an expert-generalist learning framework that combines motion\nclustering and sim-to-real adaptation to overcome these challenges. BB first\nleverages an autoencoder-based clustering method to group behaviorally similar\nmotions using motion features and motion descriptions. Expert policies are then\ntrained within each cluster and refined with real-world data through iterative\ndelta action modeling to bridge the sim-to-real gap. Finally, these experts are\ndistilled into a unified generalist controller that preserves agility and\nrobustness across all motion types. Experiments on two simulations and a real\nhumanoid robot demonstrate that BB achieves state-of-the-art general whole-body\ncontrol, setting a new benchmark for agile, robust, and generalizable humanoid\nperformance in the real world."}
{"id": "2506.12851", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12851", "abs": "https://arxiv.org/abs/2506.12851", "authors": ["Weiji Xie", "Jinrui Han", "Jiakun Zheng", "Huanyu Li", "Xinzhe Liu", "Jiyuan Shi", "Weinan Zhang", "Chenjia Bai", "Xuelong Li"], "title": "KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills", "comment": null, "summary": "Humanoid robots are promising to acquire various skills by imitating human\nbehaviors. However, existing algorithms are only capable of tracking smooth,\nlow-speed human motions, even with delicate reward and curriculum design. This\npaper presents a physics-based humanoid control framework, aiming to master\nhighly-dynamic human behaviors such as Kungfu and dancing through multi-steps\nmotion processing and adaptive motion tracking. For motion processing, we\ndesign a pipeline to extract, filter out, correct, and retarget motions, while\nensuring compliance with physical constraints to the maximum extent. For motion\nimitation, we formulate a bi-level optimization problem to dynamically adjust\nthe tracking accuracy tolerance based on the current tracking error, creating\nan adaptive curriculum mechanism. We further construct an asymmetric\nactor-critic framework for policy training. In experiments, we train whole-body\ncontrol policies to imitate a set of highly-dynamic motions. Our method\nachieves significantly lower tracking errors than existing approaches and is\nsuccessfully deployed on the Unitree G1 robot, demonstrating stable and\nexpressive behaviors. The project page is https://kungfu-bot.github.io."}
{"id": "2506.13019", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13019", "abs": "https://arxiv.org/abs/2506.13019", "authors": ["Jiachen Li", "Jian Chu", "Feiyang Zhao", "Shihao Li", "Wei Li", "Dongmei Chen"], "title": "Constrained Optimal Planning to Minimize Battery Degradation of Autonomous Mobile Robots", "comment": null, "summary": "This paper proposes an optimization framework that addresses both cycling\ndegradation and calendar aging of batteries for autonomous mobile robot (AMR)\nto minimize battery degradation while ensuring task completion. A rectangle\nmethod of piecewise linear approximation is employed to linearize the bilinear\noptimization problem. We conduct a case study to validate the efficiency of the\nproposed framework in achieving an optimal path planning for AMRs while\nreducing battery aging."}
{"id": "2506.13079", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.13079", "abs": "https://arxiv.org/abs/2506.13079", "authors": ["Qidi Fang", "Hang Yu", "Shijie Fang", "Jindan Huang", "Qiuyu Chen", "Reuben M. Aronson", "Elaine S. Short"], "title": "CHARM: Considering Human Attributes for Reinforcement Modeling", "comment": null, "summary": "Reinforcement Learning from Human Feedback has recently achieved significant\nsuccess in various fields, and its performance is highly related to feedback\nquality. While much prior work acknowledged that human teachers'\ncharacteristics would affect human feedback patterns, there is little work that\nhas closely investigated the actual effects. In this work, we designed an\nexploratory study investigating how human feedback patterns are associated with\nhuman characteristics. We conducted a public space study with two long horizon\ntasks and 46 participants. We found that feedback patterns are not only\ncorrelated with task statistics, such as rewards, but also correlated with\nparticipants' characteristics, especially robot experience and educational\nbackground. Additionally, we demonstrated that human feedback value can be more\naccurately predicted with human characteristics compared to only using task\nstatistics. All human feedback and characteristics we collected, and codes for\nour data collection and predicting more accurate human feedback are available\nat https://github.com/AABL-Lab/CHARM"}
{"id": "2506.13087", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13087", "abs": "https://arxiv.org/abs/2506.13087", "authors": ["Zeyu Zhang", "Ziyuan Jiao"], "title": "IKDiffuser: Fast and Diverse Inverse Kinematics Solution Generation for Multi-arm Robotic Systems", "comment": "under review", "summary": "Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has\nprimarily been successful with single serial manipulators. For multi-arm\nrobotic systems, IK remains challenging due to complex self-collisions, coupled\njoints, and high-dimensional redundancy. These complexities make traditional IK\nsolvers slow, prone to failure, and lacking in solution diversity. In this\npaper, we present IKDiffuser, a diffusion-based model designed for fast and\ndiverse IK solution generation for multi-arm robotic systems. IKDiffuser learns\nthe joint distribution over the configuration space, capturing complex\ndependencies and enabling seamless generalization to multi-arm robotic systems\nof different structures. In addition, IKDiffuser can incorporate additional\nobjectives during inference without retraining, offering versatility and\nadaptability for task-specific requirements. In experiments on 6 different\nmulti-arm systems, the proposed IKDiffuser achieves superior solution accuracy,\nprecision, diversity, and computational efficiency compared to existing\nsolvers. The proposed IKDiffuser framework offers a scalable, unified approach\nto solving multi-arm IK problems, facilitating the potential of multi-arm\nrobotic systems in real-time manipulation tasks."}
{"id": "2506.13100", "categories": ["cs.RO", "cs.CV", "93C85", "I.4"], "pdf": "https://arxiv.org/pdf/2506.13100", "abs": "https://arxiv.org/abs/2506.13100", "authors": ["Zhanhua Xin", "Zhihao Wang", "Shenghao Zhang", "Wanchao Chi", "Yan Meng", "Shihan Kong", "Yan Xiong", "Chong Zhang", "Yuzhen Liu", "Junzhi Yu"], "title": "A Novel ViDAR Device With Visual Inertial Encoder Odometry and Reinforcement Learning-Based Active SLAM Method", "comment": "12 pages, 13 figures", "summary": "In the field of multi-sensor fusion for simultaneous localization and mapping\n(SLAM), monocular cameras and IMUs are widely used to build simple and\neffective visual-inertial systems. However, limited research has explored the\nintegration of motor-encoder devices to enhance SLAM performance. By\nincorporating such devices, it is possible to significantly improve active\ncapability and field of view (FOV) with minimal additional cost and structural\ncomplexity. This paper proposes a novel visual-inertial-encoder tightly coupled\nodometry (VIEO) based on a ViDAR (Video Detection and Ranging) device. A ViDAR\ncalibration method is introduced to ensure accurate initialization for VIEO. In\naddition, a platform motion decoupled active SLAM method based on deep\nreinforcement learning (DRL) is proposed. Experimental data demonstrate that\nthe proposed ViDAR and the VIEO algorithm significantly increase cross-frame\nco-visibility relationships compared to its corresponding visual-inertial\nodometry (VIO) algorithm, improving state estimation accuracy. Additionally,\nthe DRL-based active SLAM algorithm, with the ability to decouple from platform\nmotion, can increase the diversity weight of the feature points and further\nenhance the VIEO algorithm's performance. The proposed methodology sheds fresh\ninsights into both the updated platform design and decoupled approach of active\nSLAM systems in complex environments."}
{"id": "2506.13105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13105", "abs": "https://arxiv.org/abs/2506.13105", "authors": ["Fen Liu", "Chengfeng Jia", "Na Zhang", "Shenghai Yuan", "Rong Su"], "title": "Underwater target 6D State Estimation via UUV Attitude Enhance Observability", "comment": "Paper has been accepted in IROS 2025", "summary": "Accurate relative state observation of Unmanned Underwater Vehicles (UUVs)\nfor tracking uncooperative targets remains a significant challenge due to the\nabsence of GPS, complex underwater dynamics, and sensor limitations. Existing\nlocalization approaches rely on either global positioning infrastructure or\nmulti-UUV collaboration, both of which are impractical for a single UUV\noperating in large or unknown environments. To address this, we propose a novel\npersistent relative 6D state estimation framework that enables a single UUV to\nestimate its relative motion to a non-cooperative target using only successive\nnoisy range measurements from two monostatic sonar sensors. Our key\ncontribution is an observability-enhanced attitude control strategy, which\noptimally adjusts the UUV's orientation to improve the observability of\nrelative state estimation using a Kalman filter, effectively mitigating the\nimpact of sensor noise and drift accumulation. Additionally, we introduce a\nrigorously proven Lyapunov-based tracking control strategy that guarantees\nlong-term stability by ensuring that the UUV maintains an optimal measurement\nrange, preventing localization errors from diverging over time. Through\ntheoretical analysis and simulations, we demonstrate that our method\nsignificantly improves 6D relative state estimation accuracy and robustness\ncompared to conventional approaches. This work provides a scalable,\ninfrastructure-free solution for UUVs tracking uncooperative targets\nunderwater."}
{"id": "2506.13106", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13106", "abs": "https://arxiv.org/abs/2506.13106", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Rong Su"], "title": "Autonomous 3D Moving Target Encirclement and Interception with Range measurement", "comment": "Paper has been accepted into IROS 2025", "summary": "Commercial UAVs are an emerging security threat as they are capable of\ncarrying hazardous payloads or disrupting air traffic. To counter UAVs, we\nintroduce an autonomous 3D target encirclement and interception strategy.\nUnlike traditional ground-guided systems, this strategy employs autonomous\ndrones to track and engage non-cooperative hostile UAVs, which is effective in\nnon-line-of-sight conditions, GPS denial, and radar jamming, where conventional\ndetection and neutralization from ground guidance fail. Using two noisy\nreal-time distances measured by drones, guardian drones estimate the relative\nposition from their own to the target using observation and velocity\ncompensation methods, based on anti-synchronization (AS) and an X$-$Y circular\nmotion combined with vertical jitter. An encirclement control mechanism is\nproposed to enable UAVs to adaptively transition from encircling and protecting\na target to encircling and monitoring a hostile target. Upon breaching a\nwarning threshold, the UAVs may even employ a suicide attack to neutralize the\nhostile target. We validate this strategy through real-world UAV experiments\nand simulated analysis in MATLAB, demonstrating its effectiveness in detecting,\nencircling, and intercepting hostile drones. More details:\nhttps://youtu.be/5eHW56lPVto."}
{"id": "2506.13149", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13149", "abs": "https://arxiv.org/abs/2506.13149", "authors": ["Jaehong Oh"], "title": "Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots", "comment": null, "summary": "This paper presents SEGO (Semantic Graph Ontology), a cognitive mapping\narchitecture designed to integrate geometric perception, semantic reasoning,\nand explanation generation into a unified framework for human-centric\ncollaborative robotics. SEGO constructs dynamic cognitive scene graphs that\nrepresent not only the spatial configuration of the environment but also the\nsemantic relations and ontological consistency among detected objects. The\narchitecture seamlessly combines SLAM-based localization, deep-learning-based\nobject detection and tracking, and ontology-driven reasoning to enable\nreal-time, semantically coherent mapping."}
{"id": "2506.13198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13198", "abs": "https://arxiv.org/abs/2506.13198", "authors": ["Bin-Bin Hu", "Bayu Jayawardhana", "Ming Cao"], "title": "Equilibrium-Driven Smooth Separation and Navigation of Marsupial Robotic Systems", "comment": null, "summary": "In this paper, we propose an equilibrium-driven controller that enables a\nmarsupial carrier-passenger robotic system to achieve smooth carrier-passenger\nseparation and then to navigate the passenger robot toward a predetermined\ntarget point. Particularly, we design a potential gradient in the form of a\ncubic polynomial for the passenger's controller as a function of the\ncarrier-passenger and carrier-target distances in the moving carrier's frame.\nThis introduces multiple equilibrium points corresponding to the zero state of\nthe error dynamic system during carrier-passenger separation. The change of\nequilibrium points is associated with the change in their attraction regions,\nenabling smooth carrier-passenger separation and afterwards seamless navigation\ntoward the target. Finally, simulations demonstrate the effectiveness and\nadaptability of the proposed controller in environments containing obstacles."}
{"id": "2506.13202", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13202", "abs": "https://arxiv.org/abs/2506.13202", "authors": ["Bin-Bin Hu", "Yanxin Zhou", "Henglai Wei", "Shuo Cheng", "Chen Lv"], "title": "C2TE: Coordinated Constrained Task Execution Design for Ordering-Flexible Multi-Vehicle Platoon Merging", "comment": null, "summary": "In this paper, we propose a distributed coordinated constrained task\nexecution (C2TE) algorithm that enables a team of vehicles from different lanes\nto cooperatively merge into an {\\it ordering-flexible platoon} maneuvering on\nthe desired lane. Therein, the platoon is flexible in the sense that no\nspecific spatial ordering sequences of vehicles are predetermined. To attain\nsuch a flexible platoon, we first separate the multi-vehicle platoon (MVP)\nmerging mission into two stages, namely, pre-merging regulation and {\\it\nordering-flexible platoon} merging, and then formulate them into distributed\nconstraint-based optimization problems. Particularly, by encoding\nlongitudinal-distance regulation and same-lane collision avoidance subtasks\ninto the corresponding control barrier function (CBF) constraints, the proposed\nalgorithm in Stage 1 can safely enlarge sufficient longitudinal distances among\nadjacent vehicles. Then, by encoding lateral convergence, longitudinal-target\nattraction, and neighboring collision avoidance subtasks into CBF constraints,\nthe proposed algorithm in Stage~2 can efficiently achieve the {\\it\nordering-flexible platoon}. Note that the {\\it ordering-flexible platoon} is\nrealized through the interaction of the longitudinal-target attraction and\ntime-varying neighboring collision avoidance constraints simultaneously.\nFeasibility guarantee and rigorous convergence analysis are both provided under\nstrong nonlinear couplings induced by flexible orderings. Finally, experiments\nusing three autonomous mobile vehicles (AMVs) are conducted to verify the\neffectiveness and flexibility of the proposed algorithm, and extensive\nsimulations are performed to demonstrate its robustness, adaptability, and\nscalability when tackling vehicles' sudden breakdown, new appearing, different\nnumber of lanes, mixed autonomy, and large-scale scenarios, respectively."}
{"id": "2506.13367", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13367", "abs": "https://arxiv.org/abs/2506.13367", "authors": ["Utkarsh Bajpai", "Julius Rückin", "Cyrill Stachniss", "Marija Popović"], "title": "Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation", "comment": "7 pages, 3 figures", "summary": "Mobile robots exploring indoor environments increasingly rely on\nvision-language models to perceive high-level semantic cues in camera images,\nsuch as object categories. Such models offer the potential to substantially\nadvance robot behaviour for tasks such as object-goal navigation (ObjectNav),\nwhere the robot must locate objects specified in natural language by exploring\nthe environment. Current ObjectNav methods heavily depend on prompt engineering\nfor perception and do not address the semantic uncertainty induced by\nvariations in prompt phrasing. Ignoring semantic uncertainty can lead to\nsuboptimal exploration, which in turn limits performance. Hence, we propose a\nsemantic uncertainty-informed active perception pipeline for ObjectNav in\nindoor environments. We introduce a novel probabilistic sensor model for\nquantifying semantic uncertainty in vision-language models and incorporate it\ninto a probabilistic geometric-semantic map to enhance spatial understanding.\nBased on this map, we develop a frontier exploration planner with an\nuncertainty-informed multi-armed bandit objective to guide efficient object\nsearch. Experimental results demonstrate that our method achieves ObjectNav\nsuccess rates comparable to those of state-of-the-art approaches, without\nrequiring extensive prompt engineering."}
{"id": "2506.13420", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13420", "abs": "https://arxiv.org/abs/2506.13420", "authors": ["Jiang Wang", "Yaozhong Kang", "Linya Fu", "Kazuhiro Nakadai", "He Kong"], "title": "Observability-Aware Active Calibration of Multi-Sensor Extrinsics for Ground Robots via Online Trajectory Optimization", "comment": "Accepted and to appear in the IEEE Sensors Journal", "summary": "Accurate calibration of sensor extrinsic parameters for ground robotic\nsystems (i.e., relative poses) is crucial for ensuring spatial alignment and\nachieving high-performance perception. However, existing calibration methods\ntypically require complex and often human-operated processes to collect data.\nMoreover, most frameworks neglect acoustic sensors, thereby limiting the\nassociated systems' auditory perception capabilities. To alleviate these\nissues, we propose an observability-aware active calibration method for ground\nrobots with multimodal sensors, including a microphone array, a LiDAR\n(exteroceptive sensors), and wheel encoders (proprioceptive sensors). Unlike\ntraditional approaches, our method enables active trajectory optimization for\nonline data collection and calibration, contributing to the development of more\nintelligent robotic systems. Specifically, we leverage the Fisher information\nmatrix (FIM) to quantify parameter observability and adopt its minimum\neigenvalue as an optimization metric for trajectory generation via B-spline\ncurves. Through planning and replanning of robot trajectory online, the method\nenhances the observability of multi-sensor extrinsic parameters. The\neffectiveness and advantages of our method have been demonstrated through\nnumerical simulations and real-world experiments. For the benefit of the\ncommunity, we have also open-sourced our code and data at\nhttps://github.com/AISLAB-sustech/Multisensor-Calibration."}
{"id": "2506.13421", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13421", "abs": "https://arxiv.org/abs/2506.13421", "authors": ["Dongliang Zheng", "Yebin Wang", "Stefano Di Cairano", "Panagiotis Tsiotras"], "title": "Delayed Expansion AGT: Kinodynamic Planning with Application to Tractor-Trailer Parking", "comment": null, "summary": "Kinodynamic planning of articulated vehicles in cluttered environments faces\nadditional challenges arising from high-dimensional state space and complex\nsystem dynamics. Built upon [1],[2], this work proposes the DE-AGT algorithm\nthat grows a tree using pre-computed motion primitives (MPs) and A* heuristics.\nThe first feature of DE-AGT is a delayed expansion of MPs. In particular, the\nMPs are divided into different modes, which are ranked online. With the MP\nclassification and prioritization, DE-AGT expands the most promising mode of\nMPs first, which eliminates unnecessary computation and finds solutions faster.\nTo obtain the cost-to-go heuristic for nonholonomic articulated vehicles, we\nrely on supervised learning and train neural networks for fast and accurate\ncost-to-go prediction. The learned heuristic is used for online mode ranking\nand node selection. Another feature of DE-AGT is the improved goal-reaching.\nExactly reaching a goal state usually requires a constant connection checking\nwith the goal by solving steering problems -- non-trivial and time-consuming\nfor articulated vehicles. The proposed termination scheme overcomes this\nchallenge by tightly integrating a light-weight trajectory tracking controller\nwith the search process. DE-AGT is implemented for autonomous parking of a\ngeneral car-like tractor with 3-trailer. Simulation results show an average of\n10x acceleration compared to a previous method."}
{"id": "2506.13425", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13425", "abs": "https://arxiv.org/abs/2506.13425", "authors": ["Sai Srinivas Jeevanandam", "Sandeep Inuganti", "Shreedhar Govil", "Didier Stricker", "Jason Rambach"], "title": "JENGA: Object selection and pose estimation for robotic grasping from a stack", "comment": null, "summary": "Vision-based robotic object grasping is typically investigated in the context\nof isolated objects or unstructured object sets in bin picking scenarios.\nHowever, there are several settings, such as construction or warehouse\nautomation, where a robot needs to interact with a structured object formation\nsuch as a stack. In this context, we define the problem of selecting suitable\nobjects for grasping along with estimating an accurate 6DoF pose of these\nobjects. To address this problem, we propose a camera-IMU based approach that\nprioritizes unobstructed objects on the higher layers of stacks and introduce a\ndataset for benchmarking and evaluation, along with a suitable evaluation\nmetric that combines object selection with pose accuracy. Experimental results\nshow that although our method can perform quite well, this is a challenging\nproblem if a completely error-free solution is needed. Finally, we show results\nfrom the deployment of our method for a brick-picking application in a\nconstruction scenario."}
{"id": "2506.13428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13428", "abs": "https://arxiv.org/abs/2506.13428", "authors": ["Jiaming Chen", "Yiyu Jiang", "Aoshen Huang", "Yang Li", "Wei Pan"], "title": "VLM-SFD: VLM-Assisted Siamese Flow Diffusion Framework for Dual-Arm Cooperative Manipulation", "comment": null, "summary": "Dual-arm cooperative manipulation holds great promise for tackling complex\nreal-world tasks that demand seamless coordination and adaptive dynamics.\nDespite substantial progress in learning-based motion planning, most approaches\nstruggle to generalize across diverse manipulation tasks and adapt to dynamic,\nunstructured environments, particularly in scenarios involving interactions\nbetween two objects such as assembly, tool use, and bimanual grasping. To\naddress these challenges, we introduce a novel VLM-Assisted Siamese Flow\nDiffusion (VLM-SFD) framework for efficient imitation learning in dual-arm\ncooperative manipulation. The proposed VLM-SFD framework exhibits outstanding\nadaptability, significantly enhancing the ability to rapidly adapt and\ngeneralize to diverse real-world tasks from only a minimal number of human\ndemonstrations. Specifically, we propose a Siamese Flow Diffusion Network\n(SFDNet) employs a dual-encoder-decoder Siamese architecture to embed two\ntarget objects into a shared latent space, while a diffusion-based conditioning\nprocess-conditioned by task instructions-generates two-stream object-centric\nmotion flows that guide dual-arm coordination. We further design a dynamic task\nassignment strategy that seamlessly maps the predicted 2D motion flows into 3D\nspace and incorporates a pre-trained vision-language model (VLM) to adaptively\nassign the optimal motion to each robotic arm over time. Experiments validate\nthe effectiveness of the proposed method, demonstrating its ability to\ngeneralize to diverse manipulation tasks while maintaining high efficiency and\nadaptability. The code and demo videos are publicly available on our project\nwebsite https://sites.google.com/view/vlm-sfd/."}
{"id": "2506.13432", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13432", "abs": "https://arxiv.org/abs/2506.13432", "authors": ["Jonas Haack", "Franek Stark", "Shubham Vyas", "Frank Kirchner", "Shivesh Kumar"], "title": "Adaptive Model-Base Control of Quadrupeds via Online System Identification using Kalman Filter", "comment": "6 pages, 5 figures, 1 table, accepted for IEEE IROS 2025", "summary": "Many real-world applications require legged robots to be able to carry\nvariable payloads. Model-based controllers such as model predictive control\n(MPC) have become the de facto standard in research for controlling these\nsystems. However, most model-based control architectures use fixed plant\nmodels, which limits their applicability to different tasks. In this paper, we\npresent a Kalman filter (KF) formulation for online identification of the mass\nand center of mass (COM) of a four-legged robot. We evaluate our method on a\nquadrupedal robot carrying various payloads and find that it is more robust to\nstrong measurement noise than classical recursive least squares (RLS) methods.\nMoreover, it improves the tracking performance of the model-based controller\nwith varying payloads when the model parameters are adjusted at runtime."}
{"id": "2506.13453", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13453", "abs": "https://arxiv.org/abs/2506.13453", "authors": ["YR Darr", "MA Niazi"], "title": "Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics", "comment": null, "summary": "The self-organization of robots for the formation of structures and shapes is\na stimulating application of the swarm robotic system. It involves a large\nnumber of autonomous robots of heterogeneous behavior, coordination among them,\nand their interaction with the dynamic environment. This process of complex\nstructure formation is considered a complex system, which needs to be modeled\nby using any modeling approach. Although the formal specification approach\nalong with other formal methods has been used to model the behavior of robots\nin a swarm. However, to the best of our knowledge, the formal specification\napproach has not been used to model the self-organization process in swarm\nrobotic systems for shape formation. In this paper, we use a formal\nspecification approach to model the shape formation task of swarm robots. We\nuse Z (Zed) language of formal specification, which is a state-based language,\nto model the states of the entities of the systems. We demonstrate the\neffectiveness of Z for the self-organized shape formation. The presented formal\nspecification model gives the outlines for designing and implementing the swarm\nrobotic system for the formation of complex shapes and structures. It also\nprovides the foundation for modeling the complex shape formation process for\nswarm robotics using a multi-agent system in a simulation-based environment.\nKeywords: Swarm robotics, Self-organization, Formal specification, Complex\nsystems"}
{"id": "2506.13478", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13478", "abs": "https://arxiv.org/abs/2506.13478", "authors": ["Hemjyoti Das", "Minh Nhat Vu", "Christian Ott"], "title": "Learning Swing-up Maneuvers for a Suspended Aerial Manipulation Platform in a Hierarchical Control Framework", "comment": "6 pages, 10 figures", "summary": "In this work, we present a novel approach to augment a model-based control\nmethod with a reinforcement learning (RL) agent and demonstrate a swing-up\nmaneuver with a suspended aerial manipulation platform. These platforms are\ntargeted towards a wide range of applications on construction sites involving\ncranes, with swing-up maneuvers allowing it to perch at a given location,\ninaccessible with purely the thrust force of the platform. Our proposed\napproach is based on a hierarchical control framework, which allows different\ntasks to be executed according to their assigned priorities. An RL agent is\nthen subsequently utilized to adjust the reference set-point of the\nlower-priority tasks to perform the swing-up maneuver, which is confined in the\nnullspace of the higher-priority tasks, such as maintaining a specific\norientation and position of the end-effector. Our approach is validated using\nextensive numerical simulation studies."}
{"id": "2506.13498", "categories": ["cs.RO", "cs.HC", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.13498", "abs": "https://arxiv.org/abs/2506.13498", "authors": ["Toshiaki Tsuji", "Yasuhiro Kato", "Gokhan Solak", "Heng Zhang", "Tadej Petrič", "Francesco Nori", "Arash Ajoudani"], "title": "A Survey on Imitation Learning for Contact-Rich Tasks in Robotics", "comment": "47pages, 1 figures", "summary": "This paper comprehensively surveys research trends in imitation learning for\ncontact-rich robotic tasks. Contact-rich tasks, which require complex physical\ninteractions with the environment, represent a central challenge in robotics\ndue to their nonlinear dynamics and sensitivity to small positional deviations.\nThe paper examines demonstration collection methodologies, including teaching\nmethods and sensory modalities crucial for capturing subtle interaction\ndynamics. We then analyze imitation learning approaches, highlighting their\napplications to contact-rich manipulation. Recent advances in multimodal\nlearning and foundation models have significantly enhanced performance in\ncomplex contact tasks across industrial, household, and healthcare domains.\nThrough systematic organization of current research and identification of\nchallenges, this survey provides a foundation for future advancements in\ncontact-rich robotic manipulation."}
{"id": "2506.13536", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13536", "abs": "https://arxiv.org/abs/2506.13536", "authors": ["Vaibhav Saxena", "Matthew Bronars", "Nadun Ranawaka Arachchige", "Kuancheng Wang", "Woo Chul Shin", "Soroush Nasiriany", "Ajay Mandlekar", "Danfei Xu"], "title": "What Matters in Learning from Large-Scale Datasets for Robot Manipulation", "comment": null, "summary": "Imitation learning from large multi-task demonstration datasets has emerged\nas a promising path for building generally-capable robots. As a result, 1000s\nof hours have been spent on building such large-scale datasets around the\nglobe. Despite the continuous growth of such efforts, we still lack a\nsystematic understanding of what data should be collected to improve the\nutility of a robotics dataset and facilitate downstream policy learning. In\nthis work, we conduct a large-scale dataset composition study to answer this\nquestion. We develop a data generation framework to procedurally emulate common\nsources of diversity in existing datasets (such as sensor placements and object\ntypes and arrangements), and use it to generate large-scale robot datasets with\ncontrolled compositions, enabling a suite of dataset composition studies that\nwould be prohibitively expensive in the real world. We focus on two practical\nsettings: (1) what types of diversity should be emphasized when future\nresearchers collect large-scale datasets for robotics, and (2) how should\ncurrent practitioners retrieve relevant demonstrations from existing datasets\nto maximize downstream policy performance on tasks of interest. Our study\nyields several critical insights -- for example, we find that camera poses and\nspatial arrangements are crucial dimensions for both diversity in collection\nand alignment in retrieval. In real-world robot learning settings, we find that\nnot only do our insights from simulation carry over, but our retrieval\nstrategies on existing datasets such as DROID allow us to consistently\noutperform existing training strategies by up to 70%. More results at\nhttps://robo-mimiclabs.github.io/"}
{"id": "2506.13622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13622", "abs": "https://arxiv.org/abs/2506.13622", "authors": ["Martino Gulisano", "Matteo Masoni", "Marco Gabiccini", "Massimo Guiggiani"], "title": "Disturbance-aware minimum-time planning strategies for motorsport vehicles with probabilistic safety certificates", "comment": "24 pages, 11 figures, paper under review", "summary": "This paper presents a disturbance-aware framework that embeds robustness into\nminimum-lap-time trajectory optimization for motorsport. Two formulations are\nintroduced. (i) Open-loop, horizon-based covariance propagation uses worst-case\nuncertainty growth over a finite window to tighten tire-friction and\ntrack-limit constraints. (ii) Closed-loop, covariance-aware planning\nincorporates a time-varying LQR feedback law in the optimizer, providing a\nfeedback-consistent estimate of disturbance attenuation and enabling sharper\nyet reliable constraint tightening. Both methods yield reference trajectories\nfor human or artificial drivers: in autonomous applications the modelled\ncontroller can replicate the on-board implementation, while for human driving\naccuracy increases with the extent to which the driver can be approximated by\nthe assumed time-varying LQR policy. Computational tests on a representative\nBarcelona-Catalunya sector show that both schemes meet the prescribed safety\nprobability, yet the closed-loop variant incurs smaller lap-time penalties than\nthe more conservative open-loop solution, while the nominal (non-robust)\ntrajectory remains infeasible under the same uncertainties. By accounting for\nuncertainty growth and feedback action during planning, the proposed framework\ndelivers trajectories that are both performance-optimal and probabilistically\nsafe, advancing minimum-time optimization toward real-world deployment in\nhigh-performance motorsport and autonomous racing."}
{"id": "2506.13640", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13640", "abs": "https://arxiv.org/abs/2506.13640", "authors": ["Cedric Le Gentil", "Cedric Pradalier", "Timothy D. Barfoot"], "title": "Towards Efficient Occupancy Mapping via Gaussian Process Latent Field Shaping", "comment": "Presented at RSS 2025 Workshop: Gaussian Representations for Robot\n  Autonomy: Challenges and Opportunities", "summary": "Occupancy mapping has been a key enabler of mobile robotics. Originally based\non a discrete grid representation, occupancy mapping has evolved towards\ncontinuous representations that can predict the occupancy status at any\nlocation and account for occupancy correlations between neighbouring areas.\nGaussian Process (GP) approaches treat this task as a binary classification\nproblem using both observations of occupied and free space. Conceptually, a GP\nlatent field is passed through a logistic function to obtain the output class\nwithout actually manipulating the GP latent field. In this work, we propose to\nact directly on the latent function to efficiently integrate free space\ninformation as a prior based on the shape of the sensor's field-of-view. A\nmajor difference with existing methods is the change in the classification\nproblem, as we distinguish between free and unknown space. The `occupied' area\nis the infinitesimally thin location where the class transitions from free to\nunknown. We demonstrate in simulated environments that our approach is sound\nand leads to competitive reconstruction accuracy."}
{"id": "2506.13679", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13679", "abs": "https://arxiv.org/abs/2506.13679", "authors": ["Yuqing Wen", "Kefan Gu", "Haoxuan Liu", "Yucheng Zhao", "Tiancai Wang", "Haoqiang Fan", "Xiaoyan Sun"], "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently made significant advance in\nmulti-task, end-to-end robotic control, due to the strong generalization\ncapabilities of Vision-Language Models (VLMs). A fundamental challenge in\ndeveloping such models is effectively aligning the vision-language space with\nthe robotic action space. Existing approaches typically rely on directly\nfine-tuning VLMs using expert demonstrations. However, this strategy suffers\nfrom a spatio-temporal gap, resulting in considerable data inefficiency and\nheavy reliance on human labor. Spatially, VLMs operate within a high-level\nsemantic space, whereas robotic actions are grounded in low-level 3D physical\nspace; temporally, VLMs primarily interpret the present, while VLA models\nanticipate future actions. To overcome these challenges, we propose a novel\ntraining paradigm, ROSA, which leverages robot state estimation to improve\nalignment between vision-language and action spaces. By integrating robot state\nestimation data obtained via an automated process, ROSA enables the VLA model\nto gain enhanced spatial understanding and self-awareness, thereby boosting\nperformance and generalization. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of ROSA, particularly in\nlow-data regimes."}
{"id": "2506.13704", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13704", "abs": "https://arxiv.org/abs/2506.13704", "authors": ["V. Sripada", "A. Khan", "J. Föcker", "S. Parsa", "Susmitha P", "H Maior", "A. Ghalamzan-E"], "title": "HARMONI: Haptic-Guided Assistance for Unified Robotic Tele-Manipulation and Tele-Navigation", "comment": "To appear in IEEE CASE 2025", "summary": "Shared control, which combines human expertise with autonomous assistance, is\ncritical for effective teleoperation in complex environments. While recent\nadvances in haptic-guided teleoperation have shown promise, they are often\nlimited to simplified tasks involving 6- or 7-DoF manipulators and rely on\nseparate control strategies for navigation and manipulation. This increases\nboth cognitive load and operational overhead. In this paper, we present a\nunified tele-mobile manipulation framework that leverages haptic-guided shared\ncontrol. The system integrates a 9-DoF follower mobile manipulator and a 7-DoF\nleader robotic arm, enabling seamless transitions between tele-navigation and\ntele-manipulation through real-time haptic feedback. A user study with 20\nparticipants under real-world conditions demonstrates that our framework\nsignificantly improves task accuracy and efficiency without increasing\ncognitive load. These findings highlight the potential of haptic-guided shared\ncontrol for enhancing operator performance in demanding teleoperation\nscenarios."}
{"id": "2506.13725", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13725", "abs": "https://arxiv.org/abs/2506.13725", "authors": ["Wenxuan Song", "Jiayi Chen", "Pengxiang Ding", "Yuxin Huang", "Han Zhao", "Donglin Wang", "Haoang Li"], "title": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding", "comment": "16 pages", "summary": "In recent years, Vision-Language-Action (VLA) models have become a vital\nresearch direction in robotics due to their impressive multimodal understanding\nand generalization capabilities. Despite the progress, their practical\ndeployment is severely constrained by inference speed bottlenecks, particularly\nin high-frequency and dexterous manipulation tasks. While recent studies have\nexplored Jacobi decoding as a more efficient alternative to traditional\nautoregressive decoding, its practical benefits are marginal due to the lengthy\niterations. To address it, we introduce consistency distillation training to\npredict multiple correct action tokens in each iteration, thereby achieving\nacceleration. Besides, we design mixed-label supervision to mitigate the error\naccumulation during distillation. Although distillation brings acceptable\nspeedup, we identify that certain inefficient iterations remain a critical\nbottleneck. To tackle this, we propose an early-exit decoding strategy that\nmoderately relaxes convergence conditions, which further improves average\ninference efficiency. Experimental results show that the proposed method\nachieves more than 4 times inference acceleration across different baselines\nwhile maintaining high task success rates in both simulated and real-world\nrobot tasks. These experiments validate that our approach provides an efficient\nand general paradigm for accelerating multimodal decision-making in robotics.\nOur project page is available at https://irpn-eai.github.io/CEED-VLA/."}
{"id": "2506.13739", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.13739", "abs": "https://arxiv.org/abs/2506.13739", "authors": ["Guy Laban", "Micol Spitale", "Minja Axelsson", "Nida Itrat Abbasi", "Hatice Gunes"], "title": "Critical Insights about Robots for Mental Wellbeing", "comment": null, "summary": "Social robots are increasingly being explored as tools to support emotional\nwellbeing, particularly in non-clinical settings. Drawing on a range of\nempirical studies and practical deployments, this paper outlines six key\ninsights that highlight both the opportunities and challenges in using robots\nto promote mental wellbeing. These include (1) the lack of a single, objective\nmeasure of wellbeing, (2) the fact that robots don't need to act as companions\nto be effective, (3) the growing potential of virtual interactions, (4) the\nimportance of involving clinicians in the design process, (5) the difference\nbetween one-off and long-term interactions, and (6) the idea that adaptation\nand personalization are not always necessary for positive outcomes. Rather than\npositioning robots as replacements for human therapists, we argue that they are\nbest understood as supportive tools that must be designed with care, grounded\nin evidence, and shaped by ethical and psychological considerations. Our aim is\nto inform future research and guide responsible, effective use of robots in\nmental health and wellbeing contexts."}
{"id": "2506.13751", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13751", "abs": "https://arxiv.org/abs/2506.13751", "authors": ["Haoru Xue", "Xiaoyu Huang", "Dantong Niu", "Qiayuan Liao", "Thomas Kragerud", "Jan Tommy Gravdahl", "Xue Bin Peng", "Guanya Shi", "Trevor Darrell", "Koushil Screenath", "Shankar Sastry"], "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction", "comment": null, "summary": "Vision-language-action (VLA) models have demonstrated strong semantic\nunderstanding and zero-shot generalization, yet most existing systems assume an\naccurate low-level controller with hand-crafted action \"vocabulary\" such as\nend-effector pose or root velocity. This assumption confines prior work to\nquasi-static tasks and precludes the agile, whole-body behaviors required by\nhumanoid whole-body control (WBC) tasks. To capture this gap in the literature,\nwe start by introducing the first sim-to-real-ready, vision-language,\nclosed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10\ncategories. We then propose LeVERB: Latent Vision-Language-Encoded Robot\nBehavior, a hierarchical latent instruction-following framework for humanoid\nvision-language WBC, the first of its kind. At the top level, a vision-language\npolicy learns a latent action vocabulary from synthetically rendered kinematic\ndemonstrations; at the low level, a reinforcement-learned WBC policy consumes\nthese latent verbs to generate dynamics-level commands. In our benchmark,\nLeVERB can zero-shot attain a 80% success rate on simple visual navigation\ntasks, and 58.5% success rate overall, outperforming naive hierarchical\nwhole-body VLA implementation by 7.8 times."}
{"id": "2506.13753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13753", "abs": "https://arxiv.org/abs/2506.13753", "authors": ["Stav Ashur", "Nancy M. Amato", "Sariel Har-Peled"], "title": "Edge Nearest Neighbor in Sampling-Based Motion Planning", "comment": null, "summary": "Neighborhood finders and nearest neighbor queries are fundamental parts of\nsampling based motion planning algorithms. Using different distance metrics or\notherwise changing the definition of a neighborhood produces different\nalgorithms with unique empiric and theoretical properties. In \\cite{l-pa-06}\nLaValle suggests a neighborhood finder for the Rapidly-exploring Random Tree\nRRT\n  algorithm \\cite{l-rrtnt-98} which finds the nearest neighbor of the sampled\npoint on the swath of the tree, that is on the set of all of the points on the\ntree edges, using a hierarchical data structure. In this paper we implement\nsuch a neighborhood finder and show, theoretically and experimentally, that\nthis results in more efficient algorithms, and suggest a variant of the\nRapidly-exploring Random Graph RRG algorithm \\cite{f-isaom-10} that better\nexploits the exploration properties of the newly described subroutine for\nfinding narrow passages."}
{"id": "2506.13761", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.13761", "abs": "https://arxiv.org/abs/2506.13761", "authors": ["Chuanruo Ning", "Kuan Fang", "Wei-Chiu Ma"], "title": "Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins", "comment": null, "summary": "Recent advancements in open-world robot manipulation have been largely driven\nby vision-language models (VLMs). While these models exhibit strong\ngeneralization ability in high-level planning, they struggle to predict\nlow-level robot controls due to limited physical-world understanding. To\naddress this issue, we propose a model predictive control framework for\nopen-world manipulation that combines the semantic reasoning capabilities of\nVLMs with physically-grounded, interactive digital twins of the real-world\nenvironments. By constructing and simulating the digital twins, our approach\ngenerates feasible motion trajectories, simulates corresponding outcomes, and\nprompts the VLM with future observations to evaluate and select the most\nsuitable outcome based on language instructions of the task. To further enhance\nthe capability of pre-trained VLMs in understanding complex scenes for robotic\ncontrol, we leverage the flexible rendering capabilities of the digital twin to\nsynthesize the scene at various novel, unoccluded viewpoints. We validate our\napproach on a diverse set of complex manipulation tasks, demonstrating superior\nperformance compared to baseline methods for language-conditioned robotic\ncontrol using VLMs."}
{"id": "2506.13762", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.13762", "abs": "https://arxiv.org/abs/2506.13762", "authors": ["Zifan Zhao", "Siddhant Haldar", "Jinda Cui", "Lerrel Pinto", "Raunaq Bhirangi"], "title": "Touch begins where vision ends: Generalizable policies for contact-rich manipulation", "comment": null, "summary": "Data-driven approaches struggle with precise manipulation; imitation learning\nrequires many hard-to-obtain demonstrations, while reinforcement learning\nyields brittle, non-generalizable policies. We introduce VisuoTactile Local\n(ViTaL) policy learning, a framework that solves fine-grained manipulation\ntasks by decomposing them into two phases: a reaching phase, where a\nvision-language model (VLM) enables scene-level reasoning to localize the\nobject of interest, and a local interaction phase, where a reusable,\nscene-agnostic ViTaL policy performs contact-rich manipulation using egocentric\nvision and tactile sensing. This approach is motivated by the observation that\nwhile scene context varies, the low-level interaction remains consistent across\ntask instances. By training local policies once in a canonical setting, they\ncan generalize via a localize-then-execute strategy. ViTaL achieves around 90%\nsuccess on contact-rich tasks in unseen environments and is robust to\ndistractors. ViTaL's effectiveness stems from three key insights: (1)\nfoundation models for segmentation enable training robust visual encoders via\nbehavior cloning; (2) these encoders improve the generalizability of policies\nlearned using residual RL; and (3) tactile sensing significantly boosts\nperformance in contact-rich tasks. Ablation studies validate each of these\ninsights, and we demonstrate that ViTaL integrates well with high-level VLMs,\nenabling robust, reusable low-level skills. Results and videos are available at\nhttps://vitalprecise.github.io."}
