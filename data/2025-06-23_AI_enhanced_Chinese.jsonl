{"id": "2506.15788", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15788", "abs": "https://arxiv.org/abs/2506.15788", "authors": ["Baxi Chong", "Juntao He", "Daniel Irvine", "Tianyu Wang", "Esteban Flores", "Daniel Soto", "Jianfeng Lin", "Zhaochen Xu", "Vincent R Nienhusser", "Grigoriy Blekherman", "Daniel I. Goldman"], "title": "Robust control for multi-legged elongate robots in noisy environments", "comment": null, "summary": "Modern two and four legged robots exhibit impressive mobility on complex\nterrain, largely attributed to advancement in learning algorithms. However,\nthese systems often rely on high-bandwidth sensing and onboard computation to\nperceive/respond to terrain uncertainties. Further, current locomotion\nstrategies typically require extensive robot-specific training, limiting their\ngeneralizability across platforms. Building on our prior research connecting\nrobot-environment interaction and communication theory, we develop a new\nparadigm to construct robust and simply controlled multi-legged elongate robots\n(MERs) capable of operating effectively in cluttered, unstructured\nenvironments. In this framework, each leg-ground contact is thought of as a\nbasic active contact (bac), akin to bits in signal transmission. Reliable\nlocomotion can be achieved in open-loop on \"noisy\" landscapes via sufficient\nredundancy in bacs. In such situations, robustness is achieved through passive\nmechanical responses. We term such processes as those displaying mechanical\nintelligence (MI) and analogize these processes to forward error correction\n(FEC) in signal transmission. To augment MI, we develop feedback control\nschemes, which we refer to as computational intelligence (CI) and such\nprocesses analogize automatic repeat request (ARQ) in signal transmission.\nIntegration of these analogies between locomotion and communication theory\nallow analysis, design, and prediction of embodied intelligence control schemes\n(integrating MI and CI) in MERs, showing effective and reliable performance\n(approximately half body lengths per cycle) on complex landscapes with terrain\n\"noise\" over twice the robot's height. Our work provides a foundation for\nsystematic development of MER control, paving the way for terrain-agnostic,\nagile, and resilient robotic systems capable of operating in extreme\nenvironments.", "AI": {"tldr": "提出了一种新范式，通过将机器人-环境交互与通信理论结合，开发了多足细长机器人（MERs）的稳健控制方案，实现了在复杂地形中的高效运动。", "motivation": "现有机器人系统依赖高带宽传感和计算，且训练泛化性差，需要一种更通用、稳健的控制方法。", "method": "将每条腿与地面的接触视为基本主动接触（bac），类比信号传输中的比特，通过冗余和被动机械响应实现稳健运动，并结合反馈控制（CI）增强性能。", "result": "MERs在复杂地形中表现出高效可靠的运动性能（每周期约半个身长），地形噪声超过机器人高度的两倍。", "conclusion": "该研究为MERs控制提供了系统化开发基础，推动了地形无关、敏捷且稳健的机器人系统发展。"}}
{"id": "2506.15799", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15799", "abs": "https://arxiv.org/abs/2506.15799", "authors": ["Andrew Wagenmaker", "Mitsuhiko Nakamoto", "Yunchu Zhang", "Seohong Park", "Waleed Yagoub", "Anusha Nagabandi", "Abhishek Gupta", "Sergey Levine"], "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning", "comment": null, "summary": "Robotic control policies learned from human demonstrations have achieved\nimpressive results in many real-world applications. However, in scenarios where\ninitial performance is not satisfactory, as is often the case in novel\nopen-world settings, such behavioral cloning (BC)-learned policies typically\nrequire collecting additional human demonstrations to further improve their\nbehavior -- an expensive and time-consuming process. In contrast, reinforcement\nlearning (RL) holds the promise of enabling autonomous online policy\nimprovement, but often falls short of achieving this due to the large number of\nsamples it typically requires. In this work we take steps towards enabling fast\nautonomous adaptation of BC-trained policies via efficient real-world RL.\nFocusing in particular on diffusion policies -- a state-of-the-art BC\nmethodology -- we propose diffusion steering via reinforcement learning (DSRL):\nadapting the BC policy by running RL over its latent-noise space. We show that\nDSRL is highly sample efficient, requires only black-box access to the BC\npolicy, and enables effective real-world autonomous policy improvement.\nFurthermore, DSRL avoids many of the challenges associated with finetuning\ndiffusion policies, obviating the need to modify the weights of the base policy\nat all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,\nand for adapting pretrained generalist policies, illustrating its sample\nefficiency and effective performance at real-world policy improvement.", "AI": {"tldr": "该论文提出了一种名为DSRL的方法，通过强化学习在行为克隆（BC）策略的潜在噪声空间中进行调整，以实现快速自主适应。DSRL高效且无需修改基础策略权重。", "motivation": "行为克隆策略在初始性能不足时需要额外的人类演示，成本高且耗时。强化学习虽能自主改进策略，但样本效率低。DSRL旨在结合两者优势。", "method": "提出DSRL方法，在扩散策略（一种先进的BC方法）的潜在噪声空间中运行强化学习，实现高效自主适应。", "result": "DSRL在模拟和真实机器人任务中表现出高样本效率和有效性能改进，且无需修改基础策略权重。", "conclusion": "DSRL为行为克隆策略的快速自主适应提供了一种高效且实用的解决方案。"}}
{"id": "2506.15828", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15828", "abs": "https://arxiv.org/abs/2506.15828", "authors": ["Emanuele Musumeci", "Michele Brienza", "Francesco Argenziano", "Vincenzo Suriani", "Daniele Nardi", "Domenico D. Bloisi"], "title": "Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning", "comment": null, "summary": "Classical planning in AI and Robotics addresses complex tasks by shifting\nfrom imperative to declarative approaches (e.g., PDDL). However, these methods\noften fail in real scenarios due to limited robot perception and the need to\nground perceptions to planning predicates. This often results in heavily\nhard-coded behaviors that struggle to adapt, even with scenarios where goals\ncan be achieved through relaxed planning. Meanwhile, Large Language Models\n(LLMs) lead to planning systems that leverage commonsense reasoning but often\nat the cost of generating unfeasible and/or unsafe plans. To address these\nlimitations, we present an approach integrating classical planning with LLMs,\nleveraging their ability to extract commonsense knowledge and ground actions.\nWe propose a hierarchical formulation that enables robots to make unfeasible\ntasks tractable by defining functionally equivalent goals through gradual\nrelaxation. This mechanism supports partial achievement of the intended\nobjective, suited to the agent's specific context. Our method demonstrates its\nability to adapt and execute tasks effectively within environments modeled\nusing 3D Scene Graphs through comprehensive qualitative and quantitative\nevaluations. We also show how this method succeeds in complex scenarios where\nother benchmark methods are more likely to fail. Code, dataset, and additional\nmaterial are released to the community.", "AI": {"tldr": "论文提出了一种结合经典规划与大型语言模型（LLMs）的方法，以解决机器人规划中的适应性和可行性问题。", "motivation": "传统规划方法（如PDDL）在真实场景中因感知限制和难以将感知映射到规划谓词而表现不佳，而LLMs虽然能利用常识推理，但常生成不可行或不安全的计划。", "method": "通过分层规划框架，结合LLMs的常识推理能力，逐步放松任务目标，使其适应机器人具体环境。", "result": "方法在3D场景图中表现出高效的任务适应和执行能力，优于其他基准方法。", "conclusion": "该方法有效整合了经典规划与LLMs的优势，提升了机器人在复杂场景中的规划能力。"}}
{"id": "2506.15847", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15847", "abs": "https://arxiv.org/abs/2506.15847", "authors": ["Arpit Bahety", "Arnav Balaji", "Ben Abbatematteo", "Roberto Martín-Martín"], "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation", "comment": null, "summary": "For robots to become efficient helpers in the home, they must learn to\nperform new mobile manipulation tasks simply by watching humans perform them.\nLearning from a single video demonstration from a human is challenging as the\nrobot needs to first extract from the demo what needs to be done and how,\ntranslate the strategy from a third to a first-person perspective, and then\nadapt it to be successful with its own morphology. Furthermore, to mitigate the\ndependency on costly human monitoring, this learning process should be\nperformed in a safe and autonomous manner. We present SafeMimic, a framework to\nlearn new mobile manipulation skills safely and autonomously from a single\nthird-person human video. Given an initial human video demonstration of a\nmulti-step mobile manipulation task, SafeMimic first parses the video into\nsegments, inferring both the semantic changes caused and the motions the human\nexecuted to achieve them and translating them to an egocentric reference. Then,\nit adapts the behavior to the robot's own morphology by sampling candidate\nactions around the human ones, and verifying them for safety before execution\nin a receding horizon fashion using an ensemble of safety Q-functions trained\nin simulation. When safe forward progression is not possible, SafeMimic\nbacktracks to previous states and attempts a different sequence of actions,\nadapting both the trajectory and the grasping modes when required for its\nmorphology. As a result, SafeMimic yields a strategy that succeeds in the\ndemonstrated behavior and learns task-specific actions that reduce exploration\nin future attempts. Our experiments show that our method allows robots to\nsafely and efficiently learn multi-step mobile manipulation behaviors from a\nsingle human demonstration, from different users, and in different\nenvironments, with improvements over state-of-the-art baselines across seven\ntasks", "AI": {"tldr": "SafeMimic框架通过单次人类视频演示，让机器人安全自主地学习移动操作任务。", "motivation": "解决机器人从人类视频中学习新任务时的视角转换、形态适应和安全执行问题。", "method": "解析视频为语义和动作段，转换为第一人称视角，采样候选动作并通过安全验证执行。", "result": "实验表明，SafeMimic能在不同环境和用户下安全高效地学习任务，优于现有方法。", "conclusion": "SafeMimic为机器人学习复杂任务提供了一种安全、高效的解决方案。"}}
{"id": "2506.15849", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15849", "abs": "https://arxiv.org/abs/2506.15849", "authors": ["Kirill Muravyev", "Vasily Yuryev", "Oleg Bulichev", "Dmitry Yudin", "Konstantin Yakovlev"], "title": "PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps", "comment": "This version was submitted and rejected from IROS 2025 conference", "summary": "Localization in the environment is one of the crucial tasks of navigation of\na mobile robot or a self-driving vehicle. For long-range routes, performing\nlocalization within a dense global lidar map in real time may be difficult, and\nthe creation of such a map may require much memory. To this end, leveraging\ntopological maps may be useful. In this work, we propose PRISM-Loc -- a\ntopological map-based approach for localization in large environments. The\nproposed approach leverages a twofold localization pipeline, which consists of\nglobal place recognition and estimation of the local pose inside the found\nlocation. For local pose estimation, we introduce an original lidar scan\nmatching algorithm, which is based on 2D features and point-based optimization.\nWe evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and\ncompare it against the state-of-the-art metric map-based and place\nrecognition-based competitors. The results of the experiments show that the\nproposed method outperforms its competitors both quality-wise and\ncomputationally-wise.", "AI": {"tldr": "PRISM-Loc是一种基于拓扑地图的定位方法，适用于大范围环境，通过全局地点识别和局部位姿估计实现高效定位。", "motivation": "在长距离导航中，实时定位和全局激光雷达地图的高内存需求是挑战，拓扑地图提供了一种解决方案。", "method": "采用双重定位流程：全局地点识别和局部位姿估计，后者使用基于2D特征和点优化的激光雷达扫描匹配算法。", "result": "在3公里路线的ITLP-Campus数据集上，PRISM-Loc在质量和计算效率上均优于现有方法。", "conclusion": "PRISM-Loc是一种高效且高质量的定位方法，适用于大范围环境。"}}
{"id": "2506.15870", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15870", "abs": "https://arxiv.org/abs/2506.15870", "authors": ["Hossein Maghsoumi", "Yaser Fallah"], "title": "A Small-Scale Robot for Autonomous Driving: Design, Challenges, and Best Practices", "comment": null, "summary": "Small-scale autonomous vehicle platforms provide a cost-effective environment\nfor developing and testing advanced driving systems. However, specific\nconfigurations within this scale are underrepresented, limiting full awareness\nof their potential. This paper focuses on a one-sixth-scale setup, offering a\nhigh-level overview of its design, hardware and software integration, and\ntypical challenges encountered during development. We discuss methods for\naddressing mechanical and electronic issues common to this scale and propose\nguidelines for improving reliability and performance. By sharing these\ninsights, we aim to expand the utility of small-scale vehicles for testing\nautonomous driving algorithms and to encourage further research in this domain.", "AI": {"tldr": "本文探讨了六分之一比例的小型自动驾驶车辆平台的设计、硬件与软件集成，以及开发中的常见挑战，旨在提升其可靠性和性能。", "motivation": "小型自动驾驶车辆平台成本低，但某些配置未被充分研究，限制了其潜力。本文希望通过分享经验，推动该领域的研究。", "method": "介绍了六分之一比例平台的设计、硬件与软件集成方法，并提出了解决机械和电子问题的方案。", "result": "提出了提升小型车辆平台可靠性和性能的指导原则。", "conclusion": "通过分享经验，本文旨在扩展小型车辆平台在自动驾驶算法测试中的应用，并鼓励进一步研究。"}}
{"id": "2506.15851", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15851", "abs": "https://arxiv.org/abs/2506.15851", "authors": ["Qiyuan Wu", "Mark Campbell"], "title": "Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles", "comment": "Accepted by ICRA 2025", "summary": "The uncertainty quantification of sensor measurements coupled with deep\nlearning networks is crucial for many robotics systems, especially for\nsafety-critical applications such as self-driving cars. This paper develops an\nuncertainty quantification approach in the context of visual localization for\nautonomous driving, where locations are selected based on images. Key to our\napproach is to learn the measurement uncertainty using light-weight sensor\nerror model, which maps both image feature and semantic information to\n2-dimensional error distribution. Our approach enables uncertainty estimation\nconditioned on the specific context of the matched image pair, implicitly\ncapturing other critical, unannotated factors (e.g., city vs highway, dynamic\nvs static scenes, winter vs summer) in a latent manner. We demonstrate the\naccuracy of our uncertainty prediction framework using the Ithaca365 dataset,\nwhich includes variations in lighting and weather (sunny, night, snowy). Both\nthe uncertainty quantification of the sensor+network is evaluated, along with\nBayesian localization filters using unique sensor gating method. Results show\nthat the measurement error does not follow a Gaussian distribution with poor\nweather and lighting conditions, and is better predicted by our Gaussian\nMixture model.", "AI": {"tldr": "本文提出了一种用于自动驾驶视觉定位的轻量级传感器误差模型，通过学习图像特征和语义信息映射到二维误差分布，实现不确定性量化。", "motivation": "传感器测量与深度学习网络的不确定性量化对机器人系统（如自动驾驶汽车）至关重要，尤其是在安全关键应用中。", "method": "开发了一种基于图像特征和语义信息的轻量级传感器误差模型，用于预测测量误差分布。", "result": "在Ithaca365数据集上验证了方法的准确性，结果显示在恶劣天气和光照条件下，测量误差不符合高斯分布，而高斯混合模型表现更好。", "conclusion": "该方法能够隐式捕捉未标注的关键因素（如场景类型、季节变化），为自动驾驶提供更准确的不确定性估计。"}}
{"id": "2506.15899", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15899", "abs": "https://arxiv.org/abs/2506.15899", "authors": ["Israel Charles", "Hossein Maghsoumi", "Yaser Fallah"], "title": "Advancing Autonomous Racing: A Comprehensive Survey of the RoboRacer (F1TENTH) Platform", "comment": null, "summary": "The RoboRacer (F1TENTH) platform has emerged as a leading testbed for\nadvancing autonomous driving research, offering a scalable, cost-effective, and\ncommunity-driven environment for experimentation. This paper presents a\ncomprehensive survey of the platform, analyzing its modular hardware and\nsoftware architecture, diverse research applications, and role in autonomous\nsystems education. We examine critical aspects such as bridging the\nsimulation-to-reality (Sim2Real) gap, integration with simulation environments,\nand the availability of standardized datasets and benchmarks. Furthermore, the\nsurvey highlights advancements in perception, planning, and control algorithms,\nas well as insights from global competitions and collaborative research\nefforts. By consolidating these contributions, this study positions RoboRacer\nas a versatile framework for accelerating innovation and bridging the gap\nbetween theoretical research and real-world deployment. The findings underscore\nthe platform's significance in driving forward developments in autonomous\nracing and robotics.", "AI": {"tldr": "本文综述了RoboRacer（F1TENTH）平台作为自动驾驶研究的领先测试平台，分析了其硬件和软件架构、研究应用及教育价值，并探讨了其在仿真到现实（Sim2Real）转换、算法开发和竞赛中的作用。", "motivation": "RoboRacer平台因其可扩展性、成本效益和社区驱动特性成为自动驾驶研究的重要工具，本文旨在全面评估其贡献和潜力。", "method": "通过分析平台的模块化架构、仿真环境集成、标准化数据集及算法进展，结合全球竞赛和合作研究的案例，进行综合调查。", "result": "研究发现RoboRacer是一个多功能框架，能加速创新并弥合理论研究与实际部署之间的差距。", "conclusion": "RoboRacer在自动驾驶赛车和机器人领域具有重要推动作用，为未来研究提供了坚实基础。"}}
{"id": "2506.15865", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15865", "abs": "https://arxiv.org/abs/2506.15865", "authors": ["Viral Rasik Galaiya"], "title": "Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples", "comment": "Thesis", "summary": "To use robots in more unstructured environments, we have to accommodate for\nmore complexities. Robotic systems need more awareness of the environment to\nadapt to uncertainty and variability. Although cameras have been predominantly\nused in robotic tasks, the limitations that come with them, such as occlusion,\nvisibility and breadth of information, have diverted some focus to tactile\nsensing. In this thesis, we explore the use of tactile sensing to determine the\npose of the object using the temporal features. We then use reinforcement\nlearning with tactile collisions to reduce the number of attempts required to\ngrasp an object resulting from positional uncertainty from camera estimates.\nFinally, we use information provided by these tactile sensors to a\nreinforcement learning agent to determine the trajectory to take to remove an\nobject from a restricted passage while reducing training time by pertaining\nfrom human examples.", "AI": {"tldr": "论文探讨了在非结构化环境中使用触觉传感和强化学习来改进机器人抓取和移动物体的方法。", "motivation": "在非结构化环境中，机器人需要更高的环境感知能力以应对不确定性。尽管相机在机器人任务中占主导地位，但其局限性（如遮挡和视野限制）促使研究转向触觉传感。", "method": "利用触觉传感通过时间特征确定物体姿态；结合强化学习和触觉碰撞减少抓取尝试次数；使用触觉信息指导强化学习代理规划轨迹，并通过人类示例减少训练时间。", "result": "方法有效减少了因相机估计位置不确定性导致的抓取尝试次数，并优化了物体移动轨迹的训练效率。", "conclusion": "触觉传感与强化学习的结合为机器人在复杂环境中的操作提供了更高效的解决方案。"}}
{"id": "2506.16427", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16427", "abs": "https://arxiv.org/abs/2506.16427", "authors": ["Mohamad Hachem", "Clément Roos", "Thierry Miquel", "Murat Bronz"], "title": "Full-Pose Tracking via Robust Control for Over-Actuated Multirotors", "comment": null, "summary": "This paper presents a robust cascaded control architecture for over-actuated\nmultirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)\ncontrol combined with structured H_inf control, initially proposed for\nunder-actuated multirotors, to a broader range of multirotor configurations. To\nachieve precise and robust attitude and position tracking, we employ a weighted\nleast-squares geometric guidance control allocation method, formulated as a\nquadratic optimization problem, enabling full-pose tracking. The proposed\napproach effectively addresses key challenges, such as preventing infeasible\npose references and enhancing robustness against disturbances, as well as\nconsidering multirotor's actual physical limitations. Numerical simulations\nwith an over-actuated hexacopter validate the method's effectiveness,\ndemonstrating its adaptability to diverse mission scenarios and its potential\nfor real-world aerial applications.", "AI": {"tldr": "本文提出了一种针对过驱动多旋翼的级联控制架构，结合INDI和结构化H_inf控制，通过几何引导控制分配实现精确跟踪。", "motivation": "扩展INDI和H_inf控制的应用范围至过驱动多旋翼，解决姿态和位置跟踪的精确性与鲁棒性问题。", "method": "采用加权最小二乘几何引导控制分配方法，将其建模为二次优化问题，实现全姿态跟踪。", "result": "数值模拟验证了方法在过驱动六旋翼上的有效性，适应性强且具备实际应用潜力。", "conclusion": "该方法成功解决了不可行姿态参考和抗干扰等挑战，适用于多样化任务场景。"}}
{"id": "2506.15868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15868", "abs": "https://arxiv.org/abs/2506.15868", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jia Hu", "Jiaqi Ma"], "title": "CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction", "comment": "IROS2025", "summary": "Risk quantification is a critical component of safe autonomous driving,\nhowever, constrained by the limited perception range and occlusion of\nsingle-vehicle systems in complex and dense scenarios. Vehicle-to-everything\n(V2X) paradigm has been a promising solution to sharing complementary\nperception information, nevertheless, how to ensure the risk interpretability\nwhile understanding multi-agent interaction with V2X remains an open question.\nIn this paper, we introduce the first V2X-enabled risk quantification pipeline,\nCooperRisk, to fuse perception information from multiple agents and quantify\nthe scenario driving risk in future multiple timestamps. The risk is\nrepresented as a scenario risk map to ensure interpretability based on risk\nseverity and exposure, and the multi-agent interaction is captured by the\nlearning-based cooperative prediction model. We carefully design a\nrisk-oriented transformer-based prediction model with multi-modality and\nmulti-agent considerations. It aims to ensure scene-consistent future behaviors\nof multiple agents and avoid conflicting predictions that could lead to overly\nconservative risk quantification and cause the ego vehicle to become overly\nhesitant to drive. Then, the temporal risk maps could serve to guide a model\npredictive control planner. We evaluate the CooperRisk pipeline in a real-world\nV2X dataset V2XPnP, and the experiments demonstrate its superior performance in\nrisk quantification, showing a 44.35% decrease in conflict rate between the ego\nvehicle and background traffic participants.", "AI": {"tldr": "论文提出了一种基于V2X的风险量化框架CooperRisk，通过多智能体感知信息融合和未来时间戳的风险量化，解决了单车辆系统在复杂场景中的局限性。", "motivation": "单车辆系统在复杂和密集场景中感知范围有限且易受遮挡，而V2X技术虽能共享感知信息，但如何确保风险可解释性并理解多智能体交互仍是一个开放问题。", "method": "设计了基于Transformer的风险导向预测模型，融合多模态和多智能体信息，确保场景一致性和避免冲突预测，生成时间风险地图以指导规划。", "result": "在真实V2X数据集V2XPnP上评估，冲突率降低了44.35%，证明了其在风险量化中的优越性能。", "conclusion": "CooperRisk框架通过多智能体协作显著提升了风险量化的准确性和可解释性，为自动驾驶安全提供了有效解决方案。"}}
{"id": "2506.16535", "categories": ["cs.RO", "cs.MA", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16535", "abs": "https://arxiv.org/abs/2506.16535", "authors": ["Tyler Landle", "Jordan Rapp", "Dean Blank", "Chandramouli Amarnath", "Abhijit Chatterjee", "Alex Daglis", "Umakishore Ramachandran"], "title": "eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles", "comment": null, "summary": "As autonomous vehicles edge closer to widespread adoption, enhancing road\nsafety through collision avoidance and minimization of collateral damage\nbecomes imperative. Vehicle-to-everything (V2X) technologies, which include\nvehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud\n(V2C), are being proposed as mechanisms to achieve this safety improvement.\n  Simulation-based testing is crucial for early-stage evaluation of Connected\nAutonomous Vehicle (CAV) control systems, offering a safer and more\ncost-effective alternative to real-world tests. However, simulating large 3D\nenvironments with many complex single- and multi-vehicle sensors and\ncontrollers is computationally intensive. There is currently no evaluation\nframework that can effectively evaluate realistic scenarios involving large\nnumbers of autonomous vehicles.\n  We propose eCAV -- an efficient, modular, and scalable evaluation platform to\nfacilitate both functional validation of algorithmic approaches to increasing\nroad safety, as well as performance prediction of algorithms of various V2X\ntechnologies, including a futuristic Vehicle-to-Edge control plane and\ncorrespondingly designed control algorithms. eCAV can model up to 256 vehicles\nrunning individual control algorithms without perception enabled, which is\n$8\\times$ more vehicles than what is possible with state-of-the-art\nalternatives. %faster than state-of-the-art alternatives that can simulate\n$8\\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64\nvehicles with a step time under 800ms, which is $4\\times$ more and $1.5\\times$\nfaster than the state-of-the-art OpenCDA framework.", "AI": {"tldr": "论文提出eCAV平台，用于高效评估自动驾驶车辆的控制算法和V2X技术，支持256辆无感知车辆或64辆有感知车辆的模拟，性能优于现有方案。", "motivation": "随着自动驾驶车辆普及，提升道路安全（如避免碰撞和减少附带损害）变得至关重要，而V2X技术是实现这一目标的关键。目前缺乏能有效评估大规模自动驾驶车辆场景的框架。", "method": "提出eCAV平台，该平台高效、模块化且可扩展，支持功能验证和性能预测，包括V2X技术和未来车辆到边缘控制算法。", "result": "eCAV可模拟256辆无感知车辆或64辆有感知车辆，性能分别提升8倍和4倍，且速度快1.5倍。", "conclusion": "eCAV为自动驾驶车辆控制算法和V2X技术的评估提供了高效且可扩展的解决方案，填补了现有框架的不足。"}}
{"id": "2506.16537", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16537", "abs": "https://arxiv.org/abs/2506.16537", "authors": ["Sreeja Roy-Singh", "Alan P. Li", "Vinay Ravindra", "Roderick Lammers", "Marc Sanchez Net"], "title": "Agile, Autonomous Spacecraft Constellations with Disruption Tolerant Networking to Monitor Precipitation and Urban Floods", "comment": null, "summary": "Fully re-orientable small spacecraft are now supported by commercial\ntechnologies, allowing them to point their instruments in any direction and\ncapture images, with short notice. When combined with improved onboard\nprocessing, and implemented on a constellation of inter-communicable\nsatellites, this intelligent agility can significantly increase responsiveness\nto transient or evolving phenomena. We demonstrate a ground-based and onboard\nalgorithmic framework that combines orbital mechanics, attitude control,\ninter-satellite communication, intelligent prediction and planning to schedule\nthe time-varying, re-orientation of agile, small satellites in a constellation.\nPlanner intelligence is improved by updating the predictive value of future\nspace-time observations based on shared observations of evolving episodic\nprecipitation and urban flood forecasts. Reliable inter-satellite communication\nwithin a fast, dynamic constellation topology is modeled in the physical,\naccess control and network layer. We apply the framework on a representative\n24-satellite constellation observing 5 global regions. Results show\nappropriately low latency in information exchange (average within 1/3rd\navailable time for implicit consensus), enabling the onboard scheduler to\nobserve ~7% more flood magnitude than a ground-based implementation. Both\nonboard and offline versions performed ~98% better than constellations without\nagility.", "AI": {"tldr": "论文提出了一种基于轨道力学、姿态控制和卫星间通信的算法框架，用于调度小型敏捷卫星星座的动态重定向，显著提高了对瞬态或演化现象的响应能力。", "motivation": "利用商业技术支持的完全可重定向小型航天器，结合改进的机载处理和星座内通信，实现对瞬态或演化现象的高效观测。", "method": "结合轨道力学、姿态控制、卫星间通信、智能预测和规划，开发了地面和机载算法框架，动态调度卫星星座的重定向。", "result": "在24颗卫星的星座中，信息交换延迟低（平均在可用时间的1/3内），机载调度器观测到的洪水幅度比地面实现多7%，性能比非敏捷星座高98%。", "conclusion": "该框架通过智能规划和实时通信，显著提升了卫星星座对动态现象的观测能力，验证了敏捷卫星星座的实用价值。"}}
{"id": "2506.15890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15890", "abs": "https://arxiv.org/abs/2506.15890", "authors": ["Thomas Manzini", "Priyankari Perali", "Robin R. Murphy", "David Merrick"], "title": "Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene", "comment": "6 pages, 5 Figures, 1 Table", "summary": "This paper details four principal challenges encountered with machine\nlearning (ML) damage assessment using small uncrewed aerial systems (sUAS) at\nHurricanes Debby and Helene that prevented, degraded, or delayed the delivery\nof data products during operations and suggests three research directions for\nfuture real-world deployments. The presence of these challenges is not\nsurprising given that a review of the literature considering both datasets and\nproposed ML models suggests this is the first sUAS-based ML system for disaster\ndamage assessment actually deployed as a part of real-world operations. The\nsUAS-based ML system was applied by the State of Florida to Hurricanes Helene\n(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne\nsUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a\nWintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial\nimagery of inland flood damage resulting from post-tropical remnants of\nHurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing\nfurther insights into the advantages and limitations of sUAS for disaster\nresponse. The four challenges (variationin spatial resolution of input imagery,\nspatial misalignment between imagery and geospatial data, wireless\nconnectivity, and data product format) lead to three recommendations that\nspecify research needed to improve ML model capabilities to accommodate the\nwide variation of potential spatial resolutions used in practice, handle\nspatial misalignment, and minimize the dependency on wireless connectivity.\nThese recommendations are expected to improve the effective operational use of\nsUAS and sUAS-based ML damage assessment systems for disaster response.", "AI": {"tldr": "本文总结了在飓风Debby和Helene中使用小型无人机（sUAS）进行机器学习（ML）损害评估时遇到的四个主要挑战，并提出了未来实际部署的三个研究方向。", "motivation": "研究动机是解决在真实灾害响应中首次部署基于sUAS的ML损害评估系统时遇到的挑战，以提升其操作有效性。", "method": "方法包括在佛罗里达州和宾夕法尼亚州使用Wintra WingtraOne sUAS收集图像数据，并应用同一ML模型进行分析。", "result": "结果揭示了四个主要挑战（输入图像空间分辨率变化、图像与地理空间数据不对齐、无线连接问题和数据产品格式），并提出了三个改进建议。", "conclusion": "结论是这些建议有望提升sUAS和基于sUAS的ML损害评估系统在灾害响应中的实际应用效果。"}}
{"id": "2506.16546", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16546", "abs": "https://arxiv.org/abs/2506.16546", "authors": ["Liyang Yu", "Tianyi Wang", "Junfeng Jiao", "Fengwu Shan", "Hongqing Chu", "Bingzhao Gao"], "title": "BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios", "comment": "6 pages, 3 figures, 4 tables, accepted for IEEE Intelligent Vehicles\n  (IV) Symposium 2025", "summary": "In complex real-world traffic environments, autonomous vehicles (AVs) need to\ninteract with other traffic participants while making real-time and\nsafety-critical decisions accordingly. The unpredictability of human behaviors\nposes significant challenges, particularly in dynamic scenarios, such as\nmulti-lane highways and unsignalized T-intersections. To address this gap, we\ndesign a bi-level interaction decision-making algorithm (BIDA) that integrates\ninteractive Monte Carlo tree search (MCTS) with deep reinforcement learning\n(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs\nin dynamic key traffic scenarios. Specifically, we adopt three types of DRL\nalgorithms to construct a reliable value network and policy network, which\nguide the online deduction process of interactive MCTS by assisting in value\nupdate and node selection. Then, a dynamic trajectory planner and a trajectory\ntracking controller are designed and implemented in CARLA to ensure smooth\nexecution of planned maneuvers. Experimental evaluations demonstrate that our\nBIDA not only enhances interactive deduction and reduces computational costs,\nbut also outperforms other latest benchmarks, which exhibits superior safety,\nefficiency and interaction rationality under varying traffic conditions.", "AI": {"tldr": "论文提出了一种双层交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）和深度强化学习（DRL），以提升自动驾驶车辆在动态交通场景中的交互合理性、效率和安全性。", "motivation": "在复杂的真实交通环境中，自动驾驶车辆需要与其他交通参与者交互并做出实时且安全关键决策，而人类行为的不可预测性带来了显著挑战。", "method": "采用三种DRL算法构建可靠的价值网络和策略网络，指导交互式MCTS的在线推理过程，并通过动态轨迹规划器和轨迹跟踪控制器在CARLA中实现。", "result": "实验表明，BIDA不仅提升了交互推理能力并降低了计算成本，还在不同交通条件下表现出优于其他基准的安全性和效率。", "conclusion": "BIDA算法在动态关键交通场景中显著提升了自动驾驶车辆的交互合理性、效率和安全性。"}}
{"id": "2506.16555", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16555", "abs": "https://arxiv.org/abs/2506.16555", "authors": ["Melih Özcan", "Ozgur S. Oguz"], "title": "An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation", "comment": "8 pages, 8 figures, accepted for oral presentation at IROS 2025.\n  Supplementary site: https://sites.google.com/view/komo-force/home", "summary": "Robotic manipulation demands precise control over both contact forces and\nmotion trajectories. While force control is essential for achieving compliant\ninteraction and high-frequency adaptation, it is limited to operations in close\nproximity to the manipulated object and often fails to maintain stable\norientation during extended motion sequences. Conversely, optimization-based\nmotion planning excels in generating collision-free trajectories over the\nrobot's configuration space but struggles with dynamic interactions where\ncontact forces play a crucial role. To address these limitations, we propose a\nmulti-modal control framework that combines force control and\noptimization-augmented motion planning to tackle complex robotic manipulation\ntasks in a sequential manner, enabling seamless switching between control modes\nbased on task requirements. Our approach decomposes complex tasks into\nsubtasks, each dynamically assigned to one of three control modes: Pure\noptimization for global motion planning, pure force control for precise\ninteraction, or hybrid control for tasks requiring simultaneous trajectory\ntracking and force regulation. This framework is particularly advantageous for\nbimanual and multi-arm manipulation, where synchronous motion and coordination\namong arms are essential while considering both the manipulated object and\nenvironmental constraints. We demonstrate the versatility of our method through\na range of long-horizon manipulation tasks, including single-arm, bimanual, and\nmulti-arm applications, highlighting its ability to handle both free-space\nmotion and contact-rich manipulation with robustness and precision.", "AI": {"tldr": "提出了一种结合力控制和优化运动规划的多模态控制框架，用于复杂机器人操作任务。", "motivation": "力控制在高频适应和柔顺交互中表现优异，但在远距离运动和稳定姿态上受限；优化运动规划擅长全局轨迹生成，但难以处理动态接触力。", "method": "将任务分解为子任务，动态分配三种控制模式：纯优化、纯力控制或混合控制。", "result": "在单臂、双臂和多臂操作任务中展示了方法的鲁棒性和精确性。", "conclusion": "该框架能无缝切换控制模式，适用于自由空间运动和接触丰富的操作任务。"}}
{"id": "2506.15920", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15920", "abs": "https://arxiv.org/abs/2506.15920", "authors": ["Liang Qin", "Weiwei Wan", "Jun Takahashi", "Ryo Negishi", "Masaki Matsushita", "Kensuke Harada"], "title": "Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency", "comment": null, "summary": "This work proposes a learning method to accelerate robotic pick-and-place\nplanning by predicting shared grasps. Shared grasps are defined as grasp poses\nfeasible to both the initial and goal object configurations in a pick-and-place\ntask. Traditional analytical methods for solving shared grasps evaluate grasp\ncandidates separately, leading to substantial computational overhead as the\ncandidate set grows. To overcome the limitation, we introduce an Energy-Based\nModel (EBM) that predicts shared grasps by combining the energies of feasible\ngrasps at both object poses. This formulation enables early identification of\npromising candidates and significantly reduces the search space. Experiments\nshow that our method improves grasp selection performance, offers higher data\nefficiency, and generalizes well to unseen grasps and similarly shaped objects.", "AI": {"tldr": "提出一种基于能量模型的学习方法，通过预测共享抓取加速机器人抓取-放置任务规划。", "motivation": "传统分析方法需单独评估每个抓取候选，计算开销大。", "method": "引入能量模型（EBM），结合初始和目标物体位姿的可行抓取能量，预测共享抓取。", "result": "实验表明，该方法提升了抓取选择性能，数据效率更高，且能泛化到未见过的抓取和类似形状物体。", "conclusion": "该方法显著减少了搜索空间，提高了机器人任务规划的效率。"}}
{"id": "2506.16593", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16593", "abs": "https://arxiv.org/abs/2506.16593", "authors": ["Nicolas Samson", "William Larrivée-Hardy", "William Dubois", "Élie Roy-Brouard", "Edith Brotherton", "Dominic Baril", "Julien Lépine", "François Pomerleau"], "title": "DRIVE Through the Unpredictability:From a Protocol Investigating Slip to a Metric Estimating Command Uncertainty", "comment": "This version is the preprint of a journal article with the same\n  title, accepted in the IEEE Transactions on Field Robotics. To have a look at\n  the early access version, use the following link\n  https://ieeexplore.ieee.org/document/11037776", "summary": "Off-road autonomous navigation is a challenging task as it is mainly\ndependent on the accuracy of the motion model. Motion model performances are\nlimited by their ability to predict the interaction between the terrain and the\nUGV, which an onboard sensor can not directly measure. In this work, we propose\nusing the DRIVE protocol to standardize the collection of data for system\nidentification and characterization of the slip state space. We validated this\nprotocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on\nsix terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9\nhours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to\nexplore the velocity command space and identify the reachable velocities for\nterrain-robot interactions. We investigated the transfer function between the\ncommand velocity space and the resulting steady-state slip for an SSMR. An\nunpredictability metric is proposed to estimate command uncertainty and help\nassess risk likelihood and severity in deployment. Finally, we share our\nlessons learned on running system identification on large UGV to help the\ncommunity.", "AI": {"tldr": "论文提出使用DRIVE协议标准化数据收集，用于系统识别和滑移状态空间表征，并通过实验验证其有效性。", "motivation": "越野自主导航依赖运动模型的准确性，但运动模型受限于对地形与无人地面车辆（UGV）交互的预测能力。", "method": "采用DRIVE协议收集数据，在两平台（75 kg至470 kg）和六种地形上验证，评估协议对速度命令空间的探索能力。", "result": "通过数据分析了速度命令空间与稳态滑移的传递函数，提出了不可预测性度量以评估风险。", "conclusion": "DRIVE协议有效支持系统识别，并分享了大型UGV系统识别的经验。"}}
{"id": "2506.15945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15945", "abs": "https://arxiv.org/abs/2506.15945", "authors": ["Kowndinya Boyalakuntla", "Abdeslam Boularias", "Jingjin Yu"], "title": "KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping", "comment": null, "summary": "We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic\nobject tracking and grasping over eye-on-hand (EoH) systems, significantly\nexpanding such systems capabilities in challenging, realistic environments. In\ncomparison to the previous state-of-the-art, KARL (1) incorporates a novel\nsix-stage RL curriculum that doubles the system's motion range, thereby greatly\nenhancing the system's grasping performance, (2) integrates a robust Kalman\nfilter layer between the perception and reinforcement learning (RL) control\nmodules, enabling the system to maintain an uncertain but continuous 6D pose\nestimate even when the target object temporarily exits the camera's\nfield-of-view or undergoes rapid, unpredictable motion, and (3) introduces\nmechanisms to allow retries to gracefully recover from unavoidable policy\nexecution failures. Extensive evaluations conducted in both simulation and\nreal-world experiments qualitatively and quantitatively corroborate KARL's\nadvantage over earlier systems, achieving higher grasp success rates and faster\nrobot execution speed. Source code and supplementary materials for KARL will be\nmade available at: https://github.com/arc-l/karl.", "AI": {"tldr": "KARL是一种结合卡尔曼滤波和强化学习的动态目标跟踪与抓取系统，显著提升眼在手（EoH）系统在复杂环境中的性能。", "motivation": "解决动态目标跟踪与抓取在复杂环境中的挑战，提升系统在目标短暂消失或快速运动时的鲁棒性。", "method": "1. 六阶段强化学习课程扩展运动范围；2. 卡尔曼滤波层增强6D姿态估计；3. 引入失败恢复机制。", "result": "在仿真和实际实验中，KARL表现出更高的抓取成功率和更快的执行速度。", "conclusion": "KARL通过结合卡尔曼滤波和强化学习，显著提升了动态目标跟踪与抓取的性能。"}}
{"id": "2506.16892", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16892", "abs": "https://arxiv.org/abs/2506.16892", "authors": ["Partha Chowdhury", "Harsha M", "Ayush Gupta", "Sanat K Biswas"], "title": "Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform", "comment": "This work has been already submitted for STEP-IPSC 2025 Conference\n  Proceedings", "summary": "This work presents an indigenous web based platform Orbital Collision (OrCo),\ncreated by the Space Systems Laboratory at IIIT Delhi, to enhance Space\nSituational Awareness (SSA) by predicting collision probabilities of space\nobjects using Two Line Elements (TLE) data. The work highlights the growing\nchallenges of congestion in the Earth's orbital environment, mainly due to\nspace debris and defunct satellites, which increase collision risks. It employs\nseveral methods for propagating orbital uncertainty and calculating the\ncollision probability. The performance of the platform is evaluated through\naccuracy assessments and efficiency metrics, in order to improve the tracking\nof space objects and ensure the safety of the satellite in congested space.", "AI": {"tldr": "OrCo是一个基于网络的平台，用于通过TLE数据预测空间物体的碰撞概率，以增强空间态势感知。", "motivation": "地球轨道环境日益拥挤，主要由空间碎片和失效卫星引起，增加了碰撞风险。", "method": "采用多种方法传播轨道不确定性并计算碰撞概率。", "result": "通过准确性和效率评估验证了平台性能。", "conclusion": "该平台有助于改善空间物体跟踪，确保卫星在拥挤空间中的安全。"}}
{"id": "2506.15953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15953", "abs": "https://arxiv.org/abs/2506.15953", "authors": ["Liang Heng", "Haoran Geng", "Kaifeng Zhang", "Pieter Abbeel", "Jitendra Malik"], "title": "ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation", "comment": null, "summary": "Dexterous manipulation is a cornerstone capability for robotic systems aiming\nto interact with the physical world in a human-like manner. Although\nvision-based methods have advanced rapidly, tactile sensing remains crucial for\nfine-grained control, particularly in unstructured or visually occluded\nsettings. We present ViTacFormer, a representation-learning approach that\ncouples a cross-attention encoder to fuse high-resolution vision and touch with\nan autoregressive tactile prediction head that anticipates future contact\nsignals. Building on this architecture, we devise an easy-to-challenging\ncurriculum that steadily refines the visual-tactile latent space, boosting both\naccuracy and robustness. The learned cross-modal representation drives\nimitation learning for multi-fingered hands, enabling precise and adaptive\nmanipulation. Across a suite of challenging real-world benchmarks, our method\nachieves approximately 50% higher success rates than prior state-of-the-art\nsystems. To our knowledge, it is also the first to autonomously complete\nlong-horizon dexterous manipulation tasks that demand highly precise control\nwith an anthropomorphic hand, successfully executing up to 11 sequential stages\nand sustaining continuous operation for 2.5 minutes.", "AI": {"tldr": "ViTacFormer是一种结合视觉和触觉的表示学习方法，通过跨模态融合和自回归预测提升机器人灵巧操作的精度和鲁棒性，在真实任务中表现优于现有技术。", "motivation": "灵巧操作是机器人实现类人交互的关键能力，但现有视觉方法在非结构化或视觉遮挡场景中表现不足，触觉感知对精细控制至关重要。", "method": "提出ViTacFormer，结合跨注意力编码器融合视觉和触觉信号，并设计自回归触觉预测头。采用由易到难的课程学习策略优化多模态表示。", "result": "在真实任务中，成功率比现有技术高50%，首次实现长时序灵巧操作任务（11个连续阶段，持续2.5分钟）。", "conclusion": "ViTacFormer通过跨模态学习和课程训练显著提升了机器人灵巧操作的性能，为复杂任务提供了新解决方案。"}}
{"id": "2506.17184", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17184", "abs": "https://arxiv.org/abs/2506.17184", "authors": ["Albert H. Li", "Brandon Hung", "Aaron D. Ames", "Jiuguang Wang", "Simon Le Cleac'h", "Preston Culbertson"], "title": "Judo: A User-Friendly Open-Source Package for Sampling-Based Model Predictive Control", "comment": "Accepted at the 2025 RSS Workshop on Fast Motion Planning and Control\n  in the Era of Parallelism. 5 Pages", "summary": "Recent advancements in parallel simulation and successful robotic\napplications are spurring a resurgence in sampling-based model predictive\ncontrol. To build on this progress, however, the robotics community needs\ncommon tooling for prototyping, evaluating, and deploying sampling-based\ncontrollers. We introduce Judo, a software package designed to address this\nneed. To facilitate rapid prototyping and evaluation, Judo provides robust\nimplementations of common sampling-based MPC algorithms and standardized\nbenchmark tasks. It further emphasizes usability with simple but extensible\ninterfaces for controller and task definitions, asynchronous execution for\nstraightforward simulation-to-hardware transfer, and a highly customizable\ninteractive GUI for tuning controllers interactively. While written in Python,\nthe software leverages MuJoCo as its physics backend to achieve real-time\nperformance, which we validate across both consumer and server-grade hardware.\nCode at https://github.com/bdaiinstitute/judo.", "AI": {"tldr": "Judo是一个用于快速原型设计和评估采样基于MPC算法的软件包，提供标准化任务和易用接口。", "motivation": "机器人社区需要通用工具来支持采样基于控制器的开发、评估和部署。", "method": "Judo提供常见采样基于MPC算法的实现、标准化任务、易用接口和异步执行功能。", "result": "Judo在消费级和服务器级硬件上实现实时性能。", "conclusion": "Judo是一个高效、易用的工具，支持采样基于控制器的快速开发和部署。"}}
{"id": "2506.15983", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15983", "abs": "https://arxiv.org/abs/2506.15983", "authors": ["Jianzhu Huai", "Yuxin Shao", "Yujia Zhang", "Alper Yilmaz"], "title": "A Low-Cost Portable Lidar-based Mobile Mapping System on an Android Smartphone", "comment": "ISPRS GSW2025 Dubai UAE", "summary": "The rapid advancement of the metaverse, digital twins, and robotics\nunderscores the demand for low-cost, portable mapping systems for reality\ncapture. Current mobile solutions, such as the Leica BLK2Go and lidar-equipped\nsmartphones, either come at a high cost or are limited in range and accuracy.\nLeveraging the proliferation and technological evolution of mobile devices\nalongside recent advancements in lidar technology, we introduce a novel,\nlow-cost, portable mobile mapping system. Our system integrates a lidar unit,\nan Android smartphone, and an RTK-GNSS stick. Running on the Android platform,\nit features lidar-inertial odometry built with the NDK, and logs data from the\nlidar, wide-angle camera, IMU, and GNSS. With a total bill of materials (BOM)\ncost under 2,000 USD and a weight of about 1 kilogram, the system achieves a\ngood balance between affordability and portability. We detail the system\ndesign, multisensor calibration, synchronization, and evaluate its performance\nfor tracking and mapping. To further contribute to the community, the system's\ndesign and software are made open source at:\nhttps://github.com/OSUPCVLab/marslogger_android/releases/tag/v2.1", "AI": {"tldr": "提出了一种低成本、便携的移动测绘系统，结合激光雷达、智能手机和RTK-GNSS，适用于元宇宙和数字孪生等领域。", "motivation": "当前移动测绘解决方案成本高或性能有限，需要更经济便携的方案。", "method": "系统集成激光雷达、Android手机和RTK-GNSS，通过NDK实现激光雷达惯性里程计，记录多传感器数据。", "result": "系统成本低于2000美元，重量约1公斤，性能平衡。", "conclusion": "系统设计开源，为社区提供低成本测绘解决方案。"}}
{"id": "2506.16012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16012", "abs": "https://arxiv.org/abs/2506.16012", "authors": ["Boyu Li", "Siyuan He", "Hang Xu", "Haoqi Yuan", "Yu Zang", "Liwei Hu", "Junpeng Yue", "Zhenxiong Jiang", "Pengbo Hu", "Börje F. Karlsson", "Yehui Tang", "Zongqing Lu"], "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning", "comment": null, "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.", "AI": {"tldr": "DualTHOR是一个基于物理的仿真平台，专为复杂双臂人形机器人设计，旨在解决现有仿真平台在真实世界机器人应用中的局限性。", "motivation": "当前仿真平台依赖简化的机器人形态并忽略低级执行的随机性，限制了其在真实世界机器人中的可迁移性。", "method": "扩展AI2-THOR，引入真实机器人资产、双臂协作任务套件、逆运动学求解器及基于物理的低级执行故障机制。", "result": "评估显示当前视觉语言模型在双臂协调和真实环境中的鲁棒性表现不佳。", "conclusion": "DualTHOR为开发更强大的视觉语言模型提供了更全面的评估工具，有助于提升其在真实世界任务中的能力。"}}
{"id": "2506.16050", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16050", "abs": "https://arxiv.org/abs/2506.16050", "authors": ["Jiawen Yu", "Jieji Ren", "Yang Chang", "Qiaojun Yu", "Xuan Tong", "Boyang Wang", "Yan Song", "You Li", "Xinji Mai", "Wenqiang Zhang"], "title": "Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments", "comment": "IROS 2025 Oral", "summary": "Anomaly detection and localization in automated industrial manufacturing can\nsignificantly enhance production efficiency and product quality. Existing\nmethods are capable of detecting surface defects in pre-defined or controlled\nimaging environments. However, accurately detecting workpiece defects in\ncomplex and unstructured industrial environments with varying views, poses and\nillumination remains challenging. We propose a novel anomaly detection and\nlocalization method specifically designed to handle inputs with perturbative\npatterns. Our approach introduces a new framework based on a collaborative\ndistillation heterogeneous teacher network (HetNet), an adaptive local-global\nfeature fusion module, and a local multivariate Gaussian noise generation\nmodule. HetNet can learn to model the complex feature distribution of normal\npatterns using limited information about local disruptive changes. We conducted\nextensive experiments on mainstream benchmarks. HetNet demonstrates superior\nperformance with approximately 10% improvement across all evaluation metrics on\nMSC-AD under industrial conditions, while achieving state-of-the-art results on\nother datasets, validating its resilience to environmental fluctuations and its\ncapability to enhance the reliability of industrial anomaly detection systems\nacross diverse scenarios. Tests in real-world environments further confirm that\nHetNet can be effectively integrated into production lines to achieve robust\nand real-time anomaly detection. Codes, images and videos are published on the\nproject website at: https://zihuatanejoyu.github.io/HetNet/", "AI": {"tldr": "提出了一种基于异构教师网络（HetNet）的新方法，用于复杂工业环境中的异常检测与定位，显著提升了性能。", "motivation": "现有方法在复杂、非结构化的工业环境中难以准确检测工件缺陷，亟需一种能适应环境波动的方法。", "method": "采用异构教师网络（HetNet）、自适应局部-全局特征融合模块和局部多元高斯噪声生成模块。", "result": "在主流基准测试中表现优异，MSC-AD指标提升约10%，并在其他数据集上达到最优。", "conclusion": "HetNet能有效提升工业异常检测系统的可靠性，适用于多样化场景，并已在实际生产环境中验证。"}}
{"id": "2506.16079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16079", "abs": "https://arxiv.org/abs/2506.16079", "authors": ["Prakrut Kotecha", "Aditya Shirwatkar", "Shishir Kolathaya"], "title": "Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion", "comment": "6 pages, 5 figures, Accepted at Advances in Robotics (AIR) Conference\n  2025", "summary": "Lagrangian Neural Networks (LNNs) present a principled and interpretable\nframework for learning the system dynamics by utilizing inductive biases. While\ntraditional dynamics models struggle with compounding errors over long\nhorizons, LNNs intrinsically preserve the physical laws governing any system,\nenabling accurate and stable predictions essential for sustainable locomotion.\nThis work evaluates LNNs for infinite horizon planning in quadrupedal robots\nthrough four dynamics models: (1) full-order forward dynamics (FD) training and\ninference, (2) diagonalized representation of Mass Matrix in full order FD, (3)\nfull-order inverse dynamics (ID) training with FD inference, (4) reduced-order\nmodeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that\nLNNs bring improvements in sample efficiency (10x) and superior prediction\naccuracy (up to 2-10x) compared to baseline methods. Notably, the\ndiagonalization approach of LNNs reduces computational complexity while\nretaining some interpretability, enabling real-time receding horizon control.\nThese findings highlight the advantages of LNNs in capturing the underlying\nstructure of system dynamics in quadrupeds, leading to improved performance and\nefficiency in locomotion planning and control. Additionally, our approach\nachieves a higher control frequency than previous LNN methods, demonstrating\nits potential for real-world deployment on quadrupeds.", "AI": {"tldr": "Lagrangian Neural Networks (LNNs) 通过利用归纳偏置学习系统动力学，在四足机器人无限视野规划中表现出高效性和准确性，优于基线方法。", "motivation": "传统动力学模型在长时间视野下存在误差累积问题，LNNs 通过保持物理定律的完整性，提供更准确和稳定的预测，适用于可持续运动。", "method": "评估了四种动力学模型：全阶正向动力学、质量矩阵对角化表示、全阶逆向动力学训练与正向推理、以及基于躯干质心的降阶模型。", "result": "LNNs 在样本效率（10倍）和预测准确性（2-10倍）上显著优于基线方法，对角化方法降低了计算复杂度并保留可解释性。", "conclusion": "LNNs 在四足机器人动力学建模中表现出优越性，提高了运动规划与控制的性能，并展示了实时部署潜力。"}}
{"id": "2506.16143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16143", "abs": "https://arxiv.org/abs/2506.16143", "authors": ["Stephane Ngnepiepaye Wembe", "Vincent Rousseau", "Johann Laconte", "Roland Lenain"], "title": "From Theory to Practice: Identifying the Optimal Approach for Offset Point Tracking in the Context of Agricultural Robotics", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Modern agriculture faces escalating challenges: increasing demand for food,\nlabor shortages, and the urgent need to reduce environmental impact.\nAgricultural robotics has emerged as a promising response to these pressures,\nenabling the automation of precise and suitable field operations. In\nparticular, robots equipped with implements for tasks such as weeding or sowing\nmust interact delicately and accurately with the crops and soil. Unlike robots\nin other domains, these agricultural platforms typically use rigidly mounted\nimplements, where the implement's position is more critical than the robot's\ncenter in determining task success. Yet, most control strategies in the\nliterature focus on the vehicle body, often neglecting the acctual working\npoint of the system. This is particularly important when considering new\nagriculture practices where crops row are not necessary straights. This paper\npresents a predictive control strategy targeting the implement's reference\npoint. The method improves tracking performance by anticipating the motion of\nthe implement, which, due to its offset from the vehicle's center of rotation,\nis prone to overshooting during turns if not properly accounted for.", "AI": {"tldr": "本文提出了一种针对农业机器人工具的预测控制策略，通过预测工具的运动来提升跟踪性能，解决了传统控制策略忽视工具实际工作点的问题。", "motivation": "现代农业面临劳动力短缺和环境压力，农业机器人成为解决方案。然而，现有控制策略多关注机器人本体，忽视了工具的实际工作点，特别是在非直线作物行的情况下。", "method": "提出了一种预测控制策略，专注于工具参考点的运动预测，避免因工具偏移导致的转向过冲问题。", "result": "该方法显著提升了工具在复杂农业环境中的跟踪性能。", "conclusion": "通过优化工具控制策略，农业机器人的精确性和适应性得到提升，为现代农业实践提供了更高效的解决方案。"}}
{"id": "2506.16173", "categories": ["cs.RO", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16173", "abs": "https://arxiv.org/abs/2506.16173", "authors": ["Jiang Wang", "Runwu Shi", "Benjamin Yen", "He Kong", "Kazuhiro Nakadai"], "title": "Single-Microphone-Based Sound Source Localization for Mobile Robots in Reverberant Environments", "comment": "This paper was accepted and going to appear in the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Accurately estimating sound source positions is crucial for robot audition.\nHowever, existing sound source localization methods typically rely on a\nmicrophone array with at least two spatially preconfigured microphones. This\nrequirement hinders the applicability of microphone-based robot audition\nsystems and technologies. To alleviate these challenges, we propose an online\nsound source localization method that uses a single microphone mounted on a\nmobile robot in reverberant environments. Specifically, we develop a\nlightweight neural network model with only 43k parameters to perform real-time\ndistance estimation by extracting temporal information from reverberant\nsignals. The estimated distances are then processed using an extended Kalman\nfilter to achieve online sound source localization. To the best of our\nknowledge, this is the first work to achieve online sound source localization\nusing a single microphone on a moving robot, a gap that we aim to fill in this\nwork. Extensive experiments demonstrate the effectiveness and merits of our\napproach. To benefit the broader research community, we have open-sourced our\ncode at https://github.com/JiangWAV/single-mic-SSL.", "AI": {"tldr": "提出了一种基于单麦克风的在线声源定位方法，适用于移动机器人在混响环境中实时定位声源。", "motivation": "现有声源定位方法通常依赖多麦克风阵列，限制了其应用范围，因此需要一种单麦克风解决方案。", "method": "使用轻量级神经网络（43k参数）提取混响信号中的时间信息进行实时距离估计，并结合扩展卡尔曼滤波器实现在线定位。", "result": "实验证明该方法有效且性能优越，填补了单麦克风移动机器人声源定位的空白。", "conclusion": "该方法为机器人听觉系统提供了一种更灵活、实用的解决方案，并开源了代码以促进研究。"}}
{"id": "2506.16201", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16201", "abs": "https://arxiv.org/abs/2506.16201", "authors": ["Sen Wang", "Le Wang", "Sanping Zhou", "Jingyi Tian", "Jiayi Li", "Haowen Sun", "Wei Tang"], "title": "FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation in high-precision tasks is essential for numerous\nindustrial and real-world applications where accuracy and speed are required.\nYet current diffusion-based policy learning methods generally suffer from low\ncomputational efficiency due to the iterative denoising process during\ninference. Moreover, these methods do not fully explore the potential of\ngenerative models for enhancing information exploration in 3D environments. In\nresponse, we propose FlowRAM, a novel framework that leverages generative\nmodels to achieve region-aware perception, enabling efficient multimodal\ninformation processing. Specifically, we devise a Dynamic Radius Schedule,\nwhich allows adaptive perception, facilitating transitions from global scene\ncomprehension to fine-grained geometric details. Furthermore, we integrate\nstate space models to integrate multimodal information, while preserving linear\ncomputational complexity. In addition, we employ conditional flow matching to\nlearn action poses by regressing deterministic vector fields, simplifying the\nlearning process while maintaining performance. We verify the effectiveness of\nthe FlowRAM in the RLBench, an established manipulation benchmark, and achieve\nstate-of-the-art performance. The results demonstrate that FlowRAM achieves a\nremarkable improvement, particularly in high-precision tasks, where it\noutperforms previous methods by 12.0% in average success rate. Additionally,\nFlowRAM is able to generate physically plausible actions for a variety of\nreal-world tasks in less than 4 time steps, significantly increasing inference\nspeed.", "AI": {"tldr": "FlowRAM是一种基于生成模型的新型框架，通过动态半径调度和条件流匹配，实现了高效的多模态信息处理和动作生成，显著提升了高精度任务的性能。", "motivation": "当前基于扩散的策略学习方法在推理过程中计算效率低，且未充分利用生成模型在3D环境中的信息探索潜力。", "method": "提出FlowRAM框架，结合动态半径调度实现自适应感知，集成状态空间模型处理多模态信息，并使用条件流匹配学习动作位姿。", "result": "在RLBench基准测试中，FlowRAM的平均成功率提升了12.0%，且推理速度显著提高（少于4个时间步骤）。", "conclusion": "FlowRAM在高精度任务中表现出色，为机器人操作提供了高效且性能优越的解决方案。"}}
{"id": "2506.16211", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16211", "abs": "https://arxiv.org/abs/2506.16211", "authors": ["Puhao Li", "Yingying Wu", "Ziheng Xi", "Wanlin Li", "Yuzhe Huang", "Zhiyuan Zhang", "Yinghan Chen", "Jianan Wang", "Song-Chun Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models", "comment": "Website: https://controlvla.github.io", "summary": "Learning real-world robotic manipulation is challenging, particularly when\nlimited demonstrations are available. Existing methods for few-shot\nmanipulation often rely on simulation-augmented data or pre-built modules like\ngrasping and pose estimation, which struggle with sim-to-real gaps and lack\nextensibility. While large-scale imitation pre-training shows promise, adapting\nthese general-purpose policies to specific tasks in data-scarce settings\nremains unexplored. To achieve this, we propose ControlVLA, a novel framework\nthat bridges pre-trained VLA models with object-centric representations via a\nControlNet-style architecture for efficient fine-tuning. Specifically, to\nintroduce object-centric conditions without overwriting prior knowledge,\nControlVLA zero-initializes a set of projection layers, allowing them to\ngradually adapt the pre-trained manipulation policies. In real-world\nexperiments across 6 diverse tasks, including pouring cubes and folding\nclothes, our method achieves a 76.7% success rate while requiring only 10-20\ndemonstrations -- a significant improvement over traditional approaches that\nrequire more than 100 demonstrations to achieve comparable success. Additional\nexperiments highlight ControlVLA's extensibility to long-horizon tasks and\nrobustness to unseen objects and backgrounds.", "AI": {"tldr": "ControlVLA是一个新框架，通过结合预训练的视觉语言动作（VLA）模型和对象中心表示，实现了在少量演示下高效微调机器人操作策略，显著提升了任务成功率。", "motivation": "现有方法在少量演示下难以适应真实世界机器人操作任务，且依赖仿真数据或预建模块，存在仿真到现实的差距和扩展性问题。", "method": "ControlVLA通过ControlNet风格的架构，零初始化投影层，逐步调整预训练策略，实现对象中心条件的引入。", "result": "在6个多样化任务中，仅需10-20次演示即达到76.7%的成功率，显著优于传统方法。", "conclusion": "ControlVLA展示了在少量数据下高效适应特定任务的能力，并具备扩展性和鲁棒性。"}}
{"id": "2506.16219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16219", "abs": "https://arxiv.org/abs/2506.16219", "authors": ["Amine Tourki", "Paul Prevel", "Nils Einecke", "Tim Puphal", "Alexandre Alahi"], "title": "Probabilistic Collision Risk Estimation for Pedestrian Navigation", "comment": null, "summary": "Intelligent devices for supporting persons with vision impairment are\nbecoming more widespread, but they are lacking behind the advancements in\nintelligent driver assistant system. To make a first step forward, this work\ndiscusses the integration of the risk model technology, previously used in\nautonomous driving and advanced driver assistance systems, into an assistance\ndevice for persons with vision impairment. The risk model computes a\nprobabilistic collision risk given object trajectories which has previously\nbeen shown to give better indications of an object's collision potential\ncompared to distance or time-to-contact measures in vehicle scenarios. In this\nwork, we show that the risk model is also superior in warning persons with\nvision impairment about dangerous objects. Our experiments demonstrate that the\nwarning accuracy of the risk model is 67% while both distance and\ntime-to-contact measures reach only 51% accuracy for real-world data.", "AI": {"tldr": "将自动驾驶中的风险模型技术应用于视觉障碍辅助设备，实验证明其警告准确性优于传统距离和时间测量方法。", "motivation": "智能视觉障碍辅助设备的发展滞后于智能驾驶辅助系统，本研究旨在将自动驾驶中的风险模型技术引入视觉障碍辅助领域。", "method": "将用于自动驾驶的风险模型技术整合到视觉障碍辅助设备中，计算物体轨迹的碰撞风险概率。", "result": "风险模型的警告准确性为67%，而距离和时间测量方法的准确性仅为51%。", "conclusion": "风险模型技术在视觉障碍辅助设备中表现优于传统方法，具有实际应用潜力。"}}
{"id": "2506.16263", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16263", "abs": "https://arxiv.org/abs/2506.16263", "authors": ["Xiting He", "Mingwu Su", "Xinqi Jiang", "Long Bai", "Jiewen Lai", "Hongliang Ren"], "title": "CapsDT: Diffusion-Transformer for Capsule Robot Manipulation", "comment": "IROS 2025", "summary": "Vision-Language-Action (VLA) models have emerged as a prominent research\narea, showcasing significant potential across a variety of applications.\nHowever, their performance in endoscopy robotics, particularly endoscopy\ncapsule robots that perform actions within the digestive system, remains\nunexplored. The integration of VLA models into endoscopy robots allows more\nintuitive and efficient interactions between human operators and medical\ndevices, improving both diagnostic accuracy and treatment outcomes. In this\nwork, we design CapsDT, a Diffusion Transformer model for capsule robot\nmanipulation in the stomach. By processing interleaved visual inputs, and\ntextual instructions, CapsDT can infer corresponding robotic control signals to\nfacilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot\nsystem, a capsule robot controlled by a robotic arm-held magnet, addressing\ndifferent levels of four endoscopy tasks and creating corresponding capsule\nrobot datasets within the stomach simulator. Comprehensive evaluations on\nvarious robotic tasks indicate that CapsDT can serve as a robust\nvision-language generalist, achieving state-of-the-art performance in various\nlevels of endoscopy tasks while achieving a 26.25% success rate in real-world\nsimulation manipulation.", "AI": {"tldr": "CapsDT是一种基于扩散变换器的模型，用于胶囊机器人在胃部的操作，结合视觉和文本输入生成控制信号，提升内窥镜任务的效率。", "motivation": "探索VLA模型在内窥镜机器人中的应用潜力，以改善人机交互和医疗效果。", "method": "设计CapsDT模型，处理视觉和文本输入，生成机器人控制信号，并开发胶囊内窥镜机器人系统。", "result": "CapsDT在多种内窥镜任务中表现优异，真实模拟操作成功率达26.25%。", "conclusion": "CapsDT作为视觉语言通用模型，在内窥镜任务中展现出卓越性能和应用前景。"}}
{"id": "2506.16301", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16301", "abs": "https://arxiv.org/abs/2506.16301", "authors": ["Nadine Imholz", "Maurice Brunner", "Nicolas Baumann", "Edoardo Ghignone", "Michele Magno"], "title": "M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing", "comment": null, "summary": "Unrestricted multi-agent racing presents a significant research challenge,\nrequiring decision-making at the limits of a robot's operational capabilities.\nWhile previous approaches have either ignored spatiotemporal information in the\ndecision-making process or been restricted to single-opponent scenarios, this\nwork enables arbitrary multi-opponent head-to-head racing while considering the\nopponents' future intent. The proposed method employs a KF-based multi-opponent\ntracker to effectively perform opponent ReID by associating them across\nobservations. Simultaneously, spatial and velocity GPR is performed on all\nobserved opponent trajectories, providing predictive information to compute the\novertaking maneuvers. This approach has been experimentally validated on a\nphysical 1:10 scale autonomous racing car, achieving an overtaking success rate\nof up to 91.65% and demonstrating an average 10.13%-point improvement in safety\nat the same speed as the previous SotA. These results highlight its potential\nfor high-performance autonomous racing.", "AI": {"tldr": "该论文提出了一种基于KF的多对手跟踪器和GPR的方法，用于多智能体赛车场景，实现了高成功率的超车和安全性提升。", "motivation": "多智能体赛车决策在机器人极限操作能力下具有挑战性，现有方法忽略时空信息或仅适用于单对手场景。", "method": "使用KF多对手跟踪器进行对手ReID，结合空间和速度GPR预测对手轨迹，计算超车策略。", "result": "在1:10比例自主赛车实验中，超车成功率高达91.65%，安全性平均提升10.13%。", "conclusion": "该方法在高性能自主赛车中具有潜力。"}}
{"id": "2506.16336", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16336", "abs": "https://arxiv.org/abs/2506.16336", "authors": ["Yiou Huang"], "title": "Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections", "comment": null, "summary": "Reinforcement learning (RL) exhibits remarkable potential in addressing\nautonomous driving tasks. However, it is difficult to train a sample-efficient\nand safe policy in complex scenarios. In this article, we propose a novel\nhierarchical reinforcement learning (HRL) framework with a goal-conditioned\ncollision prediction (GCCP) module. In the hierarchical structure, the GCCP\nmodule predicts collision risks according to different potential subgoals of\nthe ego vehicle. A high-level decision-maker choose the best safe subgoal. A\nlow-level motion-planner interacts with the environment according to the\nsubgoal. Compared to traditional RL methods, our algorithm is more\nsample-efficient, since its hierarchical structure allows reusing the policies\nof subgoals across similar tasks for various navigation scenarios. In\nadditional, the GCCP module's ability to predict both the ego vehicle's and\nsurrounding vehicles' future actions according to different subgoals, ensures\nthe safety of the ego vehicle throughout the decision-making process.\nExperimental results demonstrate that the proposed method converges to an\noptimal policy faster and achieves higher safety than traditional RL methods.", "AI": {"tldr": "提出了一种基于目标条件碰撞预测（GCCP）模块的分层强化学习（HRL）框架，用于自动驾驶任务，提高了样本效率和安全性。", "motivation": "传统强化学习在复杂场景中难以高效且安全地训练策略，需要一种更优的解决方案。", "method": "采用分层结构，高层决策器选择安全子目标，低层运动规划器执行；GCCP模块预测碰撞风险。", "result": "相比传统方法，算法收敛更快，安全性更高，且子目标策略可跨任务复用。", "conclusion": "提出的HRL框架显著提升了自动驾驶任务的样本效率和安全性。"}}
{"id": "2506.16356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16356", "abs": "https://arxiv.org/abs/2506.16356", "authors": ["Aman Singh", "Deepak Kapa", "Prasham Chedda", "Shishir N. Y. Kolathaya"], "title": "Comparison between External and Internal Single Stage Planetary gearbox actuators for legged robots", "comment": "6 pages, 5 figures, Accepted at Advances in Robotics 2025", "summary": "Legged robots, such as quadrupeds and humanoids, require high-performance\nactuators for efficient locomotion. Quasi-Direct-Drive (QDD) actuators with\nsingle-stage planetary gearboxes offer low inertia, high efficiency, and\ntransparency. Among planetary gearbox architectures, Internal (ISSPG) and\nExternal Single-Stage Planetary Gearbox (ESSPG) are the two predominant\ndesigns. While ISSPG is often preferred for its compactness and high torque\ndensity at certain gear ratios, no objective comparison between the two\narchitectures exists. Additionally, existing designs rely on heuristics rather\nthan systematic optimization. This paper presents a design framework for\noptimally selecting actuator parameters based on given performance requirements\nand motor specifications. Using this framework, we generate and analyze various\noptimized gearbox designs for both architectures. Our results demonstrate that\nfor the T-motor U12, ISSPG is the superior choice within the lower gear ratio\nrange of 5:1 to 7:1, offering a lighter design. However, for gear ratios\nexceeding 7:1, ISSPG becomes infeasible, making ESSPG the better option in the\n7:1 to 11:1 range. To validate our approach, we designed and optimized two\nactuators for manufacturing: an ISSPG with a 6.0:1 gear ratio and an ESSPG with\na 7.2:1 gear ratio. Their respective masses closely align with our optimization\nmodel predictions, confirming the effectiveness of our methodology.", "AI": {"tldr": "论文提出了一个设计框架，用于优化选择执行器参数，比较了内单级行星齿轮箱（ISSPG）和外单级行星齿轮箱（ESSPG）的性能，并验证了其方法的有效性。", "motivation": "目前缺乏对ISSPG和ESSPG两种行星齿轮箱架构的客观比较，且现有设计依赖启发式方法而非系统优化。", "method": "提出了一个基于性能要求和电机规格的设计框架，用于优化执行器参数，并生成和分析两种架构的优化齿轮箱设计。", "result": "对于T-motor U12，ISSPG在5:1至7:1的低传动比范围内表现更优，而ESSPG在7:1至11:1范围内更优。优化设计的执行器质量与模型预测一致。", "conclusion": "提出的设计框架有效，能够为不同传动比范围选择最优的行星齿轮箱架构。"}}
{"id": "2506.16386", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16386", "abs": "https://arxiv.org/abs/2506.16386", "authors": ["Leesai Park", "Keunwoo Jang", "Sanghyun Kim"], "title": "CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance", "comment": null, "summary": "This paper proposes Constrained Sampling Cluster Model Predictive Path\nIntegral (CSC-MPPI), a novel constrained formulation of MPPI designed to\nenhance trajectory optimization while enforcing strict constraints on system\nstates and control inputs. Traditional MPPI, which relies on a probabilistic\nsampling process, often struggles with constraint satisfaction and generates\nsuboptimal trajectories due to the weighted averaging of sampled trajectories.\nTo address these limitations, the proposed framework integrates a primal-dual\ngradient-based approach and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) to steer sampled input trajectories into feasible regions\nwhile mitigating risks associated with weighted averaging. First, to ensure\nthat sampled trajectories remain within the feasible region, the primal-dual\ngradient method is applied to iteratively shift sampled inputs while enforcing\nstate and control constraints. Then, DBSCAN groups the sampled trajectories,\nenabling the selection of representative control inputs within each cluster.\nFinally, among the representative control inputs, the one with the lowest cost\nis chosen as the optimal action. As a result, CSC-MPPI guarantees constraint\nsatisfaction, improves trajectory selection, and enhances robustness in complex\nenvironments. Simulation and real-world experiments demonstrate that CSC-MPPI\noutperforms traditional MPPI in obstacle avoidance, achieving improved\nreliability and efficiency. The experimental videos are available at\nhttps://cscmppi.github.io", "AI": {"tldr": "CSC-MPPI是一种改进的MPPI方法，通过结合原始对偶梯度方法和DBSCAN聚类，在轨迹优化中严格满足约束条件，提升性能。", "motivation": "传统MPPI在满足约束条件和生成最优轨迹方面存在不足，CSC-MPPI旨在解决这些问题。", "method": "结合原始对偶梯度方法确保轨迹可行性，使用DBSCAN聚类选择代表性控制输入，从中选取最优动作。", "result": "CSC-MPPI在避障任务中表现优于传统MPPI，提高了可靠性和效率。", "conclusion": "CSC-MPPI通过严格约束和聚类优化，显著提升了轨迹优化的性能和鲁棒性。"}}
{"id": "2506.16475", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16475", "abs": "https://arxiv.org/abs/2506.16475", "authors": ["Yaru Niu", "Yunzhe Zhang", "Mingyang Yu", "Changyi Lin", "Chenhao Li", "Yikai Wang", "Yuxiang Yang", "Wenhao Yu", "Tingnan Zhang", "Bingqing Chen", "Jonathan Francis", "Zhenzhen Li", "Jie Tan", "Ding Zhao"], "title": "Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining", "comment": null, "summary": "Quadrupedal robots have demonstrated impressive locomotion capabilities in\ncomplex environments, but equipping them with autonomous versatile manipulation\nskills in a scalable way remains a significant challenge. In this work, we\nintroduce a cross-embodiment imitation learning system for quadrupedal\nmanipulation, leveraging data collected from both humans and LocoMan, a\nquadruped equipped with multiple manipulation modes. Specifically, we develop a\nteleoperation and data collection pipeline, which unifies and modularizes the\nobservation and action spaces of the human and the robot. To effectively\nleverage the collected data, we propose an efficient modularized architecture\nthat supports co-training and pretraining on structured modality-aligned data\nacross different embodiments. Additionally, we construct the first manipulation\ndataset for the LocoMan robot, covering various household tasks in both\nunimanual and bimanual modes, supplemented by a corresponding human dataset. We\nvalidate our system on six real-world manipulation tasks, where it achieves an\naverage success rate improvement of 41.9% overall and 79.7% under\nout-of-distribution (OOD) settings compared to the baseline. Pretraining with\nhuman data contributes a 38.6% success rate improvement overall and 82.7% under\nOOD settings, enabling consistently better performance with only half the\namount of robot data. Our code, hardware, and data are open-sourced at:\nhttps://human2bots.github.io.", "AI": {"tldr": "该论文提出了一种跨具身模仿学习系统，用于四足机器人的多功能操作，通过结合人类和机器人数据，显著提升了操作任务的性能。", "motivation": "四足机器人在复杂环境中的运动能力已很出色，但实现自主多功能操作仍具挑战性。", "method": "开发了统一的遥操作和数据收集流程，提出模块化架构支持跨具身数据训练和预训练。", "result": "在六项真实操作任务中，系统平均成功率提升41.9%，OOD设置下提升79.7%。", "conclusion": "结合人类数据预训练可显著提升性能，且仅需一半机器人数据即可实现更好效果。"}}
{"id": "2506.16493", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16493", "abs": "https://arxiv.org/abs/2506.16493", "authors": ["Mehreen Naeem", "Andrew Melnik", "Michael Beetz"], "title": "Grounding Language Models with Semantic Digital Twins for Robotic Planning", "comment": null, "summary": "We introduce a novel framework that integrates Semantic Digital Twins (SDTs)\nwith Large Language Models (LLMs) to enable adaptive and goal-driven robotic\ntask execution in dynamic environments. The system decomposes natural language\ninstructions into structured action triplets, which are grounded in contextual\nenvironmental data provided by the SDT. This semantic grounding allows the\nrobot to interpret object affordances and interaction rules, enabling action\nplanning and real-time adaptability. In case of execution failures, the LLM\nutilizes error feedback and SDT insights to generate recovery strategies and\niteratively revise the action plan. We evaluate our approach using tasks from\nthe ALFRED benchmark, demonstrating robust performance across various household\nscenarios. The proposed framework effectively combines high-level reasoning\nwith semantic environment understanding, achieving reliable task completion in\nthe face of uncertainty and failure.", "AI": {"tldr": "提出了一种结合语义数字孪生（SDT）和大语言模型（LLM）的框架，用于动态环境中机器人任务的适应性执行。", "motivation": "解决动态环境中机器人任务执行的高层推理和语义环境理解的结合问题。", "method": "将自然语言指令分解为结构化动作三元组，并通过SDT提供环境数据支持语义理解，LLM用于错误恢复和动作计划修订。", "result": "在ALFRED基准测试中表现稳健，适用于多种家庭场景。", "conclusion": "该框架成功结合了高层推理与语义环境理解，在不确定性和失败情况下实现了可靠的任务完成。"}}
{"id": "2506.16565", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16565", "abs": "https://arxiv.org/abs/2506.16565", "authors": ["Yuxin Chen", "Jianglan Wei", "Chenfeng Xu", "Boyi Li", "Masayoshi Tomizuka", "Andrea Bajcsy", "Ran Tian"], "title": "Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control", "comment": null, "summary": "World models enable robots to \"imagine\" future observations given current\nobservations and planned actions, and have been increasingly adopted as\ngeneralized dynamics models to facilitate robot learning. Despite their\npromise, these models remain brittle when encountering novel visual distractors\nsuch as objects and background elements rarely seen during training.\nSpecifically, novel distractors can corrupt action outcome predictions, causing\ndownstream failures when robots rely on the world model imaginations for\nplanning or action verification. In this work, we propose Reimagination with\nObservation Intervention (ReOI), a simple yet effective test-time strategy that\nenables world models to predict more reliable action outcomes in open-world\nscenarios where novel and unanticipated visual distractors are inevitable.\nGiven the current robot observation, ReOI first detects visual distractors by\nidentifying which elements of the scene degrade in physically implausible ways\nduring world model prediction. Then, it modifies the current observation to\nremove these distractors and bring the observation closer to the training\ndistribution. Finally, ReOI \"reimagines\" future outcomes with the modified\nobservation and reintroduces the distractors post-hoc to preserve visual\nconsistency for downstream planning and verification. We validate our approach\non a suite of robotic manipulation tasks in the context of action verification,\nwhere the verifier needs to select desired action plans based on predictions\nfrom a world model. Our results show that ReOI is robust to both\nin-distribution and out-of-distribution visual distractors. Notably, it\nimproves task success rates by up to 3x in the presence of novel distractors,\nsignificantly outperforming action verification that relies on world model\npredictions without imagination interventions.", "AI": {"tldr": "论文提出ReOI方法，通过检测并干预视觉干扰物，提升世界模型在开放环境中的预测可靠性。", "motivation": "现有世界模型在面对训练中未见的视觉干扰物时表现脆弱，影响动作预测和下游任务。", "method": "提出ReOI方法，检测并移除干扰物，修正观测后重新预测未来结果。", "result": "ReOI显著提升任务成功率，最高达3倍，优于未干预的模型。", "conclusion": "ReOI是一种简单有效的测试时策略，适用于开放世界中的机器人任务。"}}
{"id": "2506.16623", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16623", "abs": "https://arxiv.org/abs/2506.16623", "authors": ["Mobin Habibpour", "Fatemeh Afghah"], "title": "History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation", "comment": null, "summary": "Object Goal Navigation (ObjectNav) challenges robots to find objects in\nunseen environments, demanding sophisticated reasoning. While Vision-Language\nModels (VLMs) show potential, current ObjectNav methods often employ them\nsuperficially, primarily using vision-language embeddings for object-scene\nsimilarity checks rather than leveraging deeper reasoning. This limits\ncontextual understanding and leads to practical issues like repetitive\nnavigation behaviors. This paper introduces a novel zero-shot ObjectNav\nframework that pioneers the use of dynamic, history-aware prompting to more\ndeeply integrate VLM reasoning into frontier-based exploration. Our core\ninnovation lies in providing the VLM with action history context, enabling it\nto generate semantic guidance scores for navigation actions while actively\navoiding decision loops. We also introduce a VLM-assisted waypoint generation\nmechanism for refining the final approach to detected objects. Evaluated on the\nHM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and\n24.8% Success weighted by Path Length (SPL). These results are comparable to\nstate-of-the-art zero-shot methods, demonstrating the significant potential of\nour history-augmented VLM prompting strategy for more robust and context-aware\nrobotic navigation.", "AI": {"tldr": "本文提出了一种新的零样本目标导航框架，通过动态历史感知提示深度整合视觉语言模型（VLM）推理，解决了现有方法在上下文理解和导航行为上的局限性。", "motivation": "当前目标导航方法对视觉语言模型（VLM）的利用较浅，仅用于对象-场景相似性检查，缺乏深度推理能力，导致上下文理解不足和重复导航行为。", "method": "采用动态历史感知提示技术，为VLM提供动作历史上下文，生成导航动作的语义指导分数，并引入VLM辅助的路径点生成机制。", "result": "在HM3D数据集上的实验显示，成功率为46%，路径长度加权的成功率为24.8%，与现有零样本方法相当。", "conclusion": "历史增强的VLM提示策略显著提升了机器人导航的鲁棒性和上下文感知能力。"}}
{"id": "2506.16643", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16643", "abs": "https://arxiv.org/abs/2506.16643", "authors": ["Matthew Ebisu", "Hang Yu", "Reuben Aronson", "Elaine Short"], "title": "See What I Mean? Expressiveness and Clarity in Robot Display Design", "comment": null, "summary": "Nonverbal visual symbols and displays play an important role in communication\nwhen humans and robots work collaboratively. However, few studies have\ninvestigated how different types of non-verbal cues affect objective task\nperformance, especially in a dynamic environment that requires real time\ndecision-making. In this work, we designed a collaborative navigation task\nwhere the user and the robot only had partial information about the map on each\nend and thus the users were forced to communicate with a robot to complete the\ntask. We conducted our study in a public space and recruited 37 participants\nwho randomly passed by our setup. Each participant collaborated with a robot\nutilizing either animated anthropomorphic eyes and animated icons, or static\nanthropomorphic eyes and static icons. We found that participants that\ninteracted with a robot with animated displays reported the greatest level of\ntrust and satisfaction; that participants interpreted static icons the best;\nand that participants with a robot with static eyes had the highest completion\nsuccess. These results suggest that while animation can foster trust with\nrobots, human-robot communication can be optimized by the addition of familiar\nstatic icons that may be easier for users to interpret. We published our code,\ndesigned symbols, and collected results online at:\nhttps://github.com/mattufts/huamn_Cozmo_interaction.", "AI": {"tldr": "研究探讨了非语言视觉符号在人与机器人协作中的作用，发现动画显示能提升信任感，而静态图标更易解读。", "motivation": "研究动机是探索不同类型的非语言提示如何影响动态环境中人与机器人协作的任务表现。", "method": "设计了一个协作导航任务，参与者与机器人使用动画或静态的非语言符号进行交流，并在公共空间招募了37名参与者进行实验。", "result": "结果显示，动画显示提升了信任感，静态图标更易解读，而静态眼睛的机器人任务完成率最高。", "conclusion": "结论是动画能增强信任，但结合熟悉的静态图标可以优化人机交流效果。"}}
{"id": "2506.16652", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16652", "abs": "https://arxiv.org/abs/2506.16652", "authors": ["Guang Yin", "Yitong Li", "Yixuan Wang", "Dale McConachie", "Paarth Shah", "Kunimatsu Hashimoto", "Huan Zhang", "Katherine Liu", "Yunzhu Li"], "title": "CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025. The first three\n  authors contributed equally. Project Page:\n  https://robopil.github.io/code-diffuser/", "summary": "Natural language instructions for robotic manipulation tasks often exhibit\nambiguity and vagueness. For instance, the instruction \"Hang a mug on the mug\ntree\" may involve multiple valid actions if there are several mugs and branches\nto choose from. Existing language-conditioned policies typically rely on\nend-to-end models that jointly handle high-level semantic understanding and\nlow-level action generation, which can result in suboptimal performance due to\ntheir lack of modularity and interpretability. To address these challenges, we\nintroduce a novel robotic manipulation framework that can accomplish tasks\nspecified by potentially ambiguous natural language. This framework employs a\nVision-Language Model (VLM) to interpret abstract concepts in natural language\ninstructions and generates task-specific code - an interpretable and executable\nintermediate representation. The generated code interfaces with the perception\nmodule to produce 3D attention maps that highlight task-relevant regions by\nintegrating spatial and semantic information, effectively resolving ambiguities\nin instructions. Through extensive experiments, we identify key limitations of\ncurrent imitation learning methods, such as poor adaptation to language and\nenvironmental variations. We show that our approach excels across challenging\nmanipulation tasks involving language ambiguity, contact-rich manipulation, and\nmulti-object interactions.", "AI": {"tldr": "论文提出了一种新型机器人操作框架，通过视觉语言模型（VLM）解析自然语言指令中的模糊概念，并生成可执行代码，以解决指令歧义问题。", "motivation": "自然语言指令在机器人操作任务中常存在模糊性，现有端到端模型因缺乏模块化和可解释性导致性能不佳。", "method": "框架结合VLM生成任务特定代码，通过感知模块生成3D注意力图，整合空间与语义信息以消除指令歧义。", "result": "实验表明，该方法在语言模糊性、接触丰富操作和多物体交互任务中表现优异。", "conclusion": "该框架有效解决了自然语言指令的模糊性问题，提升了机器人操作的适应性和性能。"}}
{"id": "2506.16685", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16685", "abs": "https://arxiv.org/abs/2506.16685", "authors": ["Xiaomeng Xu", "Yifan Hou", "Zeyi Liu", "Shuran Song"], "title": "Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections", "comment": null, "summary": "We address key challenges in Dataset Aggregation (DAgger) for real-world\ncontact-rich manipulation: how to collect informative human correction data and\nhow to effectively update policies with this new data. We introduce Compliant\nResidual DAgger (CR-DAgger), which contains two novel components: 1) a\nCompliant Intervention Interface that leverages compliance control, allowing\nhumans to provide gentle, accurate delta action corrections without\ninterrupting the ongoing robot policy execution; and 2) a Compliant Residual\nPolicy formulation that learns from human corrections while incorporating force\nfeedback and force control. Our system significantly enhances performance on\nprecise contact-rich manipulation tasks using minimal correction data,\nimproving base policy success rates by over 50\\% on two challenging tasks (book\nflipping and belt assembly) while outperforming both retraining-from-scratch\nand finetuning approaches. Through extensive real-world experiments, we provide\npractical guidance for implementing effective DAgger in real-world robot\nlearning tasks. Result videos are available at:\nhttps://compliant-residual-dagger.github.io/", "AI": {"tldr": "论文提出了一种改进的DAgger方法（CR-DAgger），通过合规干预接口和合规残差策略，显著提升了接触密集型操作任务的性能。", "motivation": "解决在真实世界接触密集型操作中，如何收集有效的人类校正数据以及如何利用这些数据更新策略的关键挑战。", "method": "引入合规干预接口和合规残差策略，结合力反馈和力控制，从人类校正中学习。", "result": "在书籍翻页和皮带组装任务中，基础策略成功率提升超过50%，优于从头训练和微调方法。", "conclusion": "CR-DAgger为真实世界机器人学习任务提供了有效的DAgger实现指南。"}}
{"id": "2506.16703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16703", "abs": "https://arxiv.org/abs/2506.16703", "authors": ["Sinuo Cheng", "Ruyi Zhou", "Wenhao Feng", "Huaiguang Yang", "Haibo Gao", "Zongquan Deng", "Liang Ding"], "title": "VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation", "comment": "accepted by IROS 2025", "summary": "The increasingly complex and diverse planetary exploration environment\nrequires more adaptable and flexible rover navigation strategy. In this study,\nwe propose a VLM-empowered multi-mode system to achieve efficient while safe\nautonomous navigation for planetary rovers. Vision-Language Model (VLM) is used\nto parse scene information by image inputs to achieve a human-level\nunderstanding of terrain complexity. Based on the complexity classification,\nthe system switches to the most suitable navigation mode, composing of\nperception, mapping and planning modules designed for different terrain types,\nto traverse the terrain ahead before reaching the next waypoint. By integrating\nthe local navigation system with a map server and a global waypoint generation\nmodule, the rover is equipped to handle long-distance navigation tasks in\ncomplex scenarios. The navigation system is evaluated in various simulation\nenvironments. Compared to the single-mode conservative navigation method, our\nmulti-mode system is able to bootstrap the time and energy efficiency in a\nlong-distance traversal with varied type of obstacles, enhancing efficiency by\n79.5%, while maintaining its avoidance capabilities against terrain hazards to\nguarantee rover safety. More system information is shown at\nhttps://chengsn1234.github.io/multi-mode-planetary-navigation/.", "AI": {"tldr": "提出了一种基于视觉语言模型（VLM）的多模式导航系统，用于行星探测车的高效安全自主导航。", "motivation": "行星探测环境的复杂性和多样性要求更灵活适应的导航策略。", "method": "利用VLM解析场景信息，根据地形复杂度分类切换不同导航模式（感知、建图、规划模块）。", "result": "多模式系统在模拟环境中效率提升79.5%，同时保持对地形危险的规避能力。", "conclusion": "该系统显著提升了行星探测车在复杂环境中的导航效率和安全性。"}}
{"id": "2506.16710", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16710", "abs": "https://arxiv.org/abs/2506.16710", "authors": ["Aditya Bhatt", "Mary Katherine Corra", "Franklin Merlo", "Prajit KrisshnaKumar", "Souma Chowdhury"], "title": "Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms", "comment": "to be published in IDETC 2025 conference proceedings", "summary": "Signal source localization has been a problem of interest in the multi-robot\nsystems domain given its applications in search \\& rescue and hazard\nlocalization in various industrial and outdoor settings. A variety of\nmulti-robot search algorithms exist that usually formulate and solve the\nassociated autonomous motion planning problem as a heuristic model-free or\nbelief model-based optimization process. Most of these algorithms however\nremains tested only in simulation, thereby losing the opportunity to generate\nknowledge about how such algorithms would compare/contrast in a real physical\nsetting in terms of search performance and real-time computing performance. To\naddress this gap, this paper presents a new lab-scale physical setup and\nassociated open-source software pipeline to evaluate and benchmark multi-robot\nsearch algorithms. The presented physical setup innovatively uses an acoustic\nsource (that is safe and inexpensive) and small ground robots (e-pucks)\noperating in a standard motion-capture environment. This setup can be easily\nrecreated and used by most robotics researchers. The acoustic source also\npresents interesting uncertainty in terms of its noise-to-signal ratio, which\nis useful to assess sim-to-real gaps. The overall software pipeline is designed\nto readily interface with any multi-robot search algorithm with minimal effort\nand is executable in parallel asynchronous form. This pipeline includes a\nframework for distributed implementation of multi-robot or swarm search\nalgorithms, integrated with a ROS (Robotics Operating System)-based software\nstack for motion capture supported localization. The utility of this novel\nsetup is demonstrated by using it to evaluate two state-of-the-art multi-robot\nsearch algorithms, based on swarm optimization and batch-Bayesian Optimization\n(called Bayes-Swarm), as well as a random walk baseline.", "AI": {"tldr": "本文提出了一种新的实验室规模物理设置和开源软件管道，用于评估和基准测试多机器人搜索算法，填补了算法在真实物理环境中性能测试的空白。", "motivation": "多机器人信号源定位在搜索救援和危险定位中有广泛应用，但现有算法多在仿真中测试，缺乏真实物理环境中的性能对比。", "method": "使用声源和小型地面机器人（e-pucks）在标准运动捕捉环境中构建物理设置，开发开源软件管道支持多机器人搜索算法的分布式实现。", "result": "通过评估两种先进的多机器人搜索算法（群优化和批量贝叶斯优化）以及随机行走基线，验证了该设置的实用性。", "conclusion": "该物理设置和软件管道为多机器人搜索算法的真实环境测试提供了便捷工具，有助于缩小仿真与现实的差距。"}}
{"id": "2506.16720", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16720", "abs": "https://arxiv.org/abs/2506.16720", "authors": ["Weitao Zhou", "Bo Zhang", "Zhong Cao", "Xiang Li", "Qian Cheng", "Chunyang Liu", "Yaqin Zhang", "Diange Yang"], "title": "DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy", "comment": null, "summary": "With the increasing presence of automated vehicles on open roads under driver\nsupervision, disengagement cases are becoming more prevalent. While some\ndata-driven planning systems attempt to directly utilize these disengagement\ncases for policy improvement, the inherent scarcity of disengagement data\n(often occurring as a single instances) restricts training effectiveness.\nFurthermore, some disengagement data should be excluded since the disengagement\nmay not always come from the failure of driving policies, e.g. the driver may\ncasually intervene for a while. To this end, this work proposes\ndisengagement-reason-augmented reinforcement learning (DRARL), which enhances\ndriving policy improvement process according to the reason of disengagement\ncases. Specifically, the reason of disengagement is identified by a\nout-of-distribution (OOD) state estimation model. When the reason doesn't\nexist, the case will be identified as a casual disengagement case, which\ndoesn't require additional policy adjustment. Otherwise, the policy can be\nupdated under a reason-augmented imagination environment, improving the policy\nperformance of disengagement cases with similar reasons. The method is\nevaluated using real-world disengagement cases collected by autonomous driving\nrobotaxi. Experimental results demonstrate that the method accurately\nidentifies policy-related disengagement reasons, allowing the agent to handle\nboth original and semantically similar cases through reason-augmented training.\nFurthermore, the approach prevents the agent from becoming overly conservative\nafter policy adjustments. Overall, this work provides an efficient way to\nimprove driving policy performance with disengagement cases.", "AI": {"tldr": "论文提出了一种基于脱钩原因增强的强化学习方法（DRARL），通过识别脱钩原因来优化自动驾驶策略，避免无效数据干扰。", "motivation": "自动驾驶车辆在开放道路上的脱钩案例日益增多，但数据稀缺且部分脱钩并非策略失败导致，直接利用这些数据效果有限。", "method": "使用OOD状态估计模型识别脱钩原因，区分无效脱钩案例，并在原因增强的想象环境中更新策略。", "result": "实验表明，该方法能准确识别策略相关脱钩原因，提升策略性能，同时避免策略过于保守。", "conclusion": "DRARL为利用脱钩案例优化驾驶策略提供了高效方法。"}}
{"id": "2506.16748", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16748", "abs": "https://arxiv.org/abs/2506.16748", "authors": ["Arjo Chakravarty", "Michael X. Grey", "M. A. Viraj J. Muthugala", "Mohan Rajesh Elara"], "title": "A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT", "comment": null, "summary": "Free-space multi-agent path planning remains challenging at large scales.\nMost existing methods either offer optimality guarantees but do not scale\nbeyond a few dozen agents, or rely on grid-world assumptions that do not\ngeneralize well to continuous space. In this work, we propose a hybrid,\nrule-based planning framework that combines Priority Inheritance with\nBacktracking (PiBT) with a novel safety-aware path smoothing method. Our\napproach extends PiBT to 8-connected grids and selectively applies\nstring-pulling based smoothing while preserving collision safety through local\ninteraction awareness and a fallback collision resolution step based on Safe\nInterval Path Planning (SIPP). This design allows us to reduce overall path\nlengths while maintaining real-time performance. We demonstrate that our method\ncan scale to over 500 agents in large free-space environments, outperforming\nexisting any-angle and optimal methods in terms of runtime, while producing\nnear-optimal trajectories in sparse domains. Our results suggest this framework\nis a promising building block for scalable, real-time multi-agent navigation in\nrobotics systems operating beyond grid constraints.", "AI": {"tldr": "提出了一种结合优先级继承回溯（PiBT）和安全感知路径平滑的混合规划框架，用于大规模自由空间多智能体路径规划。", "motivation": "现有方法在大规模场景下难以兼顾最优性和实时性，或依赖网格假设。", "method": "扩展PiBT至8连通网格，结合安全感知路径平滑和局部交互感知，使用SIPP解决碰撞。", "result": "方法支持500+智能体，实时性能优于现有方法，路径接近最优。", "conclusion": "该框架为机器人系统在大规模自由空间中的实时导航提供了可行方案。"}}
{"id": "2506.16822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16822", "abs": "https://arxiv.org/abs/2506.16822", "authors": ["Daniel Frau-Alfaro", "Julio Castaño-Amoros", "Santiago Puente", "Pablo Gil", "Roberto Calandra"], "title": "Learning Dexterous Object Handover", "comment": "Paper accepted for presentation in RoMan 2025", "summary": "Object handover is an important skill that we use daily when interacting with\nother humans. To deploy robots in collaborative setting, like houses, being\nable to receive and handing over objects safely and efficiently becomes a\ncrucial skill. In this work, we demonstrate the use of Reinforcement Learning\n(RL) for dexterous object handover between two multi-finger hands. Key to this\ntask is the use of a novel reward function based on dual quaternions to\nminimize the rotation distance, which outperforms other rotation\nrepresentations such as Euler and rotation matrices. The robustness of the\ntrained policy is experimentally evaluated by testing w.r.t. objects that are\nnot included in the training distribution, and perturbations during the\nhandover process. The results demonstrate that the trained policy successfully\nperform this task, achieving a total success rate of 94% in the best-case\nscenario after 100 experiments, thereby showing the robustness of our policy\nwith novel objects. In addition, the best-case performance of the policy\ndecreases by only 13.8% when the other robot moves during the handover, proving\nthat our policy is also robust to this type of perturbation, which is common in\nreal-world object handovers.", "AI": {"tldr": "使用强化学习（RL）实现多指手之间的灵巧物体交接，提出基于双四元数的新奖励函数以减少旋转距离，实验证明其鲁棒性。", "motivation": "在协作环境中（如家庭），机器人需要安全高效地交接物体，这是日常人机交互的重要技能。", "method": "采用强化学习方法，引入基于双四元数的奖励函数优化旋转距离，优于欧拉角和旋转矩阵。", "result": "训练策略在未见过物体和扰动情况下表现良好，最佳成功率94%，扰动下性能仅下降13.8%。", "conclusion": "该方法在物体交接任务中表现出高效性和鲁棒性，适用于实际场景。"}}
{"id": "2506.16936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16936", "abs": "https://arxiv.org/abs/2506.16936", "authors": ["Shengpeng Wang", "Xin Luo", "Yulong Xie", "Wei Wang"], "title": "SDDiff: Boost Radar Perception via Spatial-Doppler Diffusion", "comment": null, "summary": "Point cloud extraction (PCE) and ego velocity estimation (EVE) are key\ncapabilities gaining attention in 3D radar perception. However, existing work\ntypically treats these two tasks independently, which may neglect the interplay\nbetween radar's spatial and Doppler domain features, potentially introducing\nadditional bias. In this paper, we observe an underlying correlation between 3D\npoints and ego velocity, which offers reciprocal benefits for PCE and EVE. To\nfully unlock such inspiring potential, we take the first step to design a\nSpatial-Doppler Diffusion (SDDiff) model for simultaneously dense PCE and\naccurate EVE. To seamlessly tailor it to radar perception, SDDiff improves the\nconventional latent diffusion process in three major aspects. First, we\nintroduce a representation that embodies both spatial occupancy and Doppler\nfeatures. Second, we design a directional diffusion with radar priors to\nstreamline the sampling. Third, we propose Iterative Doppler Refinement to\nenhance the model's adaptability to density variations and ghosting effects.\nExtensive evaluations show that SDDiff significantly outperforms\nstate-of-the-art baselines by achieving 59% higher in EVE accuracy, 4X greater\nin valid generation density while boosting PCE effectiveness and reliability.", "AI": {"tldr": "论文提出了一种名为SDDiff的模型，通过结合空间和多普勒特征，同时优化点云提取（PCE）和自车速度估计（EVE），显著提升了性能。", "motivation": "现有研究通常将PCE和EVE视为独立任务，忽略了雷达空间和多普勒特征的相互作用，可能引入偏差。论文发现两者存在潜在关联，可以相互促进。", "method": "设计了Spatial-Doppler Diffusion（SDDiff）模型，改进传统潜在扩散过程：1）引入结合空间和多普勒特征的表示；2）设计基于雷达先验的方向性扩散；3）提出迭代多普勒细化以增强适应性。", "result": "SDDiff在EVE准确性上提升59%，生成密度提高4倍，同时显著提升了PCE的有效性和可靠性。", "conclusion": "SDDiff通过结合空间和多普勒特征，为雷达感知任务提供了更高效的解决方案，展示了联合建模的潜力。"}}
{"id": "2506.16986", "categories": ["cs.RO", "68T40, 93C85, 70E60", "I.2.9; I.2.10; I.2.8"], "pdf": "https://arxiv.org/pdf/2506.16986", "abs": "https://arxiv.org/abs/2506.16986", "authors": ["Yuntao Ma", "Yang Liu", "Kaixian Qu", "Marco Hutter"], "title": "Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration", "comment": "8 pages, IROS 2025", "summary": "Throwing is a fundamental skill that enables robots to manipulate objects in\nways that extend beyond the reach of their arms. We present a control framework\nthat combines learning and model-based control for prehensile whole-body\nthrowing with legged mobile manipulators. Our framework consists of three\ncomponents: a nominal tracking policy for the end-effector, a high-frequency\nresidual policy to enhance tracking accuracy, and an optimization-based module\nto improve end-effector acceleration control. The proposed controller achieved\nthe average of 0.28 m landing error when throwing at targets located 6 m away.\nFurthermore, in a comparative study with university students, the system\nachieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,\nhitting small targets randomly placed at distances of 3-5 m while throwing at a\nspecified speed of 6 m/s. In contrast, humans have a success rate of only\n15.2%. This work provides an early demonstration of prehensile throwing with\nquantified accuracy on hardware, contributing to progress in dynamic whole-body\nmanipulation.", "AI": {"tldr": "论文提出了一种结合学习和模型控制的方法，用于腿式移动机械臂的投掷任务，实现了较高的准确性和成功率。", "motivation": "投掷是机器人扩展操作范围的重要技能，但现有方法在动态全身操作中的准确性不足。", "method": "框架包括末端执行器的名义跟踪策略、高频残差策略和基于优化的加速度控制模块。", "result": "投掷6米目标时平均着陆误差0.28米，与大学生对比实验中成功率达56.8%，远超人类的15.2%。", "conclusion": "该工作展示了动态全身操作中投掷任务的硬件实现，为相关领域提供了进展。"}}
{"id": "2506.17110", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17110", "abs": "https://arxiv.org/abs/2506.17110", "authors": ["Teng Guo", "Baichuan Huang", "Jingjin Yu"], "title": "Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping", "comment": "Accepted to IROS 2025", "summary": "Accurate 6D object pose estimation is a prerequisite for successfully\ncompleting robotic prehensile and non-prehensile manipulation tasks. At\npresent, 6D pose estimation for robotic manipulation generally relies on depth\nsensors based on, e.g., structured light, time-of-flight, and stereo-vision,\nwhich can be expensive, produce noisy output (as compared with RGB cameras),\nand fail to handle transparent objects. On the other hand, state-of-the-art\nmonocular depth estimation models (MDEMs) provide only affine-invariant depths\nup to an unknown scale and shift. Metric MDEMs achieve some successful\nzero-shot results on public datasets, but fail to generalize. We propose a\nnovel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover\nmetric depth from a single RGB image, through a one-shot adaptation building on\nMDEM techniques. MOMA performs scale-rotation-shift alignments during camera\ncalibration, guided by sparse ground-truth depth points, enabling accurate\ndepth estimation without additional data collection or model retraining on the\ntesting setup. MOMA supports fine-tuning the MDEM on transparent objects,\ndemonstrating strong generalization capabilities. Real-world experiments on\ntabletop 2-finger grasping and suction-based bin-picking applications show MOMA\nachieves high success rates in diverse tasks, confirming its effectiveness.", "AI": {"tldr": "论文提出了一种名为MOMA的单目RGB图像度量深度估计框架，通过一次适应性调整实现高精度深度估计，适用于透明物体，并在实际机器人操作任务中表现出色。", "motivation": "当前6D物体姿态估计依赖昂贵的深度传感器，且对透明物体效果不佳；单目深度估计模型（MDEMs）虽能提供深度信息，但无法直接度量。因此，需要一种低成本、高精度的解决方案。", "method": "提出MOMA框架，通过一次适应性调整（基于稀疏真实深度点）实现尺度-旋转-平移对齐，无需额外数据收集或模型重训练。", "result": "MOMA在透明物体上表现良好，并在实际机器人操作任务（如夹取和吸盘式分拣）中取得高成功率。", "conclusion": "MOMA是一种高效、通用的单目度量深度估计方法，适用于复杂场景和透明物体，具有实际应用潜力。"}}
{"id": "2506.17198", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17198", "abs": "https://arxiv.org/abs/2506.17198", "authors": ["Jianglong Ye", "Keyi Wang", "Chengjing Yuan", "Ruihan Yang", "Yiquan Li", "Jiyue Zhu", "Yuzhe Qin", "Xueyan Zou", "Xiaolong Wang"], "title": "Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation", "comment": "Accepted to RSS 2025. Project page: https://jianglongye.com/dex1b", "summary": "Generating large-scale demonstrations for dexterous hand manipulation remains\nchallenging, and several approaches have been proposed in recent years to\naddress this. Among them, generative models have emerged as a promising\nparadigm, enabling the efficient creation of diverse and physically plausible\ndemonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and\nhigh-quality demonstration dataset produced with generative models. The dataset\ncontains one billion demonstrations for two fundamental tasks: grasping and\narticulation. To construct it, we propose a generative model that integrates\ngeometric constraints to improve feasibility and applies additional conditions\nto enhance diversity. We validate the model on both established and newly\nintroduced simulation benchmarks, where it significantly outperforms prior\nstate-of-the-art methods. Furthermore, we demonstrate its effectiveness and\nrobustness through real-world robot experiments. Our project page is at\nhttps://jianglongye.com/dex1b", "AI": {"tldr": "论文介绍了Dex1B数据集，通过生成模型创建大规模、多样化的灵巧手操作演示，并在仿真和实际机器人实验中验证其优越性。", "motivation": "灵巧手操作的大规模演示生成具有挑战性，生成模型为解决这一问题提供了新思路。", "method": "提出一种生成模型，结合几何约束提升可行性，并通过附加条件增强多样性，构建了包含10亿演示的Dex1B数据集。", "result": "在仿真和实际实验中，该方法显著优于现有技术，表现出高效性和鲁棒性。", "conclusion": "Dex1B数据集和生成模型为灵巧手操作提供了高质量演示，推动了相关研究的发展。"}}
