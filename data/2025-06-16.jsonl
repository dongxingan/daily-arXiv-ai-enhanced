{"id": "2506.11234", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11234", "abs": "https://arxiv.org/abs/2506.11234", "authors": ["Luke Rowe", "Rodrigue de Schaetzen", "Roger Girgis", "Christopher Pal", "Liam Paull"], "title": "Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving", "comment": null, "summary": "We present Poutine, a 3B-parameter vision-language model (VLM) tailored for\nend-to-end autonomous driving in long-tail driving scenarios. Poutine is\ntrained in two stages. To obtain strong base driving capabilities, we train\nPoutine-Base in a self-supervised vision-language-trajectory (VLT) next-token\nprediction fashion on 83 hours of CoVLA nominal driving and 11 hours of Waymo\nlong-tail driving. Accompanying language annotations are auto-generated with a\n72B-parameter VLM. Poutine is obtained by fine-tuning Poutine-Base with Group\nRelative Policy Optimization (GRPO) using less than 500 preference-labeled\nframes from the Waymo validation set. We show that both VLT pretraining and RL\nfine-tuning are critical to attain strong driving performance in the long-tail.\nPoutine-Base achieves a rater-feedback score (RFS) of 8.12 on the validation\nset, nearly matching Waymo's expert ground-truth RFS. The final Poutine model\nachieves an RFS of 7.99 on the official Waymo test set, placing 1st in the 2025\nWaymo Vision-Based End-to-End Driving Challenge by a significant margin. These\nresults highlight the promise of scalable VLT pre-training and lightweight RL\nfine-tuning to enable robust and generalizable autonomy."}
{"id": "2506.11261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11261", "abs": "https://arxiv.org/abs/2506.11261", "authors": ["Shizhe Chen", "Ricardo Garcia", "Paul Pacaud", "Cordelia Schmid"], "title": "Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation", "comment": null, "summary": "Robotic manipulation faces a significant challenge in generalizing across\nunseen objects, environments and tasks specified by diverse language\ninstructions. To improve generalization capabilities, recent research has\nincorporated large language models (LLMs) for planning and action execution.\nWhile promising, these methods often fall short in generating grounded plans in\nvisual environments. Although efforts have been made to perform visual\ninstructional tuning on LLMs for robotic manipulation, existing methods are\ntypically constrained by single-view image input and struggle with precise\nobject grounding. In this work, we introduce Gondola, a novel grounded\nvision-language planning model based on LLMs for generalizable robotic\nmanipulation. Gondola takes multi-view images and history plans to produce the\nnext action plan with interleaved texts and segmentation masks of target\nobjects and locations. To support the training of Gondola, we construct three\ntypes of datasets using the RLBench simulator, namely robot grounded planning,\nmulti-view referring expression and pseudo long-horizon task datasets. Gondola\noutperforms the state-of-the-art LLM-based method across all four\ngeneralization levels of the GemBench dataset, including novel placements,\nrigid objects, articulated objects and long-horizon tasks."}
{"id": "2506.11262", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11262", "abs": "https://arxiv.org/abs/2506.11262", "authors": ["Shijie Fang", "Hang Yu", "Qidi Fang", "Reuben M. Aronson", "Elaine S. Short"], "title": "Demonstration Sidetracks: Categorizing Systematic Non-Optimality in Human Demonstrations", "comment": null, "summary": "Learning from Demonstration (LfD) is a popular approach for robots to acquire\nnew skills, but most LfD methods suffer from imperfections in human\ndemonstrations. Prior work typically treats these suboptimalities as random\nnoise. In this paper we study non-optimal behaviors in non-expert\ndemonstrations and show that they are systematic, forming what we call\ndemonstration sidetracks. Using a public space study with 40 participants\nperforming a long-horizon robot task, we recreated the setup in simulation and\nannotated all demonstrations. We identify four types of sidetracks\n(Exploration, Mistake, Alignment, Pause) and one control pattern (one-dimension\ncontrol). Sidetracks appear frequently across participants, and their temporal\nand spatial distribution is tied to task context. We also find that users'\ncontrol patterns depend on the control interface. These insights point to the\nneed for better models of suboptimal demonstrations to improve LfD algorithms\nand bridge the gap between lab training and real-world deployment. All\ndemonstrations, infrastructure, and annotations are available at\nhttps://github.com/AABL-Lab/Human-Demonstration-Sidetracks."}
{"id": "2506.11263", "categories": ["cs.RO", "cs.IT", "cs.NA", "cs.SY", "eess.SY", "math.IT", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.11263", "abs": "https://arxiv.org/abs/2506.11263", "authors": ["Christian Brommer", "Alessandro Fornasier", "Jan Steinbrener", "Stephan Weiss"], "title": "Sensor Model Identification via Simultaneous Model Selection and State Variable Determination", "comment": null, "summary": "We present a method for the unattended gray-box identification of sensor\nmodels commonly used by localization algorithms in the field of robotics. The\nobjective is to determine the most likely sensor model for a time series of\nunknown measurement data, given an extendable catalog of predefined sensor\nmodels. Sensor model definitions may require states for rigid-body calibrations\nand dedicated reference frames to replicate a measurement based on the robot's\nlocalization state. A health metric is introduced, which verifies the outcome\nof the selection process in order to detect false positives and facilitate\nreliable decision-making. In a second stage, an initial guess for identified\ncalibration states is generated, and the necessity of sensor world reference\nframes is evaluated. The identified sensor model with its parameter information\nis then used to parameterize and initialize a state estimation application,\nthus ensuring a more accurate and robust integration of new sensor elements.\nThis method is helpful for inexperienced users who want to identify the source\nand type of a measurement, sensor calibrations, or sensor reference frames. It\nwill also be important in the field of modular multi-agent scenarios and\nmodularized robotic platforms that are augmented by sensor modalities during\nruntime. Overall, this work aims to provide a simplified integration of sensor\nmodalities to downstream applications and circumvent common pitfalls in the\nusage and development of localization approaches."}
{"id": "2506.11263", "categories": ["cs.RO", "cs.IT", "cs.NA", "cs.SY", "eess.SY", "math.IT", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.11263", "abs": "https://arxiv.org/abs/2506.11263", "authors": ["Christian Brommer", "Alessandro Fornasier", "Jan Steinbrener", "Stephan Weiss"], "title": "Sensor Model Identification via Simultaneous Model Selection and State Variable Determination", "comment": null, "summary": "We present a method for the unattended gray-box identification of sensor\nmodels commonly used by localization algorithms in the field of robotics. The\nobjective is to determine the most likely sensor model for a time series of\nunknown measurement data, given an extendable catalog of predefined sensor\nmodels. Sensor model definitions may require states for rigid-body calibrations\nand dedicated reference frames to replicate a measurement based on the robot's\nlocalization state. A health metric is introduced, which verifies the outcome\nof the selection process in order to detect false positives and facilitate\nreliable decision-making. In a second stage, an initial guess for identified\ncalibration states is generated, and the necessity of sensor world reference\nframes is evaluated. The identified sensor model with its parameter information\nis then used to parameterize and initialize a state estimation application,\nthus ensuring a more accurate and robust integration of new sensor elements.\nThis method is helpful for inexperienced users who want to identify the source\nand type of a measurement, sensor calibrations, or sensor reference frames. It\nwill also be important in the field of modular multi-agent scenarios and\nmodularized robotic platforms that are augmented by sensor modalities during\nruntime. Overall, this work aims to provide a simplified integration of sensor\nmodalities to downstream applications and circumvent common pitfalls in the\nusage and development of localization approaches."}
{"id": "2506.11264", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.11264", "abs": "https://arxiv.org/abs/2506.11264", "authors": ["Jiachen Li", "Chu Jian", "Feiyang Zhao", "Shihao Li", "Wei Li", "Dongmei Chen"], "title": "Robust Optimal Task Planning to Maximize Battery Life", "comment": null, "summary": "This paper proposes a control-oriented optimization platform for autonomous\nmobile robots (AMRs), focusing on extending battery life while ensuring task\ncompletion. The requirement of fast AMR task planning while maintaining minimum\nbattery state of charge, thus maximizing the battery life, renders a bilinear\noptimization problem. McCormick envelop technique is proposed to linearize the\nbilinear term. A novel planning algorithm with relaxed constraints is also\ndeveloped to handle parameter uncertainties robustly with high efficiency\nensured. Simulation results are provided to demonstrate the utility of the\nproposed methods in reducing battery degradation while satisfying task\ncompletion requirements."}
{"id": "2506.11264", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.11264", "abs": "https://arxiv.org/abs/2506.11264", "authors": ["Jiachen Li", "Chu Jian", "Feiyang Zhao", "Shihao Li", "Wei Li", "Dongmei Chen"], "title": "Robust Optimal Task Planning to Maximize Battery Life", "comment": null, "summary": "This paper proposes a control-oriented optimization platform for autonomous\nmobile robots (AMRs), focusing on extending battery life while ensuring task\ncompletion. The requirement of fast AMR task planning while maintaining minimum\nbattery state of charge, thus maximizing the battery life, renders a bilinear\noptimization problem. McCormick envelop technique is proposed to linearize the\nbilinear term. A novel planning algorithm with relaxed constraints is also\ndeveloped to handle parameter uncertainties robustly with high efficiency\nensured. Simulation results are provided to demonstrate the utility of the\nproposed methods in reducing battery degradation while satisfying task\ncompletion requirements."}
{"id": "2506.11335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11335", "abs": "https://arxiv.org/abs/2506.11335", "authors": ["Levi Cai", "Youenn Jézéquel", "T. Aran Mooney", "Yogesh Girdhar"], "title": "Measuring and Minimizing Disturbance of Marine Animals to Underwater Vehicles", "comment": "Accepted to ISER 2025", "summary": "Do fish respond to the presence of underwater vehicles, potentially biasing\nour estimates about them? If so, are there strategies to measure and mitigate\nthis response? This work provides a theoretical and practical framework towards\nbias-free estimation of animal behavior from underwater vehicle observations.\nWe also provide preliminary results from the field in coral reef environments\nto address these questions."}
{"id": "2506.11387", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "93C85 (Primary), 93B52 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.11387", "abs": "https://arxiv.org/abs/2506.11387", "authors": ["Rongfei Li"], "title": "Control Architecture and Design for a Multi-robotic Visual Servoing System in Automated Manufacturing Environment", "comment": "272 pages, 171 figures, PhD dissertation, University of California,\n  Davis, 2025. To be published in ProQuest ETD", "summary": "The use of robotic technology has drastically increased in manufacturing in\nthe 21st century. But by utilizing their sensory cues, humans still outperform\nmachines, especially in micro scale manufacturing, which requires\nhigh-precision robot manipulators. These sensory cues naturally compensate for\nhigh levels of uncertainties that exist in the manufacturing environment.\nUncertainties in performing manufacturing tasks may come from measurement\nnoise, model inaccuracy, joint compliance (e.g., elasticity), etc. Although\nadvanced metrology sensors and high precision microprocessors, which are\nutilized in modern robots, have compensated for many structural and dynamic\nerrors in robot positioning, a well-designed control algorithm still works as a\ncomparable and cheaper alternative to reduce uncertainties in automated\nmanufacturing. Our work illustrates that a multi-robot control system that\nsimulates the positioning process for fastening and unfastening applications\ncan reduce various uncertainties, which may occur in this process, to a great\nextent. In addition, most research papers in visual servoing mainly focus on\ndeveloping control and observation architectures in various scenarios, but few\nhave discussed the importance of the camera's location in the configuration. In\na manufacturing environment, the quality of camera estimations may vary\nsignificantly from one observation location to another, as the combined effects\nof environmental conditions result in different noise levels of a single image\nshot at different locations. Therefore, in this paper, we also propose a novel\nalgorithm for the camera's moving policy so that it explores the camera\nworkspace and searches for the optimal location where the image noise level is\nminimized."}
{"id": "2506.11384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11384", "abs": "https://arxiv.org/abs/2506.11384", "authors": ["Hikaru Sasaki", "Naoto Komeno", "Takumi Hachimine", "Kei Takahashi", "Yu-ya Ohnishi", "Tetsunori Sugawara", "Araki Wakiuchi", "Miho Hatanaka", "Tomoyuki Miyao", "Hiroharu Ajiro", "Mikiya Fujii", "Takamitsu Matsubara"], "title": "Robotic System for Chemical Experiment Automation with Dual Demonstration of End-effector and Jig Operations", "comment": null, "summary": "While robotic automation has demonstrated remarkable performance, such as\nexecuting hundreds of experiments continuously over several days, it is\nchallenging to design a program that synchronizes the robot's movements with\nthe experimental jigs to conduct an experiment. We propose a concept that\nenables the automation of experiments by utilizing dual demonstrations of robot\nmotions and jig operations by chemists in an experimental environment\nconstructed to be controlled by a robot. To verify this concept, we developed a\nchemical-experiment-automation system consisting of jigs to assist the robot in\nexperiments, a motion-demonstration interface, a jig-control interface, and a\nmobile manipulator. We validate the concept through polymer-synthesis\nexperiments, focusing on critical liquid-handling tasks such as pipetting and\ndilution. The experimental results indicate high reproducibility of the\ndemonstrated motions and robust task-success rates. This comprehensive concept\nnot only simplifies the robot programming process for chemists but also\nprovides a flexible and efficient solution to accommodate a wide range of\nexperimental conditions, contributing significantly to the field of chemical\nexperiment automation."}
{"id": "2506.11387", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "93C85 (Primary), 93B52 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.11387", "abs": "https://arxiv.org/abs/2506.11387", "authors": ["Rongfei Li"], "title": "Control Architecture and Design for a Multi-robotic Visual Servoing System in Automated Manufacturing Environment", "comment": "272 pages, 171 figures, PhD dissertation, University of California,\n  Davis, 2025. To be published in ProQuest ETD", "summary": "The use of robotic technology has drastically increased in manufacturing in\nthe 21st century. But by utilizing their sensory cues, humans still outperform\nmachines, especially in micro scale manufacturing, which requires\nhigh-precision robot manipulators. These sensory cues naturally compensate for\nhigh levels of uncertainties that exist in the manufacturing environment.\nUncertainties in performing manufacturing tasks may come from measurement\nnoise, model inaccuracy, joint compliance (e.g., elasticity), etc. Although\nadvanced metrology sensors and high precision microprocessors, which are\nutilized in modern robots, have compensated for many structural and dynamic\nerrors in robot positioning, a well-designed control algorithm still works as a\ncomparable and cheaper alternative to reduce uncertainties in automated\nmanufacturing. Our work illustrates that a multi-robot control system that\nsimulates the positioning process for fastening and unfastening applications\ncan reduce various uncertainties, which may occur in this process, to a great\nextent. In addition, most research papers in visual servoing mainly focus on\ndeveloping control and observation architectures in various scenarios, but few\nhave discussed the importance of the camera's location in the configuration. In\na manufacturing environment, the quality of camera estimations may vary\nsignificantly from one observation location to another, as the combined effects\nof environmental conditions result in different noise levels of a single image\nshot at different locations. Therefore, in this paper, we also propose a novel\nalgorithm for the camera's moving policy so that it explores the camera\nworkspace and searches for the optimal location where the image noise level is\nminimized."}
{"id": "2506.11470", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11470", "abs": "https://arxiv.org/abs/2506.11470", "authors": ["Shunpeng Yang", "Zhen Fu", "Zhefeng Cao", "Guo Junde", "Patrick Wensing", "Wei Zhang", "Hua Chen"], "title": "Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion", "comment": "19 pages", "summary": "Generalizing locomotion policies across diverse legged robots with varying\nmorphologies is a key challenge due to differences in observation/action\ndimensions and system dynamics. In this work, we propose Multi-Loco, a novel\nunified framework combining a morphology-agnostic generative diffusion model\nwith a lightweight residual policy optimized via reinforcement learning (RL).\nThe diffusion model captures morphology-invariant locomotion patterns from\ndiverse cross-embodiment datasets, improving generalization and robustness. The\nresidual policy is shared across all embodiments and refines the actions\ngenerated by the diffusion model, enhancing task-aware performance and\nrobustness for real-world deployment. We evaluated our method with a rich\nlibrary of four legged robots in both simulation and real-world experiments.\nCompared to a standard RL framework with PPO, our approach -- replacing the\nGaussian policy with a diffusion model and residual term -- achieves a 10.35%\naverage return improvement, with gains up to 13.57% in wheeled-biped locomotion\ntasks. These results highlight the benefits of cross-embodiment data and\ncomposite generative architectures in learning robust, generalized locomotion\nskills."}
{"id": "2506.11526", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11526", "abs": "https://arxiv.org/abs/2506.11526", "authors": ["Yuan Gao", "Mattia Piccinini", "Yuchen Zhang", "Dingrui Wang", "Korbinian Moller", "Roberto Brusnicki", "Baha Zarrouki", "Alessio Gambi", "Jan Frederik Totz", "Kai Storms", "Steven Peters", "Andrea Stocco", "Bassam Alrifaee", "Marco Pavone", "Johannes Betz"], "title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis", "comment": null, "summary": "For autonomous vehicles, safe navigation in complex environments depends on\nhandling a broad range of diverse and rare driving scenarios. Simulation- and\nscenario-based testing have emerged as key approaches to development and\nvalidation of autonomous driving systems. Traditional scenario generation\nrelies on rule-based systems, knowledge-driven models, and data-driven\nsynthesis, often producing limited diversity and unrealistic safety-critical\ncases. With the emergence of foundation models, which represent a new\ngeneration of pre-trained, general-purpose AI models, developers can process\nheterogeneous inputs (e.g., natural language, sensor data, HD maps, and control\nactions), enabling the synthesis and interpretation of complex driving\nscenarios. In this paper, we conduct a survey about the application of\nfoundation models for scenario generation and scenario analysis in autonomous\ndriving (as of May 2025). Our survey presents a unified taxonomy that includes\nlarge language models, vision-language models, multimodal large language\nmodels, diffusion models, and world models for the generation and analysis of\nautonomous driving scenarios. In addition, we review the methodologies,\nopen-source datasets, simulation platforms, and benchmark challenges, and we\nexamine the evaluation metrics tailored explicitly to scenario generation and\nanalysis. Finally, the survey concludes by highlighting the open challenges and\nresearch questions, and outlining promising future research directions. All\nreviewed papers are listed in a continuously maintained repository, which\ncontains supplementary materials and is available at\nhttps://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis."}
{"id": "2506.11570", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11570", "abs": "https://arxiv.org/abs/2506.11570", "authors": ["Jihao Li", "Keqi Zhu", "Guodong Lu", "I-Ming Chen", "Huixu Dong"], "title": "Construction of a Multiple-DOF Under-actuated Gripper with Force-Sensing via Deep Learning", "comment": null, "summary": "We present a novel under-actuated gripper with two 3-joint fingers, which\nrealizes force feedback control by the deep learning technique- Long Short-Term\nMemory (LSTM) model, without any force sensor. First, a five-linkage mechanism\nstacked by double four-linkages is designed as a finger to automatically\nachieve the transformation between parallel and enveloping grasping modes. This\nenables the creation of a low-cost under-actuated gripper comprising a single\nactuator and two 3-phalange fingers. Second, we devise theoretical models of\nkinematics and power transmission based on the proposed gripper, accurately\nobtaining fingertip positions and contact forces. Through coupling and\ndecoupling of five-linkage mechanisms, the proposed gripper offers the expected\ncapabilities of grasping payload/force/stability and objects with large\ndimension ranges. Third, to realize the force control, an LSTM model is\nproposed to determine the grasping mode for synthesizing force-feedback control\npolicies that exploit contact sensing after outlining the uncertainty of\ncurrents using a statistical method. Finally, a series of experiments are\nimplemented to measure quantitative indicators, such as the payload, grasping\nforce, force sensing, grasping stability and the dimension ranges of objects to\nbe grasped. Additionally, the grasping performance of the proposed gripper is\nverified experimentally to guarantee the high versatility and robustness of the\nproposed gripper."}
{"id": "2506.11650", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11650", "abs": "https://arxiv.org/abs/2506.11650", "authors": ["Lambert Lee", "Joshua Lau"], "title": "Robot Context Protocol (RCP): A Runtime-Agnostic Interface for Agent-Aware Robot Control", "comment": null, "summary": "The Robot Context Protocol (RCP) is a lightweight, middleware-agnostic\ncommunication protocol designed to simplify the complexity of robotic systems\nand enable seamless interaction between robots, users, and autonomous agents.\nRCP provides a unified and semantically meaningful interface that decouples\nclient-facing operations from backend implementations, supporting a wide range\nof deployment environments including physical robots, cloud-based\norchestrators, and simulated platforms. Built on HTTP and WebSocket transport\nlayers, the protocol defines a schema-driven message format with structured\noperations such as read, write, execute, and subscribe. It integrates features\nsuch as runtime introspection, asynchronous feedback, multi-tenant namespace\nisolation, and strict type validation to ensure robustness, scalability, and\nsecurity. The architecture, message structure, interface model, and\nadapter-based backend integration strategy of RCP are described, along with\ndeployment practices and applicability across industries including\nmanufacturing, logistics, and healthcare. RCP enables intelligent, resilient,\nand safe robotic operations in complex, multi-agent ecosystems."}
{"id": "2506.11723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11723", "abs": "https://arxiv.org/abs/2506.11723", "authors": ["Ziren Xiao", "Ruxin Xiao", "Chang Liu", "Xinheng Wang"], "title": "Dynamic Collaborative Material Distribution System for Intelligent Robots In Smart Manufacturing", "comment": null, "summary": "The collaboration and interaction of multiple robots have become integral\naspects of smart manufacturing. Effective planning and management play a\ncrucial role in achieving energy savings and minimising overall costs. This\npaper addresses the real-time Dynamic Multiple Sources to Single Destination\n(DMS-SD) navigation problem, particularly with a material distribution case for\nmultiple intelligent robots in smart manufacturing. Enumerated solutions, such\nas in \\cite{xiao2022efficient}, tackle the problem by generating as many\noptimal or near-optimal solutions as possible but do not learn patterns from\nthe previous experience, whereas the method in \\cite{xiao2023collaborative}\nonly uses limited information from the earlier trajectories. Consequently,\nthese methods may take a considerable amount of time to compute results on\nlarge maps, rendering real-time operations impractical. To overcome this\nchallenge, we propose a lightweight Deep Reinforcement Learning (DRL) method to\naddress the DMS-SD problem. The proposed DRL method can be efficiently trained\nand rapidly converges to the optimal solution using the designed target-guided\nreward function. A well-trained DRL model significantly reduces the computation\ntime for the next movement to a millisecond level, which improves the time up\nto 100 times in our experiments compared to the enumerated solutions. Moreover,\nthe trained DRL model can be easily deployed on lightweight devices in smart\nmanufacturing, such as Internet of Things devices and mobile phones, which only\nrequire limited computational resources."}
{"id": "2506.11748", "categories": ["cs.RO", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.11748", "abs": "https://arxiv.org/abs/2506.11748", "authors": ["Federico Zocco", "Monica Malvezzi"], "title": "CIRO7.2: A Material Network with Circularity of -7.2 and Reinforcement-Learning-Controlled Robotic Disassembler", "comment": "To be submitted", "summary": "The competition over natural reserves of minerals is expected to increase in\npart because of the linear-economy paradigm based on take-make-dispose.\nSimultaneously, the linear economy considers end-of-use products as waste\nrather than as a resource, which results in large volumes of waste whose\nmanagement remains an unsolved problem. Since a transition to a circular\neconomy can mitigate these open issues, in this paper we begin by enhancing the\nnotion of circularity based on compartmental dynamical thermodynamics, namely,\n$\\lambda$, and then, we model a thermodynamical material network processing a\nbatch of 2 solid materials of criticality coefficients of 0.1 and 0.95, with a\nrobotic disassembler compartment controlled via reinforcement learning (RL),\nand processing 2-7 kg of materials. Subsequently, we focused on the design of\nthe robotic disassembler compartment using state-of-the-art RL algorithms and\nassessing the algorithm performance with respect to $\\lambda$ (Fig. 1). The\nhighest circularity is -2.1 achieved in the case of disassembling 2 parts of 1\nkg each, whereas it reduces to -7.2 in the case of disassembling 4 parts of 1\nkg each contained inside a chassis of 3 kg. Finally, a sensitivity analysis\nhighlighted that the impact on $\\lambda$ of the performance of an RL controller\nhas a positive correlation with the quantity and the criticality of the\nmaterials to be disassembled. This work also gives the principles of the\nemerging research fields indicated as circular intelligence and robotics\n(CIRO). Source code is publicly available."}
{"id": "2506.11775", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11775", "abs": "https://arxiv.org/abs/2506.11775", "authors": ["Zilin Si", "Jose Enrique Chen", "M. Emre Karagozler", "Antonia Bronars", "Jonathan Hutchinson", "Thomas Lampe", "Nimrod Gileadi", "Taylor Howell", "Stefano Saliceti", "Lukasz Barczyk", "Ilan Olivarez Correa", "Tom Erez", "Mohit Shridhar", "Murilo Fernandes Martins", "Konstantinos Bousmalis", "Nicolas Heess", "Francesco Nori", "Maria Bauza Villalonga"], "title": "ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations", "comment": null, "summary": "Recent advancements in teleoperation systems have enabled high-quality data\ncollection for robotic manipulators, showing impressive results in learning\nmanipulation at scale. This progress suggests that extending these capabilities\nto robotic hands could unlock an even broader range of manipulation skills,\nespecially if we could achieve the same level of dexterity that human hands\nexhibit. However, teleoperating robotic hands is far from a solved problem, as\nit presents a significant challenge due to the high degrees of freedom of\nrobotic hands and the complex dynamics occurring during contact-rich settings.\nIn this work, we present ExoStart, a general and scalable learning framework\nthat leverages human dexterity to improve robotic hand control. In particular,\nwe obtain high-quality data by collecting direct demonstrations without a robot\nin the loop using a sensorized low-cost wearable exoskeleton, capturing the\nrich behaviors that humans can demonstrate with their own hands. We also\npropose a simulation-based dynamics filter that generates dynamically feasible\ntrajectories from the collected demonstrations and use the generated\ntrajectories to bootstrap an auto-curriculum reinforcement learning method that\nrelies only on simple sparse rewards. The ExoStart pipeline is generalizable\nand yields robust policies that transfer zero-shot to the real robot. Our\nresults demonstrate that ExoStart can generate dexterous real-world hand\nskills, achieving a success rate above 50% on a wide range of complex tasks\nsuch as opening an AirPods case or inserting and turning a key in a lock. More\ndetails and videos can be found in https://sites.google.com/view/exostart."}
{"id": "2506.11827", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.11827", "abs": "https://arxiv.org/abs/2506.11827", "authors": ["Saitarun Nadipineni", "Chapa Sirithunge", "Yue Xie", "Fumiya Iida", "Thilina Dulantha Lalitharatne"], "title": "Auditory-Tactile Congruence for Synthesis of Adaptive Pain Expressions in RoboPatients", "comment": "17 pages, 9 figures, journal", "summary": "Misdiagnosis can lead to delayed treatments and harm. Robotic patients offer\na controlled way to train and evaluate clinicians in rare, subtle, or complex\ncases, reducing diagnostic errors. We present RoboPatient, a medical robotic\nsimulator aimed at multimodal pain synthesis based on haptic and auditory\nfeedback during palpation-based training scenarios. The robopatient functions\nas an adaptive intermediary, capable of synthesizing plausible pain expressions\nvocal and facial in response to tactile stimuli generated during palpation.\nUsing an abdominal phantom, robopatient captures and processes haptic input via\nan internal palpation-to-pain mapping model. To evaluate perceptual congruence\nbetween palpation and the corresponding auditory output, we conducted a study\ninvolving 7680 trials across 20 participants, where they evaluated pain\nintensity through sound. Results show that amplitude and pitch significantly\ninfluence agreement with the robot's pain expressions, irrespective of pain\nsounds. Stronger palpation forces elicited stronger agreement, aligning with\npsychophysical patterns. The study revealed two key dimensions: pitch and\namplitude are central to how people perceive pain sounds, with pitch being the\nmost influential cue. These acoustic features shape how well the sound matches\nthe applied force during palpation, impacting perceived realism. This approach\nlays the groundwork for high-fidelity robotic patients in clinical education\nand diagnostic simulation."}
{"id": "2506.11829", "categories": ["cs.RO", "cs.HC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2506.11829", "abs": "https://arxiv.org/abs/2506.11829", "authors": ["Ana Müller", "Anja Richert"], "title": "The Space Between Us: A Methodological Framework for Researching Bonding and Proxemics in Situated Group-Agent Interactions", "comment": "Accepted for presentation at the Workshop on Advancing Group\n  Understanding and Robots' Adaptive Behavior (GROUND), held at the Intelligent\n  Autonomous Systems (IAS) Conference 2025, Genoa, Italy", "summary": "This paper introduces a multimethod framework for studying spatial and social\ndynamics in real-world group-agent interactions with socially interactive\nagents. Drawing on proxemics and bonding theories, the method combines\nsubjective self-reports and objective spatial tracking. Applied in two field\nstudies in a museum (N = 187) with a robot and a virtual agent, the paper\naddresses the challenges in aligning human perception and behavior. We focus on\npresenting an open source, scalable, and field-tested toolkit for future\nstudies."}
{"id": "2506.11842", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11842", "abs": "https://arxiv.org/abs/2506.11842", "authors": ["Zhipeng Bao", "Qianwen Li"], "title": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated Driving Systems", "comment": "10 figures,29 pages, one colummn", "summary": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving."}
{"id": "2506.11906", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11906", "abs": "https://arxiv.org/abs/2506.11906", "authors": ["Chapa Sirithunge", "Yue Xie", "Saitarun Nadipineni", "Fumiya Iida", "Thilina Dulantha Lalitharatne"], "title": "Palpation Alters Auditory Pain Expressions with Gender-Specific Variations in Robopatients", "comment": "11 pages, 9 figures, journal", "summary": "Diagnostic errors remain a major cause of preventable deaths, particularly in\nresource-limited regions. Medical training simulators, including robopatients,\nplay a vital role in reducing these errors by mimicking real patients for\nprocedural training such as palpation. However, generating multimodal feedback,\nespecially auditory pain expressions, remains challenging due to the complex\nrelationship between palpation behavior and sound. The high-dimensional nature\nof pain sounds makes exploration challenging with conventional methods. This\nstudy introduces a novel experimental paradigm for pain expressivity in\nrobopatients where they dynamically generate auditory pain expressions in\nresponse to palpation force, by co-optimizing human feedback using machine\nlearning. Using Proximal Policy Optimization (PPO), a reinforcement learning\n(RL) technique optimized for continuous adaptation, our robot iteratively\nrefines pain sounds based on real-time human feedback. This robot initializes\nrandomized pain responses to palpation forces, and the RL agent learns to\nadjust these sounds to align with human preferences. The results demonstrated\nthat the system adapts to an individual's palpation forces and sound\npreferences and captures a broad spectrum of pain intensity, from mild\ndiscomfort to acute distress, through RL-guided exploration of the auditory\npain space. The study further showed that pain sound perception exhibits\nsaturation at lower forces with gender specific thresholds. These findings\nhighlight the system's potential to enhance abdominal palpation training by\noffering a controllable and immersive simulation platform."}
{"id": "2506.11916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11916", "abs": "https://arxiv.org/abs/2506.11916", "authors": ["Elvis Nava", "Victoriano Montesinos", "Erik Bauer", "Benedek Forrai", "Jonas Pai", "Stefan Weirich", "Stephan-Daniel Gravert", "Philipp Wand", "Stephan Polinski", "Benjamin F. Grewe", "Robert K. Katzschmann"], "title": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity", "comment": null, "summary": "We present a diffusion-based model recipe for real-world control of a highly\ndexterous humanoid robotic hand, designed for sample-efficient learning and\nsmooth fine-motor action inference. Our system features a newly designed 16-DoF\ntendon-driven hand, equipped with wide angle wrist cameras and mounted on a\nFranka Emika Panda arm. We develop a versatile teleoperation pipeline and data\ncollection protocol using both glove-based and VR interfaces, enabling\nhigh-quality data collection across diverse tasks such as pick and place, item\nsorting and assembly insertion. Leveraging high-frequency generative control,\nwe train end-to-end policies from raw sensory inputs, enabling smooth,\nself-correcting motions in complex manipulation scenarios. Real-world\nevaluations demonstrate up to 93.3% out of distribution success rates, with up\nto a +33.3% performance boost due to emergent self-correcting behaviors, while\nalso revealing scaling trends in policy performance. Our results advance the\nstate-of-the-art in dexterous robotic manipulation through a fully integrated,\npractical approach to hardware, learning, and real-world deployment."}
{"id": "2506.11948", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11948", "abs": "https://arxiv.org/abs/2506.11948", "authors": ["Nadun Ranawaka Arachchige", "Zhenyang Chen", "Wonsuhk Jung", "Woo Chul Shin", "Rohan Bansal", "Pierre Barroso", "Yu Hang He", "Yingyang Celine Lin", "Benjamin Joffe", "Shreyas Kousik", "Danfei Xu"], "title": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies", "comment": "The first two authors contributed equally", "summary": "Offline Imitation Learning (IL) methods such as Behavior Cloning are\neffective at acquiring complex robotic manipulation skills. However, existing\nIL-trained policies are confined to executing the task at the same speed as\nshown in demonstration data. This limits the task throughput of a robotic\nsystem, a critical requirement for applications such as industrial automation.\nIn this paper, we introduce and formalize the novel problem of enabling\nfaster-than-demonstration execution of visuomotor policies and identify\nfundamental challenges in robot dynamics and state-action distribution shifts.\nWe instantiate the key insights as SAIL (Speed Adaptation for Imitation\nLearning), a full-stack system integrating four tightly-connected components:\n(1) a consistency-preserving action inference algorithm for smooth motion at\nhigh speed, (2) high-fidelity tracking of controller-invariant motion targets,\n(3) adaptive speed modulation that dynamically adjusts execution speed based on\nmotion complexity, and (4) action scheduling to handle real-world system\nlatencies. Experiments on 12 tasks across simulation and two real, distinct\nrobot platforms show that SAIL achieves up to a 4x speedup over demonstration\nspeed in simulation and up to 3.2x speedup in the real world. Additional detail\nis available at https://nadunranawaka1.github.io/sail-policy"}
