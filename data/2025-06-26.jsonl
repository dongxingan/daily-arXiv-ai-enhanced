{"id": "2506.19968", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19968", "abs": "https://arxiv.org/abs/2506.19968", "authors": ["Sahand Farghdani", "Robin Chhabra"], "title": "Evolutionary Gait Reconfiguration in Damaged Legged Robots", "comment": null, "summary": "Multi-legged robots deployed in complex missions are susceptible to physical\ndamage in their legs, impairing task performance and potentially compromising\nmission success. This letter presents a rapid, training-free damage recovery\nalgorithm for legged robots subject to partial or complete loss of functional\nlegs. The proposed method first stabilizes locomotion by generating a new gait\nsequence and subsequently optimally reconfigures leg gaits via a developed\ndifferential evolution algorithm to maximize forward progression while\nminimizing body rotation and lateral drift. The algorithm successfully restores\nlocomotion in a 24-degree-of-freedom hexapod within one hour, demonstrating\nboth high efficiency and robustness to structural damage."}
{"id": "2506.19984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19984", "abs": "https://arxiv.org/abs/2506.19984", "authors": ["Sahand Farghdani", "Mili Patel", "Robin Chhabra"], "title": "Robust Embodied Self-Identification of Morphology in Damaged Multi-Legged Robots", "comment": null, "summary": "Multi-legged robots (MLRs) are vulnerable to leg damage during complex\nmissions, which can impair their performance. This paper presents a\nself-modeling and damage identification algorithm that enables autonomous\nadaptation to partial or complete leg loss using only data from a low-cost IMU.\nA novel FFT-based filter is introduced to address time-inconsistent signals,\nimproving damage detection by comparing body orientation between the robot and\nits model. The proposed method identifies damaged legs and updates the robot's\nmodel for integration into its control system. Experiments on uneven terrain\nvalidate its robustness and computational efficiency."}
{"id": "2506.20036", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20036", "abs": "https://arxiv.org/abs/2506.20036", "authors": ["Jeremiah Coholich", "Muhammad Ali Murtaza", "Seth Hutchinson", "Zsolt Kira"], "title": "Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion", "comment": null, "summary": "We propose a novel hierarchical reinforcement learning framework for\nquadruped locomotion over challenging terrain. Our approach incorporates a\ntwo-layer hierarchy in which a high-level policy (HLP) selects optimal goals\nfor a low-level policy (LLP). The LLP is trained using an on-policy\nactor-critic RL algorithm and is given footstep placements as goals. We propose\nan HLP that does not require any additional training or environment samples and\ninstead operates via an online optimization process over the learned value\nfunction of the LLP. We demonstrate the benefits of this framework by comparing\nit with an end-to-end reinforcement learning (RL) approach. We observe\nimprovements in its ability to achieve higher rewards with fewer collisions\nacross an array of different terrains, including terrains more difficult than\nany encountered during training."}
{"id": "2506.20045", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20045", "abs": "https://arxiv.org/abs/2506.20045", "authors": ["Eric C. Joyce", "Qianwen Zhao", "Nathaniel Burgdorfer", "Long Wang", "Philippos Mordohai"], "title": "Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception", "comment": null, "summary": "Deep object pose estimators are notoriously overconfident. A grasping agent\nthat both estimates the 6-DoF pose of a target object and predicts the\nuncertainty of its own estimate could avoid task failure by choosing not to act\nunder high uncertainty. Even though object pose estimation improves and\nuncertainty quantification research continues to make strides, few studies have\nconnected them to the downstream task of robotic grasping. We propose a method\nfor training lightweight, deep networks to predict whether a grasp guided by an\nimage-based pose estimate will succeed before that grasp is attempted. We\ngenerate training data for our networks via object pose estimation on real\nimages and simulated grasping. We also find that, despite high object\nvariability in grasping trials, networks benefit from training on all objects\njointly, suggesting that a diverse variety of objects can nevertheless\ncontribute to the same goal."}
{"id": "2506.20311", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.20311", "abs": "https://arxiv.org/abs/2506.20311", "authors": ["Jingwen Wei"], "title": "Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles", "comment": null, "summary": "The growing use of mobile robots in sectors such as automotive, agriculture,\nand rescue operations reflects progress in robotics and autonomy. In unmanned\naerial vehicles (UAVs), most research emphasizes visual SLAM, sensor fusion,\nand path planning. However, applying UAVs to search and rescue missions in\ndisaster zones remains underexplored, especially for autonomous navigation.\n  This report develops methods for real-time and secure UAV maneuvering in\ncomplex 3D environments, crucial during forest fires. Building upon past\nresearch, it focuses on designing navigation algorithms for unfamiliar and\nhazardous environments, aiming to improve rescue efficiency and safety through\nUAV-based early warning and rapid response.\n  The work unfolds in phases. First, a 2D fusion navigation strategy is\nexplored, initially for mobile robots, enabling safe movement in dynamic\nsettings. This sets the stage for advanced features such as adaptive obstacle\nhandling and decision-making enhancements. Next, a novel 3D reactive navigation\nstrategy is introduced for collision-free movement in forest fire simulations,\naddressing the unique challenges of UAV operations in such scenarios.\n  Finally, the report proposes a unified control approach that integrates UAVs\nand unmanned ground vehicles (UGVs) for coordinated rescue missions in forest\nenvironments. Each phase presents challenges, proposes control models, and\nvalidates them with mathematical and simulation-based evidence. The study\noffers practical value and academic insights for improving the role of UAVs in\nnatural disaster rescue operations."}
{"id": "2506.20049", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20049", "abs": "https://arxiv.org/abs/2506.20049", "authors": ["Lorin Achey", "Alec Reed", "Brendan Crowe", "Bradley Hayes", "Christoffer Heckman"], "title": "Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis", "comment": "arXiv admin note: text overlap with arXiv:2409.10681", "summary": "We present a novel approach for enhancing robotic exploration by using\ngenerative occupancy mapping. We introduce SceneSense, a diffusion model\ndesigned and trained for predicting 3D occupancy maps given partial\nobservations. Our proposed approach probabilistically fuses these predictions\ninto a running occupancy map in real-time, resulting in significant\nimprovements in map quality and traversability. We implement SceneSense onboard\na quadruped robot and validate its performance with real-world experiments to\ndemonstrate the effectiveness of the model. In these experiments, we show that\noccupancy maps enhanced with SceneSense predictions better represent our fully\nobserved ground truth data (24.44% FID improvement around the robot and 75.59%\nimprovement at range). We additionally show that integrating\nSceneSense-enhanced maps into our robotic exploration stack as a \"drop-in\" map\nimprovement, utilizing an existing off-the-shelf planner, results in\nimprovements in robustness and traversability time. Finally we show results of\nfull exploration evaluations with our proposed system in two dissimilar\nenvironments and find that locally enhanced maps provide more consistent\nexploration results than maps constructed only from direct sensor measurements."}
{"id": "2506.20097", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20097", "abs": "https://arxiv.org/abs/2506.20097", "authors": ["Wang Bill Zhu", "Miaosen Chai", "Ishika Singh", "Robin Jia", "Jesse Thomason"], "title": "PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models", "comment": null, "summary": "We propose PSALM-V, the first autonomous neuro-symbolic learning system able\nto induce symbolic action semantics (i.e., pre- and post-conditions) in visual\nenvironments through interaction. PSALM-V bootstraps reliable symbolic planning\nwithout expert action definitions, using LLMs to generate heuristic plans and\ncandidate symbolic semantics. Previous work has explored using large language\nmodels to generate action semantics for Planning Domain Definition Language\n(PDDL)-based symbolic planners. However, these approaches have primarily\nfocused on text-based domains or relied on unrealistic assumptions, such as\naccess to a predefined problem file, full observability, or explicit error\nmessages. By contrast, PSALM-V dynamically infers PDDL problem files and domain\naction semantics by analyzing execution outcomes and synthesizing possible\nerror explanations. The system iteratively generates and executes plans while\nmaintaining a tree-structured belief over possible action semantics for each\naction, iteratively refining these beliefs until a goal state is reached.\nSimulated experiments of task completion in ALFRED demonstrate that PSALM-V\nincreases the plan success rate from 37% (Claude-3.7) to 74% in partially\nobserved setups. Results on two 2D game environments, RTFM and Overcooked-AI,\nshow that PSALM-V improves step efficiency and succeeds in domain induction in\nmulti-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions\nfor real-world robot BlocksWorld tasks, despite low-level manipulation failures\nfrom the robot."}
{"id": "2506.20212", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20212", "abs": "https://arxiv.org/abs/2506.20212", "authors": ["Andrea Bussolan", "Oliver Avram", "Andrea Pignata", "Gianvito Urgese", "Stefano Baraldo", "Anna Valente"], "title": "Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning", "comment": null, "summary": "With the advent of Industry 5.0, manufacturers are increasingly prioritizing\nworker well-being alongside mass customization. Stress-aware Human-Robot\nCollaboration (HRC) plays a crucial role in this paradigm, where robots must\nadapt their behavior to human mental states to improve collaboration fluency\nand safety. This paper presents a novel framework that integrates Federated\nLearning (FL) to enable personalized mental state evaluation while preserving\nuser privacy. By leveraging physiological signals, including EEG, ECG, EDA,\nEMG, and respiration, a multimodal model predicts an operator's stress level,\nfacilitating real-time robot adaptation. The FL-based approach allows\ndistributed on-device training, ensuring data confidentiality while improving\nmodel generalization and individual customization. Results demonstrate that the\ndeployment of an FL approach results in a global model with performance in\nstress prediction accuracy comparable to a centralized training approach.\nMoreover, FL allows for enhancing personalization, thereby optimizing\nhuman-robot interaction in industrial settings, while preserving data privacy.\nThe proposed framework advances privacy-preserving, adaptive robotics to\nenhance workforce well-being in smart manufacturing."}
{"id": "2506.20259", "categories": ["cs.RO", "cs.AI", "68T40, 93C85, 70E60", "I.2.9"], "pdf": "https://arxiv.org/pdf/2506.20259", "abs": "https://arxiv.org/abs/2506.20259", "authors": ["Andrej Lúčny", "Matilde Antonj", "Carlo Mazzola", "Hana Hornáčková", "Igor Farkaš"], "title": "Generating and Customizing Robotic Arm Trajectories using Neural Networks", "comment": "The code is released at\n  https://github.com/andylucny/nico2/tree/main/generate", "summary": "We introduce a neural network approach for generating and customizing the\ntrajectory of a robotic arm, that guarantees precision and repeatability. To\nhighlight the potential of this novel method, we describe the design and\nimplementation of the technique and show its application in an experimental\nsetting of cognitive robotics. In this scenario, the NICO robot was\ncharacterized by the ability to point to specific points in space with precise\nlinear movements, increasing the predictability of the robotic action during\nits interaction with humans. To achieve this goal, the neural network computes\nthe forward kinematics of the robot arm. By integrating it with a generator of\njoint angles, another neural network was developed and trained on an artificial\ndataset created from suitable start and end poses of the robotic arm. Through\nthe computation of angular velocities, the robot was characterized by its\nability to perform the movement, and the quality of its action was evaluated in\nterms of shape and accuracy. Thanks to its broad applicability, our approach\nsuccessfully generates precise trajectories that could be customized in their\nshape and adapted to different settings."}
{"id": "2506.20268", "categories": ["cs.RO", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20268", "abs": "https://arxiv.org/abs/2506.20268", "authors": ["Ruben Janssens", "Jens De Bock", "Sofie Labat", "Eva Verhelst", "Veronique Hoste", "Tony Belpaeme"], "title": "Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue", "comment": "Accepted at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN 2025)", "summary": "Detecting miscommunication in human-robot interaction is a critical function\nfor maintaining user engagement and trust. While humans effortlessly detect\ncommunication errors in conversations through both verbal and non-verbal cues,\nrobots face significant challenges in interpreting non-verbal feedback, despite\nadvances in computer vision for recognizing affective expressions. This\nresearch evaluates the effectiveness of machine learning models in detecting\nmiscommunications in robot dialogue. Using a multi-modal dataset of 240\nhuman-robot conversations, where four distinct types of conversational failures\nwere systematically introduced, we assess the performance of state-of-the-art\ncomputer vision models. After each conversational turn, users provided feedback\non whether they perceived an error, enabling an analysis of the models' ability\nto accurately detect robot mistakes. Despite using state-of-the-art models, the\nperformance barely exceeds random chance in identifying miscommunication, while\non a dataset with more expressive emotional content, they successfully\nidentified confused states. To explore the underlying cause, we asked human\nraters to do the same. They could also only identify around half of the induced\nmiscommunications, similarly to our model. These results uncover a fundamental\nlimitation in identifying robot miscommunications in dialogue: even when users\nperceive the induced miscommunication as such, they often do not communicate\nthis to their robotic conversation partner. This knowledge can shape\nexpectations of the performance of computer vision models and can help\nresearchers to design better human-robot conversations by deliberately\neliciting feedback where needed."}
{"id": "2506.20311", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.20311", "abs": "https://arxiv.org/abs/2506.20311", "authors": ["Jingwen Wei"], "title": "Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles", "comment": null, "summary": "The growing use of mobile robots in sectors such as automotive, agriculture,\nand rescue operations reflects progress in robotics and autonomy. In unmanned\naerial vehicles (UAVs), most research emphasizes visual SLAM, sensor fusion,\nand path planning. However, applying UAVs to search and rescue missions in\ndisaster zones remains underexplored, especially for autonomous navigation.\n  This report develops methods for real-time and secure UAV maneuvering in\ncomplex 3D environments, crucial during forest fires. Building upon past\nresearch, it focuses on designing navigation algorithms for unfamiliar and\nhazardous environments, aiming to improve rescue efficiency and safety through\nUAV-based early warning and rapid response.\n  The work unfolds in phases. First, a 2D fusion navigation strategy is\nexplored, initially for mobile robots, enabling safe movement in dynamic\nsettings. This sets the stage for advanced features such as adaptive obstacle\nhandling and decision-making enhancements. Next, a novel 3D reactive navigation\nstrategy is introduced for collision-free movement in forest fire simulations,\naddressing the unique challenges of UAV operations in such scenarios.\n  Finally, the report proposes a unified control approach that integrates UAVs\nand unmanned ground vehicles (UGVs) for coordinated rescue missions in forest\nenvironments. Each phase presents challenges, proposes control models, and\nvalidates them with mathematical and simulation-based evidence. The study\noffers practical value and academic insights for improving the role of UAVs in\nnatural disaster rescue operations."}
{"id": "2506.20314", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20314", "abs": "https://arxiv.org/abs/2506.20314", "authors": ["Marc-Philip Ecker", "Bernhard Bischof", "Minh Nhat Vu", "Christoph Fröhlich", "Tobias Glück", "Wolfgang Kemmetmüller"], "title": "Near Time-Optimal Hybrid Motion Planning for Timber Cranes", "comment": "Accepted at ICRA 2025", "summary": "Efficient, collision-free motion planning is essential for automating\nlarge-scale manipulators like timber cranes. They come with unique challenges\nsuch as hydraulic actuation constraints and passive joints-factors that are\nseldom addressed by current motion planning methods. This paper introduces a\nnovel approach for time-optimal, collision-free hybrid motion planning for a\nhydraulically actuated timber crane with passive joints. We enhance the\nvia-point-based stochastic trajectory optimization (VP-STO) algorithm to\ninclude pump flow rate constraints and develop a novel collision cost\nformulation to improve robustness. The effectiveness of the enhanced VP-STO as\nan optimal single-query global planner is validated by comparison with an\ninformed RRT* algorithm using a time-optimal path parameterization (TOPP). The\noverall hybrid motion planning is formed by combination with a gradient-based\nlocal planner that is designed to follow the global planner's reference and to\nsystematically consider the passive joint dynamics for both collision avoidance\nand sway damping."}
{"id": "2506.20315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20315", "abs": "https://arxiv.org/abs/2506.20315", "authors": ["Matías Mattamala", "Nived Chebrolu", "Jonas Frey", "Leonard Freißmuth", "Haedam Oh", "Benoit Casseau", "Marco Hutter", "Maurice Fallon"], "title": "Building Forest Inventories with Autonomous Legged Robots -- System, Lessons, and Challenges Ahead", "comment": "20 pages, 13 figures. Pre-print version of the accepted paper for\n  IEEE Transactions on Field Robotics (T-FR)", "summary": "Legged robots are increasingly being adopted in industries such as oil, gas,\nmining, nuclear, and agriculture. However, new challenges exist when moving\ninto natural, less-structured environments, such as forestry applications. This\npaper presents a prototype system for autonomous, under-canopy forest inventory\nwith legged platforms. Motivated by the robustness and mobility of modern\nlegged robots, we introduce a system architecture which enabled a quadruped\nplatform to autonomously navigate and map forest plots. Our solution involves a\ncomplete navigation stack for state estimation, mission planning, and tree\ndetection and trait estimation. We report the performance of the system from\ntrials executed over one and a half years in forests in three European\ncountries. Our results with the ANYmal robot demonstrate that we can survey\nplots up to 1 ha plot under 30 min, while also identifying trees with typical\nDBH accuracy of 2cm. The findings of this project are presented as five lessons\nand challenges. Particularly, we discuss the maturity of hardware development,\nstate estimation limitations, open problems in forest navigation, future\navenues for robotic forest inventory, and more general challenges to assess\nautonomous systems. By sharing these lessons and challenges, we offer insight\nand new directions for future research on legged robots, navigation systems,\nand applications in natural environments. Additional videos can be found in\nhttps://dynamic.robots.ox.ac.uk/projects/legged-robots"}
{"id": "2506.20320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20320", "abs": "https://arxiv.org/abs/2506.20320", "authors": ["Malte Probst", "Raphael Wenzel", "Tim Puphal", "Monica Dasi", "Nico A. Steinhardt", "Sango Matsuzaki", "Misa Komuro"], "title": "Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation", "comment": null, "summary": "In Social Robot Navigation, autonomous agents need to resolve many sequential\ninteractions with other agents. State-of-the art planners can efficiently\nresolve the next, imminent interaction cooperatively and do not focus on longer\nplanning horizons. This makes it hard to maneuver scenarios where the agent\nneeds to select a good strategy to find gaps or channels in the crowd. We\npropose to decompose trajectory planning into two separate steps: Conflict\navoidance for finding good, macroscopic trajectories, and cooperative collision\navoidance (CCA) for resolving the next interaction optimally. We propose the\nProbabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies\nan established probabilistic collision risk model to include a general\nassumption of cooperativity. PGP biases the short-term CCA planner to head\ntowards gaps in the crowd. In extensive simulations with crowds of varying\ndensity, we show that using PGP in addition to state-of-the-art CCA planners\nimproves the agents' performance: On average, agents keep more space to others,\ncreate less tension, and cause fewer collisions. This typically comes at the\nexpense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot\nby Honda R&D."}
{"id": "2506.20343", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20343", "abs": "https://arxiv.org/abs/2506.20343", "authors": ["Kento Kawaharazuka", "Takahiro Hattori", "Keita Yoneda", "Kei Okada"], "title": "PIMBS: Efficient Body Schema Learning for Musculoskeletal Humanoids with Physics-Informed Neural Networks", "comment": "Accepted at IEEE Robotics and Automation Letters", "summary": "Musculoskeletal humanoids are robots that closely mimic the human\nmusculoskeletal system, offering various advantages such as variable stiffness\ncontrol, redundancy, and flexibility. However, their body structure is complex,\nand muscle paths often significantly deviate from geometric models. To address\nthis, numerous studies have been conducted to learn body schema, particularly\nthe relationships among joint angles, muscle tension, and muscle length. These\nstudies typically rely solely on data collected from the actual robot, but this\ndata collection process is labor-intensive, and learning becomes difficult when\nthe amount of data is limited. Therefore, in this study, we propose a method\nthat applies the concept of Physics-Informed Neural Networks (PINNs) to the\nlearning of body schema in musculoskeletal humanoids, enabling high-accuracy\nlearning even with a small amount of data. By utilizing not only data obtained\nfrom the actual robot but also the physical laws governing the relationship\nbetween torque and muscle tension under the assumption of correct joint\nstructure, more efficient learning becomes possible. We apply the proposed\nmethod to both simulation and an actual musculoskeletal humanoid and discuss\nits effectiveness and characteristics."}
{"id": "2506.20373", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20373", "abs": "https://arxiv.org/abs/2506.20373", "authors": ["Joerg Deigmoeller", "Stephan Hasler", "Nakul Agarwal", "Daniel Tanneberg", "Anna Belardinelli", "Reza Ghoddoosian", "Chao Wang", "Felix Ocker", "Fan Zhang", "Behzad Dariush", "Michael Gienger"], "title": "CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition", "comment": null, "summary": "We introduce CARMA, a system for situational grounding in human-robot group\ninteractions. Effective collaboration in such group settings requires\nsituational awareness based on a consistent representation of present persons\nand objects coupled with an episodic abstraction of events regarding actors and\nmanipulated objects. This calls for a clear and consistent assignment of\ninstances, ensuring that robots correctly recognize and track actors, objects,\nand their interactions over time. To achieve this, CARMA uniquely identifies\nphysical instances of such entities in the real world and organizes them into\ngrounded triplets of actors, objects, and actions.\n  To validate our approach, we conducted three experiments, where multiple\nhumans and a robot interact: collaborative pouring, handovers, and sorting.\nThese scenarios allow the assessment of the system's capabilities as to role\ndistinction, multi-actor awareness, and consistent instance identification. Our\nexperiments demonstrate that the system can reliably generate accurate\nactor-action-object triplets, providing a structured and robust foundation for\napplications requiring spatiotemporal reasoning and situated decision-making in\ncollaborative settings."}
{"id": "2506.20376", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20376", "abs": "https://arxiv.org/abs/2506.20376", "authors": ["Lingyun Chen", "Xinrui Zhao", "Marcos P. S. Campanha", "Alexander Wegener", "Abdeldjallil Naceri", "Abdalla Swikir", "Sami Haddadin"], "title": "Enhanced Robotic Navigation in Deformable Environments using Learning from Demonstration and Dynamic Modulation", "comment": "Accepted to IROS 2025", "summary": "This paper presents a novel approach for robot navigation in environments\ncontaining deformable obstacles. By integrating Learning from Demonstration\n(LfD) with Dynamical Systems (DS), we enable adaptive and efficient navigation\nin complex environments where obstacles consist of both soft and hard regions.\nWe introduce a dynamic modulation matrix within the DS framework, allowing the\nsystem to distinguish between traversable soft regions and impassable hard\nareas in real-time, ensuring safe and flexible trajectory planning. We validate\nour method through extensive simulations and robot experiments, demonstrating\nits ability to navigate deformable environments. Additionally, the approach\nprovides control over both trajectory and velocity when interacting with\ndeformable objects, including at intersections, while maintaining adherence to\nthe original DS trajectory and dynamically adapting to obstacles for smooth and\nreliable navigation."}
{"id": "2506.20394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20394", "abs": "https://arxiv.org/abs/2506.20394", "authors": ["Mimo Shirasaka", "Yuya Ikeda", "Tatsuya Matsushima", "Yutaka Matsuo", "Yusuke Iwasawa"], "title": "SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning", "comment": null, "summary": "The ability to update information acquired through various means online\nduring task execution is crucial for a general-purpose service robot. This\ninformation includes geometric and semantic data. While SLAM handles geometric\nupdates on 2D maps or 3D point clouds, online updates of semantic information\nremain unexplored. We attribute the challenge to the online scene graph\nrepresentation, for its utility and scalability. Building on previous works\nregarding offline scene graph representations, we study online graph\nrepresentations of semantic information in this work. We introduce SPARK:\nSpatial Perception and Robot Knowledge Integration. This framework extracts\nsemantic information from environment-embedded cues and updates the scene graph\naccordingly, which is then used for subsequent task planning. We demonstrate\nthat graph representations of spatial relationships enhance the robot system's\nability to perform tasks in dynamic environments and adapt to unconventional\nspatial cues, like gestures."}
{"id": "2506.20399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20399", "abs": "https://arxiv.org/abs/2506.20399", "authors": ["Hatem Fakhruldeen", "Arvind Raveendran Nambiar", "Satheeshkumar Veeramani", "Bonilkumar Vijaykumar Tailor", "Hadi Beyzaee Juneghani", "Gabriella Pizzuto", "Andrew Ian Cooper"], "title": "Multimodal Behaviour Trees for Robotic Laboratory Task Automation", "comment": "7 pages, 5 figures, accepted and presented in ICRA 2025", "summary": "Laboratory robotics offer the capability to conduct experiments with a high\ndegree of precision and reproducibility, with the potential to transform\nscientific research. Trivial and repeatable tasks; e.g., sample transportation\nfor analysis and vial capping are well-suited for robots; if done successfully\nand reliably, chemists could contribute their efforts towards more critical\nresearch activities. Currently, robots can perform these tasks faster than\nchemists, but how reliable are they? Improper capping could result in human\nexposure to toxic chemicals which could be fatal. To ensure that robots perform\nthese tasks as accurately as humans, sensory feedback is required to assess the\nprogress of task execution. To address this, we propose a novel methodology\nbased on behaviour trees with multimodal perception. Along with automating\nrobotic tasks, this methodology also verifies the successful execution of the\ntask, a fundamental requirement in safety-critical environments. The\nexperimental evaluation was conducted on two lab tasks: sample vial capping and\nlaboratory rack insertion. The results show high success rate, i.e., 88% for\ncapping and 92% for insertion, along with strong error detection capabilities.\nThis ultimately proves the robustness and reliability of our approach and that\nusing multimodal behaviour trees should pave the way towards the next\ngeneration of robotic chemists."}
{"id": "2506.20445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20445", "abs": "https://arxiv.org/abs/2506.20445", "authors": ["Dongkun Wang", "Junkai Zhao", "Yunfei Teng", "Jieyang Peng", "Wenjing Xue", "Xiaoming Tao"], "title": "Learn to Position -- A Novel Meta Method for Robotic Positioning", "comment": null, "summary": "Absolute positioning accuracy is a vital specification for robots. Achieving\nhigh position precision can be challenging due to the presence of various\nsources of errors. Meanwhile, accurately depicting these errors is difficult\ndue to their stochastic nature. Vision-based methods are commonly integrated to\nguide robotic positioning, but their performance can be highly impacted by\ninevitable occlusions or adverse lighting conditions. Drawing on the\naforementioned considerations, a vision-free, model-agnostic meta-method for\ncompensating robotic position errors is proposed, which maximizes the\nprobability of accurate robotic position via interactive feedback. Meanwhile,\nthe proposed method endows the robot with the capability to learn and adapt to\nvarious position errors, which is inspired by the human's instinct for grasping\nunder uncertainties. Furthermore, it is a self-learning and self-adaptive\nmethod able to accelerate the robotic positioning process as more examples are\nincorporated and learned. Empirical studies validate the effectiveness of the\nproposed method. As of the writing of this paper, the proposed meta search\nmethod has already been implemented in a robotic-based assembly line for\nodd-form electronic components."}
{"id": "2506.20447", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20447", "abs": "https://arxiv.org/abs/2506.20447", "authors": ["James Fant-Male", "Roel Pieters"], "title": "A Review of Personalisation in Human-Robot Collaboration and Future Perspectives Towards Industry 5.0", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "The shift in research focus from Industry 4.0 to Industry 5.0 (I5.0) promises\na human-centric workplace, with social and well-being values at the centre of\ntechnological implementation. Human-Robot Collaboration (HRC) is a core aspect\nof I5.0 development, with an increase in adaptive and personalised interactions\nand behaviours. This review investigates recent advancements towards\npersonalised HRC, where user-centric adaption is key. There is a growing trend\nfor adaptable HRC research, however there lacks a consistent and unified\napproach. The review highlights key research trends on which personal factors\nare considered, workcell and interaction design, and adaptive task completion.\nThis raises various key considerations for future developments, particularly\naround the ethical and regulatory development of personalised systems, which\nare discussed in detail."}
{"id": "2506.20485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20485", "abs": "https://arxiv.org/abs/2506.20485", "authors": ["Tian Liu", "Han Liu", "Boyang Li", "Long Chen", "Kai Huang"], "title": "EANS: Reducing Energy Consumption for UAV with an Environmental Adaptive Navigation Strategy", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVS) are limited by the onboard energy. Refinement\nof the navigation strategy directly affects both the flight velocity and the\ntrajectory based on the adjustment of key parameters in the UAVS pipeline, thus\nreducing energy consumption. However, existing techniques tend to adopt static\nand conservative strategies in dynamic scenarios, leading to inefficient energy\nreduction. Dynamically adjusting the navigation strategy requires overcoming\nthe challenges including the task pipeline interdependencies, the\nenvironmental-strategy correlations, and the selecting parameters. To solve the\naforementioned problems, this paper proposes a method to dynamically adjust the\nnavigation strategy of the UAVS by analyzing its dynamic characteristics and\nthe temporal characteristics of the autonomous navigation pipeline, thereby\nreducing UAVS energy consumption in response to environmental changes. We\ncompare our method with the baseline through hardware-in-the-loop (HIL)\nsimulation and real-world experiments, showing our method 3.2X and 2.6X\nimprovements in mission time, 2.4X and 1.6X improvements in energy,\nrespectively."}
{"id": "2506.20487", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20487", "abs": "https://arxiv.org/abs/2506.20487", "authors": ["Mingqi Yuan", "Tao Yu", "Wenqi Ge", "Xiuyong Yao", "Dapeng Li", "Huijiang Wang", "Jiayu Chen", "Xin Jin", "Bo Li", "Hua Chen", "Wei Zhang", "Wenjun Zeng"], "title": "Behavior Foundation Model: Towards Next-Generation Whole-Body Control System of Humanoid Robots", "comment": "19 pages, 8 figures", "summary": "Humanoid robots are drawing significant attention as versatile platforms for\ncomplex motor control, human-robot interaction, and general-purpose physical\nintelligence. However, achieving efficient whole-body control (WBC) in\nhumanoids remains a fundamental challenge due to sophisticated dynamics,\nunderactuation, and diverse task requirements. While learning-based controllers\nhave shown promise for complex tasks, their reliance on labor-intensive and\ncostly retraining for new scenarios limits real-world applicability. To address\nthese limitations, behavior(al) foundation models (BFMs) have emerged as a new\nparadigm that leverages large-scale pretraining to learn reusable primitive\nskills and behavioral priors, enabling zero-shot or rapid adaptation to a wide\nrange of downstream tasks. In this paper, we present a comprehensive overview\nof BFMs for humanoid WBC, tracing their development across diverse pre-training\npipelines. Furthermore, we discuss real-world applications, current\nlimitations, urgent challenges, and future opportunities, positioning BFMs as a\nkey approach toward scalable and general-purpose humanoid intelligence.\nFinally, we provide a curated and long-term list of BFM papers and projects to\nfacilitate more subsequent research, which is available at\nhttps://github.com/yuanmingqi/awesome-bfm-papers."}
{"id": "2506.20496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20496", "abs": "https://arxiv.org/abs/2506.20496", "authors": ["Jonathan Wang", "Hisashi Ishida", "David Usevitch", "Kesavan Venkatesh", "Yi Wang", "Mehran Armand", "Rachel Bronheim", "Amit Jain", "Adnan Munawar"], "title": "Critical Anatomy-Preserving & Terrain-Augmenting Navigation (CAPTAiN): Application to Laminectomy Surgical Education", "comment": null, "summary": "Surgical training remains a crucial milestone in modern medicine, with\nprocedures such as laminectomy exemplifying the high risks involved.\nLaminectomy drilling requires precise manual control to mill bony tissue while\npreserving spinal segment integrity and avoiding breaches in the dura: the\nprotective membrane surrounding the spinal cord. Despite unintended tears\noccurring in up to 11.3% of cases, no assistive tools are currently utilized to\nreduce this risk. Variability in patient anatomy further complicates learning\nfor novice surgeons. This study introduces CAPTAiN, a critical\nanatomy-preserving and terrain-augmenting navigation system that provides\nlayered, color-coded voxel guidance to enhance anatomical awareness during\nspinal drilling. CAPTAiN was evaluated against a standard non-navigated\napproach through 110 virtual laminectomies performed by 11 orthopedic residents\nand medical students. CAPTAiN significantly improved surgical completion rates\nof target anatomy (87.99% vs. 74.42%) and reduced cognitive load across\nmultiple NASA-TLX domains. It also minimized performance gaps across experience\nlevels, enabling novices to perform on par with advanced trainees. These\nfindings highlight CAPTAiN's potential to optimize surgical execution and\nsupport skill development across experience levels. Beyond laminectomy, it\ndemonstrates potential for broader applications across various surgical and\ndrilling procedures, including those in neurosurgery, otolaryngology, and other\nmedical fields."}
{"id": "2506.20553", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20553", "abs": "https://arxiv.org/abs/2506.20553", "authors": ["Rachel Luo", "Heng Yang", "Michael Watson", "Apoorva Sharma", "Sushant Veer", "Edward Schmerling", "Marco Pavone"], "title": "Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation", "comment": null, "summary": "Learning-based robotic systems demand rigorous validation to assure reliable\nperformance, but extensive real-world testing is often prohibitively expensive,\nand if conducted may still yield insufficient data for high-confidence\nguarantees. In this work, we introduce a general estimation framework that\nleverages paired data across test platforms, e.g., paired simulation and\nreal-world observations, to achieve better estimates of real-world metrics via\nthe method of control variates. By incorporating cheap and abundant auxiliary\nmeasurements (for example, simulator outputs) as control variates for costly\nreal-world samples, our method provably reduces the variance of Monte Carlo\nestimates and thus requires significantly fewer real-world samples to attain a\nspecified confidence bound on the mean performance. We provide theoretical\nanalysis characterizing the variance and sample-efficiency improvement, and\ndemonstrate empirically in autonomous driving and quadruped robotics settings\nthat our approach achieves high-probability bounds with markedly improved\nsample efficiency. Our technique can lower the real-world testing burden for\nvalidating the performance of the stack, thereby enabling more efficient and\ncost-effective experimental evaluation of robotic systems."}
{"id": "2506.20566", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20566", "abs": "https://arxiv.org/abs/2506.20566", "authors": ["Zhonghao Shi", "Enyu Zhao", "Nathaniel Dennler", "Jingzhen Wang", "Xinyang Xu", "Kaleen Shrestha", "Mengxue Fu", "Daniel Seita", "Maja Matarić"], "title": "HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction", "comment": "Accepted to the 19th International Symposium on Experimental Robotics\n  (ISER 2025)", "summary": "Real-time human perception is crucial for effective human-robot interaction\n(HRI). Large vision-language models (VLMs) offer promising generalizable\nperceptual capabilities but often suffer from high latency, which negatively\nimpacts user experience and limits VLM applicability in real-world scenarios.\nTo systematically study VLM capabilities in human perception for HRI and\nperformance-latency trade-offs, we introduce HRIBench, a visual\nquestion-answering (VQA) benchmark designed to evaluate VLMs across a diverse\nset of human perceptual tasks critical for HRI. HRIBench covers five key\ndomains: (1) non-verbal cue understanding, (2) verbal instruction\nunderstanding, (3) human-robot object relationship understanding, (4) social\nnavigation, and (5) person identification. To construct HRIBench, we collected\ndata from real-world HRI environments to curate questions for non-verbal cue\nunderstanding, and leveraged publicly available datasets for the remaining four\ndomains. We curated 200 VQA questions for each domain, resulting in a total of\n1000 questions for HRIBench. We then conducted a comprehensive evaluation of\nboth state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.\nOur results show that, despite their generalizability, current VLMs still\nstruggle with core perceptual capabilities essential for HRI. Moreover, none of\nthe models within our experiments demonstrated a satisfactory\nperformance-latency trade-off suitable for real-time deployment, underscoring\nthe need for future research on developing smaller, low-latency VLMs with\nimproved human perception capabilities. HRIBench and our results can be found\nin this Github repository: https://github.com/interaction-lab/HRIBench."}
{"id": "2506.20579", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20579", "abs": "https://arxiv.org/abs/2506.20579", "authors": ["Ali Reza Pedram", "Evangelos Psomiadis", "Dipankar Maity", "Panagiotis Tsiotras"], "title": "Communication-Aware Map Compression for Online Path-Planning: A Rate-Distortion Approach", "comment": null, "summary": "This paper addresses the problem of collaborative navigation in an unknown\nenvironment, where two robots, referred to in the sequel as the Seeker and the\nSupporter, traverse the space simultaneously. The Supporter assists the Seeker\nby transmitting a compressed representation of its local map under bandwidth\nconstraints to support the Seeker's path-planning task. We introduce a bit-rate\nmetric based on the expected binary codeword length to quantify communication\ncost. Using this metric, we formulate the compression design problem as a\nrate-distortion optimization problem that determines when to communicate, which\nregions of the map should be included in the compressed representation, and at\nwhat resolution (i.e., quantization level) they should be encoded. Our\nformulation allows different map regions to be encoded at varying quantization\nlevels based on their relevance to the Seeker's path-planning task. We\ndemonstrate that the resulting optimization problem is convex, and admits a\nclosed-form solution known in the information theory literature as reverse\nwater-filling, enabling efficient, low-computation, and real-time\nimplementation. Additionally, we show that the Seeker can infer the compression\ndecisions of the Supporter independently, requiring only the encoded map\ncontent and not the encoding policy itself to be transmitted, thereby reducing\ncommunication overhead. Simulation results indicate that our method effectively\nconstructs compressed, task-relevant map representations, both in content and\nresolution, that guide the Seeker's planning decisions even under tight\nbandwidth limitations."}
{"id": "2506.20636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20636", "abs": "https://arxiv.org/abs/2506.20636", "authors": ["Venkat Karramreddy", "Rangarajan Ramanujam"], "title": "A Computationally Aware Multi Objective Framework for Camera LiDAR Calibration", "comment": "16 pages, 10 figures", "summary": "Accurate extrinsic calibration between LiDAR and camera sensors is important\nfor reliable perception in autonomous systems. In this paper, we present a\nnovel multi-objective optimization framework that jointly minimizes the\ngeometric alignment error and computational cost associated with camera-LiDAR\ncalibration. We optimize two objectives: (1) error between projected LiDAR\npoints and ground-truth image edges, and (2) a composite metric for\ncomputational cost reflecting runtime and resource usage. Using the NSGA-II\n\\cite{deb2002nsga2} evolutionary algorithm, we explore the parameter space\ndefined by 6-DoF transformations and point sampling rates, yielding a\nwell-characterized Pareto frontier that exposes trade-offs between calibration\nfidelity and resource efficiency. Evaluations are conducted on the KITTI\ndataset using its ground-truth extrinsic parameters for validation, with\nresults verified through both multi-objective and constrained single-objective\nbaselines. Compared to existing gradient-based and learned calibration methods,\nour approach demonstrates interpretable, tunable performance with lower\ndeployment overhead. Pareto-optimal configurations are further analyzed for\nparameter sensitivity and innovation insights. A preference-based\ndecision-making strategy selects solutions from the Pareto knee region to suit\nthe constraints of the embedded system. The robustness of calibration is tested\nacross variable edge-intensity weighting schemes, highlighting optimal balance\npoints. Although real-time deployment on embedded platforms is deferred to\nfuture work, this framework establishes a scalable and transparent method for\ncalibration under realistic misalignment and resource-limited conditions,\ncritical for long-term autonomy, particularly in SAE L3+ vehicles receiving OTA\nupdates."}
{"id": "2506.20668", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20668", "abs": "https://arxiv.org/abs/2506.20668", "authors": ["Sungjae Park", "Homanga Bharadhwaj", "Shubham Tulsiani"], "title": "DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy", "comment": "Preprint(17 pages). Under Review", "summary": "We propose DemoDiffusion, a simple and scalable method for enabling robots to\nperform manipulation tasks in natural environments by imitating a single human\ndemonstration. Our approach is based on two key insights. First, the hand\nmotion in a human demonstration provides a useful prior for the robot's\nend-effector trajectory, which we can convert into a rough open-loop robot\nmotion trajectory via kinematic retargeting. Second, while this retargeted\nmotion captures the overall structure of the task, it may not align well with\nplausible robot actions in-context. To address this, we leverage a pre-trained\ngeneralist diffusion policy to modify the trajectory, ensuring it both follows\nthe human motion and remains within the distribution of plausible robot\nactions. Our approach avoids the need for online reinforcement learning or\npaired human-robot data, enabling robust adaptation to new tasks and scenes\nwith minimal manual effort. Experiments in both simulation and real-world\nsettings show that DemoDiffusion outperforms both the base policy and the\nretargeted trajectory, enabling the robot to succeed even on tasks where the\npre-trained generalist policy fails entirely. Project page:\nhttps://demodiffusion.github.io/"}
