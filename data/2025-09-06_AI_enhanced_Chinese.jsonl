{"id": "2509.03563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03563", "abs": "https://arxiv.org/abs/2509.03563", "authors": ["Quan Quan", "Jiwen Xu", "Runxiao Liu", "Yi Ding", "Jiaxing Che", "Kai-Yuan Cai"], "title": "Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach", "comment": null, "summary": "In comparison with existing approaches, which struggle with scalability,\ncommunication dependency, and robustness against dynamic failures, cooperative\naerial transportation via robot swarms holds transformative potential for\nlogistics and disaster response. Here, we present a physics-inspired\ncooperative transportation approach for flying robot swarms that imitates the\ndissipative mechanics of table-leg load distribution. By developing a\ndecentralized dissipative force model, our approach enables autonomous\nformation stabilization and adaptive load allocation without the requirement of\nexplicit communication. Based on local neighbor robots and the suspended\npayload, each robot dynamically adjusts its position. This is similar to\nenergy-dissipating table leg reactions. The stability of the resultant control\nsystem is rigorously proved. Simulations demonstrate that the tracking errors\nof the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches\nunder the cases of capability variation, cable uncertainty, limited vision, and\npayload variation, respectively. In real-world experiments with six flying\nrobots, the cooperative aerial transportation system achieved a 94% success\nrate under single-robot failure, disconnection events, 25% payload variation,\nand 40% cable length uncertainty, demonstrating strong robustness under outdoor\nwinds up to Beaufort scale 4. Overall, this physics-inspired approach bridges\nswarm intelligence and mechanical stability principles, offering a scalable\nframework for heterogeneous aerial systems to collectively handle complex\ntransportation tasks in communication-constrained environments.", "AI": {"tldr": "提出了一种基于物理启发的无人机群协同运输方法，模仿桌腿负载分布的耗散力学机制，实现了无需显式通信的自主编队稳定和自适应负载分配。", "motivation": "现有方法在可扩展性、通信依赖性和动态故障鲁棒性方面存在不足，而无人机群协同运输在物流和灾难响应领域具有变革潜力。", "method": "开发了去中心化的耗散力模型，每个机器人根据邻近机器人和悬挂负载动态调整位置，类似于能量耗散的桌腿反应机制。", "result": "仿真显示跟踪误差比现有方法减少20%-68.5%，真实实验中6架无人机系统在单机故障、断开事件、25%负载变化和40%缆长不确定性下达到94%成功率，在4级风力下表现强鲁棒性。", "conclusion": "该方法将群体智能与机械稳定性原理相结合，为异构空中系统在通信受限环境中处理复杂运输任务提供了可扩展框架。"}}
{"id": "2509.03638", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.03638", "abs": "https://arxiv.org/abs/2509.03638", "authors": ["David Alvear", "George Turkiyyah", "Shinkyu Park"], "title": "Cooperative Grasping for Collective Object Transport in Constrained Environments", "comment": null, "summary": "We propose a novel framework for decision-making in cooperative grasping for\ntwo-robot object transport in constrained environments. The core of the\nframework is a Conditional Embedding (CE) model consisting of two neural\nnetworks that map grasp configuration information into an embedding space. The\nresulting embedding vectors are then used to identify feasible grasp\nconfigurations that allow two robots to collaboratively transport an object. To\nensure generalizability across diverse environments and object geometries, the\nneural networks are trained on a dataset comprising a range of environment maps\nand object shapes. We employ a supervised learning approach with negative\nsampling to ensure that the learned embeddings effectively distinguish between\nfeasible and infeasible grasp configurations. Evaluation results across a wide\nrange of environments and objects in simulations demonstrate the model's\nability to reliably identify feasible grasp configurations. We further validate\nthe framework through experiments on a physical robotic platform, confirming\nits practical applicability.", "AI": {"tldr": "提出了一种用于双机器人协作抓取决策的新框架，通过条件嵌入模型将抓取配置信息映射到嵌入空间，以识别可行的协作抓取配置。", "motivation": "解决在受限环境中双机器人协作物体运输的抓取决策问题，需要能够适应不同环境和物体几何形状的通用解决方案。", "method": "使用包含两个神经网络的条件嵌入模型，将抓取配置信息映射到嵌入空间；采用监督学习和负采样方法训练神经网络，数据集包含多种环境地图和物体形状。", "result": "在模拟环境中对多种环境和物体进行评估，模型能够可靠地识别可行的抓取配置；在物理机器人平台上的实验进一步验证了框架的实际适用性。", "conclusion": "该框架通过条件嵌入模型有效解决了双机器人协作抓取的决策问题，具有良好的泛化能力和实际应用价值。"}}
{"id": "2509.03658", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03658", "abs": "https://arxiv.org/abs/2509.03658", "authors": ["Antonio Guillen-Perez"], "title": "Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning", "comment": null, "summary": "The ability to generate a diverse and plausible distribution of future\ntrajectories is a critical capability for autonomous vehicle planning systems.\nWhile recent generative models have shown promise, achieving high fidelity,\ncomputational efficiency, and precise control remains a significant challenge.\nIn this paper, we present the \\textbf{Efficient Virtuoso}, a conditional latent\ndiffusion model for goal-conditioned trajectory planning. Our approach\nintroduces a novel two-stage normalization pipeline that first scales\ntrajectories to preserve their geometric aspect ratio and then normalizes the\nresulting PCA latent space to ensure a stable training target. The denoising\nprocess is performed efficiently in this low-dimensional latent space by a\nsimple MLP denoiser, which is conditioned on a rich scene context fused by a\npowerful Transformer-based StateEncoder. We demonstrate that our method\nachieves state-of-the-art performance on the Waymo Open Motion Dataset,\nreaching a \\textbf{minADE of 0.25}. Furthermore, through a rigorous ablation\nstudy on goal representation, we provide a key insight: while a single endpoint\ngoal can resolve strategic ambiguity, a richer, multi-step sparse route is\nessential for enabling the precise, high-fidelity tactical execution that\nmirrors nuanced human driving behavior.", "AI": {"tldr": "Efficient Virtuoso是一个基于条件潜在扩散模型的目标条件轨迹规划方法，通过两阶段归一化管道和低维潜在空间去噪，在Waymo数据集上达到0.25 minADE的SOTA性能。", "motivation": "解决自动驾驶规划系统中生成多样化、合理未来轨迹的挑战，需要在保持高保真度的同时实现计算效率和精确控制。", "method": "提出条件潜在扩散模型，采用两阶段归一化管道：先缩放轨迹保持几何纵横比，再对PCA潜在空间归一化；使用简单MLP去噪器在低维潜在空间高效去噪，结合基于Transformer的StateEncoder融合丰富场景上下文。", "result": "在Waymo Open Motion Dataset上达到最先进性能，minADE为0.25；通过目标表示消融研究发现，多步稀疏路径比单端点目标更能实现精确的高保真战术执行。", "conclusion": "该方法在轨迹规划任务中实现了高保真度、计算效率和精确控制的平衡，多步稀疏路径表示对于模拟人类驾驶行为的细微差别至关重要。"}}
{"id": "2509.03690", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.03690", "abs": "https://arxiv.org/abs/2509.03690", "authors": ["Kelvin Daniel Gonzalez Amador"], "title": "Low-Cost Open-Source Ambidextrous Robotic Hand with 23 Direct-Drive servos for American Sign Language Alphabet", "comment": "9 pages, 8 figures, 4 tables. Submitted as preprint", "summary": "Accessible communication through sign language is vital for deaf communities,\n1 yet robotic solutions are often costly and limited. This study presents\nVulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand\ncapable of reproducing the full 3 American Sign Language (ASL) alphabet (52\nsigns for right- and left-hand configurations). 4 The system employs 23\ndirect-drive servo actuators for precise finger and wrist movements, 5\ncontrolled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid\nupper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3\nachieves complete ASL 7 coverage with a reversible design. All CAD files and\ncode are released under permissive 8 open-source licenses to enable\nreplication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL\nhandshapes, while a participant study (n = 33) achieved 96.97% recognition 10\naccuracy, improving to 98.78% after video demonstration. VulcanV3 advances\nassistive 11 robotics by combining affordability, full ASL coverage, and\nambidexterity in an openly 12 shared platform, contributing to accessible\ncommunication technologies and inclusive 13 innovation.", "AI": {"tldr": "VulcanV3是一个低成本、开源、3D打印的双手灵巧机器人手，能够完整再现美国手语字母表（52个手势），通过33人参与的研究实现了96.97%的识别准确率。", "motivation": "为聋人社区提供可访问的手语交流解决方案，现有机器人方案成本高且功能有限，需要开发低成本、开源的全功能手语机器人。", "method": "采用23个直接驱动伺服执行器实现精确的手指和手腕运动，由Arduino Mega和双PCA9685模块控制，设计为可逆的双手灵巧结构，所有CAD文件和代码开源。", "result": "实证测试确认了所有52个ASL手形的准确再现，参与者研究（n=33）达到96.97%的识别准确率，视频演示后提升至98.78%。", "conclusion": "VulcanV3通过结合 affordability、完整ASL覆盖和双手灵巧性，在开放共享平台上推进了辅助机器人技术，为可访问通信技术和包容性创新做出贡献。"}}
{"id": "2509.03804", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.03804", "abs": "https://arxiv.org/abs/2509.03804", "authors": ["Ad-Deen Mahbub", "Md Ragib Shaharear"], "title": "Real-Time Buoyancy Estimation for AUV Simulations Using Convex Hull-Based Submerged Volume Calculation", "comment": "7 pages, 10 figures", "summary": "Accurate real-time buoyancy modeling is essential for high-fidelity\nAutonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a\nnative buoyancy system, requiring external solutions for precise underwater\nphysics. This paper presents a novel convex hull-based approach to dynamically\ncompute the submerged volume of an AUV in real time. By extracting mesh\ngeometry from the simulation environment and calculating the hull portion\nintersecting the water level along the z-axis, our method enhances accuracy\nover traditional geometric approximations. A cross-sectional area extension\nreduces computational overhead, enabling efficient buoyant force updates that\nadapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested\non a custom AUV design for SAUVC 2025, this approach delivers real-time\nperformance and scalability, improving simulation fidelity for underwater\nrobotics research without precomputed hydrodynamic models.", "AI": {"tldr": "基于凸包算法的实时浮力模型，通过动态计算AUV沉水体积来提高NVIDIA Isaac Sim水下模拟的精度", "motivation": "NVIDIA Isaac Sim缺乏原生浮力系统，需要外部解决方案来实现精确的水下物理模拟", "method": "提出了一种新的凸包基础方法，通过提取模拟环境中的网格几何形状，动态计算AUV沉水部分的体积，并使用截面积扩展来降低计算开销", "result": "在SAUVC 2025自定制AUV设计上进行测试，该方法能够处理方位、深度和正弦波动（±0.3m），实现实时性能和可扩展性", "conclusion": "该方法提供了一种无需预计算流体动力学模型的高保真度水下模拟解决方案，为水下机器人研究提供了更准确的模拟环境"}}
{"id": "2509.03842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03842", "abs": "https://arxiv.org/abs/2509.03842", "authors": ["Guanglu Jia", "Ceng Zhang", "Gregory S. Chirikjian"], "title": "INGRID: Intelligent Generative Robotic Design Using Large Language Models", "comment": "15 pages, 6 figures", "summary": "The integration of large language models (LLMs) into robotic systems has\naccelerated progress in embodied artificial intelligence, yet current\napproaches remain constrained by existing robotic architectures, particularly\nserial mechanisms. This hardware dependency fundamentally limits the scope of\nrobotic intelligence. Here, we present INGRID (Intelligent Generative Robotic\nDesign), a framework that enables the automated design of parallel robotic\nmechanisms through deep integration with reciprocal screw theory and kinematic\nsynthesis methods. We decompose the design challenge into four progressive\ntasks: constraint analysis, kinematic joint generation, chain construction, and\ncomplete mechanism design. INGRID demonstrates the ability to generate novel\nparallel mechanisms with both fixed and variable mobility, discovering\nkinematic configurations not previously documented in the literature. We\nvalidate our approach through three case studies demonstrating how INGRID\nassists users in designing task-specific parallel robots based on desired\nmobility requirements. By bridging the gap between mechanism theory and machine\nlearning, INGRID enables researchers without specialized robotics training to\ncreate custom parallel mechanisms, thereby decoupling advances in robotic\nintelligence from hardware constraints. This work establishes a foundation for\nmechanism intelligence, where AI systems actively design robotic hardware,\npotentially transforming the development of embodied AI systems.", "AI": {"tldr": "INGRID框架通过结合互易螺旋理论和运动学综合方法，实现并联机器人机构的自动化设计，突破了传统串行机构的硬件限制。", "motivation": "当前机器人系统受限于串行机构硬件，限制了机器人智能的发展范围，需要开发能够自动设计并联机构的新方法。", "method": "将设计任务分解为四个渐进步骤：约束分析、运动学关节生成、链构造和完整机构设计，深度整合互易螺旋理论和运动学综合方法。", "result": "INGRID能够生成具有固定和可变自由度的新型并联机构，发现了文献中未记载的运动学配置，并通过三个案例研究验证了其有效性。", "conclusion": "该工作建立了机制智能的基础，使AI系统能够主动设计机器人硬件，有望彻底改变具身AI系统的开发方式。"}}
{"id": "2509.03859", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03859", "abs": "https://arxiv.org/abs/2509.03859", "authors": ["Haichao Zhang", "Haonan Yu", "Le Zhao", "Andrew Choi", "Qinxun Bai", "Yiqing Yang", "Wei Xu"], "title": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator", "comment": "Project: https://horizonrobotics.github.io/gail/SLIM", "summary": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.", "AI": {"tldr": "提出了一种在模拟中训练视觉运动策略的方法，用于四足机器人的移动操作任务，在真实世界中达到近80%的成功率，能够执行搜索、接近、抓取、运输和放置等动作。", "motivation": "四足机器人的移动操作面临技能多样性、长任务周期和部分可观测性等挑战，需要一个能够在模拟中训练并有效转移到真实世界的解决方案。", "method": "采用模拟训练方法，通过多阶段拾取放置任务训练视觉运动策略，使用关键技术实现高效的训练和有效的模拟到真实转移。", "result": "在真实世界中实现了近80%的成功率，策略能够执行完整的操作流程并展现出重新抓取和任务链式执行等涌现行为，在各种室内外环境中成功部署。", "conclusion": "该方法证明了在模拟中训练四足机器人移动操作策略的可行性，通过有效的模拟到真实转移技术实现了高性能的实际部署。"}}
{"id": "2509.03889", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03889", "abs": "https://arxiv.org/abs/2509.03889", "authors": ["Neha Sunil", "Megha Tippur", "Arnau Saumell", "Edward Adelson", "Alberto Rodriguez"], "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance", "comment": "Accepted at CoRL 2025. Project website:\n  https://mhtippur.github.io/inairclothmanipulation/", "summary": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.", "AI": {"tldr": "一种结合视觉-触觉感的双臂框架，通过自信度感知和感知不确定性适应，实现对磨纳和悬挂衣物的操作", "motivation": "传统衣物操作系统常将衣物展平或假设关键特征可见，无法处理复杂配置、变化材料动态和频繁自戒的挑战", "method": "组合自信度感知的密集视视对应和触觉监督的把握支撑力。使用分布损失训练对应模型，通过反应式状态机适应折叠策略，并使用高分辨率触觉反馈进行自监督把握支撑力学习", "result": "系统能够处理高度遮挡的桌面和空中配置，通过在低信心状态下延迟动作来应对不确定性。在折叠和悬挂任务中展示了任务无关的把握选择模块", "conclusion": "该框架为衣物操作提供了可重用的中间表示，如从人类视频示范中提取把握目标，为更通用和可扩展的衣物操作抓基础"}}
{"id": "2509.04016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04016", "abs": "https://arxiv.org/abs/2509.04016", "authors": ["Branimir Ćaran", "Vladimir Milić", "Marko Švaco", "Bojan Jerbić"], "title": "Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot", "comment": "ACCEPTED FOR IEEE EUROPEAN CONFERENCE ON MOBILE ROBOTS 2025. PREPRINT\n  VERSION. ACCEPTED JUNE, 2025 AND PRESENTED SEPTEMBER, 2025", "summary": "This paper presents the design of a pose estimator for a four wheel\nindependent steer four wheel independent drive (4WIS4WID) wall climbing mobile\nrobot, based on the fusion of multimodal measurements, including wheel\nodometry, visual odometry, and an inertial measurement unit (IMU) data using\nExtended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose\nestimator is a critical component of wall climbing mobile robots, as their\noperational environment involves carrying precise measurement equipment and\nmaintenance tools in construction, requiring information about pose on the\nbuilding at the time of measurement. Due to the complex geometry and material\nproperties of building facades, the use of traditional localization sensors\nsuch as laser, ultrasonic, or radar is often infeasible for wall-climbing\nrobots. Moreover, GPS-based localization is generally unreliable in these\nenvironments because of signal degradation caused by reinforced concrete and\nelectromagnetic interference. Consequently, robot odometry remains the primary\nsource of velocity and position information, despite being susceptible to drift\ncaused by both systematic and non-systematic errors. The calibrations of the\nrobot's systematic parameters were conducted using nonlinear optimization and\nLevenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting\nmethods, while Genetic algorithm and Particle swarm were used as\nstochastic-based methods for kinematic parameter calibration. Performance and\nresults of the calibration methods and pose estimators were validated in detail\nwith experiments on the experimental mobile wall climbing robot.", "AI": {"tldr": "本文提出了一种基于多模态测量融合的4WIS4WID爬墙机器人位姿估计器，使用EKF和UKF融合轮式里程计、视觉里程计和IMU数据，解决了传统传感器在建筑立面上的局限性问题。", "motivation": "爬墙机器人在建筑维护作业中需要精确的位姿信息，但由于建筑立面的复杂几何和材料特性，传统定位传感器（激光、超声波、雷达）不可行，GPS信号也因钢筋混凝土和电磁干扰而不可靠，轮式里程计又存在漂移问题。", "method": "使用扩展卡尔曼滤波(EKF)和无迹卡尔曼滤波(UKF)融合多模态测量数据（轮式里程计、视觉里程计、IMU）。采用非线性优化和Levenberg-Marquardt方法进行系统参数标定，使用遗传算法和粒子群算法进行运动学参数标定。", "result": "在实验性爬墙移动机器人上详细验证了标定方法和位姿估计器的性能和结果。", "conclusion": "提出的多模态融合位姿估计方法有效解决了爬墙机器人在复杂建筑环境中的定位问题，为建筑维护作业提供了可靠的位姿信息。"}}
{"id": "2509.04018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04018", "abs": "https://arxiv.org/abs/2509.04018", "authors": ["Yifan Yang", "Zhixiang Duan", "Tianshi Xie", "Fuyu Cao", "Pinxi Shen", "Peili Song", "Piaopiao Jin", "Guokang Sun", "Shaoqing Xu", "Yangwei You", "Jingtai Liu"], "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction", "comment": null, "summary": "Robotic manipulation is a fundamental component of automation. However,\ntraditional perception-planning pipelines often fall short in open-ended tasks\ndue to limited flexibility, while the architecture of a single end-to-end\nVision-Language-Action (VLA) offers promising capabilities but lacks crucial\nmechanisms for anticipating and recovering from failure. To address these\nchallenges, we propose FPC-VLA, a dual-model framework that integrates VLA with\na supervisor for failure prediction and correction. The supervisor evaluates\naction viability through vision-language queries and generates corrective\nstrategies when risks arise, trained efficiently without manual labeling. A\nsimilarity-guided fusion module further refines actions by leveraging past\npredictions. Evaluation results on multiple simulation platforms (SIMPLER and\nLIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA\noutperforms state-of-the-art models in both zero-shot and fine-tuned settings.\nBy activating the supervisor only at keyframes, our approach significantly\nincreases task success rates with minimal impact on execution time. Successful\nreal-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong\ngeneralization and practical utility for building more reliable autonomous\nsystems.", "AI": {"tldr": "FPC-VLA是一个双模型框架，将视觉-语言-动作模型与故障预测和纠正监督器相结合，通过视觉语言查询评估动作可行性并在风险出现时生成纠正策略，显著提高了任务成功率。", "motivation": "传统感知-规划流水线在开放任务中灵活性不足，而单端到端VLA模型缺乏故障预测和恢复机制。需要一种能够预见和处理故障的可靠自主系统。", "method": "提出FPC-VLA双模型框架：VLA模型负责主要动作生成，监督器通过视觉语言查询评估动作可行性并生成纠正策略。采用相似性引导融合模块优化动作，仅在关键帧激活监督器以减少计算开销。", "result": "在多个仿真平台和机器人平台上，FPC-VLA在零样本和微调设置下均优于最先进模型，任务成功率显著提升，执行时间影响最小。在真实世界的长时程任务中成功部署。", "conclusion": "FPC-VLA通过集成故障预测和纠正机制，提供了强大的泛化能力和实际效用，为构建更可靠的自主系统提供了有效解决方案。"}}
{"id": "2509.04061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04061", "abs": "https://arxiv.org/abs/2509.04061", "authors": ["Ventseslav Yordanov", "Simon Schäfer", "Alexander Mann", "Stefan Kowalewski", "Bassam Alrifaee", "Lutz Eckstein"], "title": "Integrated Wheel Sensor Communication using ESP32 -- A Contribution towards a Digital Twin of the Road System", "comment": "6 pages, 2 figures, this work was submitted to and accepted by IEEE\n  International Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "While current onboard state estimation methods are adequate for most driving\nand safety-related applications, they do not provide insights into the\ninteraction between tires and road surfaces. This paper explores a novel\ncommunication concept for efficiently transmitting integrated wheel sensor data\nfrom an ESP32 microcontroller. Our proposed approach utilizes a\npublish-subscribe system, surpassing comparable solutions in the literature\nregarding data transmission volume. We tested this approach on a drum tire test\nrig with our prototype sensors system utilizing a diverse selection of sample\nfrequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our\ncommunication concept. The implemented prototype sensor showcases minimal data\nloss, approximately 0.1 % of the sampled data, validating the reliability of\nour developed communication system. This work contributes to advancing\nreal-time data acquisition, providing insights into optimizing integrated wheel\nsensor communication.", "AI": {"tldr": "本文提出了一种基于ESP32微控制器的连接轮汽传感器数据的新题通信方案，通过发布-订阅系统实现低数据损耗的高效传输，为轮胎-路面交互分析提供实时数据支持。", "motivation": "当前的车载状态估计方法虽能满足驾驶和安全需求，但无法揭示轮胎与路面之间的交互信息，需要一种能够高效传输集成轮汽传感器数据的通信方案。", "method": "设计并实现了一种基于ESP32微控制器的发布-订阅通信系统，在模拟轮胎测试台上进行测试，采用了1 Hz到32000 Hz之间的多种采样频率。", "result": "原型系统展现了极低的数据损耗率（约0.1%），在各种采样频率下都保持了高可靠性，数据传输量超越了相关文献中的类似方案。", "conclusion": "该通信方案成功实现了高效的实时数据获取，为轮胎传感器通信优化提供了有价值的见解，有助于进一步研究轮胎-路面交互机制。"}}
{"id": "2509.04063", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04063", "abs": "https://arxiv.org/abs/2509.04063", "authors": ["Hongyin Zhang", "Shiyuan Zhang", "Junxi Jin", "Qixin Zeng", "Yifan Qiao", "Hongchao Lu", "Donglin Wang"], "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models", "comment": null, "summary": "Vision-Language-Action (VLA) models based on flow matching have shown\nexcellent performance in general-purpose robotic manipulation tasks. However,\nthe action accuracy of these models on complex downstream tasks is\nunsatisfactory. One important reason is that these models rely solely on the\npost-training paradigm of imitation learning, which makes it difficult to have\na deeper understanding of the distribution properties of data quality, which is\nexactly what Reinforcement Learning (RL) excels at. In this paper, we\ntheoretically propose an offline RL post-training objective for VLA flow models\nand induce an efficient and feasible offline RL fine-tuning algorithm --\nAdaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted\nscaling factor in the VLA flow model loss, we construct a principled\nbias-variance trade-off objective function to optimally control the impact of\nRL signal on flow loss. ARFM adaptively balances RL advantage preservation and\nflow loss gradient variance control, resulting in a more stable and efficient\nfine-tuning process. Extensive simulation and real-world experimental results\nshow that ARFM exhibits excellent generalization, robustness, few-shot\nlearning, and continuous learning performance.", "AI": {"tldr": "提出了自适应强化流匹配(ARFM)算法，通过离线强化学习后训练提升VLA流模型的行动精度，在复杂任务中表现出优秀的泛化性和稳定性", "motivation": "现有基于流匹配的VLA模型在复杂下游任务中行动精度不足，仅依赖模仿学习的后训练范式难以深入理解数据质量分布特性，而强化学习正好擅长这方面", "method": "理论提出VLA流模型的离线RL后训练目标，引入自适应调整的缩放因子构建偏差-方差权衡目标函数，控制RL信号对流损失的影响，平衡RL优势保持和流损失梯度方差控制", "result": "大量仿真和真实实验表明ARFM具有优秀的泛化性、鲁棒性、少样本学习和持续学习性能", "conclusion": "ARFM算法通过离线RL后训练有效提升了VLA流模型的行动精度，为复杂机器人操作任务提供了更稳定高效的解决方案"}}
{"id": "2509.04069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04069", "abs": "https://arxiv.org/abs/2509.04069", "authors": ["Chengyandan Shen", "Christoffer Sloth"], "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning", "comment": null, "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.", "AI": {"tldr": "提出DRLR框架，通过改进IBRL算法的动作选择模块来减少bootstrapping误差，使用SAC代替TD3防止次优策略，在机器人任务中验证了有效性并实现了sim2real部署。", "motivation": "解决深度强化学习在机器人任务中的探索效率问题，特别是bootstrapping误差导致的低效探索和策略收敛到次优解的问题。", "method": "基于IBRL算法改进动作选择模块，提供校准的Q值来减少bootstrapping误差；使用SAC作为RL策略替代TD3；在装桶和开抽屉等机器人任务上进行验证。", "result": "实验验证了方法在减少bootstrapping误差和防止过拟合方面的有效性，框架在不同状态-动作维度和演示质量的任务中表现出鲁棒性，成功实现了从仿真到真实的部署。", "conclusion": "DRLR框架通过改进的动作选择模块和SAC策略，有效提高了探索效率，减少了bootstrapping误差，在机器人任务中表现出良好的性能和实际部署能力。"}}
{"id": "2509.04076", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04076", "abs": "https://arxiv.org/abs/2509.04076", "authors": ["Lennart Clasmeier", "Jan-Gerrit Habekost", "Connor Gäde", "Philipp Allgeuer", "Stefan Wermter"], "title": "Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot", "comment": "Submitted to ICANN 20255 Special Session on Neural Robotics", "summary": "We propose a novel diffusion-based action model for robotic motion planning.\nCommonly, established numerical planning approaches are used to solve general\nmotion planning problems, but have significant runtime requirements. By\nleveraging the power of deep learning, we are able to achieve good results in a\nmuch smaller runtime by learning from a dataset generated by these planners.\nWhile our initial model uses point cloud embeddings in the input to predict\nkeypoint-based joint sequences in its output, we observed in our ablation study\nthat it remained challenging to condition the network on the point cloud\nembeddings. We identified some biases in our dataset and refined it, which\nimproved the model's performance. Our model, even without the use of the point\ncloud encodings, outperforms numerical models by an order of magnitude\nregarding the runtime, while reaching a success rate of up to 90% of collision\nfree solutions on the test set.", "AI": {"tldr": "推出基于液散模型的机器人运动规划方法，通过深度学习大幅缩短运行时间，达到传统数值规划方法90%的成功率", "motivation": "传统数值规划方法虽能解决一般运动规划问题，但需要昂贵的运行时间成本，需要更高效的解决方案", "method": "使用液散模型，通过从数值规划器生成的数据集中学习，初始模型使用点云嵌入作为输入预测关键点基于的关节序列", "result": "模型在运行时间上比数值模型提升了一个数量级，在测试集上达到了至90%的无碰撞解决方案成功率", "conclusion": "深度学习基于液散模型的方法可以在保持高成功率的同时大幅缩短机器人运动规划的运行时间，具有强大的应用潜力"}}
{"id": "2509.04094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04094", "abs": "https://arxiv.org/abs/2509.04094", "authors": ["Fatih Dursun", "Bruno Vilhena Adorno", "Simon Watson", "Wei Pan"], "title": "Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators", "comment": "14 pages, 13 figures, 3 tables. Under Review for the IEEE\n  Transactions on Robotics (T-RO)", "summary": "Object reconstruction and inspection tasks play a crucial role in various\nrobotics applications. Identifying paths that reveal the most unknown areas of\nthe object becomes paramount in this context, as it directly affects\nefficiency, and this problem is known as the view path planning problem.\nCurrent methods often use sampling-based path planning techniques, evaluating\npotential views along the path to enhance reconstruction performance. However,\nthese methods are computationally expensive as they require evaluating several\ncandidate views on the path. To this end, we propose a computationally\nefficient solution that relies on calculating a focus point in the most\ninformative (unknown) region and having the robot maintain this point in the\ncamera field of view along the path. We incorporated this strategy into the\nwhole-body control of a mobile manipulator employing a visibility constraint\nwithout the need for an additional path planner. We conducted comprehensive and\nrealistic simulations using a large dataset of 114 diverse objects of varying\nsizes from 57 categories to compare our method with a sampling-based planning\nstrategy using Bayesian data analysis. Furthermore, we performed real-world\nexperiments with an 8-DoF mobile manipulator to demonstrate the proposed\nmethod's performance in practice. Our results suggest that there is no\nsignificant difference in object coverage and entropy. In contrast, our method\nis approximately nine times faster than the baseline sampling-based method in\nterms of the average time the robot spends between views.", "AI": {"tldr": "这篇论文提出了一种计算效率更高的视角路径规划方法，通过计算最信息区域的焦点并使机器人保持该点在视野内，实现了与采样基准方法相似的物体覆盖效果，但速度提升了8倍。", "motivation": "现有的采样基础视角路径规划方法需要评估多个候选视图，计算成本高，影响了物体重建和检测任务的效率。", "method": "提出一种计算效率高的方法：计算最信息区域（未知区域）的焦点，并让机器人在路径上保持该点在摄像头视野内，通过可视性约束集成到移动操纳器的全身控制中。", "result": "通过在114个多样化物体的实际模拟实验和超过8自由度移动操纳器的实际实验，结果显示：在物体覆盖率和信息量上与基准采样方法没有显著差异，但平均视图切换时间约提升了9倍。", "conclusion": "该方法在保持与采样基准方法相似性能的同时，显著提高了计算效率，为机器人物体重建和检测任务提供了更高效的解决方案。"}}
{"id": "2509.04095", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.04095", "abs": "https://arxiv.org/abs/2509.04095", "authors": ["Achilleas Santi Seisa", "Viswa Narayanan Sankaranarayanan", "Gerasimos Damigos", "Sumeet Gajanan Satpute", "George Nikolakopoulos"], "title": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation", "comment": "6 pages, 7 figures, CCGridW 2025", "summary": "Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments.", "AI": {"tldr": "一种基于容器化技术的可扩展测试框架，用于测试云端和边缘端机器人系统，通过UDP隧道模拟实际网络条件", "motivation": "云机器人技术虽有优势，但集成云计算与机器人存在网络延迟、安全问题和资源管理挑战，需要有效的测试框架", "method": "开发包含容器化云集群和容器化机器人模拟环境两个组件，通过UDP隧道实现双向通信，使用Linux网络控制工具模拟延迟和振动", "result": "框架能够模拟实际云机器人部署中遇到的变化网络条件，以云辅助航空机器人远程控制为例进行验证", "conclusion": "该框架为云机器人系统提供了可扩展、直观的测试解决方案，有助于解决云机器人集成中的技术挑战"}}
{"id": "2509.04119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04119", "abs": "https://arxiv.org/abs/2509.04119", "authors": ["Ke Wu", "Yuhao Wang", "Kevin Henry", "Cesare Stefanini", "Gang Zheng"], "title": "Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation", "comment": "Journal", "summary": "Continuum robots, inspired by octopus arms and elephant trunks, combine\ndexterity with intrinsic compliance, making them well suited for unstructured\nand confined environments. Yet their continuously deformable morphology poses\nchallenges for motion planning and control, calling for accurate but\nlightweight models. We propose the Lightweight Actuation Space Energy Modeling\n(LASEM) framework for cable driven continuum robots, which formulates actuation\npotential energy directly in actuation space. LASEM yields an analytical\nforward model derived from geometrically nonlinear beam and rod theories via\nHamilton's principle, while avoiding explicit modeling of cable backbone\ncontact. It accepts both force and displacement inputs, thereby unifying\nkinematic and static formulations. Assuming the friction is neglected, the\nframework generalizes to nonuniform geometries, arbitrary cable routings,\ndistributed loading and axial extensibility, while remaining computationally\nefficient for real-time use. Numerical simulations validate its accuracy, and a\nsemi-analytical iterative scheme is developed for inverse kinematics. To\naddress discretization in practical robots, LASEM further reformulates the\nfunctional minimization as a numerical optimization, which also naturally\nincorporates cable potential energy without explicit contact modeling.", "AI": {"tldr": "提出了LASEM框架，为缆绳驱动连续体机器人建立轻量级的驱动空间能量模型，统一运动学和静力学建模，避免显式接触建模，支持实时计算。", "motivation": "连续体机器人具有柔顺性和灵活性，但其连续变形形态给运动规划和控制带来挑战，需要准确但轻量化的模型。", "method": "基于几何非线性梁杆理论和哈密顿原理，在驱动空间中直接建立驱动势能模型，避免缆绳-骨架接触显式建模，支持力和位移输入。", "result": "数值模拟验证了模型准确性，开发了半解析迭代方案用于逆运动学，并通过数值优化处理实际机器人的离散化问题。", "conclusion": "LASEM框架为缆绳驱动连续体机器人提供了统一、准确且计算高效的建模方法，适用于非均匀几何、任意缆绳布线等复杂情况。"}}
{"id": "2509.04324", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04324", "abs": "https://arxiv.org/abs/2509.04324", "authors": ["Chen Hu", "Shan Luo", "Letizia Gionfrida"], "title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection", "comment": null, "summary": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.", "AI": {"tldr": "OVGrasp是一个基于软体外骨骼的分层抓取辅助框架，整合了RGB-D视觉、开放词汇提示和语音命令，实现了多模态交互和零样本物体检测。", "motivation": "为运动障碍患者在非结构化环境中提供抓取辅助，应对物体类别和用户意图的多样性和不可预测性。", "method": "采用分层控制框架，集成视觉-语言基础模型和开放词汇机制，通过多模态决策器融合空间和语言线索来推断用户意图。", "result": "在15个物体和3种抓取类型的系统评估中，10名参与者的抓取能力得分达到87.00%，优于现有基线方法，运动学对齐性更好。", "conclusion": "OVGrasp框架在开放环境中表现出强大的泛化能力和多模态交互性能，为运动障碍患者的自主性恢复提供了有效解决方案。"}}
{"id": "2509.04441", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04441", "abs": "https://arxiv.org/abs/2509.04441", "authors": ["Hao-Shu Fang", "Branden Romero", "Yichen Xie", "Arthur Hu", "Bo-Ruei Huang", "Juan Alvarez", "Matthew Kim", "Gabriel Margolis", "Kavya Anbarasu", "Masayoshi Tomizuka", "Edward Adelson", "Pulkit Agrawal"], "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "comment": "project page: https://dex-op.github.io", "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.", "AI": {"tldr": "DEXOP是一种被动手部外骨骼系统，通过perioperation范式收集人类操作数据，提供直接接触反馈和姿态镜像，相比遥操作能更高效地收集高质量演示数据用于机器人技能学习。", "motivation": "传统遥操作方式收集机器人演示数据存在效率低、不自然的问题，需要一种能够最大化人类操作能力同时保证数据可迁移性的新范式。", "method": "开发DEXOP被动手部外骨骼，机械连接人类手指和机器人手指，提供直接接触反馈和姿态镜像功能，在自然环境中收集视觉+触觉的丰富感官数据。", "result": "DEXOP在多种灵巧接触密集型任务中表现出色，相比遥操作显著提高了单位时间数据收集的任务性能，能够规模化收集高质量演示数据。", "conclusion": "DEXOP通过perioperation范式为提升机器人灵巧性提供了强大工具，其收集的数据训练的策略在任务表现上优于遥操作数据。"}}
{"id": "2509.04443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04443", "abs": "https://arxiv.org/abs/2509.04443", "authors": ["Lawrence Y. Zhu", "Pranav Kuppili", "Ryan Punamiya", "Patcharapong Aphiwetsa", "Dhruv Patel", "Simar Kareer", "Sehoon Ha", "Danfei Xu"], "title": "EMMA: Scaling Mobile Manipulation via Egocentric Human Data", "comment": null, "summary": "Scaling mobile manipulation imitation learning is bottlenecked by expensive\nmobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),\nan end-to-end framework training mobile manipulation policies from human mobile\nmanipulation data with static robot data, sidestepping mobile teleoperation. To\naccomplish this, we co-train human full-body motion data with static robot\ndata. In our experiments across three real-world tasks, EMMA demonstrates\ncomparable performance to baselines trained on teleoperated mobile robot data\n(Mobile ALOHA), achieving higher or equivalent task performance in full task\nsuccess. We find that EMMA is able to generalize to new spatial configurations\nand scenes, and we observe positive performance scaling as we increase the\nhours of human data, opening new avenues for scalable robotic learning in\nreal-world environments. Details of this project can be found at\nhttps://ego-moma.github.io/.", "AI": {"tldr": "EMMA框架通过结合人类全身运动数据和静态机器人数据来训练移动操作策略，避免了昂贵的移动机器人遥操作，在真实任务中达到与基于遥操作数据训练的方法相当的性能。", "motivation": "移动操作模仿学习的规模化受限于昂贵的移动机器人遥操作成本，需要找到替代方案来降低数据收集成本。", "method": "提出EMMA端到端框架，通过共同训练人类全身运动数据和静态机器人数据来学习移动操作策略，无需移动遥操作数据。", "result": "在三个真实世界任务中，EMMA表现出与基于遥操作移动机器人数据训练的方法（Mobile ALOHA）相当的性能，在完整任务成功率上达到更高或相当的水平，能够泛化到新的空间配置和场景。", "conclusion": "EMMA为真实世界环境中的可扩展机器人学习开辟了新途径，随着人类数据量的增加，性能呈现正向扩展趋势。"}}
