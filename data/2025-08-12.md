<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 60]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing](https://arxiv.org/abs/2508.06518)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: 该研究设计了一款自动化折叠和缝纫机，用于生产褶皱裤，显著提升了传统手工缝纫的效率。


<details>
  <summary>Details</summary>
Motivation: 传统褶皱制作方法劳动密集、易出错且技术要求高，自动化需求迫切。

Method: 集成精密折叠机制和实时监控功能的自动化缝纫单元。

Result: 劳动力时间减少93%，机器时间提升73%，总产出率增加72%。

Conclusion: 自动化机器提高了效率，降低了成本和浪费，符合可持续性趋势。

Abstract: The applied research is the design and development of an automated folding
and sewing machine for pleated pants. It represents a significant advancement
in addressing the challenges associated with manual sewing processes.
Traditional methods for creating pleats are labour-intensive, prone to
inconsistencies, and require high levels of skill, making automation a critical
need in the apparel industry. This research explores the technical feasibility
and operational benefits of integrating advanced technologies into garment
production, focusing on the creation of an automated machine capable of precise
folding and sewing operations and eliminating the marking operation.
  The proposed machine incorporates key features such as a precision folding
mechanism integrated into the automated sewing unit with real-time monitoring
capabilities. The results demonstrate remarkable improvements: the standard
labour time has been reduced by 93%, dropping from 117 seconds per piece to
just 8 seconds with the automated system. Similarly, machinery time improved by
73%, and the total output rate increased by 72%. These enhancements translate
into a cycle time reduction from 117 seconds per piece to an impressive 33
seconds, enabling manufacturers to meet customer demand more swiftly. By
eliminating manual marking processes, the machine not only reduces labour costs
but also minimizes waste through consistent pleat formation. This automation
aligns with industry trends toward sustainability and efficiency, potentially
reducing environmental impact by decreasing material waste and energy
consumption.

</details>


### [2] [Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator](https://arxiv.org/abs/2508.06520)
*Liwei Chen,Tong Qin,Zhenhua Huangfu,Li Li,Wei Wei*

Main category: cs.RO

TL;DR: 提出了一种可微分优化框架，用于可重复使用航天器的翻转和着陆轨迹设计，结合深度学习与动力学求解器实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 解决传统轨迹优化方法中线性化或凸松弛的限制，实现高非线性复杂机动的建模与优化。

Method: 使用深度神经网络代理模型预测气动力和力矩，结合可微分刚体动力学求解器，支持端到端梯度优化。

Result: 框架成功处理执行器限制和终端着陆约束，生成物理一致的优化控制序列。

Conclusion: 为未来涉及非定常气动力学、羽流交互和智能制导设计的扩展奠定了基础。

Abstract: We propose a differentiable optimization framework for flip-and-landing
trajectory design of reusable spacecraft, exemplified by the Starship vehicle.
A deep neural network surrogate, trained on high-fidelity CFD data, predicts
aerodynamic forces and moments, and is tightly coupled with a differentiable
rigid-body dynamics solver. This enables end-to-end gradient-based trajectory
optimization without linearization or convex relaxation. The framework handles
actuator limits and terminal landing constraints, producing physically
consistent, optimized control sequences. Both standard automatic
differentiation and Neural ODEs are applied to support long-horizon rollouts.
Results demonstrate the framework's effectiveness in modeling and optimizing
complex maneuvers with high nonlinearities. This work lays the groundwork for
future extensions involving unsteady aerodynamics, plume interactions, and
intelligent guidance design.

</details>


### [3] [Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments](https://arxiv.org/abs/2508.06521)
*H. Liu,L. S. Moreu,T. S. Andersen,V. V. Puche,M. Fumagalli*

Main category: cs.RO

TL;DR: 论文提出了一种名为Stinger Robot的新型紧凑型机器人平台，专为在废弃地下矿山中自主高力钻孔而设计，解决了传统钻机在狭窄、无结构环境中的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于对关键原材料的需求增加，废弃地下矿山重新受到关注，但这些环境对传统钻机提出了极端挑战。

Method: 机器人采用机械自锁三腿支撑机制，结合力感知闭环控制策略，通过ROS 2中的有限状态机动态调整腿部部署。

Result: 模拟和初步硬件测试表明，Stinger Robot能在传统采矿机器无法工作的环境中自主稳定和钻孔。

Conclusion: 该研究首次验证了在地下环境中集成分布式力支撑和自主钻孔的机器人架构，为未来模块化机器人协作采矿奠定了基础。

Abstract: The increasing demand for critical raw materials has revitalized interest in
abandoned underground mines, which pose extreme challenges for conventional
drilling machinery due to confined, unstructured, and infrastructure-less
environments. This paper presents the Stinger Robot, a novel compact robotic
platform specifically designed for autonomous high-force drilling in such
settings. The robot features a mechanically self-locking tri-leg bracing
mechanism that enables stable anchoring to irregular tunnel surfaces. A key
innovation lies in its force-aware, closed-loop control strategy, which enables
force interaction with unstructured environments during bracing and drilling.
Implemented as a finite-state machine in ROS 2, the control policy dynamically
adapts leg deployment based on real-time contact feedback and load thresholds,
ensuring stability without external supports. We demonstrate, through
simulation and preliminary hardware tests, that the Stinger Robot can
autonomously stabilize and drill in conditions previously inaccessible to
nowadays mining machines. This work constitutes the first validated robotic
architecture to integrate distributed force-bracing and autonomous drilling in
underground environments, laying the groundwork for future collaborative mining
operations using modular robot systems.

</details>


### [4] [MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](https://arxiv.org/abs/2508.06534)
*Aishan Liu,Jiakai Wang,Tianyuan Zhang,Hainan Li,Jiangfan Liu,Siyuan Liang,Yilong Ren,Xianglong Liu,Dacheng Tao*

Main category: cs.RO

TL;DR: MetAdv是一个新型对抗测试平台，通过虚拟仿真与物理车辆反馈的紧密结合，实现了自动驾驶系统的动态、交互式对抗评估。


<details>
  <summary>Details</summary>
Motivation: 评估和确保自动驾驶系统的对抗鲁棒性是一个关键且未解决的挑战。

Method: MetAdv建立了一个混合虚拟-物理沙盒，设计了三层闭环测试环境，支持端到端对抗评估，涵盖从高层对抗生成到低层物理车辆执行的整个过程。

Result: MetAdv支持广泛的自动驾驶任务和算法范式，具有灵活的3D车辆建模和模拟-物理环境无缝切换能力，并具备人机交互功能。

Conclusion: MetAdv为对抗评估提供了一个可扩展的统一框架，有助于实现更安全的自动驾驶。

Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD)
systems is a critical and unresolved challenge. This paper introduces MetAdv, a
novel adversarial testing platform that enables realistic, dynamic, and
interactive evaluation by tightly integrating virtual simulation with physical
vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical
sandbox, within which we design a three-layer closed-loop testing environment
with dynamic adversarial test evolution. This architecture facilitates
end-to-end adversarial evaluation, ranging from high-level unified adversarial
generation, through mid-level simulation-based interaction, to low-level
execution on physical vehicles. Additionally, MetAdv supports a broad spectrum
of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,
end-to-end learning, vision-language models). It supports flexible 3D vehicle
modeling and seamless transitions between simulated and physical environments,
with built-in compatibility for commercial platforms such as Apollo and Tesla.
A key feature of MetAdv is its human-in-the-loop capability: besides flexible
environmental configuration for more customized evaluation, it enables
real-time capture of physiological signals and behavioral feedback from
drivers, offering new insights into human-machine trust under adversarial
conditions. We believe MetAdv can offer a scalable and unified framework for
adversarial assessment, paving the way for safer AD.

</details>


### [5] [Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots](https://arxiv.org/abs/2508.06538)
*Gioele Buriani,Jingyue Liu,Maximilian Stölzle,Cosimo Della Santina,Jiatao Ding*

Main category: cs.RO

TL;DR: 提出了一种结合SINDy与物理结构先验的学习架构，用于构建四足机器人跳跃的可解释降阶模型，其精度优于传统aSLIP模型。


<details>
  <summary>Details</summary>
Motivation: 降阶模型对四足机器人的运动规划与控制至关重要，但需在简化复杂动力学的同时保留关键行为。

Method: 结合Sparse Identification of Nonlinear Dynamics (SINDy)与跳跃动力学的物理结构先验，构建低维潜在空间模型。

Result: 新方法在仿真和硬件实验中表现出优于传统aSLIP模型的精度，适用于多种跳跃策略。

Conclusion: 该方法为四足机器人跳跃提供了高精度且可解释的降阶模型，验证了其有效性。

Abstract: Reduced-order models are essential for motion planning and control of
quadruped robots, as they simplify complex dynamics while preserving critical
behaviors. This paper introduces a novel methodology for deriving such
interpretable dynamic models, specifically for jumping. We capture the
high-dimensional, nonlinear jumping dynamics in a low-dimensional latent space
by proposing a learning architecture combining Sparse Identification of
Nonlinear Dynamics (SINDy) with physical structural priors on the jump
dynamics. Our approach demonstrates superior accuracy to the traditional
actuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through
simulation and hardware experiments across different jumping strategies.

</details>


### [6] [A tutorial note on collecting simulated data for vision-language-action models](https://arxiv.org/abs/2508.06547)
*Heran Wu,Zirun Zhou,Jingfeng Zhang*

Main category: cs.RO

TL;DR: VLA模型通过单一神经网络统一处理视觉、语言和动作，但依赖高质量数据集。本文介绍了三种代表性系统：PyBullet、LIBERO和RT-X。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统将智能分解为独立模块，而VLA模型通过统一框架实现多任务处理，但需要高质量数据集支持。

Method: 介绍了三种系统：PyBullet用于灵活数据生成，LIBERO用于标准化任务评估，RT-X用于大规模多机器人数据采集。

Result: 展示了PyBullet中的数据生成方法和LIBERO中的定制数据收集，并概述了RT-X数据集的特点和作用。

Conclusion: VLA模型依赖高质量数据集，PyBullet、LIBERO和RT-X为数据生成和评估提供了有效工具。

Abstract: Traditional robotic systems typically decompose intelligence into independent
modules for computer vision, natural language processing, and motion control.
Vision-Language-Action (VLA) models fundamentally transform this approach by
employing a single neural network that can simultaneously process visual
observations, understand human instructions, and directly output robot actions
-- all within a unified framework. However, these systems are highly dependent
on high-quality training datasets that can capture the complex relationships
between visual observations, language instructions, and robotic actions. This
tutorial reviews three representative systems: the PyBullet simulation
framework for flexible customized data generation, the LIBERO benchmark suite
for standardized task definition and evaluation, and the RT-X dataset
collection for large-scale multi-robot data acquisition. We demonstrated
dataset generation approaches in PyBullet simulation and customized data
collection within LIBERO, and provide an overview of the characteristics and
roles of the RT-X dataset for large-scale multi-robot data acquisition.

</details>


### [7] [Learning Causal Structure Distributions for Robust Planning](https://arxiv.org/abs/2508.06742)
*Alejandro Murillo-Gonzalez,Junhong Xu,Lantao Liu*

Main category: cs.RO

TL;DR: 论文提出了一种通过学习功能关系并考虑结构信息不确定性的方法，提高了机器人动力学模型的鲁棒性，同时减少了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统模型学习方法忽略了因果结构，未能利用机器人系统中交互的稀疏性。本文旨在通过结合结构信息的不确定性，提升模型的鲁棒性和效率。

Method: 通过估计因果结构分布，采样因果图以指导编码器-多解码器概率模型中的潜在空间表示。

Result: 模型能够学习机器人动力学，结合采样规划器完成新任务，并在仿真和实际环境中验证了其适应性和鲁棒性。

Conclusion: 该方法显著提升了机器人动力学模型的鲁棒性和适应性，适用于复杂现实场景。

Abstract: Structural causal models describe how the components of a robotic system
interact. They provide both structural and functional information about the
relationships that are present in the system. The structural information
outlines the variables among which there is interaction. The functional
information describes how such interactions work, via equations or learned
models. In this paper we find that learning the functional relationships while
accounting for the uncertainty about the structural information leads to more
robust dynamics models which improves downstream planning, while using
significantly lower computational resources. This in contrast with common
model-learning methods that ignore the causal structure and fail to leverage
the sparsity of interactions in robotic systems. We achieve this by estimating
a causal structure distribution that is used to sample causal graphs that
inform the latent-space representations in an encoder-multidecoder
probabilistic model. We show that our model can be used to learn the dynamics
of a robot, which together with a sampling-based planner can be used to perform
new tasks in novel environments, provided an objective function for the new
requirement is available. We validate our method using manipulators and mobile
robots in both simulation and the real-world. Additionally, we validate the
learned dynamics' adaptability and increased robustness to corrupted inputs and
changes in the environment, which is highly desirable in challenging real-world
robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.

</details>


### [8] [AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance](https://arxiv.org/abs/2508.06554)
*Abdelhaleem Saad,Waseem Akram,Irfan Hussain*

Main category: cs.RO

TL;DR: AquaChat++ 是一个基于大型语言模型的多 ROV 检查框架，用于提高水产养殖网箱检查的适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的水产养殖网箱检查方法适应性差，无法应对实时约束（如能耗、硬件故障和动态水下环境）。

Method: AquaChat++ 采用双层架构：高层计划生成层使用 LLM 将自然语言命令转换为多智能体检查计划；低层控制层确保轨迹跟踪和故障补偿。

Result: 模拟实验显示 AquaChat++ 提高了检查覆盖率、能效和故障恢复能力。

Conclusion: LLM 驱动的框架有望支持水产养殖领域的智能自主水下机器人操作。

Abstract: Inspection of aquaculture net pens is essential for ensuring the structural
integrity and sustainable operation of offshore fish farming systems.
Traditional methods, typically based on manually operated or single-ROV
systems, offer limited adaptability to real-time constraints such as energy
consumption, hardware faults, and dynamic underwater conditions. This paper
introduces AquaChat++, a novel multi-ROV inspection framework that uses Large
Language Models (LLMs) to enable adaptive mission planning, coordinated task
execution, and fault-tolerant control in complex aquaculture environments. The
proposed system consists of a two-layered architecture. The high-level plan
generation layer employs an LLM, such as ChatGPT-4, to translate natural
language user commands into symbolic, multi-agent inspection plans. A task
manager dynamically allocates and schedules actions among ROVs based on their
real-time status and operational constraints, including thruster faults and
battery levels. The low-level control layer ensures accurate trajectory
tracking and integrates thruster fault detection and compensation mechanisms.
By incorporating real-time feedback and event-triggered replanning, AquaChat++
enhances system robustness and operational efficiency. Simulated experiments in
a physics-based aquaculture environment demonstrate improved inspection
coverage, energy-efficient behavior, and resilience to actuator failures. These
findings highlight the potential of LLM-driven frameworks to support scalable,
intelligent, and autonomous underwater robotic operations within the
aquaculture sector.

</details>


### [9] [Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery](https://arxiv.org/abs/2508.06744)
*Yunke Ao,Manish Prajapat,Yarden As,Yassine Taoudi-Benchekroun,Fabio Carrillo,Hooman Esfandiari,Benjamin F. Grewe,Andreas Krause,Philipp Fürnstahl*

Main category: cs.RO

TL;DR: 论文提出了一种基于高维光学数据的安全关键控制方法，通过子高斯噪声模型处理估计误差，并结合MPC框架确保线性系统的闭环安全性，应用于机器人脊柱手术。


<details>
  <summary>Details</summary>
Motivation: 高维光学数据（如图像、点云）在自动驾驶和机器人手术等领域的安全控制面临挑战，传统概率模型难以捕捉复杂的估计误差分布，导致安全保证困难。

Method: 提出子高斯噪声模型描述估计误差，结合鲁棒集方法和子高斯方差传播技术，开发了具有安全保证的MPC框架。

Result: 在机器人脊柱手术的仿真环境中验证了方法的有效性，展示了其在复杂图像引导任务中的潜力。

Conclusion: 该方法为高维数据驱动的安全控制提供了新思路，尤其在医疗机器人领域具有应用前景。

Abstract: Safety-critical control using high-dimensional sensory feedback from optical
data (e.g., images, point clouds) poses significant challenges in domains like
autonomous driving and robotic surgery. Control can rely on low-dimensional
states estimated from high-dimensional data. However, the estimation errors
often follow complex, unknown distributions that standard probabilistic models
fail to capture, making formal safety guarantees challenging. In this work, we
introduce a novel characterization of these general estimation errors using
sub-Gaussian noise with bounded mean. We develop a new technique for
uncertainty propagation of proposed noise characterization in linear systems,
which combines robust set-based methods with the propagation of sub-Gaussian
variance proxies. We further develop a Model Predictive Control (MPC) framework
that provides closed-loop safety guarantees for linear systems under the
proposed noise assumption. We apply this MPC approach in an
ultrasound-image-guided robotic spinal surgery pipeline, which contains
deep-learning-based semantic segmentation, image-based registration, high-level
optimization-based planning, and low-level robotic control. To validate the
pipeline, we developed a realistic simulation environment integrating real
human anatomy, robot dynamics, efficient ultrasound simulation, as well as
in-vivo data of breathing motion and drilling force. Evaluation results in
simulation demonstrate the potential of our approach for solving complex
image-guided robotic surgery task while ensuring safety.

</details>


### [10] [Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control](https://arxiv.org/abs/2508.06568)
*Amin Yazdanshenas,Reza Faieghi*

Main category: cs.RO

TL;DR: 提出了一种新的自适应滑模控制框架，用于四旋翼飞行器，在计算资源受限下实现鲁棒和敏捷飞行。


<details>
  <summary>Details</summary>
Motivation: 解决现有滑模控制方法的局限性，如收敛慢、全局稳定性不足、旋转动力学简化、解绕问题和增益增长问题。

Method: 利用非光滑稳定性分析，设计了在S³上的姿态滑动动力学和位置滑动动力学，并证明了全局稳定性。

Result: 在硬件实验中，控制器在130多次飞行试验中表现优于基准方法，实现了高刷新率和低控制能耗。

Conclusion: 该控制器在外部干扰和计算资源受限下展现出高性能飞行控制的潜力。

Abstract: This paper presents a new adaptive sliding mode control (SMC) framework for
quadrotors that achieves robust and agile flight under tight computational
constraints. The proposed controller addresses key limitations of prior SMC
formulations, including (i) the slow convergence and almost-global stability of
$\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational
dynamics in Euler-based controllers, (iii) the unwinding phenomenon in
quaternion-based formulations, and (iv) the gain overgrowth problem in adaptive
SMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous
global stability proofs for both the nonsmooth attitude sliding dynamics
defined on $\mathbb{S}^3$ and the position sliding dynamics. Our controller is
computationally efficient and runs reliably on a resource-constrained nano
quadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude
control, respectively. In an extensive set of hardware experiments with over
130 flight trials, the proposed controller consistently outperforms three
benchmark methods, demonstrating superior trajectory tracking accuracy and
robustness with relatively low control effort. The controller enables
aggressive maneuvers such as dynamic throw launches, flip maneuvers, and
accelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.
These results highlight promising potential for real-world applications,
particularly in scenarios requiring robust, high-performance flight control
under significant external disturbances and tight computational constraints.

</details>


### [11] [From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline](https://arxiv.org/abs/2508.07045)
*Dennis Benders,Johannes Köhler,Robert Babuška,Javier Alonso-Mora,Laura Ferranti*

Main category: cs.RO

TL;DR: 提出了一种模块化鲁棒MPC设计流程，通过闭环实验数据估计扰动边界，确保无人机导航中的安全性和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有MPC方法在真实环境中因扰动和噪声难以保证安全性，通常依赖理想假设或启发式猜测。

Method: 采用迭代流程，利用闭环实验数据估计扰动边界，并设计鲁棒输出反馈MPC方案。

Result: 在Gazebo仿真中验证了鲁棒约束满足和递归可行性。

Conclusion: 该流程高效、模块化，能系统解决现有MPC方法的局限性。

Abstract: Model predictive control (MPC) is a powerful strategy for planning and
control in autonomous mobile robot navigation. However, ensuring safety in
real-world deployments remains challenging due to the presence of disturbances
and measurement noise. Existing approaches often rely on idealized assumptions,
neglect the impact of noisy measurements, and simply heuristically guess
unrealistic bounds. In this work, we present an efficient and modular robust
MPC design pipeline that systematically addresses these limitations. The
pipeline consists of an iterative procedure that leverages closed-loop
experimental data to estimate disturbance bounds and synthesize a robust
output-feedback MPC scheme. We provide the pipeline in the form of
deterministic and reproducible code to synthesize the robust output-feedback
MPC from data. We empirically demonstrate robust constraint satisfaction and
recursive feasibility in quadrotor simulations using Gazebo.

</details>


### [12] [Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios](https://arxiv.org/abs/2508.06575)
*Rui Zhou*

Main category: cs.RO

TL;DR: 该研究提出了一种加速测试算法（ALVNS-SA），用于验证自动驾驶车辆在安全关键场景中的安全性，显著提高了测试效率和场景覆盖率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的安全性至关重要，尤其是在安全关键场景中，需要高效的测试方法来验证其驾驶能力。

Method: 研究从真实事故数据库中提取典型逻辑场景，结合百度Apollo自动驾驶系统，并提出了ALVNS-SA算法以加速测试过程。

Result: ALVNS-SA算法在安全关键场景中实现了84.00%的覆盖率，其中碰撞场景覆盖率为96.83%，接近碰撞场景覆盖率为92.07%，显著优于其他方法。

Conclusion: ALVNS-SA算法在测试效率和场景覆盖率方面表现出色，为自动驾驶车辆的安全验证提供了有效工具。

Abstract: Ensuring the safety of autonomous vehicles (AVs) is paramount in their
development and deployment. Safety-critical scenarios pose more severe
challenges, necessitating efficient testing methods to validate AVs safety.
This study focuses on designing an accelerated testing algorithm for AVs in
safety-critical scenarios, enabling swift recognition of their driving
capabilities. First, typical logical scenarios were extracted from real-world
crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)
database, obtaining pre-crash features through reconstruction. Second, Baidu
Apollo, an advanced black-box automated driving system (ADS) is integrated to
control the behavior of the ego vehicle. Third, we proposed an adaptive
large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to
expedite the testing process. Experimental results demonstrate a significant
enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an
84.00% coverage of safety-critical scenarios, with crash scenario coverage of
96.83% and near-crash scenario coverage of 92.07%. Compared to genetic
algorithm (GA), adaptive large neighborhood-simulated annealing algorithm
(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage
in safety-critical scenarios.

</details>


### [13] [Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction](https://arxiv.org/abs/2508.07079)
*Mohamed Parvez Aslam,Bojan Derajic,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Jan Oliver Ringert*

Main category: cs.RO

TL;DR: 论文研究了在行人密集环境中，将基于深度学习的Social-Implicit（SI）行人轨迹预测器与模型预测控制（MPC）框架结合，以提升自主机器人的导航安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决自主机器人在行人密集环境中的安全导航问题，传统方法（如恒定速度模型）在复杂场景中表现不佳。

Method: 将SI行人轨迹预测器集成到MPC框架中，并在物理机器人上测试，对比传统CV模型的开环预测和闭环导航性能。

Result: SI模型在低密度场景中减少轨迹预测误差达76%，在拥挤场景中提升安全性和运动平滑性。

Conclusion: SI-MPC框架在动态行人环境中展现出更高的安全性和适应性，强调系统级评估的重要性。

Abstract: Safe navigation in pedestrian-rich environments remains a key challenge for
autonomous robots. This work evaluates the integration of a deep learning-based
Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive
Control (MPC) framework on the physical Continental Corriere robot. Tested
across varied pedestrian densities, the SI-MPC system is compared to a
traditional Constant Velocity (CV) model in both open-loop prediction and
closed-loop navigation. Results show that SI improves trajectory prediction -
reducing errors by up to 76% in low-density settings - and enhances safety and
motion smoothness in crowded scenes. Moreover, real-world deployment reveals
discrepancies between open-loop metrics and closed-loop performance, as the SI
model yields broader, more cautious predictions. These findings emphasize the
importance of system-level evaluation and highlight the SI-MPC framework's
promise for safer, more adaptive navigation in dynamic, human-populated
environments.

</details>


### [14] [Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation](https://arxiv.org/abs/2508.06687)
*Sreeja Roy-Singh,Vinay Ravindra,Richard Levinson,Mahta Moghaddam,Jan Mandel,Adam Kochanski,Angel Farguell Caus,Kurtis Nelson,Samira Alkaee Taleghan,Archana Kannan,Amer Melebari*

Main category: cs.RO

TL;DR: 提出一种结合最优规划和机器学习的方法，用于收集和处理太空数据以监测野火，提升现有决策支持工具的效率。


<details>
  <summary>Details</summary>
Motivation: 通过优化数据收集和处理流程，提高野火监测和预测的准确性和时效性，支持消防员的实时决策。

Method: 采用混合整数规划调度卫星观测和数据下传，结合机器学习预测野火，生成高分辨率数据产品。

Result: 优化方案能捕获98-100%的观测机会，机器学习预测准确率提升40%，数据整合使预测准确率再提升13-15%。

Conclusion: 该方法显著提升了野火监测的效率和准确性，具有计算可扩展性和全球适用性，适合实时应用。

Abstract: We propose a novel concept of operations using optimal planning methods and
machine learning (ML) to collect spaceborne data that is unprecedented for
monitoring wildfires, process it to create new or enhanced products in the
context of wildfire danger or spread monitoring, and assimilate them to improve
existing, wildfire decision support tools delivered to firefighters within
latency appropriate for time-critical applications. The concept is studied with
respect to NASA's CYGNSS Mission, a constellation of passive microwave
receivers that measure specular GNSS-R reflections despite clouds and smoke.
Our planner uses a Mixed Integer Program formulation to schedule joint
observation data collection and downlink for all satellites. Optimal solutions
are found quickly that collect 98-100% of available observation opportunities.
ML-based fire predictions that drive the planner objective are greater than 40%
more correlated with ground truth than existing state-of-art. The presented
case study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025
represents the first high-resolution data collected by CYGNSS of active fires.
Creation of Burnt Area Maps (BAM) using ML applied to the data during active
fires and BAM assimilation into NASA's Weather Research and Forecasting Model
using ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained
soil moisture are integrated for the first time into USGS fire danger maps.
Inclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,
and inclusion of high-resolution data boosts ML recall by another 15%. The
proposed workflow has an expected latency of 6-30h, improving on the current
delivery time of multiple days. All components in the proposed concept are
shown to be computationally scalable and globally generalizable, with
sustainability considerations such as edge efficiency and low latency on small
devices.

</details>


### [15] [Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)](https://arxiv.org/abs/2508.07323)
*Adeetya Uppal,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: 提出了一种基于能量的APF框架（E-APF），结合位置和速度依赖的势函数，解决了传统APF的局部最小值和振荡问题，并通过混合轨迹优化器实现了平滑且时间高效的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 动态和复杂环境中机器人轨迹规划面临时间效率和运动平滑性的挑战，传统APF方法存在局部最小值和振荡问题。

Method: 提出E-APF框架，结合位置和速度依赖的势函数，并与混合轨迹优化器联合优化，最小化急动和执行时间。

Result: 在7自由度Kinova Gen3机械臂仿真中验证，实现了无碰撞、平滑、高效且无振荡的轨迹。

Conclusion: E-APF框架为未来与反应控制策略和实际硬件部署的集成奠定了基础。

Abstract: Robotic trajectory planning in dynamic and cluttered environments remains a
critical challenge, particularly when striving for both time efficiency and
motion smoothness under actuation constraints. Traditional path planner, such
as Artificial Potential Field (APF), offer computational efficiency but suffer
from local minima issue due to position-based potential field functions and
oscillatory motion near the obstacles due to Newtonian mechanics. To address
this limitation, an Energy-based Artificial Potential Field (APF) framework is
proposed in this paper that integrates position and velocity-dependent
potential functions. E-APF ensures dynamic adaptability and mitigates local
minima, enabling uninterrupted progression toward the goal. The proposed
framework integrates E-APF with a hybrid trajectory optimizer that jointly
minimizes jerk and execution time under velocity and acceleration constraints,
ensuring geometric smoothness and time efficiency. The entire framework is
validated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic
manipulator. The results demonstrate collision-free, smooth, time-efficient,
and oscillation-free trajectory in the presence of obstacles, highlighting the
efficacy of the combined trajectory optimization and real-time obstacle
avoidance approach. This work lays the foundation for future integration with
reactive control strategies and physical hardware deployment in real-world
manipulation tasks.

</details>


### [16] [Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC](https://arxiv.org/abs/2508.06722)
*Justin London*

Main category: cs.RO

TL;DR: ORCA-FL通过模糊逻辑控制器改进ORCA算法，以更好地处理路径规划中的不确定性和不精确性，在多智能体实验中表现优于ORCA。


<details>
  <summary>Details</summary>
Motivation: 现有避障算法（如DWA、TEB、RVO）存在路径次优、计算成本高或动态障碍适应性差的问题，ORCA虽改进RVO但仍需提升。

Method: 提出ORCA-FL，利用模糊逻辑控制器（FLCs）优化ORCA，并进一步通过模糊Q强化学习（FQL）调优FLCs。

Result: 实验表明，ORCA-FL在智能体速度超过阈值时能减少碰撞次数，优于ORCA。

Conclusion: ORCA-FL结合模糊逻辑和强化学习，显著提升了动态多智能体环境中的避障性能。

Abstract: Obstacle avoidance enables autonomous agents and robots to operate safely and
efficiently in dynamic and complex environments, reducing the risk of
collisions and damage. For a robot or autonomous system to successfully
navigate through obstacles, it must be able to detect such obstacles. While
numerous collision avoidance algorithms like the dynamic window approach (DWA),
timed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been
proposed, they may lead to suboptimal paths due to fixed weights, be
computationally expensive, or have limited adaptability to dynamic obstacles in
multi-agent environments. Optimal reciprocal collision avoidance (ORCA), which
improves on RVO, provides smoother trajectories and stronger collision
avoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy
logic controllers (FLCs) to better handle uncertainty and imprecision for
obstacle avoidance in path planning. Numerous multi-agent experiments are
conducted and it is shown that ORCA-FL can outperform ORCA in reducing the
number of collision if the agent has a velocity that exceeds a certain
threshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy
Q reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.

</details>


### [17] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: 论文提出了一种基于AI的感知系统，用于GPS缺失的室内环境中自主四轴飞行器导航，结合云计算和定制PCB，实现了高效导航和避障。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失环境下四轴飞行器的自主导航问题，提升在狭小空间中的感知和决策能力。

Method: 系统整合了YOLOv11目标检测、Depth Anything V2深度估计、定制PCB（含ToF传感器和IMU）、云端LLM决策，以及虚拟安全边界和多线程架构。

Result: 实验显示，目标检测mAP50为0.6，深度估计MAE为7.2 cm，42次试验中仅16次安全边界突破，系统延迟低于1秒。

Conclusion: 该框架为GPS缺失环境下的无人机导航提供了高效辅助感知系统，补充了现有技术的不足。

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [18] [COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models](https://arxiv.org/abs/2508.08144)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.RO

TL;DR: 本文提出了一种基于组件感知结构化剪枝的模型压缩方法，用于在资源受限设备上部署神经网络控制器（NNCs），同时保持稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 随着资源受限移动平台的快速发展，对高效神经网络控制器的需求增加，但传统深度神经网络的计算复杂性和内存需求限制了其实际应用。

Method: 采用组件感知结构化剪枝方法，结合数学稳定性保证（如Lyapunov准则），确定最优剪枝幅度，并在TD-MPC算法上进行验证。

Result: 实验表明，该方法在降低模型复杂度的同时保持了控制性能和稳定性，并确定了安全压缩比的理论边界。

Conclusion: 该方法为资源受限环境下压缩NNCs的部署提供了理论框架和实践指导。

Abstract: The rapid growth of resource-constrained mobile platforms, including mobile
robots, wearable systems, and Internet-of-Things devices, has increased the
demand for computationally efficient neural network controllers (NNCs) that can
operate within strict hardware limitations. While deep neural networks (DNNs)
demonstrate superior performance in control applications, their substantial
computational complexity and memory requirements present significant barriers
to practical deployment on edge devices. This paper introduces a comprehensive
model compression methodology that leverages component-aware structured pruning
to determine the optimal pruning magnitude for each pruning group, ensuring a
balance between compression and stability for NNC deployment. Our approach is
rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),
a state-of-the-art model-based reinforcement learning algorithm, with a
systematic integration of mathematical stability guarantee properties,
specifically Lyapunov criteria. The key contribution of this work lies in
providing a principled framework for determining the theoretical limits of
model compression while preserving controller stability. Experimental
validation demonstrates that our methodology successfully reduces model
complexity while maintaining requisite control performance and stability
characteristics. Furthermore, our approach establishes a quantitative boundary
for safe compression ratios, enabling practitioners to systematically determine
the maximum permissible model reduction before violating critical stability
properties, thereby facilitating the confident deployment of compressed NNCs in
resource-limited environments.

</details>


### [19] [Learning a Vision-Based Footstep Planner for Hierarchical Walking Control](https://arxiv.org/abs/2508.06779)
*Minku Kim,Brian Acosta,Pratik Chaudhari,Michael Posa*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的分层控制框架，结合强化学习的高层脚步规划器和低层操作空间控制器，以解决双足机器人在非结构化环境中的实时脚步规划问题。


<details>
  <summary>Details</summary>
Motivation: 当前的双足机器人框架依赖本体感知或手动设计的视觉管道，在现实环境中脆弱且难以实时规划脚步。

Method: 采用基于强化学习的高层脚步规划器生成脚步命令，结合低层操作空间控制器跟踪轨迹，并使用角动量线性倒立摆模型简化状态表示。

Result: 在双足机器人Cassie上评估了不同地形条件下的性能，并通过仿真和硬件实验验证了方法的有效性。

Conclusion: 该框架在非结构化环境中表现出潜力，但仍需进一步解决实际挑战。

Abstract: Bipedal robots demonstrate potential in navigating challenging terrains
through dynamic ground contact. However, current frameworks often depend solely
on proprioception or use manually designed visual pipelines, which are fragile
in real-world settings and complicate real-time footstep planning in
unstructured environments. To address this problem, we present a vision-based
hierarchical control framework that integrates a reinforcement learning
high-level footstep planner, which generates footstep commands based on a local
elevation map, with a low-level Operational Space Controller that tracks the
generated trajectories. We utilize the Angular Momentum Linear Inverted
Pendulum model to construct a low-dimensional state representation to capture
an informative encoding of the dynamics while reducing complexity. We evaluate
our method across different terrain conditions using the underactuated bipedal
robot Cassie and investigate the capabilities and challenges of our approach
through simulation and hardware experiments.

</details>


### [20] [D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning](https://arxiv.org/abs/2508.06804)
*Shu-Ang Yu,Feng Gao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: D3P是一种动态去噪扩散策略，通过自适应分配去噪步骤提升实时性能，在保持任务成功率的同时显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有动作采用固定去噪步骤，忽略了动作对任务成功的重要性差异。实验表明，机器人任务中动作分为关键和常规两类，影响不同。

Method: 提出D3P，通过轻量级状态感知适配器动态分配去噪步骤，并结合强化学习联合优化适配器和基础扩散策略。

Result: 在模拟任务中，D3P平均加速2.2倍且成功率不降；在物理机器人上加速1.9倍。

Conclusion: D3P通过动态去噪步骤分配，显著提升推理效率，适用于实时机器人任务。

Abstract: Diffusion policies excel at learning complex action distributions for robotic
visuomotor tasks, yet their iterative denoising process poses a major
bottleneck for real-time deployment. Existing acceleration methods apply a
fixed number of denoising steps per action, implicitly treating all actions as
equally important. However, our experiments reveal that robotic tasks often
contain a mix of \emph{crucial} and \emph{routine} actions, which differ in
their impact on task success. Motivated by this finding, we propose
\textbf{D}ynamic \textbf{D}enoising \textbf{D}iffusion \textbf{P}olicy
\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising
steps across actions at test time. D3P uses a lightweight, state-aware adaptor
to allocate the optimal number of denoising steps for each action. We jointly
optimize the adaptor and base diffusion policy via reinforcement learning to
balance task performance and inference efficiency. On simulated tasks, D3P
achieves an averaged 2.2$\times$ inference speed-up over baselines without
degrading success. Furthermore, we demonstrate D3P's effectiveness on a
physical robot, achieving a 1.9$\times$ acceleration over the baseline.

</details>


### [21] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: 提出一种通过周期性振动针头来恢复超声成像平面与针插入平面对齐的方法，即使在针头完全不可见时仍有效。


<details>
  <summary>Details</summary>
Motivation: 在机器人超声引导的针插入过程中，针的精确对齐至关重要，但超声图像中的噪声和低分辨率等问题使得针的检测困难。

Method: 通过机械系统周期性振动针头，提出一种基于振动的能量度量，并开发控制策略以调整超声探头位置。

Result: 实验显示，平移误差为0.41±0.27 mm，旋转误差为0.51±0.19度。

Conclusion: 该方法在针头不可见时仍能有效恢复对齐，具有较高的精确性和鲁棒性。

Abstract: Precise needle alignment is essential for percutaneous needle insertion in
robotic ultrasound-guided procedures. However, inherent challenges such as
speckle noise, needle-like artifacts, and low image resolution make robust
needle detection difficult, particularly when visibility is reduced or lost. In
this paper, we propose a method to restore needle alignment when the ultrasound
imaging plane and the needle insertion plane are misaligned. Unlike many
existing approaches that rely heavily on needle visibility in ultrasound
images, our method uses a more robust feature by periodically vibrating the
needle using a mechanical system. Specifically, we propose a vibration-based
energy metric that remains effective even when the needle is fully out of
plane. Using this metric, we develop a control strategy to reposition the
ultrasound probe in response to misalignments between the imaging plane and the
needle insertion plane in both translation and rotation. Experiments conducted
on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided
needle insertion system demonstrate the effectiveness of the proposed approach.
The experimental results show the translational error of 0.41$\pm$0.27 mm and
the rotational error of 0.51$\pm$0.19 degrees.

</details>


### [22] [Manipulator for people with limited abilities](https://arxiv.org/abs/2508.06969)
*Bingkun Huang,Evgeniy Kotov,Arkady Yuschenko*

Main category: cs.RO

TL;DR: 开发一种四自由度机械手，用于辅助残障人士，结合机械设计、控制系统、视觉系统和ROS软件。


<details>
  <summary>Details</summary>
Motivation: 机器人技术的发展为改善残障人士生活质量提供了新机会，设计适配的机械手是重要挑战。

Method: 综合设计机械结构、开发控制系统，并集成视觉系统和ROS软件。

Result: 提出了一种适合实际操作的机械手设计方案。

Conclusion: 该研究为残障人士辅助设备的发展提供了实用解决方案。

Abstract: The topic of this final qualification work was chosen due to the importance
of developing robotic systems designed to assist people with disabilities.
Advances in robotics and automation technologies have opened up new prospects
for creating devices that can significantly improve the quality of life for
these people. In this context, designing a robotic hand with a control system
adapted to the needs of people with disabilities is a major scientific and
practical challenge. This work addresses the problem of developing and
manufacturing a four-degree-of-freedom robotic hand suitable for practical
manipulation. Addressing this issue requires a comprehensive approach,
encompassing the design of the hand's mechanical structure, the development of
its control system, and its integration with a technical vision system and
software based on the Robot Operating System (ROS).

</details>


### [23] [Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation](https://arxiv.org/abs/2508.06990)
*Yue Hu,Junzhe Wu,Ruihan Xu,Hang Liu,Avery Xi,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: SGImagineNav是一种新颖的导航框架，通过符号世界建模和场景图预测未来场景，提升语义导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖过去观察，缺乏对未来场景的预测能力，限制了导航效率。

Method: SGImagineNav利用分层场景图和大型语言模型预测环境未知部分，采用自适应导航策略。

Result: 在HM3D和HSSD基准测试中，成功率分别提升至65.4%和66.8%，并在真实环境中展示了跨楼层和跨房间导航能力。

Conclusion: SGImagineNav通过主动预测和自适应策略，显著提升了语义导航的成功率和通用性。

Abstract: Semantic navigation requires an agent to navigate toward a specified target
in an unseen environment. Employing an imaginative navigation strategy that
predicts future scenes before taking action, can empower the agent to find
target faster. Inspired by this idea, we propose SGImagineNav, a novel
imaginative navigation framework that leverages symbolic world modeling to
proactively build a global environmental representation. SGImagineNav maintains
an evolving hierarchical scene graphs and uses large language models to predict
and explore unseen parts of the environment. While existing methods solely
relying on past observations, this imaginative scene graph provides richer
semantic context, enabling the agent to proactively estimate target locations.
Building upon this, SGImagineNav adopts an adaptive navigation strategy that
exploits semantic shortcuts when promising and explores unknown areas otherwise
to gather additional context. This strategy continuously expands the known
environment and accumulates valuable semantic contexts, ultimately guiding the
agent toward the target. SGImagineNav is evaluated in both real-world scenarios
and simulation benchmarks. SGImagineNav consistently outperforms previous
methods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and
demonstrating cross-floor and cross-room navigation in real-world environments,
underscoring its effectiveness and generalizability.

</details>


### [24] [EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events](https://arxiv.org/abs/2508.07003)
*Siyu Chen,Shenghai Yuan,Thien-Minh Nguyen,Zhuyu Huang,Chenyang Shi,Jin Jing,Lihua Xie*

Main category: cs.RO

TL;DR: EGS-SLAM通过融合事件数据和RGB-D输入，解决了传统GS-SLAM在运动模糊下的性能问题，提升了跟踪精度和3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有GS-SLAM系统在严重运动模糊下表现不佳，导致跟踪和重建质量下降。

Method: 提出EGS-SLAM框架，结合事件数据和RGB-D输入，建模相机连续轨迹，引入可学习的相机响应函数和无事件损失。

Result: 在合成和真实数据集上验证，EGS-SLAM在轨迹精度和3D重建质量上优于现有GS-SLAM系统。

Conclusion: EGS-SLAM显著提升了在运动模糊场景下的SLAM性能，具有实际应用潜力。

Abstract: Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over
traditional SLAM methods, enabling photorealistic 3D reconstruction that
conventional approaches often struggle to achieve. However, existing GS-SLAM
systems perform poorly under persistent and severe motion blur commonly
encountered in real-world scenarios, leading to significantly degraded tracking
accuracy and compromised 3D reconstruction quality. To address this limitation,
we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D
inputs to simultaneously reduce motion blur in images and compensate for the
sparse and discrete nature of event streams, enabling robust tracking and
high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system
explicitly models the camera's continuous trajectory during exposure,
supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian
Splatting scene. Furthermore, we introduce a learnable camera response function
to align the dynamic ranges of events and images, along with a no-event loss to
suppress ringing artifacts during reconstruction. We validate our approach on a
new dataset comprising synthetic and real-world sequences with significant
motion blur. Extensive experimental results demonstrate that EGS-SLAM
consistently outperforms existing GS-SLAM systems in both trajectory accuracy
and photorealistic 3D Gaussian Splatting reconstruction. The source code will
be available at https://github.com/Chensiyu00/EGS-SLAM.

</details>


### [25] [$\mathcal{P}^3$: Toward Versatile Embodied Agents](https://arxiv.org/abs/2508.07033)
*Shengli Zhou,Xiangchen Wang,Jinrui Zhang,Ruozai Tian,Rongtao Xu,Feng Zheng*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Embodied agents have shown promising generalization capabilities across
diverse physical environments, making them essential for a wide range of
real-world applications. However, building versatile embodied agents poses
critical challenges due to three key issues: dynamic environment perception,
open-ended tool usage, and complex multi-task planning. Most previous works
rely solely on feedback from tool agents to perceive environmental changes and
task status, which limits adaptability to real-time dynamics, causes error
accumulation, and restricts tool flexibility. Furthermore, multi-task
scheduling has received limited attention, primarily due to the inherent
complexity of managing task dependencies and balancing competing priorities in
dynamic and complex environments. To overcome these challenges, we introduce
$\mathcal{P}^3$, a unified framework that integrates real-time perception and
dynamic scheduling. Specifically, $\mathcal{P}^3$ enables 1) \textbf Perceive
relevant task information actively from the environment, 2) \textbf Plug and
utilize any tool without feedback requirement, and 3) \textbf Plan multi-task
execution based on prioritizing urgent tasks and dynamically adjusting task
order based on dependencies. Extensive real-world experiments show that our
approach bridges the gap between benchmarks and practical deployment,
delivering highly transferable, general-purpose embodied agents. Code and data
will be released soon.

</details>


### [26] [An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving](https://arxiv.org/abs/2508.07080)
*Haolin Liu,Zijun Guo,Yanbo Chen,Jiaqi Chen,Huilong Yu,Junqiang Xi*

Main category: cs.RO

TL;DR: 论文提出了一种基于进化博弈论（EGT）的决策框架，用于解决自动驾驶车辆在高速匝道合流时的动态复杂性和社会接受度问题。


<details>
  <summary>Details</summary>
Motivation: 现有决策算法未能充分应对动态复杂性和社会接受度，导致合流决策次优或不安全。

Method: 采用进化博弈论框架，结合多目标收益函数和实时驾驶风格估计算法，动态优化合流时机。

Result: 实验表明，该方法在效率、舒适性和安全性上优于现有博弈论和传统规划方法。

Conclusion: 提出的EGT框架有效平衡了自动驾驶车辆和主路车辆的需求，提升了合流决策的整体性能。

Abstract: Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),
since they have to proactively interact with surrounding vehicles to enter the
main road safely within limited time. However, existing decision-making
algorithms fail to adequately address dynamic complexities and social
acceptance of AVs, leading to suboptimal or unsafe merging decisions. To
address this, we propose an evolutionary game-theoretic (EGT) merging
decision-making framework, grounded in the bounded rationality of human
drivers, which dynamically balances the benefits of both AVs and main-road
vehicles (MVs). We formulate the cut-in decision-making process as an EGT
problem with a multi-objective payoff function that reflects human-like driving
preferences. By solving the replicator dynamic equation for the evolutionarily
stable strategy (ESS), the optimal cut-in timing is derived, balancing
efficiency, comfort, and safety for both AVs and MVs. A real-time driving style
estimation algorithm is proposed to adjust the game payoff function online by
observing the immediate reactions of MVs. Empirical results demonstrate that we
improve the efficiency, comfort and safety of both AVs and MVs compared with
existing game-theoretic and traditional planning approaches across multi-object
metrics.

</details>


### [27] [DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit](https://arxiv.org/abs/2508.07118)
*Aiden Swann,Alex Qiu,Matthew Strong,Angelina Zhang,Samuel Morstein,Kai Rayle,Monroe Kennedy III*

Main category: cs.RO

TL;DR: DexFruit是一个机器人操作框架，通过光学触觉感知实现脆弱水果的自主轻柔处理和损伤评估，显著减少损伤并提高成功率。


<details>
  <summary>Details</summary>
Motivation: 许多水果易碎且容易碰伤，需人工小心采摘，因此需要开发自动化解决方案以减少损伤。

Method: 使用光学触觉感知和触觉信息扩散策略，结合FruitSplat技术（基于3D高斯溅射的高分辨率3D损伤表示）。

Result: 在草莓、番茄和黑莓上，实现了92%的抓取成功率，视觉损伤减少20%，抓取成功率提升31%。

Conclusion: DexFruit框架在减少水果损伤和提高操作成功率方面表现优异，具有实际应用潜力。

Abstract: DexFruit is a robotic manipulation framework that enables gentle, autonomous
handling of fragile fruit and precise evaluation of damage. Many fruits are
fragile and prone to bruising, thus requiring humans to manually harvest them
with care. In this work, we demonstrate by using optical tactile sensing,
autonomous manipulation of fruit with minimal damage can be achieved. We show
that our tactile informed diffusion policies outperform baselines in both
reduced bruising and pick-and-place success rate across three fruits:
strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,
a novel technique to represent and quantify visual damage in high-resolution 3D
representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring
damage lack quantitative rigor or require expensive equipment. With FruitSplat,
we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into
the 3DGS representation. Furthermore, this representation is modular and
general, compatible with any relevant 2D model. Overall, we demonstrate a 92%
grasping policy success rate, up to a 20% reduction in visual bruising, and up
to an 31% improvement in grasp success rate on challenging fruit compared to
our baselines across our three tested fruits. We rigorously evaluate this
result with over 630 trials. Please checkout our website at
https://dex-fruit.github.io .

</details>


### [28] [Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey](https://arxiv.org/abs/2508.07163)
*Kamal Acharya,Iman Sharifi,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.RO

TL;DR: 神经符号AI结合神经网络适应性和符号推理，为高级空中交通（AAM）的复杂挑战提供解决方案。本文综述了其在需求预测、飞机设计和实时空中交通管理等领域的应用，揭示了研究碎片化问题，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高级空中交通（AAM）面临复杂的监管、运营和安全挑战，神经符号AI有望通过结合神经网络和符号推理的优势来解决这些问题。

Method: 本文综述了神经符号AI在AAM中的应用，包括神经符号强化学习等方法，并分析了其动态优化潜力及当前面临的挑战。

Result: 研究发现神经符号AI在AAM中显示出潜力，但仍需解决可扩展性、鲁棒性和航空标准合规性等问题。

Conclusion: 本文为研究人员和从业者提供了整合神经符号AI到可靠、透明的AAM系统中的路线图，并指明了未来研究方向。

Abstract: Neurosymbolic AI combines neural network adaptability with symbolic
reasoning, promising an approach to address the complex regulatory,
operational, and safety challenges in Advanced Air Mobility (AAM). This survey
reviews its applications across key AAM domains such as demand forecasting,
aircraft design, and real-time air traffic management. Our analysis reveals a
fragmented research landscape where methodologies, including Neurosymbolic
Reinforcement Learning, have shown potential for dynamic optimization but still
face hurdles in scalability, robustness, and compliance with aviation
standards. We classify current advancements, present relevant case studies, and
outline future research directions aimed at integrating these approaches into
reliable, transparent AAM systems. By linking advanced AI techniques with AAM's
operational demands, this work provides a concise roadmap for researchers and
practitioners developing next-generation air mobility solutions.

</details>


### [29] [3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07182)
*Xuesong Li,Lars Petersson,Vivien Rolland*

Main category: cs.RO

TL;DR: 提出了一种结合3D高斯泼溅与运动轨迹场的新方法，用于动态场景的新视角合成和运动重建。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频中重建动态场景的挑战，尤其是复杂物体运动的精确处理。

Method: 结合3D高斯泼溅与运动轨迹场，解耦动态物体与静态背景，优化运动轨迹场。

Result: 在单目视频的新视角合成和运动轨迹恢复方面取得了最先进的结果。

Conclusion: 该方法提升了动态场景重建的能力，实现了物理上合理的运动轨迹。

Abstract: This paper addresses the challenge of novel-view synthesis and motion
reconstruction of dynamic scenes from monocular video, which is critical for
many robotic applications. Although Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering
static scenes, extending them to reconstruct dynamic scenes remains
challenging. In this work, we introduce a novel approach that combines 3DGS
with a motion trajectory field, enabling precise handling of complex object
motions and achieving physically plausible motion trajectories. By decoupling
dynamic objects from static background, our method compactly optimizes the
motion trajectory field. The approach incorporates time-invariant motion
coefficients and shared motion trajectory bases to capture intricate motion
patterns while minimizing optimization complexity. Extensive experiments
demonstrate that our approach achieves state-of-the-art results in both
novel-view synthesis and motion trajectory recovery from monocular video,
advancing the capabilities of dynamic scene reconstruction.

</details>


### [30] [Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks](https://arxiv.org/abs/2508.07244)
*Ayesha Jena,Stefan Reitmann,Elin Anna Topp*

Main category: cs.RO

TL;DR: 研究分析了基于头部注视的机器人控制和焦点视觉增强在模拟搜救任务中的应用，发现焦点增强显著提升任务表现并降低认知负荷。


<details>
  <summary>Details</summary>
Motivation: 探索头部注视和焦点视觉增强在搜救任务中的效果，以优化人机交互。

Method: 通过用户研究，分析头部注视模式和焦点视觉增强对任务表现的影响。

Result: 焦点增强显著提升任务表现，降低认知负荷38%，缩短任务时间60%以上。

Conclusion: 焦点增强技术潜力巨大，需进一步研究注视模式以优化关键任务表现。

Abstract: We present a user study analyzing head-gaze-based robot control and foveated
visual augmentation in a simulated search-and-rescue task. Results show that
foveated augmentation significantly improves task performance, reduces
cognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns
analysed over both the entire task duration and shorter time segments show that
near and far attention capture is essential to better understand user intention
in critical scenarios. Our findings highlight the potential of foveation as an
augmentation technique and the need to further study gaze measures to leverage
them during critical tasks.

</details>


### [31] [Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics](https://arxiv.org/abs/2508.07267)
*Daria de Tinguy,Tim Verbelen,Emilio Gamba,Bart Dhoedt*

Main category: cs.RO

TL;DR: 本文提出了一种基于主动推理框架（AIF）的生物启发智能体，用于自主导航，无需预训练即可实时构建环境拓扑图并规划目标导向轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有自主导航方法依赖严格规则或预训练，适应性差且计算量大，难以应对动态或未知环境。

Method: 采用AIF框架，结合概率推理和模块化ROS2架构，实现实时拓扑图构建和自适应决策。

Result: 在仿真和真实环境中测试，智能体成功探索大规模环境并适应动态障碍，性能与现有方法相当。

Conclusion: 该方法为复杂非结构化环境提供了一种可扩展且透明的导航解决方案。

Abstract: Achieving fully autonomous exploration and navigation remains a critical
challenge in robotics, requiring integrated solutions for localisation,
mapping, decision-making and motion planning. Existing approaches either rely
on strict navigation rules lacking adaptability or on pre-training, which
requires large datasets. These AI methods are often computationally intensive
or based on static assumptions, limiting their adaptability in dynamic or
unknown environments. This paper introduces a bio-inspired agent based on the
Active Inference Framework (AIF), which unifies mapping, localisation, and
adaptive decision-making for autonomous navigation, including exploration and
goal-reaching. Our model creates and updates a topological map of the
environment in real-time, planning goal-directed trajectories to explore or
reach objectives without requiring pre-training. Key contributions include a
probabilistic reasoning framework for interpretable navigation, robust
adaptability to dynamic changes, and a modular ROS2 architecture compatible
with existing navigation systems. Our method was tested in simulated and
real-world environments. The agent successfully explores large-scale simulated
environments and adapts to dynamic obstacles and drift, proving to be
comparable to other exploration strategies such as Gbplanner, FAEL and
Frontiers. This approach offers a scalable and transparent approach for
navigating complex, unstructured environments.

</details>


### [32] [Navigation and Exploration with Active Inference: from Biology to Industry](https://arxiv.org/abs/2508.07269)
*Daria de Tinguy,Tim Verbelen,Bart Dhoedt*

Main category: cs.RO

TL;DR: 论文提出了一种基于主动推理框架（AIF）的实时机器人导航系统，模仿动物认知地图构建能力，无需预先训练即可在复杂动态环境中导航。


<details>
  <summary>Details</summary>
Motivation: 受动物通过构建认知地图实现高效导航的生物机制启发，旨在开发一种无需训练的机器人导航系统。

Method: 采用主动推理框架，逐步构建拓扑地图，推断位置并通过最小化预期不确定性和实现感知目标来规划动作。

Result: 在ROS2生态系统中验证了系统在2D和3D环境（模拟和现实）中的适应性和高效性，性能与传统及前沿方法相当。

Conclusion: 该系统提供了一种生物启发的导航方法，展示了在复杂动态环境中的竞争力和潜力。

Abstract: By building and updating internal cognitive maps, animals exhibit
extraordinary navigation abilities in complex, dynamic environments. Inspired
by these biological mechanisms, we present a real time robotic navigation
system grounded in the Active Inference Framework (AIF). Our model
incrementally constructs a topological map, infers the agent's location, and
plans actions by minimising expected uncertainty and fulfilling perceptual
goals without any prior training. Integrated into the ROS2 ecosystem, we
validate its adaptability and efficiency across both 2D and 3D environments
(simulated and real world), demonstrating competitive performance with
traditional and state of the art exploration approaches while offering a
biologically inspired navigation approach.

</details>


### [33] [Multimodal Spiking Neural Network for Space Robotic Manipulation](https://arxiv.org/abs/2508.07287)
*Liwen Zhang,Dong Zhou,Shibo Shao,Zihao Su,Guanghui Sun*

Main category: cs.RO

TL;DR: 提出了一种基于脉冲神经网络（SNN）的多模态控制框架，用于空间站机械臂，旨在解决资源有限问题并实现自主操作。


<details>
  <summary>Details</summary>
Motivation: 解决空间站机械臂在资源有限条件下的自主操作问题，提升环境感知能力。

Method: 结合几何状态、触觉和语义信息，采用双通道三阶段课程强化学习（CRL）方案。

Result: 在目标接近、物体抓取和稳定提升等任务中表现可靠，成功率和能效均优于基线方法。

Conclusion: 该框架适用于实际航空航天应用，具有较高的实用价值。

Abstract: This paper presents a multimodal control framework based on spiking neural
networks (SNNs) for robotic arms aboard space stations. It is designed to cope
with the constraints of limited onboard resources while enabling autonomous
manipulation and material transfer in space operations. By combining geometric
states with tactile and semantic information, the framework strengthens
environmental awareness and contributes to more robust control strategies. To
guide the learning process progressively, a dual-channel, three-stage
curriculum reinforcement learning (CRL) scheme is further integrated into the
system. The framework was tested across a range of tasks including target
approach, object grasping, and stable lifting with wall-mounted robotic arms,
demonstrating reliable performance throughout. Experimental evaluations
demonstrate that the proposed method consistently outperforms baseline
approaches in both task success rate and energy efficiency. These findings
highlight its suitability for real-world aerospace applications.

</details>


### [34] [A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks](https://arxiv.org/abs/2508.07319)
*Yanzhao Yu,Haotian Yang,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 提出了一种混合力-位置策略，用于控制可变形线性物体（DLOs）的形状，结合了力空间的状态轨迹规划和位置空间的模型预测控制（MPC）。


<details>
  <summary>Details</summary>
Motivation: DLOs（如电线和电缆）在电子组装和医疗手术中有广泛应用，但其无限自由度、复杂非线性动力学和系统欠驱动特性带来了挑战。

Method: 提出了一个框架，结合力和位置表示，包括力空间的状态轨迹规划和位置空间的MPC。模型包含动作编码器、属性提取器和基于图注意力网络的图处理器。

Result: 仿真和实际实验表明，该方法能高效稳定地控制DLOs的形状。

Conclusion: 该混合策略有效解决了DLO形状控制的挑战，模型和框架在实际应用中表现良好。

Abstract: Manipulating deformable linear objects (DLOs) such as wires and cables is
crucial in various applications like electronics assembly and medical
surgeries. However, it faces challenges due to DLOs' infinite degrees of
freedom, complex nonlinear dynamics, and the underactuated nature of the
system. To address these issues, this paper proposes a hybrid force-position
strategy for DLO shape control. The framework, combining both force and
position representations of DLO, integrates state trajectory planning in the
force space and Model Predictive Control (MPC) in the position space. We
present a dynamics model with an explicit action encoder, a property extractor
and a graph processor based on Graph Attention Networks. The model is used in
the MPC to enhance prediction accuracy. Results from both simulations and
real-world experiments demonstrate the effectiveness of our approach in
achieving efficient and stable shape control of DLOs. Codes and videos are
available at https://sites.google.com/view/dlom.

</details>


### [35] [MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control](https://arxiv.org/abs/2508.07387)
*Basant Sharma,Prajyot Jadhav,Pranjal Paul,K. Madhava Krishna,Arun Kumar Singh*

Main category: cs.RO

TL;DR: 论文提出了一种基于学习碰撞模型的方法，通过预测最小障碍物间隙分布来改进单RGB相机在未知环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 单RGB相机缺乏深度信息，导致在未知环境中导航时碰撞检测不可靠。现有方法使用估计深度构建碰撞地图，但深度估计噪声大，无法直接用于零样本导航。

Method: 提出一种替代方法：将噪声深度估计作为丰富上下文输入，训练一个学习碰撞模型，预测机器人执行给定控制序列时的最小障碍物间隙分布。结合风险感知MPC规划器，最小化碰撞风险。

Result: 实验表明，该方法在真实环境中比NoMaD和ROS堆栈的成功率分别提高了9倍和7倍。消融研究验证了设计选择的有效性。

Conclusion: 通过联合学习碰撞模型和风险度量，显著提高了在高度杂乱环境中的导航性能。

Abstract: Navigating unknown environments with a single RGB camera is challenging, as
the lack of depth information prevents reliable collision-checking. While some
methods use estimated depth to build collision maps, we found that depth
estimates from vision foundation models are too noisy for zero-shot navigation
in cluttered environments.
  We propose an alternative approach: instead of using noisy estimated depth
for direct collision-checking, we use it as a rich context input to a learned
collision model. This model predicts the distribution of minimum obstacle
clearance that the robot can expect for a given control sequence. At inference,
these predictions inform a risk-aware MPC planner that minimizes estimated
collision risk. Our joint learning pipeline co-trains the collision model and
risk metric using both safe and unsafe trajectories. Crucially, our
joint-training ensures optimal variance in our collision model that improves
navigation in highly cluttered environments. Consequently, real-world
experiments show 9x and 7x improvements in success rates over NoMaD and the ROS
stack, respectively. Ablation studies further validate the effectiveness of our
design choices.

</details>


### [36] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 论文提出了农业场景下的视觉与语言导航基准A2A和基线方法AgriVLN，解决了现有方法在农业领域的不足，并通过指令分解模块提升了性能。


<details>
  <summary>Details</summary>
Motivation: 农业机器人依赖人工操作或固定轨道，移动性和适应性差。现有视觉与语言导航方法未针对农业场景设计，因此需要专门的研究。

Method: 提出A2A基准，包含1,560个农业场景片段。基于视觉语言模型（VLM）设计AgriVLN基线方法，并引入子任务列表（STL）模块分解指令。

Result: AgriVLN在短指令上表现良好，但对长指令跟踪不足。加入STL后，成功率从0.33提升至0.47，优于现有方法。

Conclusion: A2A和AgriVLN填补了农业场景导航的空白，STL模块显著提升了性能，为农业机器人导航提供了新思路。

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [37] [Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics](https://arxiv.org/abs/2508.07421)
*Zixi Jia,Hongbin Gao,Fashe Li,Jiqiang Liu,Hexiao Li,Qinghua Liu*

Main category: cs.RO

TL;DR: 提出Triple-S框架，通过多LLM协作解决长时隐式任务中的API参数、注释和顺序错误，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 利用LLM编写机器人控制策略代码时，长时隐式任务中常出现API参数、注释和顺序错误，导致任务失败。

Method: 提出Triple-S框架，通过多LLM在简化-解决-总结闭环过程中扮演特定角色，结合演示库更新机制。

Result: 在LDIP数据集中，Triple-S在可观察和部分可观察场景下任务成功率达89%。

Conclusion: Triple-S框架显著提高了长时隐式任务的执行成功率和鲁棒性，实验验证了其有效性。

Abstract: Leveraging Large Language Models (LLMs) to write policy code for controlling
robots has gained significant attention. However, in long-horizon implicative
tasks, this approach often results in API parameter, comments and sequencing
errors, leading to task failure. To address this problem, we propose a
collaborative Triple-S framework that involves multiple LLMs. Through
In-Context Learning, different LLMs assume specific roles in a closed-loop
Simplification-Solution-Summary process, effectively improving success rates
and robustness in long-horizon implicative tasks. Additionally, a novel
demonstration library update mechanism which learned from success allows it to
generalize to previously failed tasks. We validate the framework in the
Long-horizon Desktop Implicative Placement (LDIP) dataset across various
baseline models, where Triple-S successfully executes 89% of tasks in both
observable and partially observable scenarios. Experiments in both simulation
and real-world robot settings further validated the effectiveness of Triple-S.
Our code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.

</details>


### [38] [A Learning-Based Framework for Collision-Free Motion Planning](https://arxiv.org/abs/2508.07502)
*Mateus Salomão,Tianyü Ren,Alexander König*

Main category: cs.RO

TL;DR: 论文提出了一种基于学习的运动规划方法，通过深度神经网络优化参数，实现高效无碰撞轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统手动调整力场参数的局限性，提升在复杂环境中的运动规划效率。

Method: 结合CUDA加速的感知模块、基于预测的规划策略，以及贝叶斯优化生成的数据集，训练深度神经网络推断最优参数。

Result: 实验验证了实时规划能力，任务完成率高，且比传统规划器具有更好的泛化性能。

Conclusion: 该方法无需手动调参，适用于复杂环境，并在仿真和实际机器人上验证了有效性。

Abstract: This paper presents a learning-based extension to a Circular Field (CF)-based
motion planner for efficient, collision-free trajectory generation in cluttered
environments. The proposed approach overcomes the limitations of hand-tuned
force field parameters by employing a deep neural network trained to infer
optimal planner gains from a single depth image of the scene. The pipeline
incorporates a CUDA-accelerated perception module, a predictive agent-based
planning strategy, and a dataset generated through Bayesian optimization in
simulation. The resulting framework enables real-time planning without manual
parameter tuning and is validated both in simulation and on a Franka Emika
Panda robot. Experimental results demonstrate successful task completion and
improved generalization compared to classical planners.

</details>


### [39] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: 综述首次从安全关键视角全面回顾BEV感知，分析单模态、多模态及多代理协作感知框架，评估相关数据集，并指出开放世界挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶从受控环境转向实际部署，确保BEV感知在复杂场景中的安全性和可靠性成为关键挑战。

Method: 系统分析BEV感知的三个阶段：单模态车端、多模态车端及多代理协作感知，并评估相关数据集。

Result: 识别开放世界挑战（如开放集识别、传感器退化等），并指出未来研究方向（如端到端系统集成、大语言模型等）。

Conclusion: BEV感知在安全性和可靠性方面仍需突破，未来研究需关注开放世界挑战及新技术整合。

Abstract: Bird's-Eye-View (BEV) perception has become a foundational paradigm in
autonomous driving, enabling unified spatial representations that support
robust multi-sensor fusion and multi-agent collaboration. As autonomous
vehicles transition from controlled environments to real-world deployment,
ensuring the safety and reliability of BEV perception in complex scenarios -
such as occlusions, adverse weather, and dynamic traffic - remains a critical
challenge. This survey provides the first comprehensive review of BEV
perception from a safety-critical perspective, systematically analyzing
state-of-the-art frameworks and implementation strategies across three
progressive stages: single-modality vehicle-side, multimodal vehicle-side, and
multi-agent collaborative perception. Furthermore, we examine public datasets
encompassing vehicle-side, roadside, and collaborative settings, evaluating
their relevance to safety and robustness. We also identify key open-world
challenges - including open-set recognition, large-scale unlabeled data, sensor
degradation, and inter-agent communication latency - and outline future
research directions, such as integration with end-to-end autonomous driving
systems, embodied intelligence, and large language models.

</details>


### [40] [Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer](https://arxiv.org/abs/2508.07566)
*Conor K. Trygstad,Cody R. Longwell,Francisco M. F. R. Gonçalves,Elijah K. Blankenship,Néstor O. Pérez-Arancibia*

Main category: cs.RO

TL;DR: 改进版的FRISSHBot通过新型SMA双压电晶片驱动器实现二维控制，速度提升四倍，并首次实现亚克级单尾机器人的反馈控制轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 开发一种更高效、可控的小型水生机器人，以改进原始FRISSHBot的性能和功能。

Method: 采用物理信息设计，增大头部并缩短尾部，结合新型SMA双压电晶片驱动器。

Result: 最高游泳速度达13.6 mm/s，闭环跟踪时速度为9.1 mm/s，跟踪误差低至2.6 mm，转弯半径最小10 mm。

Conclusion: 改进设计显著提升了FRISSHBot的性能，实现了亚克级机器人的精确控制。

Abstract: We present an evolved steerable version of the single-tail
Fish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg
biologically inspired swimmer, which is driven by a new shape-memory alloy
(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the
two-dimensional (2D) space, which enabled the first demonstration of
feedback-controlled trajectory tracking of a single-tail aquatic robot with
onboard actuation at the subgram scale. These new capabilities are the result
of a physics-informed design with an enlarged head and shortened tail relative
to those of the original platform. Enhanced by its design, this new platform
achieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over
four times that of the original platform. Furthermore, when following 2D
references in closed loop, the tested FRISSHBot prototype attains forward
swimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as
low as 2.6 mm, turning rates of up to 13.1 {\deg}/s, and turning radii as small
as 10 mm.

</details>


### [41] [In-situ Value-aligned Human-Robot Interactions with Physical Constraints](https://arxiv.org/abs/2508.07606)
*Hongtao Li,Ziyuan Jiao,Xiaofeng Liu,Hangxin Liu,Zilong Zheng*

Main category: cs.RO

TL;DR: 提出了一种结合人类偏好与物理约束的框架，通过上下文学习人类反馈（ICLHF），使机器人能够完成任务并适应未来场景。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）使机器人能完成复杂任务，但仅完成任务不足以满足认知需求，需学习并应用人类偏好。

Method: 开发了日常家务活动基准，引入ICLHF框架，结合直接指令和日常调整的人类反馈。

Result: 实验证明ICLHF能高效生成任务计划并平衡物理约束与人类偏好。

Conclusion: 该框架为机器人学习人类偏好提供了有效方法，适用于未来场景。

Abstract: Equipped with Large Language Models (LLMs), human-centered robots are now
capable of performing a wide range of tasks that were previously deemed
challenging or unattainable. However, merely completing tasks is insufficient
for cognitive robots, who should learn and apply human preferences to future
scenarios. In this work, we propose a framework that combines human preferences
with physical constraints, requiring robots to complete tasks while considering
both. Firstly, we developed a benchmark of everyday household activities, which
are often evaluated based on specific preferences. We then introduced
In-Context Learning from Human Feedback (ICLHF), where human feedback comes
from direct instructions and adjustments made intentionally or unintentionally
in daily life. Extensive sets of experiments, testing the ICLHF to generate
task plans and balance physical constraints with preferences, have demonstrated
the efficiency of our approach.

</details>


### [42] [End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy](https://arxiv.org/abs/2508.07611)
*Zifan Wang,Xun Yang,Jianzhuang Zhao,Jiaming Zhou,Teli Ma,Ziyao Gao,Arash Ajoudani,Junwei Liang*

Main category: cs.RO

TL;DR: 提出了一种基于LiDAR点云的端到端运动策略，结合CMDP和CBFs，实现人形机器人在复杂动态环境中的安全导航。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在非结构化环境中需要更强的导航能力，现有强化学习方法在环境感知和安全性上存在不足。

Method: 使用CMDP框架，将CBFs转化为成本函数，结合P3O算法训练，并引入舒适性奖励。

Result: 实现了从仿真到实体的成功迁移，机器人能在静态和动态障碍物中安全灵活导航。

Conclusion: 该方法有效提升了人形机器人在复杂环境中的导航安全性和社会适应性。

Abstract: The deployment of humanoid robots in unstructured, human-centric environments
requires navigation capabilities that extend beyond simple locomotion to
include robust perception, provable safety, and socially aware behavior.
Current reinforcement learning approaches are often limited by blind
controllers that lack environmental awareness or by vision-based systems that
fail to perceive complex 3D obstacles. In this work, we present an end-to-end
locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to
motor commands, enabling robust navigation in cluttered dynamic scenes. We
formulate the control problem as a Constrained Markov Decision Process (CMDP)
to formally separate safety from task objectives. Our key contribution is a
novel methodology that translates the principles of Control Barrier Functions
(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal
Policy Optimization (P3O) to enforce safety constraints during training.
Furthermore, we introduce a set of comfort-oriented rewards, grounded in
human-robot interaction research, to promote motions that are smooth,
predictable, and less intrusive. We demonstrate the efficacy of our framework
through a successful sim-to-real transfer to a physical humanoid robot, which
exhibits agile and safe navigation around both static and dynamic 3D obstacles.

</details>


### [43] [Grasp-HGN: Grasping the Unexpected](https://arxiv.org/abs/2508.07648)
*Mehrshad Zandigohar,Mallesham Dasari,Gunar Schirner*

Main category: cs.RO

TL;DR: 论文提出Grasp-LLaVA和HGN方法，解决假肢手控制中对未见物体泛化能力差和性能延迟问题，显著提升准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 当前假肢手控制模型对未见物体泛化能力差，影响用户独立性和生活质量。

Method: 提出Grasp-LLaVA（基于视觉语言模型）和HGN（混合边缘-云部署架构），结合语义投影和动态切换技术。

Result: Grasp-LLaVA对未见物体准确率提升至50.2%，HGN进一步将准确率提升至42.3%，速度提升3.5倍。

Conclusion: Grasp-LLaVA和HGN显著提升了假肢手控制的泛化能力和实时性能，为下一代假肢设计提供了有效解决方案。

Abstract: For transradial amputees, robotic prosthetic hands promise to regain the
capability to perform daily living activities. To advance next-generation
prosthetic hand control design, it is crucial to address current shortcomings
in robustness to out of lab artifacts, and generalizability to new
environments. Due to the fixed number of object to interact with in existing
datasets, contrasted with the virtually infinite variety of objects encountered
in the real world, current grasp models perform poorly on unseen objects,
negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to
generalize to unseen object types and show that conventional models like YOLO,
despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose
Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to
infer the suitable grasp type estimate based on the object's physical
characteristics resulting in a significant 50.2% accuracy over unseen object
types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp
Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp
estimation on edge and accurate cloud inference as a fail-safe, effectively
expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)
enables dynamic switching between edge and cloud models, improving semantic
projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object
types. Over a real-world sample mix, it reaches 86% average accuracy (12.2%
gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.

</details>


### [44] [GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](https://arxiv.org/abs/2508.07650)
*Helong Huang,Min Cen,Kai Tan,Xingyue Quan,Guowei Huang,Hong Zhang*

Main category: cs.RO

TL;DR: GraphCoT-VLA是一种高效的端到端视觉-语言-动作模型，通过结构化思维链推理和实时更新的3D姿态-物体图，解决了现有模型在模糊指令和未知环境状态下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在处理模糊指令和未知环境状态时表现不佳，且缺乏对三维交互的建模能力。

Method: 设计了结构化思维链推理模块和实时更新的3D姿态-物体图，结合混合推理策略。

Result: 在多个实际机器人任务中，GraphCoT-VLA在任务成功率和响应速度上显著优于现有方法。

Conclusion: GraphCoT-VLA在开放环境和不确定指令下表现出强大的泛化能力和鲁棒性。

Abstract: Vision-language-action models have emerged as a crucial paradigm in robotic
manipulation. However, existing VLA models exhibit notable limitations in
handling ambiguous language instructions and unknown environmental states.
Furthermore, their perception is largely constrained to static two-dimensional
observations, lacking the capability to model three-dimensional interactions
between the robot and its environment. To address these challenges, this paper
proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's
ability to interpret ambiguous instructions and improve task planning, we
design a structured Chain-of-Thought reasoning module that integrates
high-level task understanding and planning, failed task feedback, and low-level
imaginative reasoning about future object positions and robot actions.
Additionally, we construct a real-time updatable 3D Pose-Object graph, which
captures the spatial configuration of robot joints and the topological
relationships between objects in 3D space, enabling the model to better
understand and manipulate their interactions. We further integrates a dropout
hybrid reasoning strategy to achieve efficient control outputs. Experimental
results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA
significantly outperforms existing methods in terms of task success rate and
response speed, exhibiting strong generalization and robustness in open
environments and under uncertain instructions.

</details>


### [45] [MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication](https://arxiv.org/abs/2508.07657)
*Zhuoli Tian,Yuyang Zhang,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: MoRoCo框架支持多操作员与多机器人在有限通信下的实时交互与协调，通过三种模式实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 现有研究多忽略人类操作员与机器人团队的实时交互需求，而通信受限环境下亟需双边、上下文感知的协作。

Method: 提出MoRoCo框架，包含三种协调模式（spread、migrate、chain），通过分布式算法实现本地通信管理。

Result: 大规模仿真与硬件实验验证了MoRoCo在有限通信下的高效协调能力。

Conclusion: MoRoCo为挑战性环境中的人机协作多机器人系统提供了可靠解决方案。

Abstract: Fleets of autonomous robots are increasingly deployed alongside multiple
human operators to explore unknown environments, identify salient features, and
perform complex tasks in scenarios such as subterranean exploration,
reconnaissance, and search-and-rescue missions. In these contexts,
communication is often severely limited to short-range exchanges via ad-hoc
networks, posing challenges to coordination. While recent studies have
addressed multi-robot exploration under communication constraints, they largely
overlook the essential role of human operators and their real-time interaction
with robotic teams. Operators may demand timely updates on the exploration
progress and robot status, reprioritize or cancel tasks dynamically, or request
live video feeds and control access. Conversely, robots may seek human
confirmation for anomalous events or require help recovering from motion or
planning failures. To enable such bilateral, context-aware interactions under
restricted communication, this work proposes MoRoCo, a unified framework for
online coordination and exploration in multi-operator, multi-robot systems.
MoRoCo enables the team to adaptively switch among three coordination modes:
spread mode for parallelized exploration with intermittent data sharing,
migrate mode for coordinated relocation, and chain mode for maintaining
high-bandwidth connectivity through multi-hop links. These transitions are
managed through distributed algorithms via only local communication. Extensive
large-scale human-in-the-loop simulations and hardware experiments validate the
necessity of incorporating human robot interactions and demonstrate that MoRoCo
enables efficient, reliable coordination under limited communication, marking a
significant step toward robust human-in-the-loop multi-robot autonomy in
challenging environments.

</details>


### [46] [Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning](https://arxiv.org/abs/2508.07686)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jiaqi Ma,Jia Hu*

Main category: cs.RO

TL;DR: 提出了一种基于风险地图的中间件（RiskMM），用于构建可解释的协作端到端自动驾驶框架，解决了现有单智能体系统的遮挡、感知范围有限和黑盒问题。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体端到端自动驾驶系统因遮挡和感知范围受限导致危险驾驶，且黑盒特性缺乏可解释性。

Method: 通过风险地图学习驾驶数据，构建多智能体时空表示，利用注意力建模环境交互，并结合基于学习的模型预测控制（MPC）模块。

Result: 在V2XPnP-Seq数据集上验证，RiskMM在风险感知轨迹规划中表现优越且鲁棒，显著提升了框架的可解释性。

Conclusion: RiskMM为协作端到端自动驾驶提供了可解释且高效的解决方案，未来将开源代码以促进研究。

Abstract: End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.

</details>


### [47] [LAURON VI: A Six-Legged Robot for Dynamic Walking](https://arxiv.org/abs/2508.07689)
*Christian Eichmann,Sabine Bellmann,Nicolas Hügel,Louis-Elias Enslin,Carsten Plasberg,Georg Heppner,Arne Roennau,Ruediger Dillmann*

Main category: cs.RO

TL;DR: 本文介绍了六足机器人LAURON VI，旨在通过动态步态和自主控制技术提升其在复杂地形中的适应性，尤其是快速行走能力。


<details>
  <summary>Details</summary>
Motivation: 六足机器人在复杂地形中表现出色，但在简单地形中缺乏快速行走能力，限制了其广泛应用。

Method: 设计了三种控制方法：基于运动学的、模型预测的和强化学习的控制器，并在实验室和火星模拟任务中测试。

Result: LAURON VI通过引入快速行走策略，显著提升了六足机器人在多种实际场景中的适用性。

Conclusion: 六足机器人通过动态步态和自主控制技术的结合，能够更好地适应复杂和混合地形任务。

Abstract: Legged locomotion enables robotic systems to traverse extremely challenging
terrains. In many real-world scenarios, the terrain is not that difficult and
these mixed terrain types introduce the need for flexible use of different
walking strategies to achieve mission goals in a fast, reliable, and
energy-efficient way. Six-legged robots have a high degree of flexibility and
inherent stability that aids them in traversing even some of the most difficult
terrains, such as collapsed buildings. However, their lack of fast walking
gaits for easier surfaces is one reason why they are not commonly applied in
these scenarios.
  This work presents LAURON VI, a six-legged robot platform for research on
dynamic walking gaits as well as on autonomy for complex field missions. The
robot's 18 series elastic joint actuators offer high-frequency interfaces for
Cartesian impedance and pure torque control. We have designed, implemented, and
compared three control approaches: kinematic-based, model-predictive, and
reinforcement-learned controllers. The robot hardware and the different control
approaches were extensively tested in a lab environment as well as on a Mars
analog mission. The introduction of fast locomotion strategies for LAURON VI
makes six-legged robots vastly more suitable for a wide range of real-world
applications.

</details>


### [48] [Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation](https://arxiv.org/abs/2508.07758)
*Antonio Rosales,Alaa Abderrahim,Markku Suomalainen,Mikael Haag,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 提出了一种通过机器人与起重机协作增强载荷操纵的方案，减少人工操作的风险和复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前工业实践中，起重机载荷的精确定位和操纵需要人工引导，任务繁重且危险。

Method: 采用机器人末端执行器引导载荷，通过交互力实现机器人与起重机的协作，设计了两种导纳传递函数以实现平滑接触。

Result: 通过仿真和实验验证了方案的可行性，实现了流畅的机器人与起重机协作。

Conclusion: 该协作方案有效提升了载荷操纵的精确性和安全性，减少了人工干预的需求。

Abstract: This paper presents a scheme to enhance payload manipulation using a robot
collaborating with an overhead crane. In the current industrial practice, when
the crane's payload has to be accurately manipulated and located in a desired
position, the task becomes laborious and risky since the operators have to
guide the fine motions of the payload by hand. In the proposed collaborative
scheme, the crane lifts the payload while the robot's end-effector guides it
toward the desired position. The only link between the robot and the crane is
the interaction force produced during the guiding of the payload. Two
admittance transfer functions are considered to accomplish harmless and smooth
contact with the payload. The first is used in a position-based admittance
control integrated with the robot. The second one adds compliance to the crane
by processing the interaction force through the admittance transfer function to
generate a crane's velocity command that makes the crane follow the payload.
Then the robot's end-effector and the crane move collaboratively to guide the
payload to the desired location. A method is presented to design the admittance
controllers that accomplish a fluent robot-crane collaboration. Simulations and
experiments validating the scheme potential are shown.

</details>


### [49] [AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation](https://arxiv.org/abs/2508.07770)
*Yizheng Zhang,Zhenjun Yu,Jiaxin Lai,Cewu Lu,Lei Han*

Main category: cs.RO

TL;DR: AgentWorld是一个交互式仿真平台，用于开发家庭移动操作能力，结合自动化场景构建和双模式遥操作系统，支持从基础动作到多阶段任务的数据收集，并通过模仿学习方法验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决家庭环境中机器人技能的可扩展获取问题，缩小仿真训练与实际部署之间的差距。

Method: 平台结合自动化场景构建（布局生成、语义资产放置、视觉材料配置、物理模拟）和双模式遥操作系统（轮式基座和人形运动策略），收集多样化任务数据。

Result: 通过模仿学习方法（行为克隆、动作分块变换器、扩散策略、视觉-语言-动作模型）验证了数据集在仿真到现实转移中的有效性。

Conclusion: AgentWorld为复杂家庭环境中的机器人技能获取提供了全面解决方案，并公开了代码和数据集。

Abstract: We introduce AgentWorld, an interactive simulation platform for developing
household mobile manipulation capabilities. Our platform combines automated
scene construction that encompasses layout generation, semantic asset
placement, visual material configuration, and physics simulation, with a
dual-mode teleoperation system supporting both wheeled bases and humanoid
locomotion policies for data collection. The resulting AgentWorld Dataset
captures diverse tasks ranging from primitive actions (pick-and-place,
push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)
across living rooms, bedrooms, and kitchens. Through extensive benchmarking of
imitation learning methods including behavior cloning, action chunking
transformers, diffusion policies, and vision-language-action models, we
demonstrate the dataset's effectiveness for sim-to-real transfer. The
integrated system provides a comprehensive solution for scalable robotic skill
acquisition in complex home environments, bridging the gap between
simulation-based training and real-world deployment. The code, datasets will be
available at https://yizhengzhang1.github.io/agent_world/

</details>


### [50] [SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing](https://arxiv.org/abs/2508.07814)
*Malaika Zafar,Roohan Ahmed Khan,Faryal Batool,Yasheerah Yaqoot,Ziang Guo,Mikhail Litvinov,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: SwarmVLM通过语义协作和阻抗控制，解决了无人机与地面机器人在异构导航中的限制，实现了92%的成功率和安全导航。


<details>
  <summary>Details</summary>
Motivation: 随着物流需求增长，无人机与地面机器人协作的需求增加，但无人机受限于电池、负载和飞行时间，需要地面支持。

Method: 结合视觉语言模型（VLM）和检索增强生成（RAG）调整阻抗控制参数，无人机作为领导者使用APF规划导航，地面机器人通过虚拟阻抗链接跟随。

Result: 系统在12次真实试验中成功率92%，VLM-RAG在理想光照下物体检测和参数选择准确率8%，地面机器人能安全避开障碍。

Conclusion: SwarmVLM展示了异构导航中无人机与地面机器人协作的潜力，尤其在复杂环境中表现优异。

Abstract: With the growing demand for efficient logistics, unmanned aerial vehicles
(UAVs) are increasingly being paired with automated guided vehicles (AGVs).
While UAVs offer the ability to navigate through dense environments and varying
altitudes, they are limited by battery life, payload capacity, and flight
duration, necessitating coordinated ground support.
  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by
enabling semantic collaboration between UAVs and ground robots through
impedance control. The system leverages the Vision Language Model (VLM) and the
Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in
response to environmental changes. In this framework, the UAV acts as a leader
using Artificial Potential Field (APF) planning for real-time navigation, while
the ground robot follows via virtual impedance links with adaptive link
topology to avoid collisions with short obstacles.
  The system demonstrated a 92% success rate across 12 real-world trials. Under
optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in
object detection and selection of impedance parameters. The mobile robot
prioritized short obstacle avoidance, occasionally resulting in a lateral
deviation of up to 50 cm from the UAV path, which showcases safe navigation in
a cluttered setting.

</details>


### [51] [Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans](https://arxiv.org/abs/2508.07839)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.RO

TL;DR: 研究开发了一种多模态交互系统，结合触觉和听觉刺激，探索机器人通过触摸传达情感和社交手势的能力。实验表明，多模态显著提高了情感解码准确性，触觉和听觉各自对特定情感识别有优势。


<details>
  <summary>Details</summary>
Motivation: 探索机器人通过触觉和听觉结合的方式传达情感和社交手势的能力，填补现有研究中机器人情感表达方式的不足。

Method: 开发了一个包含25个振动马达和音频播放的多模态系统，通过振动、声音或其组合呈现十种情感和六种社交手势，32名参与者对刺激进行评分。

Result: 多模态显著提高了情感解码准确性；触觉和听觉各自对特定情感识别有优势；单独手势难以清晰传达情感。

Conclusion: 多感官整合在情感人机交互中至关重要，触觉和听觉线索在增强情感沟通中具有互补作用。

Abstract: Affective tactile interaction constitutes a fundamental component of human
communication. In natural human-human encounters, touch is seldom experienced
in isolation; rather, it is inherently multisensory. Individuals not only
perceive the physical sensation of touch but also register the accompanying
auditory cues generated through contact. The integration of haptic and auditory
information forms a rich and nuanced channel for emotional expression. While
extensive research has examined how robots convey emotions through facial
expressions and speech, their capacity to communicate social gestures and
emotions via touch remains largely underexplored. To address this gap, we
developed a multimodal interaction system incorporating a 5*5 grid of 25
vibration motors synchronized with audio playback, enabling robots to deliver
combined haptic-audio stimuli. In an experiment involving 32 Chinese
participants, ten emotions and six social gestures were presented through
vibration, sound, or their combination. Participants rated each stimulus on
arousal and valence scales. The results revealed that (1) the combined
haptic-audio modality significantly enhanced decoding accuracy compared to
single modalities; (2) each individual channel-vibration or sound-effectively
supported certain emotions recognition, with distinct advantages depending on
the emotional expression; and (3) gestures alone were generally insufficient
for conveying clearly distinguishable emotions. These findings underscore the
importance of multisensory integration in affective human-robot interaction and
highlight the complementary roles of haptic and auditory cues in enhancing
emotional communication.

</details>


### [52] [DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts](https://arxiv.org/abs/2508.07842)
*Yutong Shen,Hangxu Liu,Penghui Liu,Ruizhe Xia,Tianyi Yao,Yitong Sun,Tongtong Feng*

Main category: cs.RO

TL;DR: DETACH是一种基于生物启发的双流解耦框架，用于解决跨领域长时程任务中的泛化问题，显著提升了任务成功率和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖技能链式拼接，难以泛化到新环境和技能组合，无法完成跨领域的长时程任务。

Method: DETACH通过环境学习模块（空间理解）和技能学习模块（任务执行）实现环境和自我的完全解耦，支持跨领域和跨技能迁移。

Result: 实验表明，DETACH在任务成功率和执行效率上分别平均提升了23%和29%。

Conclusion: DETACH通过双流解耦机制有效解决了跨领域长时程任务的泛化问题，性能显著优于现有方法。

Abstract: Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex
multi-step tasks that require continuous planning, sequential decision-making,
and extended execution across domains to achieve the final goal. However,
existing methods heavily rely on skill chaining by concatenating pre-trained
subtasks, with environment observations and self-state tightly coupled, lacking
the ability to generalize to new combinations of environments and skills,
failing to complete various LH tasks across domains. To solve this problem,
this paper presents DETACH, a cross-domain learning framework for LH tasks via
biologically inspired dual-stream disentanglement. Inspired by the brain's
"where-what" dual pathway mechanism, DETACH comprises two core modules: i) an
environment learning module for spatial understanding, which captures object
functions, spatial relationships, and scene semantics, achieving cross-domain
transfer through complete environment-self disentanglement; ii) a skill
learning module for task execution, which processes self-state information
including joint degrees of freedom and motor patterns, enabling cross-skill
transfer through independent motor pattern encoding. We conducted extensive
experiments on various LH tasks in HSI scenes. Compared with existing methods,
DETACH can achieve an average subtasks success rate improvement of 23% and
average execution efficiency improvement of 29%.

</details>


### [53] [MolmoAct: Action Reasoning Models that can Reason in Space](https://arxiv.org/abs/2508.07917)
*Jason Lee,Jiafei Duan,Haoquan Fang,Yuquan Deng,Shuo Liu,Boyang Li,Bohan Fang,Jieyu Zhang,Yi Ru Wang,Sangho Lee,Winson Han,Wilbert Pumacay,Angelica Wu,Rose Hendrix,Karen Farley,Eli VanderBilt,Ali Farhadi,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: MolmoAct是一种新型视觉-语言-动作模型（ARM），通过三阶段结构化推理提升机器人适应性、泛化能力和语义理解，在仿真和现实任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型直接将感知和指令映射到控制，限制了适应性、泛化能力和语义理解。

Method: MolmoAct采用三阶段结构化推理：深度感知编码、可编辑轨迹规划、精确动作预测。

Result: 在仿真和现实任务中表现优异，如70.5%零样本准确率（SimperEnv）、86.6%成功率（LIBERO），并显著优于基线模型。

Conclusion: MolmoAct是当前最先进的机器人基础模型，通过结构化推理将感知转化为有目的的动作，并开源了模型和数据集。

Abstract: Reasoning is central to purposeful action, yet most robotic foundation models
map perception and instructions directly to control, which limits adaptability,
generalization, and semantic grounding. We introduce Action Reasoning Models
(ARMs), a class of vision-language-action models that integrate perception,
planning, and control through a structured three-stage pipeline. Our model,
MolmoAct, encodes observations and instructions into depth-aware perception
tokens, generates mid-level spatial plans as editable trajectory traces, and
predicts precise low-level actions, enabling explainable and steerable
behavior. MolmoAct-7B-D achieves strong performance across simulation and
real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching
tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on
LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;
and in real-world fine-tuning, an additional 10% (single-arm) and an additional
22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines
by an additional 23.3% on out-of-distribution generalization and achieves top
human-preference scores for open-ended instruction following and trajectory
steering. Furthermore, we release, for the first time, the MolmoAct Dataset --
a mid-training robot dataset comprising over 10,000 high quality robot
trajectories across diverse scenarios and tasks. Training with this dataset
yields an average 5.5% improvement in general performance over the base model.
We release all model weights, training code, our collected dataset, and our
action reasoning dataset, establishing MolmoAct as both a state-of-the-art
robotics foundation model and an open blueprint for building ARMs that
transform perception into purposeful action through structured reasoning.
Blogpost: https://allenai.org/blog/molmoact

</details>


### [54] [PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF](https://arxiv.org/abs/2508.07945)
*En Yen Puang,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: PCHands提出了一种通用表示方法，用于不同形态机械手的灵巧操作学习，通过锚点位置统一描述格式，提取跨机械手的主成分，并在强化学习中验证其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决不同形态机械手在灵巧操作中的通用表示问题，以提高学习效率和跨机械手的任务一致性。

Method: 提出PCHands方法，基于锚点位置统一描述格式，学习可变长度潜在表示，并提取跨机械手的主成分。

Result: PCHands在强化学习中表现优于关节空间基线，且在跨机械手的演示学习中表现鲁棒。

Conclusion: PCHands为不同形态机械手的灵巧操作提供了一种高效且通用的表示方法。

Abstract: We consider the problem of learning a common representation for dexterous
manipulation across manipulators of different morphologies. To this end, we
propose PCHands, a novel approach for extracting hand postural synergies from a
large set of manipulators. We define a simplified and unified description
format based on anchor positions for manipulators ranging from 2-finger
grippers to 5-finger anthropomorphic hands. This enables learning a
variable-length latent representation of the manipulator configuration and the
alignment of the end-effector frame of all manipulators. We show that it is
possible to extract principal components from this latent representation that
is universal across manipulators of different structures and degrees of
freedom. To evaluate PCHands, we use this compact representation to encode
observation and action spaces of control policies for dexterous manipulation
tasks learned with RL. In terms of learning efficiency and consistency, the
proposed representation outperforms a baseline that learns the same tasks in
joint space. We additionally show that PCHands performs robustly in RL from
demonstration, when demonstrations are provided from a different manipulator.
We further support our results with real-world experiments that involve a
2-finger gripper and a 4-finger anthropomorphic hand. Code and additional
material are available at https://hsp-iit.github.io/PCHands/.

</details>


### [55] [Aerial Target Encirclement and Interception with Noisy Range Observations](https://arxiv.org/abs/2508.08046)
*Fen Liu,Shenghai Yuan,Thien-Minh Nguyen,Wei Meng,Lihua Xie*

Main category: cs.RO

TL;DR: 提出一种利用噪声距离测量包围拦截非合作空中目标的策略，通过反同步3D轨迹确保目标可观测性，并设计新型控制器实现自适应拦截。


<details>
  <summary>Details</summary>
Motivation: 解决非合作空中目标的快速状态估计和拦截问题，确保目标可观测性并适应输入约束。

Method: 采用反同步3D轨迹和卡尔曼滤波进行状态估计，设计自适应控制器实现目标拦截。

Result: 仿真和无人机实验验证了系统设计的有效性，状态估计误差和包围误差收敛。

Conclusion: 该方法能有效拦截非合作目标，确保状态估计和拦截的稳定性。

Abstract: This paper proposes a strategy to encircle and intercept a non-cooperative
aerial point-mass moving target by leveraging noisy range measurements for
state estimation. In this approach, the guardians actively ensure the
observability of the target by using an anti-synchronization (AS), 3D
``vibrating string" trajectory, which enables rapid position and velocity
estimation based on the Kalman filter. Additionally, a novel anti-target
controller is designed for the guardians to enable adaptive transitions from
encircling a protected target to encircling, intercepting, and neutralizing a
hostile target, taking into consideration the input constraints of the
guardians. Based on the guaranteed uniform observability, the exponentially
bounded stability of the state estimation error and the convergence of the
encirclement error are rigorously analyzed. Simulation results and real-world
UAV experiments are presented to further validate the effectiveness of the
system design.

</details>


### [56] [Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain](https://arxiv.org/abs/2508.08108)
*Wei Zhang,Yinchuan Wang,Wangtao Lu,Pengyu Zhang,Xiang Zhang,Yue Wang,Chaoqun Wang*

Main category: cs.RO

TL;DR: 提出了一种基于翻覆感知的轨迹规划方法（CAP），用于地面机器人在复杂地形中的安全高效导航。


<details>
  <summary>Details</summary>
Motivation: 地面机器人在复杂地形中导航时面临翻覆风险，需平衡安全与效率。

Method: 分析机器人翻覆稳定性，定义可穿越方向，并将其作为约束加入轨迹优化，使用图求解器生成轨迹。

Result: 仿真和实验表明，CAP方法优于现有技术，提升了导航性能。

Conclusion: CAP方法有效解决了复杂地形中的导航问题，具有鲁棒性和实用性。

Abstract: It is a challenging task for ground robots to autonomously navigate in harsh
environments due to the presence of non-trivial obstacles and uneven terrain.
This requires trajectory planning that balances safety and efficiency. The
primary challenge is to generate a feasible trajectory that prevents robot from
tip-over while ensuring effective navigation. In this paper, we propose a
capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the
uneven terrain. The tip-over stability of the robot on rough terrain is
analyzed. Based on the tip-over stability, we define the traversable
orientation, which indicates the safe range of robot orientations. This
orientation is then incorporated into a capsizing-safety constraint for
trajectory optimization. We employ a graph-based solver to compute a robust and
feasible trajectory while adhering to the capsizing-safety constraint.
Extensive simulation and real-world experiments validate the effectiveness and
robustness of the proposed method. The results demonstrate that CAP outperforms
existing state-of-the-art approaches, providing enhanced navigation performance
on uneven terrains.

</details>


### [57] [AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies](https://arxiv.org/abs/2508.08113)
*Yinpei Dai,Jayjun Lee,Yichi Zhang,Ziqiao Ma,Jed Yang,Amir Zadeh,Chuan Li,Nima Fazeli,Joyce Chai*

Main category: cs.RO

TL;DR: AimBot是一种轻量级视觉增强技术，通过叠加辅助视觉线索（如射击线和准星）到多视角RGB图像中，提升机器人操作中的视觉运动策略学习。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，视觉运动策略学习通常缺乏明确的空间线索，AimBot旨在通过视觉增强提供空间引导，改善策略性能。

Method: AimBot利用深度图像、相机外参和末端执行器姿态计算叠加内容，替换原始RGB图像，无需修改模型架构。

Result: 实验表明，AimBot在仿真和真实环境中均能显著提升多种视觉运动策略的性能，计算开销极低（小于1毫秒）。

Conclusion: AimBot通过简单但有效的空间视觉反馈，显著提升了机器人操作的视觉运动策略学习效果。

Abstract: In this paper, we propose AimBot, a lightweight visual augmentation technique
that provides explicit spatial cues to improve visuomotor policy learning in
robotic manipulation. AimBot overlays shooting lines and scope reticles onto
multi-view RGB images, offering auxiliary visual guidance that encodes the
end-effector's state. The overlays are computed from depth images, camera
extrinsics, and the current end-effector pose, explicitly conveying spatial
relationships between the gripper and objects in the scene. AimBot incurs
minimal computational overhead (less than 1 ms) and requires no changes to
model architectures, as it simply replaces original RGB images with augmented
counterparts. Despite its simplicity, our results show that AimBot consistently
improves the performance of various visuomotor policies in both simulation and
real-world settings, highlighting the benefits of spatially grounded visual
feedback.

</details>


### [58] [Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy](https://arxiv.org/abs/2508.08226)
*Haiyue Chen,Aniket Datar,Tong Xu,Francesco Cancelliere,Harsh Rangwala,Madhan Balaji Rao,Daeun Song,David Eichinger,Xuesu Xiao*

Main category: cs.RO

TL;DR: 论文介绍了Verti-Arena，一个可重构的室内设施，用于标准化和可重复的越野自主导航实验。


<details>
  <summary>Details</summary>
Motivation: 越野导航对移动机器人在危险或难以进入的环境中（如灾难响应或行星探索）至关重要，但缺乏可控且标准化的真实测试环境限制了研究进展。

Method: 通过Verti-Arena设施，提供可重复的基准环境，支持多种垂直挑战性地形的实验，并利用机载传感器和运动捕捉系统提供精确的地面真实数据。

Result: Verti-Arena支持一致的数据收集和算法比较，并通过基于网络的界面实现全球研究团队远程进行标准化实验。

Conclusion: Verti-Arena填补了越野自主导航研究中标准化测试环境的空白，促进了可重复性和比较性研究。

Abstract: Off-road navigation is an important capability for mobile robots deployed in
environments that are inaccessible or dangerous to humans, such as disaster
response or planetary exploration. Progress is limited due to the lack of a
controllable and standardized real-world testbed for systematic data collection
and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable
indoor facility designed specifically for off-road autonomy. By providing a
repeatable benchmark environment, Verti-Arena supports reproducible experiments
across a variety of vertically challenging terrains and provides precise ground
truth measurements through onboard sensors and a motion capture system.
Verti-Arena also supports consistent data collection and comparative evaluation
of algorithms in off-road autonomy research. We also develop a web-based
interface that enables research groups worldwide to remotely conduct
standardized off-road autonomy experiments on Verti-Arena.

</details>


### [59] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: ODYSSEY是一个统一的移动操作框架，用于配备机械臂的四足机器人，结合了高级任务规划和低级全身控制。


<details>
  <summary>Details</summary>
Motivation: 解决语言引导的长时程移动操作中的三大挑战：现有方法局限于桌面场景、泛化能力不足、以及在高机动性和精确末端执行器控制之间的平衡问题。

Method: 引入分层规划器（基于视觉语言模型）和新型全身控制策略，实现任务分解和精确执行。

Result: 通过仿真到现实的迁移，展示了系统在真实环境中的泛化能力和鲁棒性。

Conclusion: ODYSSEY推动了通用机器人助手在复杂动态任务中的可行性。

Abstract: Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/

</details>


### [60] [BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion](https://arxiv.org/abs/2508.08241)
*Takara E. Truong,Qiayuan Liao,Xiaoyu Huang,Guy Tevet,C. Karen Liu,Koushil Sreenath*

Main category: cs.RO

TL;DR: BeyondMimic框架通过扩散策略从人类动作中学习，实现高动态运动跟踪和任务控制。


<details>
  <summary>Details</summary>
Motivation: 解决从人类动作中学习通用策略的两个关键问题：高质量运动跟踪和动作原语的合成与组合。

Method: 采用扩散策略，结合运动跟踪管道和任务特定控制。

Result: 实现了高动态运动（如跳跃、冲刺等）和零样本任务控制（如导航、避障）。

Conclusion: BeyondMimic成功将模拟到真实的运动跟踪与动作原语合成结合，为全身控制提供灵活解决方案。

Abstract: Learning skills from human motions offers a promising path toward
generalizable policies for whole-body humanoid control, yet two key
cornerstones are missing: (1) a high-quality motion tracking framework that
faithfully transforms large-scale kinematic references into robust and
extremely dynamic motions on real hardware, and (2) a distillation approach
that can effectively learn these motion primitives and compose them to solve
downstream tasks. We address these gaps with BeyondMimic, the first real-world
framework to learn from human motions for versatile and naturalistic humanoid
control via guided diffusion. Our framework provides a motion tracking pipeline
capable of challenging skills such as jumping spins, sprinting, and cartwheels
with state-of-the-art motion quality. Moving beyond mimicking existing motions
and synthesize novel ones, we further introduce a unified diffusion policy that
enables zero-shot task-specific control at test time using simple cost
functions. Deployed on hardware, BeyondMimic performs diverse tasks at test
time, including waypoint navigation, joystick teleoperation, and obstacle
avoidance, bridging sim-to-real motion tracking and flexible synthesis of human
motion primitives for whole-body control. https://beyondmimic.github.io/.

</details>
